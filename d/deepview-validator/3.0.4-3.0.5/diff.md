# Comparing `tmp/deepview_validator-3.0.4-py3-none-any.whl.zip` & `tmp/deepview_validator-3.0.5-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,46 +1,46 @@
-Zip file size: 98203 bytes, number of entries: 44
--rw-rw-r--  2.0 unx      760 b- defN 23-Apr-27 05:54 deepview/validator/__init__.py
--rw-rw-r--  2.0 unx    16195 b- defN 23-Apr-27 05:54 deepview/validator/__main__.py
--rw-rw-r--  2.0 unx    12847 b- defN 23-Apr-27 05:54 deepview/validator/exceptions.py
--rw-rw-r--  2.0 unx      497 b- defN 23-Apr-27 05:54 deepview/validator/datasets/__init__.py
--rw-rw-r--  2.0 unx    29989 b- defN 23-Apr-27 05:54 deepview/validator/datasets/core.py
--rw-rw-r--  2.0 unx    14688 b- defN 23-Apr-27 05:54 deepview/validator/datasets/darknet.py
--rw-rw-r--  2.0 unx     7029 b- defN 23-Apr-27 05:54 deepview/validator/datasets/tfrecord.py
--rw-rw-r--  2.0 unx     3786 b- defN 23-Apr-27 05:54 deepview/validator/datasets/utils.py
--rw-rw-r--  2.0 unx      530 b- defN 23-Apr-27 05:54 deepview/validator/evaluators/__init__.py
--rw-rw-r--  2.0 unx     5067 b- defN 23-Apr-27 05:54 deepview/validator/evaluators/core.py
--rw-rw-r--  2.0 unx    24515 b- defN 23-Apr-27 05:54 deepview/validator/evaluators/detectionevaluator.py
--rw-rw-r--  2.0 unx    19208 b- defN 23-Apr-27 05:54 deepview/validator/evaluators/segmentationevaluator.py
--rw-rw-r--  2.0 unx      830 b- defN 23-Apr-27 05:54 deepview/validator/metrics/__init__.py
--rw-rw-r--  2.0 unx    11367 b- defN 23-Apr-27 05:54 deepview/validator/metrics/core.py
--rw-rw-r--  2.0 unx    31668 b- defN 23-Apr-27 05:54 deepview/validator/metrics/detectiondata.py
--rw-rw-r--  2.0 unx    25023 b- defN 23-Apr-27 05:54 deepview/validator/metrics/detectionmetrics.py
--rw-rw-r--  2.0 unx     4548 b- defN 23-Apr-27 05:54 deepview/validator/metrics/detectionutils.py
--rw-rw-r--  2.0 unx    17192 b- defN 23-Apr-27 05:54 deepview/validator/metrics/segmentationdata.py
--rw-rw-r--  2.0 unx     6514 b- defN 23-Apr-27 05:54 deepview/validator/metrics/segmentationmetrics.py
--rw-rw-r--  2.0 unx    11690 b- defN 23-Apr-27 05:54 deepview/validator/metrics/segmentationutils.py
--rw-rw-r--  2.0 unx      735 b- defN 23-Apr-27 05:54 deepview/validator/runners/__init__.py
--rw-rw-r--  2.0 unx     5144 b- defN 23-Apr-27 05:54 deepview/validator/runners/core.py
--rw-rw-r--  2.0 unx    13520 b- defN 23-Apr-27 05:54 deepview/validator/runners/deepviewrt.py
--rw-rw-r--  2.0 unx    17082 b- defN 23-Apr-27 05:54 deepview/validator/runners/keras.py
--rw-rw-r--  2.0 unx     6147 b- defN 23-Apr-27 05:54 deepview/validator/runners/offline.py
--rw-rw-r--  2.0 unx    16285 b- defN 23-Apr-27 05:54 deepview/validator/runners/tensorrt.py
--rw-rw-r--  2.0 unx    14493 b- defN 23-Apr-27 05:54 deepview/validator/runners/tflite.py
--rw-rw-r--  2.0 unx      612 b- defN 23-Apr-27 05:54 deepview/validator/runners/modelclient/__init__.py
--rw-rw-r--  2.0 unx    27984 b- defN 23-Apr-27 05:54 deepview/validator/runners/modelclient/boxes.py
--rw-rw-r--  2.0 unx     3116 b- defN 23-Apr-27 05:54 deepview/validator/runners/modelclient/core.py
--rw-rw-r--  2.0 unx    11195 b- defN 23-Apr-27 05:54 deepview/validator/runners/modelclient/segmentation.py
--rw-rw-r--  2.0 unx      521 b- defN 23-Apr-27 05:54 deepview/validator/visualize/__init__.py
--rw-rw-r--  2.0 unx    11059 b- defN 23-Apr-27 05:54 deepview/validator/visualize/core.py
--rw-rw-r--  2.0 unx     7014 b- defN 23-Apr-27 05:54 deepview/validator/visualize/detectiondrawer.py
--rw-rw-r--  2.0 unx    14370 b- defN 23-Apr-27 05:54 deepview/validator/visualize/segmentationdrawer.py
--rw-rw-r--  2.0 unx      574 b- defN 23-Apr-27 05:54 deepview/validator/writers/__init__.py
--rw-rw-r--  2.0 unx     7611 b- defN 23-Apr-27 05:54 deepview/validator/writers/console.py
--rw-rw-r--  2.0 unx    19839 b- defN 23-Apr-27 05:54 deepview/validator/writers/core.py
--rw-rw-r--  2.0 unx     9065 b- defN 23-Apr-27 05:54 deepview/validator/writers/tensorboard.py
--rw-rw-r--  2.0 unx      468 b- defN 23-Apr-27 05:54 deepview_validator-3.0.4.dist-info/METADATA
--rw-rw-r--  2.0 unx       92 b- defN 23-Apr-27 05:54 deepview_validator-3.0.4.dist-info/WHEEL
--rw-rw-r--  2.0 unx       73 b- defN 23-Apr-27 05:54 deepview_validator-3.0.4.dist-info/entry_points.txt
--rw-rw-r--  2.0 unx        9 b- defN 23-Apr-27 05:54 deepview_validator-3.0.4.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     4268 b- defN 23-Apr-27 05:54 deepview_validator-3.0.4.dist-info/RECORD
-44 files, 435219 bytes uncompressed, 91217 bytes compressed:  79.0%
+Zip file size: 98598 bytes, number of entries: 44
+-rw-rw-r--  2.0 unx      760 b- defN 23-May-04 16:07 deepview/validator/__init__.py
+-rw-rw-r--  2.0 unx    16195 b- defN 23-May-04 16:07 deepview/validator/__main__.py
+-rw-rw-r--  2.0 unx    12847 b- defN 23-May-04 16:07 deepview/validator/exceptions.py
+-rw-rw-r--  2.0 unx      497 b- defN 23-May-04 16:07 deepview/validator/datasets/__init__.py
+-rw-rw-r--  2.0 unx    29989 b- defN 23-May-04 16:07 deepview/validator/datasets/core.py
+-rw-rw-r--  2.0 unx    14688 b- defN 23-May-04 16:07 deepview/validator/datasets/darknet.py
+-rw-rw-r--  2.0 unx     7029 b- defN 23-May-04 16:07 deepview/validator/datasets/tfrecord.py
+-rw-rw-r--  2.0 unx     3786 b- defN 23-May-04 16:07 deepview/validator/datasets/utils.py
+-rw-rw-r--  2.0 unx      530 b- defN 23-May-04 16:07 deepview/validator/evaluators/__init__.py
+-rw-rw-r--  2.0 unx     5066 b- defN 23-May-04 16:07 deepview/validator/evaluators/core.py
+-rw-rw-r--  2.0 unx    24946 b- defN 23-May-04 16:07 deepview/validator/evaluators/detectionevaluator.py
+-rw-rw-r--  2.0 unx    19326 b- defN 23-May-04 16:07 deepview/validator/evaluators/segmentationevaluator.py
+-rw-rw-r--  2.0 unx      830 b- defN 23-May-04 16:07 deepview/validator/metrics/__init__.py
+-rw-rw-r--  2.0 unx    11367 b- defN 23-May-04 16:07 deepview/validator/metrics/core.py
+-rw-rw-r--  2.0 unx    32340 b- defN 23-May-04 16:07 deepview/validator/metrics/detectiondata.py
+-rw-rw-r--  2.0 unx    25279 b- defN 23-May-04 16:07 deepview/validator/metrics/detectionmetrics.py
+-rw-rw-r--  2.0 unx     4548 b- defN 23-May-04 16:07 deepview/validator/metrics/detectionutils.py
+-rw-rw-r--  2.0 unx    17192 b- defN 23-May-04 16:07 deepview/validator/metrics/segmentationdata.py
+-rw-rw-r--  2.0 unx     6514 b- defN 23-May-04 16:07 deepview/validator/metrics/segmentationmetrics.py
+-rw-rw-r--  2.0 unx    11690 b- defN 23-May-04 16:07 deepview/validator/metrics/segmentationutils.py
+-rw-rw-r--  2.0 unx      735 b- defN 23-May-04 16:07 deepview/validator/runners/__init__.py
+-rw-rw-r--  2.0 unx     5144 b- defN 23-May-04 16:07 deepview/validator/runners/core.py
+-rw-rw-r--  2.0 unx    13520 b- defN 23-May-04 16:07 deepview/validator/runners/deepviewrt.py
+-rw-rw-r--  2.0 unx    17095 b- defN 23-May-04 16:07 deepview/validator/runners/keras.py
+-rw-rw-r--  2.0 unx     6147 b- defN 23-May-04 16:07 deepview/validator/runners/offline.py
+-rw-rw-r--  2.0 unx    16285 b- defN 23-May-04 16:07 deepview/validator/runners/tensorrt.py
+-rw-rw-r--  2.0 unx    14466 b- defN 23-May-04 16:07 deepview/validator/runners/tflite.py
+-rw-rw-r--  2.0 unx      612 b- defN 23-May-04 16:07 deepview/validator/runners/modelclient/__init__.py
+-rw-rw-r--  2.0 unx    27957 b- defN 23-May-04 16:07 deepview/validator/runners/modelclient/boxes.py
+-rw-rw-r--  2.0 unx     3116 b- defN 23-May-04 16:07 deepview/validator/runners/modelclient/core.py
+-rw-rw-r--  2.0 unx    11168 b- defN 23-May-04 16:07 deepview/validator/runners/modelclient/segmentation.py
+-rw-rw-r--  2.0 unx      521 b- defN 23-May-04 16:07 deepview/validator/visualize/__init__.py
+-rw-rw-r--  2.0 unx    11856 b- defN 23-May-04 16:07 deepview/validator/visualize/core.py
+-rw-rw-r--  2.0 unx     7277 b- defN 23-May-04 16:07 deepview/validator/visualize/detectiondrawer.py
+-rw-rw-r--  2.0 unx    14370 b- defN 23-May-04 16:07 deepview/validator/visualize/segmentationdrawer.py
+-rw-rw-r--  2.0 unx      574 b- defN 23-May-04 16:07 deepview/validator/writers/__init__.py
+-rw-rw-r--  2.0 unx     7611 b- defN 23-May-04 16:07 deepview/validator/writers/console.py
+-rw-rw-r--  2.0 unx    19839 b- defN 23-May-04 16:07 deepview/validator/writers/core.py
+-rw-rw-r--  2.0 unx     9065 b- defN 23-May-04 16:07 deepview/validator/writers/tensorboard.py
+-rw-rw-r--  2.0 unx      468 b- defN 23-May-04 16:08 deepview_validator-3.0.5.dist-info/METADATA
+-rw-rw-r--  2.0 unx       92 b- defN 23-May-04 16:08 deepview_validator-3.0.5.dist-info/WHEEL
+-rw-rw-r--  2.0 unx       73 b- defN 23-May-04 16:08 deepview_validator-3.0.5.dist-info/entry_points.txt
+-rw-rw-r--  2.0 unx        9 b- defN 23-May-04 16:08 deepview_validator-3.0.5.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     4268 b- defN 23-May-04 16:08 deepview_validator-3.0.5.dist-info/RECORD
+44 files, 437687 bytes uncompressed, 91612 bytes compressed:  79.1%
```

## zipnote {}

```diff
@@ -111,23 +111,23 @@
 
 Filename: deepview/validator/writers/core.py
 Comment: 
 
 Filename: deepview/validator/writers/tensorboard.py
 Comment: 
 
-Filename: deepview_validator-3.0.4.dist-info/METADATA
+Filename: deepview_validator-3.0.5.dist-info/METADATA
 Comment: 
 
-Filename: deepview_validator-3.0.4.dist-info/WHEEL
+Filename: deepview_validator-3.0.5.dist-info/WHEEL
 Comment: 
 
-Filename: deepview_validator-3.0.4.dist-info/entry_points.txt
+Filename: deepview_validator-3.0.5.dist-info/entry_points.txt
 Comment: 
 
-Filename: deepview_validator-3.0.4.dist-info/top_level.txt
+Filename: deepview_validator-3.0.5.dist-info/top_level.txt
 Comment: 
 
-Filename: deepview_validator-3.0.4.dist-info/RECORD
+Filename: deepview_validator-3.0.5.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## deepview/validator/evaluators/core.py

 * *Ordering differences only*

```diff
@@ -139,8 +139,8 @@
 
     def group_evaluation(self):
         """Abstract Method"""
         raise NotImplementedError("This is an abstract method")
 
     def conclude(self):
         """Abstract Method"""
-        raise NotImplementedError("This is an abstract method")
+        raise NotImplementedError("This is an abstract method")
```

## deepview/validator/evaluators/detectionevaluator.py

```diff
@@ -576,14 +576,18 @@
                 bbox_inches='tight')
             fig_prec_confidence_curve.savefig(
                 f'{self.save_path}/precision_confidence_curve.png',
                 bbox_inches='tight')
             fig_rec_confidence_curve.savefig(
                 f'{self.save_path}/rec_confidence_curve.png',
                 bbox_inches='tight')
+            
+            self.drawer.close_figures([fig_class_metrics, fig_prec_rec_curve,
+                            fig_f1_curve, fig_prec_confidence_curve,
+                            fig_rec_confidence_curve])
 
         elif self.tensorboardwriter:
             nimage_class = Drawer.figure2numpy(fig_class_metrics)
             nimage_precision_recall = Drawer.figure2numpy(fig_prec_rec_curve)
             nimage_precision_confidence = Drawer.figure2numpy(
                 fig_prec_confidence_curve)
             nimage_recall_confidence = Drawer.figure2numpy(
@@ -606,14 +610,18 @@
                 nimage_recall_confidence,
                 f"{summary.get('model')}_recall_confidence.png",
                 step=epoch)
             self.tensorboardwriter(
                 nimage_f1_curve,
                 f"{summary.get('model')}_f1.png",
                 step=epoch)
+            
+            self.drawer.close_figures([fig_class_metrics, fig_prec_rec_curve,
+                            fig_f1_curve, fig_prec_confidence_curve,
+                            fig_rec_confidence_curve])
 
         if self.tensorboardwriter:
             self.tensorboardwriter.publish_metrics(
                 message=deepcopy(summary),
                 parameters=self.parameters,
                 step=epoch,
                 validation_type="detection")
@@ -627,8 +635,9 @@
             if self.visualize:
                 with open(self.save_path + '/metrics.txt', 'w') as fp:
                     fp.write(header + '\n')
                     fp.write(format_summary + '\n')
                     if timings is not None:
                         fp.write(format_timings)
                 fp.close()
+
         return summary
```

## deepview/validator/evaluators/segmentationevaluator.py

```diff
@@ -500,20 +500,22 @@
             fig_class_metrics = Drawer.plot_classification(
                 class_histogram_data, model=model_name)
 
         if self.visualize:
             fig_class_metrics.savefig(
                 f'{self.save_path}/class_scores.png',
                 bbox_inches="tight")
+            self.drawer.close_figures([fig_class_metrics])
         elif self.tensorboardwriter:
             nimage_class = Drawer.figure2numpy(fig_class_metrics)
             self.tensorboardwriter(
                 nimage_class,
                 f"{summary.get('model')}_scores.png",
                 step=epoch)
+            self.drawer.close_figures([fig_class_metrics])
             
         if self.json_out:
             import json
             summary["class_histogram_data"] = class_histogram_data
             
             with open(self.json_out, 'w') as fp:
                 json.dump(summary, fp)
```

## deepview/validator/metrics/detectiondata.py

```diff
@@ -37,15 +37,15 @@
         self.label_data_list = list()
         # Total number of ground truths in the dataset.
         self.total_gt = 0
         # A list containing the strings of unique labels.
         self.labels = list()
 
     @staticmethod
-    def validate_score(score, eps=1e-10):
+    def validate_score(score, min=0., max=1.):
         """
         The method validates the confidence score or the
         score threshold to be a floating type and does not
         exceed defined bounds (0...1).
         Unit-test for this method is defined under:
             file: test/test_metrics.py
             function: test_validate_score_iou
@@ -57,36 +57,35 @@
                 The score to validate.
 
         Returns
         -------
             score: float
                 The validated score.
 
-            eps: float
-                Invalid score leniency.
+            min: float
+                The minimum acceptable score.
+
+            max: float
+                The maximum acceptable score.
 
         Raises
         ------
             ValueError
                 This method will raise the exception if the provided score
                 is not floating point type or it is out bounds
                 (less than 0 or greater than 1).
         """
 
         if not isinstance(score, (float, np.float32)):
             raise ValueError(
                 "The provided score is not of numeric type: float. " +
                 "Provided with type: {}".format(
                     type(score)))
-        if (score < 0 or score > 1. + eps):
-            raise ValueError(
-                "The provided score is out of bounds: {}. ".format(score) +
-                "This value can only be between 0 and 1")
-        else:
-            return score
+        
+        return min if score < min else max if score > max else score
 
     @staticmethod
     def validate_iou(iou, eps=1e-10):
         """
         The method validates the IoU score or the
         IoU threshold to be a floating type and does not
         exceed defined bounds (0...1).
@@ -1004,25 +1003,44 @@
                 iou_threshold and score_threshold are not floating
                 types or they are out of bounds
                 such as greater than 1 and less than 0.
         """
 
         score_threshold = DetectionDataCollection.validate_score(score_threshold)
         iou_threshold = DetectionDataCollection.validate_iou(iou_threshold)
+        local_fp = 0
+
         if len(self.tps):
             # Any predictions that are below the IoU thresholds are
             # localization false positives.
             fp_iou = np.array(self.tps)[:, 0] < iou_threshold
             tp_score = np.array(self.tps)[:, 1] >= score_threshold
-            return np.count_nonzero(
-                fp_iou * tp_score) + np.count_nonzero(
-                np.array(self.local_fps) >= score_threshold)
-        else:
-            return np.count_nonzero(
-                np.array(self.local_fps) >= score_threshold)
+            local_fp += np.count_nonzero(fp_iou * tp_score)
+
+        if len(self.class_fps):
+            class_fp_iou = np.array(self.class_fps)[:, 0] < iou_threshold
+            class_fp_score = np.array(self.class_fps)[:, 1] >= score_threshold
+            local_fp += np.count_nonzero(class_fp_iou * class_fp_score)
+
+        local_fp += np.count_nonzero(
+            np.array(self.local_fps) >= score_threshold)
+        
+        return local_fp
+
+        # if len(self.tps):
+        #     # Any predictions that are below the IoU thresholds are
+        #     # localization false positives.
+        #     fp_iou = np.array(self.tps)[:, 0] < iou_threshold
+        #     tp_score = np.array(self.tps)[:, 1] >= score_threshold
+        #     return np.count_nonzero(
+        #         fp_iou * tp_score) + np.count_nonzero(
+        #         np.array(self.local_fps) >= score_threshold) 
+        # else:
+        #     return np.count_nonzero(
+        #         np.array(self.local_fps) >= score_threshold)
 
     def get_fn_count(self, iou_threshold, score_threshold):
         """
         This method gets the number of false negatives
         at the specified IoU threshold and score threshold.
         Score threshold is needed because by principle fp = gt - tp,
         and score and IoU threshold is required to find the
```

## deepview/validator/metrics/detectionmetrics.py

```diff
@@ -238,15 +238,22 @@
         ------
             DivisionByZeroException
                 This method will raise an exception if a division of zero
                 is encountered when calculating precision, recall, or accuracy.
         """
 
         if total_tp == 0:
-            return [0., 0., 0.]
+            precision, recall, accuracy = 0., 0., 0.
+            if total_class_fp + total_local_fp == 0:
+                precision = np.nan
+            if total_fn == 0:
+                recall = np.nan
+            if total_class_fp + total_local_fp + total_fn == 0:
+                accuracy = np.nan
+            return [precision, recall, accuracy]
         else:
             overall_precision = self.compute_precision(
                 total_tp, total_class_fp + total_local_fp)
             overall_recall = self.compute_recall(
                 total_tp, total_fn + total_class_fp)
             overall_accuracy = self.compute_accuracy(
                 total_tp, total_class_fp + total_local_fp, total_fn)
@@ -312,25 +319,22 @@
                 tp = label_data.get_tp_count(iou_threshold, score_threshold)
                 class_fp = label_data.get_class_fp_count(
                     iou_threshold, score_threshold)
                 local_fp = label_data.get_local_fp_count(
                     iou_threshold, score_threshold)
                 fn = label_data.get_fn_count(iou_threshold, score_threshold)
 
-                # Avoid 0/0
                 if tp == 0:
-                    precision = 0.
-                    recall = 0.
-                    accuracy = 0.
+                    precision, recall, accuracy = 0., 0., 0.
                 else:
                     precision = self.compute_precision(tp, class_fp + local_fp)
                     recall = self.compute_recall(tp, fn)
                     accuracy = self.compute_accuracy(
                         tp, class_fp + local_fp, fn)
-
+      
                 mmap[it] += precision
                 mar[it] += recall
                 macc[it] += accuracy
 
                 # Only consider IoU threshold at 0.5
                 if it == 0:
                     class_histogram_data[label_data.get_label()] = {
```

## deepview/validator/runners/keras.py

```diff
@@ -264,32 +264,33 @@
                 self.max_num_detections,
                 iou_threshold=self.iou_threshold,
                 score_threshold=self.score_threshold,
                 clip_boxes=False
             )
 
         nmsed_boxes = nmsed_boxes.numpy()
-        nmsed_classes = tf.cast(nmsed_classes, tf.int32) + self.label_offset
+        nmsed_classes = tf.cast(nmsed_classes, tf.int32)
 
         nms_predicted_boxes = [nmsed_boxes[i, :valid_boxes[i], :]
                                for i in range(nmsed_boxes.shape[0])][0]
         nms_predicted_classes = [nmsed_classes.numpy()[i, :valid_boxes[i]]
                                  for i in range(nmsed_classes.shape[0])][0]
         nms_predicted_scores = [nmsed_scores.numpy()[i, :valid_boxes[i]]
                                 for i in range(nmsed_scores.shape[0])][0]
 
         if len(self.labels):
             string_nms_predicted_classes = list()
             for cls in nms_predicted_classes:
                 try:
-                    string_nms_predicted_classes.append(self.labels[int(cls)])
+                    string_nms_predicted_classes.append(
+                        self.labels[int(cls)])
                 except IndexError:
                     raise NonMatchingIndexException(cls)
             nms_predicted_classes = np.array(string_nms_predicted_classes)
-
+        
         return nms_predicted_boxes, nms_predicted_classes, nms_predicted_scores
 
     def get_input_type(self):
         """
         This method returns the model input type.
         Unit-test for this method is defined under:
             file: test/deepview/validator/runners/test_keras.py
```

## deepview/validator/runners/tflite.py

```diff
@@ -7,15 +7,14 @@
 # Modifying or copying any source code is explicitly forbidden.
 
 from deepview.validator.exceptions import MissingLibraryException
 from deepview.validator.runners.core import Runner
 from deepview.validator.datasets import Dataset
 from deepview.validator.writers import Writer
 from time import monotonic_ns as clock_now
-from os.path import exists
 from timeit import timeit
 import numpy as np
 
 
 class TFliteRunner(Runner):
     """
     This class runs TensorFlow Lite models.
```

## deepview/validator/runners/modelclient/boxes.py

```diff
@@ -6,15 +6,14 @@
 # This source code is provided solely for runtime interpretation by Python.
 # Modifying or copying any source code is explicitly forbidden.
 
 from deepview.validator.runners.modelclient.core import ModelClientRunner
 from deepview.validator.exceptions import MissingLibraryException
 from deepview.validator.datasets import Dataset
 from time import monotonic_ns as clock_now
-from os.path import exists
 import requests as req
 import numpy as np
 
 
 class BoxesModelPack(ModelClientRunner):
     """
     This class runs Yolo DeepViewRT models using modelrunner.
```

## deepview/validator/runners/modelclient/segmentation.py

```diff
@@ -6,15 +6,14 @@
 # This source code is provided solely for runtime interpretation by Python.
 # Modifying or copying any source code is explicitly forbidden.
 
 from deepview.validator.runners.modelclient.core import ModelClientRunner
 from deepview.validator.exceptions import MissingLibraryException
 from deepview.validator.datasets.core import Dataset
 from time import monotonic_ns as clock_now
-from os.path import exists
 import numpy as np
 
 
 class SegmentationRunner(ModelClientRunner):
     """
     This class uploads the model to the target and runs the model
     per image.
```

## deepview/validator/visualize/core.py

```diff
@@ -326,7 +326,38 @@
         ax.set_xlabel(xlabel)
         ax.set_ylabel(ylabel)
         ax.set_xlim(0, 1)
         ax.set_ylim(0, 1)
         ax.legend(bbox_to_anchor=(1.04, 1), loc="upper left")
         ax.set_title(f'{model} {ylabel}-Confidence Curve')
         return fig
+    
+    @staticmethod
+    def close_figures(figures):
+        """
+        This method closes the matplotlib figures opened to prevent
+        errors such as "Fail to allocate bitmap."
+
+        Parameters
+        ----------
+
+            figures: list
+                Contains matplotlib.pyplot figures
+
+        Returns
+        -------
+            None
+
+        Raises
+        ------
+            ValueError
+                This method will raise an exception if the 
+                provided figures is an empty list.
+        """
+
+        import matplotlib.pyplot as plt
+        if len(figures) == 0:
+            raise ValueError("The provided figures does not contain any " +
+                             "matplotlib.pyplot figures.")
+        for figure in figures:
+            plt.close(figure)
+
```

## deepview/validator/visualize/detectiondrawer.py

```diff
@@ -200,14 +200,19 @@
                     color = "OrangeRed"
             else:
                 color = "OrangeRed"
 
             if score >= score_t:
                 self.__drawRect(image_draw, color, text,
                                 (p1[0], p1[1] - 10), (p1, p2))
+                
+            if iou < iou_t:
+                text = f'Extra: {dt_label} {round(score * 100, 2)}% {round(float(iou), 2)}'
+                self.__drawRect(image_draw, "OrangeRed", text,
+                                (p1[0], p1[1] - 10), (p1, p2))
 
         # Draw Extra Predictions
         for extra_index in extras:
             dt_label = instance.get('dt_instance').get('labels')[extra_index]
             box = instance.get('dt_instance').get('boxes')[extra_index]
             score = instance.get('dt_instance').get('scores')[extra_index]
```

## deepview/validator/writers/core.py

 * *Ordering differences only*

```diff
@@ -206,29 +206,29 @@
             |---------------|----------------|-----------------|
             |{metrics.get('numgt').center(15)}|{metrics.get('Total TP').center(16)}|{metrics.get('Total FN').center(17)}|
             |_______________|________________|_________________|
             |    Classification FP    |    Localization FP     |
             |-------------------------|------------------------|
             |{metrics.get('Total Class FP').center(25)}|{metrics.get('Total Loc FP').center(24)}|
             |_________________________|________________________|
+            |               | Overall Accuracy  |{metrics.get('OA').center(14)}|
+            |               | mACC@0.5          |{metrics.get('mACC').get('0.5').center(14)}|
+            | Accuracy (%)  | mACC@0.75         |{metrics.get('mACC').get('0.75').center(14)}|
+            |               | mACC@0.5-0.95     |{metrics.get('mACC').get('0.5:0.95').center(14)}|
+            |_______________|___________________|______________|
             |               | Overall Precision |{metrics.get('OP').center(14)}|
             |               | mAP@0.5           |{metrics.get('mAP').get('0.5').center(14)}|
             | Precision (%) | mAP@0.75          |{metrics.get('mAP').get('0.75').center(14)}|
             |               | mAP@0.5-0.95      |{metrics.get('mAP').get('0.5:0.95').center(14)}|
             |_______________|___________________|______________|
             |               | Overall Recall    |{metrics.get('OR').center(14)}|
             |               | mAR@0.5           |{metrics.get('mAR').get('0.5').center(14)}|
             | Recall (%)    | mAR@0.75          |{metrics.get('mAR').get('0.75').center(14)}|
             |               | mAR@0.5-0.95      |{metrics.get('mAR').get('0.5:0.95').center(14)}|
             |_______________|___________________|______________|
-            |               | Overall Accuracy  |{metrics.get('OA').center(14)}|
-            |               | mACC@0.5          |{metrics.get('mACC').get('0.5').center(14)}|
-            | Accuracy (%)  | mACC@0.75         |{metrics.get('mACC').get('0.75').center(14)}|
-            |               | mACC@0.5-0.95     |{metrics.get('mACC').get('0.5:0.95').center(14)}|
-            |_______________|___________________|______________|
             |               | 0.5               |{metrics.get('LocFPErr').get('0.5').center(14)}|
             | Localization  | 0.75              |{metrics.get('LocFPErr').get('0.75').center(14)}|
             | FP Error (%)  | 0.5-0.95          |{metrics.get('LocFPErr').get('0.5:0.95').center(14)}|
             |_______________|___________________|______________|
             |               | 0.5               |{metrics.get('ClassFPErr').get('0.5').center(14)}|
             | Classification| 0.75              |{metrics.get('ClassFPErr').get('0.75').center(14)}|
             | FP Error (%)  | 0.5-0.95          |{metrics.get('ClassFPErr').get('0.5:0.95').center(14)}|
@@ -321,23 +321,23 @@
             | Dataset: {metrics.get('dataset').ljust(42)}|
             | Ground Truths: {metrics.get('numgt').ljust(36)}|
             |____________________________________________________|
             | True Positives | False Positives | False Negatives |
             |----------------|-----------------|-----------------|
             |{metrics.get('Total TP').center(16)}|{metrics.get('Total FP').center(17)}|{metrics.get('Total FN').center(17)}|
             |________________|_________________|_________________|
+            | Accuracy (%)  | Overall Accuracy  |{metrics.get('OA').center(16)}|
+            |               | mACC              |{metrics.get('mACC').center(16)}|
+            |_______________|___________________|________________|
             | Precision (%) | Overall Precision |{metrics.get('OP').center(16)}|
             |               | mAP               |{metrics.get('mAP').center(16)}|
             |_______________|___________________|________________|
             | Recall (%)    | Overall Recall    |{metrics.get('OR').center(16)}|
             |               | mAR               |{metrics.get('mAR').center(16)}|
             |_______________|___________________|________________|
-            | Accuracy (%)  | Overall Accuracy  |{metrics.get('OA').center(16)}|
-            |               | mACC              |{metrics.get('mACC').center(16)}|
-            |_______________|___________________|________________|
             """
 
         if path.splitext(str(metrics.get('model')))[1].lower(
         ) != "" or metrics.get('model') == "Training Model":
             timings = self.__format_timings(timings)
 
         return header, summary, timings
```

## Comparing `deepview_validator-3.0.4.dist-info/RECORD` & `deepview_validator-3.0.5.dist-info/RECORD`

 * *Files 13% similar despite different names*

```diff
@@ -3,42 +3,42 @@
 deepview/validator/exceptions.py,sha256=wN9LfxtSZ7jijwua-jziTBsrynjPS5Y8-nZerxBe5m4,12847
 deepview/validator/datasets/__init__.py,sha256=kn3O2am9aQkpCCUapanHpD2Hmqg2IXhMMduOY6LizYY,497
 deepview/validator/datasets/core.py,sha256=Uo5fxKb_dX2eCUAfh-1B3PJD23y_kq4FO-FDPeJp64c,29989
 deepview/validator/datasets/darknet.py,sha256=RDq4im5sR029vqko1078qN05P7Aa9_rPu0MQYOld66s,14688
 deepview/validator/datasets/tfrecord.py,sha256=tuMWxHPSazhzLdKlZv7tPVZVIzevEp7snIWgpT9zuDI,7029
 deepview/validator/datasets/utils.py,sha256=uFo47noinkjsD_88e5Gj1nSFHvyqFY0guLE7I4LWPlU,3786
 deepview/validator/evaluators/__init__.py,sha256=0UF3SF__Siagxw9ERwngKeVVz8DlNCiqUd2I_u8SJvI,530
-deepview/validator/evaluators/core.py,sha256=tcMB8G7z_5qeXMD8hojl9Avl-nOsUm4IjMD1lVX1MCg,5067
-deepview/validator/evaluators/detectionevaluator.py,sha256=z85dWpX4oZ3ckMiU4jZ2Sg_fDJjOW9er75iGRlSLchY,24515
-deepview/validator/evaluators/segmentationevaluator.py,sha256=TeSQkXk1bloGuxQxo-Ik0OTWPPuwHbnijE49As2D1WA,19208
+deepview/validator/evaluators/core.py,sha256=S4GTWtIkbesWVWSUwXJy46HRpQeRmVHMoQBzg8YEeKg,5066
+deepview/validator/evaluators/detectionevaluator.py,sha256=xCsO-dsP-lHggiYFm49__17_8NBeyXAZN5hGDg5FzXQ,24946
+deepview/validator/evaluators/segmentationevaluator.py,sha256=t2IXjx9q8tJbs2ersHMVcnmyrfgNIQwySJcY4DexIII,19326
 deepview/validator/metrics/__init__.py,sha256=vp8Dy7OR0tzfFsBPv81Y2OYWetgWrAHRzfBzkkNtazY,830
 deepview/validator/metrics/core.py,sha256=Bz8SHzepcgVZN1kplxRURn8thZ_UA9mdulWJQIpoOWc,11367
-deepview/validator/metrics/detectiondata.py,sha256=Y5XeGWdaZjcYpOcvx67d2yOSTIynCBxKTVVdOAzQbEg,31668
-deepview/validator/metrics/detectionmetrics.py,sha256=JwkVH8tvbz055ETPyChi4mcYlDu0XPs4XBUk0pE5ipM,25023
+deepview/validator/metrics/detectiondata.py,sha256=UCCT7SXkLbstJOthcaABZ5hImH_YAk2MLwZvPmEEHk4,32340
+deepview/validator/metrics/detectionmetrics.py,sha256=Lyez4pwLRnkPl0m40ghO2xt6aOIIaDTZH19IYzjAfxA,25279
 deepview/validator/metrics/detectionutils.py,sha256=4LaHrYYw9TM8C_6pWxnfOTSz2mjHMUCNnBKqfFD6NAk,4548
 deepview/validator/metrics/segmentationdata.py,sha256=28ws5aceUvg8jzNfGWtfKRp4EvFYwYsw6ddpj_SIjv4,17192
 deepview/validator/metrics/segmentationmetrics.py,sha256=Fe68fqugRkYZDTjXw5T_NbsrapKByMkZRXfvVueZXCU,6514
 deepview/validator/metrics/segmentationutils.py,sha256=HdI0xTKKO7GAfil42EjpI3_YAefKHCNDjJFD0wnt9UI,11690
 deepview/validator/runners/__init__.py,sha256=gRRd8B9lhTPg6noEF-nRipyIsawSJSp6BapSQzxDXjc,735
 deepview/validator/runners/core.py,sha256=gVikDHobZPPeQxvK1eRzaoKgert1RlJQQBHpz-3zBg4,5144
 deepview/validator/runners/deepviewrt.py,sha256=80JsM3F4tbWReeAi0r_PBpONrpuk107XfSGPZsKtr-I,13520
-deepview/validator/runners/keras.py,sha256=e1FOHMiY5AikhBDSbMTfKiyqknQU3XuG5feMjaYxJYA,17082
+deepview/validator/runners/keras.py,sha256=o0scubdTt7hDcwZvzsW_rOIcszG6y20w9l6gC8GOqgI,17095
 deepview/validator/runners/offline.py,sha256=DTcgg7LmuUXUuUmFw5qEo0IAYjjgaVaG0lpNCKAzqbM,6147
 deepview/validator/runners/tensorrt.py,sha256=OMBziiYWUeJWmjonHo5QlQZMXSDppHAOx0CDJFEZAfY,16285
-deepview/validator/runners/tflite.py,sha256=iecNtbftpzoxMRJ41xqx4QZIFuIsIK76eY8gA9Gpnk4,14493
+deepview/validator/runners/tflite.py,sha256=-9N6rShjKi0B98MzDeL77RNBF_Pt_10hCkF47XQBNxE,14466
 deepview/validator/runners/modelclient/__init__.py,sha256=5S2qsbvi8d9y8-FH6Zu7VZS8WlZZKwRg7yhFd58t3uw,612
-deepview/validator/runners/modelclient/boxes.py,sha256=FRGG_EwcyXLZq-CKqLen00HcM0M770d1m35UnfRXIkc,27984
+deepview/validator/runners/modelclient/boxes.py,sha256=ij5OAoTdQbpGJ6Cj1hXASu0J_A5tpKatX0XLLCuh5Xc,27957
 deepview/validator/runners/modelclient/core.py,sha256=1zK1SxMojNnEk6-BT8tRdyMroad2Bvg9vs5vu9-dNTw,3116
-deepview/validator/runners/modelclient/segmentation.py,sha256=TUGnwLy6Pih2Alp9W20_f4usTObLGRNJ8RKlp9Xa8x0,11195
+deepview/validator/runners/modelclient/segmentation.py,sha256=3Q2xG1c7Xlpm1_5pmGGu8V30d2ifwGm3NPXcLhsxdWE,11168
 deepview/validator/visualize/__init__.py,sha256=gGM_U0YR1SOagPIhUjQr8PLJAPZ-XzHuY9jLwg3uHds,521
-deepview/validator/visualize/core.py,sha256=qELc2h8wID5ROWiVUeLFZkgBUzO8KCQiQkJUqR_ZFgc,11059
-deepview/validator/visualize/detectiondrawer.py,sha256=3CMifGWwfx4cuKNwE8WLgmEGRIBaFewfTjmknlpPJeg,7014
+deepview/validator/visualize/core.py,sha256=ycKyQwh5Roph1iLlsuF5mNulEQJu7CWDxCwzFBVbQzg,11856
+deepview/validator/visualize/detectiondrawer.py,sha256=imlSA6GWin3zqdkMBIntgsCZBvNRN1gG8TT4BUgGRoI,7277
 deepview/validator/visualize/segmentationdrawer.py,sha256=WYMzIPbqcFy-8_RGrs_TTdqjvI-5Q8A08rQHN5Ymp1w,14370
 deepview/validator/writers/__init__.py,sha256=PAF-P5kc2h-0VYIz6KL5Z4jgzpKcu4ulIeDsOldKTjE,574
 deepview/validator/writers/console.py,sha256=oIDnZdDtlc0snO24ODN_-54vrdWrvUB0J3AI08RCtuE,7611
-deepview/validator/writers/core.py,sha256=zSZ58ByNrJbTyJqDb8Tohw-SvzJj3CoAqoyf963vfG4,19839
+deepview/validator/writers/core.py,sha256=tOARwmMr7MxKanlBtXqlvFf3HVDnF-pvt8WCWirWG4c,19839
 deepview/validator/writers/tensorboard.py,sha256=NxzZolLWfAlz1G8gyOZxYRlYmzM0F2KijPrHkKvA8nE,9065
-deepview_validator-3.0.4.dist-info/METADATA,sha256=AI09Bbnwz1dji3g6iYgqEugWBqBJQBkEJqKKf9_TdTY,468
-deepview_validator-3.0.4.dist-info/WHEEL,sha256=nvhOrkn7_9sGzJjxuUFjoJ6OkO7SJJqHSjq9VNu0Elc,92
-deepview_validator-3.0.4.dist-info/entry_points.txt,sha256=n4jIdEDC_mPGVLwmS21vEFC8_D7mqNuekZYdtupSSVE,73
-deepview_validator-3.0.4.dist-info/top_level.txt,sha256=FZ_uj5ZExs9dTNq5lw196yb-XR3VHKi6vS0EWgTQtXk,9
-deepview_validator-3.0.4.dist-info/RECORD,,
+deepview_validator-3.0.5.dist-info/METADATA,sha256=k80akiTFz7NieE3fbBQ6rkQpEWfev4S0eDp6Nf9GZs8,468
+deepview_validator-3.0.5.dist-info/WHEEL,sha256=nvhOrkn7_9sGzJjxuUFjoJ6OkO7SJJqHSjq9VNu0Elc,92
+deepview_validator-3.0.5.dist-info/entry_points.txt,sha256=n4jIdEDC_mPGVLwmS21vEFC8_D7mqNuekZYdtupSSVE,73
+deepview_validator-3.0.5.dist-info/top_level.txt,sha256=FZ_uj5ZExs9dTNq5lw196yb-XR3VHKi6vS0EWgTQtXk,9
+deepview_validator-3.0.5.dist-info/RECORD,,
```

