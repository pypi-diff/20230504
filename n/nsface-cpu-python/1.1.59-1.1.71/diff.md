# Comparing `tmp/nsface_cpu_python-1.1.59-py3-none-any.whl.zip` & `tmp/nsface_cpu_python-1.1.71-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,8 +1,8 @@
-Zip file size: 130360 bytes, number of entries: 55
+Zip file size: 132352 bytes, number of entries: 56
 -rw-r--r--  2.0 unx      107 b- defN 23-Jan-26 22:19 nsface_cpu/__init__.py
 -rw-r--r--  2.0 unx     4395 b- defN 23-Jan-26 22:19 nsface_cpu/bbox_anchor.py
 -rw-r--r--  2.0 unx    14916 b- defN 23-Jan-26 22:19 nsface_cpu/get_config.py
 -rw-r--r--  2.0 unx       99 b- defN 23-Jan-26 22:19 nsface_cpu/print_info.py
 -rw-r--r--  2.0 unx       71 b- defN 23-Jan-26 22:19 nsface_cpu/data/__init__.py
 -rw-r--r--  2.0 unx   211267 b- defN 23-Jan-26 22:19 nsface_cpu/data/constant.py
 -rw-r--r--  2.0 unx     4468 b- defN 23-Jan-26 22:19 nsface_cpu/data/image.py
@@ -16,42 +16,43 @@
 -rw-r--r--  2.0 unx      634 b- defN 23-Jan-26 22:19 nsface_cpu/data/experiments/WFLW/pip_32_16_60_mv3_small_l2_l1_10_1_nb10.py
 -rw-r--r--  2.0 unx      603 b- defN 23-Jan-26 22:19 nsface_cpu/data/experiments/WFLW/pip_32_16_60_r18_l2_l1_10_1_nb10.py
 -rw-r--r--  2.0 unx       52 b- defN 23-Jan-26 22:19 nsface_cpu/data/experiments/data_300W/__init__.py
 -rw-r--r--  2.0 unx      603 b- defN 23-Jan-26 22:19 nsface_cpu/data/experiments/data_300W/pip_32_16_60_r18_l2_l1_10_1_nb10.py
 -rw-r--r--  2.0 unx       67 b- defN 23-Jan-26 22:19 nsface_cpu/face/__init__.py
 -rw-r--r--  2.0 unx     6642 b- defN 23-Jan-27 01:05 nsface_cpu/face/blur.py
 -rw-r--r--  2.0 unx      400 b- defN 23-Jan-26 22:19 nsface_cpu/face/common.py
--rw-r--r--  2.0 unx    13915 b- defN 23-Apr-12 06:42 nsface_cpu/face/get_result.py
+-rw-r--r--  2.0 unx    14648 b- defN 23-May-04 00:29 nsface_cpu/face/get_result.py
 -rw-r--r--  2.0 unx      180 b- defN 23-Apr-12 06:09 nsface_cpu/model_zoo/__init__.py
--rw-r--r--  2.0 unx     4519 b- defN 23-Apr-12 06:03 nsface_cpu/model_zoo/get_models.py
--rw-r--r--  2.0 unx       91 b- defN 23-Jan-26 22:19 nsface_cpu/model_zoo/model_attribute/__init__.py
--rw-r--r--  2.0 unx     1572 b- defN 23-Jan-26 22:19 nsface_cpu/model_zoo/model_attribute/arcface_GenderAge.py
--rw-r--r--  2.0 unx     1368 b- defN 23-Jan-26 22:19 nsface_cpu/model_zoo/model_attribute/arcface_Race.py
+-rw-r--r--  2.0 unx     4948 b- defN 23-May-04 00:35 nsface_cpu/model_zoo/get_models.py
+-rw-r--r--  2.0 unx      202 b- defN 23-May-04 00:19 nsface_cpu/model_zoo/model_attribute/__init__.py
+-rw-r--r--  2.0 unx     2008 b- defN 23-May-03 10:35 nsface_cpu/model_zoo/model_attribute/arcface_GenderAge.py
+-rw-r--r--  2.0 unx     1647 b- defN 23-May-03 10:35 nsface_cpu/model_zoo/model_attribute/arcface_Race.py
+-rw-r--r--  2.0 unx     3371 b- defN 23-May-04 00:38 nsface_cpu/model_zoo/model_attribute/expression.py
+-rw-r--r--  2.0 unx     2845 b- defN 23-May-04 00:20 nsface_cpu/model_zoo/model_attribute/liveness.py
+-rw-r--r--  2.0 unx     2091 b- defN 23-Mar-20 01:58 nsface_cpu/model_zoo/model_attribute/mask_detector.py
 -rw-r--r--  2.0 unx       86 b- defN 23-Jan-26 22:19 nsface_cpu/model_zoo/model_common/__init__.py
 -rw-r--r--  2.0 unx     1533 b- defN 23-Apr-11 00:58 nsface_cpu/model_zoo/model_common/load_onnx.py
 -rw-r--r--  2.0 unx     3350 b- defN 23-Jan-26 22:19 nsface_cpu/model_zoo/model_common/load_openvino.py
--rw-r--r--  2.0 unx      212 b- defN 23-Apr-12 06:09 nsface_cpu/model_zoo/model_detection/__init__.py
+-rw-r--r--  2.0 unx      148 b- defN 23-May-04 00:19 nsface_cpu/model_zoo/model_detection/__init__.py
 -rw-r--r--  2.0 unx     7337 b- defN 23-Jan-26 22:19 nsface_cpu/model_zoo/model_detection/blazeface.py
 -rw-r--r--  2.0 unx     7129 b- defN 23-Jan-26 22:19 nsface_cpu/model_zoo/model_detection/blazeface640.py
--rw-r--r--  2.0 unx     2566 b- defN 23-Apr-12 06:38 nsface_cpu/model_zoo/model_detection/liveness.py
--rw-r--r--  2.0 unx     2091 b- defN 23-Mar-20 01:58 nsface_cpu/model_zoo/model_detection/mask_detector.py
 -rw-r--r--  2.0 unx     7347 b- defN 23-Jan-26 22:19 nsface_cpu/model_zoo/model_detection/retinaface_insightface.py
 -rw-r--r--  2.0 unx     7405 b- defN 23-Jan-26 22:19 nsface_cpu/model_zoo/model_detection/retinaface_torch.py
 -rw-r--r--  2.0 unx     9751 b- defN 23-Apr-11 00:58 nsface_cpu/model_zoo/model_detection/scrfd.py
 -rw-r--r--  2.0 unx       29 b- defN 23-Jan-26 22:19 nsface_cpu/model_zoo/model_landmark/__init__.py
 -rw-r--r--  2.0 unx     1560 b- defN 23-Jan-26 22:19 nsface_cpu/model_zoo/model_landmark/det2d106.py
 -rw-r--r--  2.0 unx    11018 b- defN 23-Jan-26 22:19 nsface_cpu/model_zoo/model_landmark/pipnet.py
--rw-r--r--  2.0 unx     5160 b- defN 23-Jan-26 22:19 nsface_cpu/model_zoo/model_landmark/tddfa.py
+-rw-r--r--  2.0 unx     5282 b- defN 23-May-03 10:32 nsface_cpu/model_zoo/model_landmark/tddfa.py
 -rw-r--r--  2.0 unx       35 b- defN 23-Jan-26 22:19 nsface_cpu/model_zoo/model_recognition/__init__.py
 -rw-r--r--  2.0 unx     2897 b- defN 23-Jan-26 22:19 nsface_cpu/model_zoo/model_recognition/arcface_modify.py
 -rw-r--r--  2.0 unx      173 b- defN 23-Jan-26 22:19 nsface_cpu/utils/__init__.py
--rw-r--r--  2.0 unx      522 b- defN 23-Jan-26 22:19 nsface_cpu/utils/util_attribute.py
--rw-r--r--  2.0 unx     6639 b- defN 23-Apr-12 06:44 nsface_cpu/utils/util_common.py
--rw-r--r--  2.0 unx    22889 b- defN 23-Apr-12 05:59 nsface_cpu/utils/util_detection.py
--rw-r--r--  2.0 unx    16091 b- defN 23-Mar-13 02:22 nsface_cpu/utils/util_landmark.py
+-rw-r--r--  2.0 unx     2162 b- defN 23-May-04 00:20 nsface_cpu/utils/util_attribute.py
+-rw-r--r--  2.0 unx     6737 b- defN 23-May-03 10:36 nsface_cpu/utils/util_common.py
+-rw-r--r--  2.0 unx    21262 b- defN 23-May-04 00:20 nsface_cpu/utils/util_detection.py
+-rw-r--r--  2.0 unx    16176 b- defN 23-May-03 10:32 nsface_cpu/utils/util_landmark.py
 -rw-r--r--  2.0 unx      755 b- defN 23-Jan-26 22:19 nsface_cpu/utils/util_recognition.py
 -rw-r--r--  2.0 unx     3003 b- defN 23-Jan-26 22:19 nsface_cpu/utils/util_warp.py
--rw-r--r--  2.0 unx      424 b- defN 23-Apr-12 06:44 nsface_cpu_python-1.1.59.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 23-Apr-12 06:44 nsface_cpu_python-1.1.59.dist-info/WHEEL
--rw-r--r--  2.0 unx       11 b- defN 23-Apr-12 06:44 nsface_cpu_python-1.1.59.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     5432 b- defN 23-Apr-12 06:44 nsface_cpu_python-1.1.59.dist-info/RECORD
-55 files, 393704 bytes uncompressed, 121388 bytes compressed:  69.2%
+-rw-r--r--  2.0 unx      424 b- defN 23-May-04 00:39 nsface_cpu_python-1.1.71.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-May-04 00:39 nsface_cpu_python-1.1.71.dist-info/WHEEL
+-rw-r--r--  2.0 unx       11 b- defN 23-May-04 00:39 nsface_cpu_python-1.1.71.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     5541 b- defN 23-May-04 00:39 nsface_cpu_python-1.1.71.dist-info/RECORD
+56 files, 399705 bytes uncompressed, 123204 bytes compressed:  69.2%
```

## zipnote {}

```diff
@@ -75,14 +75,23 @@
 
 Filename: nsface_cpu/model_zoo/model_attribute/arcface_GenderAge.py
 Comment: 
 
 Filename: nsface_cpu/model_zoo/model_attribute/arcface_Race.py
 Comment: 
 
+Filename: nsface_cpu/model_zoo/model_attribute/expression.py
+Comment: 
+
+Filename: nsface_cpu/model_zoo/model_attribute/liveness.py
+Comment: 
+
+Filename: nsface_cpu/model_zoo/model_attribute/mask_detector.py
+Comment: 
+
 Filename: nsface_cpu/model_zoo/model_common/__init__.py
 Comment: 
 
 Filename: nsface_cpu/model_zoo/model_common/load_onnx.py
 Comment: 
 
 Filename: nsface_cpu/model_zoo/model_common/load_openvino.py
@@ -93,20 +102,14 @@
 
 Filename: nsface_cpu/model_zoo/model_detection/blazeface.py
 Comment: 
 
 Filename: nsface_cpu/model_zoo/model_detection/blazeface640.py
 Comment: 
 
-Filename: nsface_cpu/model_zoo/model_detection/liveness.py
-Comment: 
-
-Filename: nsface_cpu/model_zoo/model_detection/mask_detector.py
-Comment: 
-
 Filename: nsface_cpu/model_zoo/model_detection/retinaface_insightface.py
 Comment: 
 
 Filename: nsface_cpu/model_zoo/model_detection/retinaface_torch.py
 Comment: 
 
 Filename: nsface_cpu/model_zoo/model_detection/scrfd.py
@@ -147,20 +150,20 @@
 
 Filename: nsface_cpu/utils/util_recognition.py
 Comment: 
 
 Filename: nsface_cpu/utils/util_warp.py
 Comment: 
 
-Filename: nsface_cpu_python-1.1.59.dist-info/METADATA
+Filename: nsface_cpu_python-1.1.71.dist-info/METADATA
 Comment: 
 
-Filename: nsface_cpu_python-1.1.59.dist-info/WHEEL
+Filename: nsface_cpu_python-1.1.71.dist-info/WHEEL
 Comment: 
 
-Filename: nsface_cpu_python-1.1.59.dist-info/top_level.txt
+Filename: nsface_cpu_python-1.1.71.dist-info/top_level.txt
 Comment: 
 
-Filename: nsface_cpu_python-1.1.59.dist-info/RECORD
+Filename: nsface_cpu_python-1.1.71.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## nsface_cpu/face/get_result.py

```diff
@@ -354,20 +354,21 @@
             r.max_flag=False
     return ret
 '''
 
 
 def get_landmark(model_name,model,img,faces,**kwargs):
     
+    eye_dist = kwargs.get('eye_dist',False)
     if model_name=='3ddfa':
         #param_lst, roi_box_lst = model.get_results(img,faces)
         param_lst=[]
         roi_box_lst=[]
         for face in faces:
-            param,roi_box,_ = model.get(img,face)
+            param,roi_box,_ = model.get(img,face,eye_dist=eye_dist)
             param_lst.append(param)
             roi_box_lst.append(roi_box)
             
         return faces, param_lst, roi_box_lst
 
     elif model_name=='2d106det':
         land_all=[]
@@ -381,44 +382,62 @@
         for face in faces:
             input_size = kwargs.get("input_size",256)
             scale_mul = kwargs.get("scale_mul",1.1)
             _ = model.get(img,face,input_size=input_size,scale_mul=scale_mul)
 
         return faces
 
-def get_ageGender(model_name,model,img,faces,to_bgr=False):
+def get_ageGender(model_name,model,img,faces,to_bgr=False,mask_off=False,eye_min=0):
         
     if model_name=='arcface_cmt':
         for face in faces:
-            model.get(img,face,to_bgr)
+            model.get(img,face,to_bgr,mask_off=mask_off,eye_min=eye_min)
             
     return faces
 
-def get_Race(model_name,model,img,faces,to_bgr=False):
+def get_Race(model_name,model,img,faces,to_bgr=False,mask_off=False,eye_min=0):
         
     if model_name=='arcface_race':
         for face in faces:
-            model.get(img,face,to_bgr)
+            model.get(img,face,to_bgr,mask_off=mask_off,eye_min=eye_min)
             
     return faces
 
 def get_features(model_name, model, img, faces,to_bgr=False):
 
     if model_name=='arcface':
         for face in faces:
             model.get(img,face,to_bgr)
 
     return faces
 
+def get_features_mask(model_name, model, model_mask, img, faces,to_bgr=False):
+
+    if model_name=='arcface':
+        for face in faces:
+            if np.argmax(face.mask_sf)==1: # mask
+                model_mask.get(img,face,to_bgr)
+            else:
+                model.get(img,face,to_bgr)
+
+    return faces
+
 
 def get_mask(model,faces,**kwargs):
 
     for face in faces:
         model.mask_check(face) # face.mask_sf
 
     return faces
 
-def get_liveness(model,faces,img,**kwargs):
+def get_liveness(model,faces,img,mask_off=False,eye_min=0,**kwargs):
+    for face in faces:
+        model.liveness_check(img,face,mask_off=mask_off,eye_min=eye_min)
+
+    return faces
+
+def get_expression(model,faces,mask_off=False,eye_min=0,**kwargs):
+
     for face in faces:
-        model.liveness_check(img,face)
+        model.get(face) # face.expression (expression result string)
 
     return faces
```

## nsface_cpu/model_zoo/get_models.py

```diff
@@ -1,10 +1,10 @@
-from .model_detection import scrfd, retinaface_torch, retinaface_insightface, blazeface, blazeface640, mask_detector, liveness
+from .model_detection import scrfd, retinaface_torch, retinaface_insightface, blazeface, blazeface640
 from .model_landmark import tddfa, pipnet,det2d106
-from .model_attribute import arcface_GenderAge,arcface_Race
+from .model_attribute import arcface_GenderAge,arcface_Race, mask_detector, liveness, expression
 from .model_recognition import arcface_modify as arcface
 
 
 def get_detection_model(name,path,**kwargs):
     model=None
     if type(path)==list:
         model_format = "openvino"
@@ -158,8 +158,24 @@
     model = liveness.Liveness(model_format,path,**kwargs)
 
     if model is None:
         print("{} is None".format(name))
         return None
     else:
         print("liveness model {} loaded".format(model_format))
-        return model
+        return model
+
+def get_expression_model(path,**kwargs):
+    model=None
+    if type(path)==list:
+        model_format = "openvino"
+    else:
+        model_format = path.split(".")[-1]
+
+    model = expression.Expression_DAN(model_format,path,**kwargs)
+
+    if model is None:
+        print("{} is None".format(name))
+        return None
+    else:
+        print("expression model {} loaded".format(model_format))
+        return model
```

## nsface_cpu/model_zoo/model_attribute/__init__.py

```diff
@@ -1,2 +1,5 @@
 from .arcface_GenderAge import Arcface_GenderAge_cmt
-from .arcface_Race import Arcface_Race
+from .arcface_Race import Arcface_Race
+from .mask_detector import MaskDetector
+from .liveness import Liveness
+from .expression import Expression_DAN
```

## nsface_cpu/model_zoo/model_attribute/arcface_GenderAge.py

```diff
@@ -21,20 +21,30 @@
         self.model_type=model_type
 
         if self.model_type=='onnx':
             self.net = load_onnx.Onnx_session(self.model_path,input_mean=0.0, input_std=1.0,onnx_device='cpu')
         elif self.model_type=='openvino':
             self.net = load_openvino.Openvino(self.model_path,device='CPU')
 
-    def get(self,img,face,to_bgr):
+    def get(self,img,face,to_bgr,mask_off=False,eye_min=0):
+        if mask_off and np.argmax(face['mask_sf'])==1:
+            face.gender=[]
+            face.age=[]
+            face.gender_sf=[]
+            return face.gender, face.age, face.gender_sf
+        if eye_min>0 and 'eye_dist' in face.keys() and face.eye_dist<eye_min:
+            face.gender=[]
+            face.age=[]
+            face.gender_sf=[]
+            return face.gender, face.age, face.gender_sf
+            
         if not 'aimg' in face.keys():
             aimg = face_align(img,LMARK_REF_ARC,face.land5,self.out_size)
             face.aimg = aimg
-        else:
-            aimg = face.aimg
+        aimg = face.aimg
 
         if self.model_type=='onnx':
             aimg = (aimg/255. - 0.5)/0.5
 
         #gender_outs, age_outs = self.net(aimg)
         output = self.net(aimg)
         if output[0].shape==(1,2):
@@ -43,15 +53,15 @@
         else:
             gender_outs=output[1]
             age_outs=output[0]
 
         pred_g,pred_a,sf_gender = get_pred(gender_outs,age_outs)
         face.gender = gender_dict[pred_g]
         face.age = pred_a
-        face.gender_sf = sf_gender
+        face.gender_sf = sf_gender.copy()
 
         return face.gender, face.age, face.gender_sf
```

## nsface_cpu/model_zoo/model_attribute/arcface_Race.py

```diff
@@ -20,15 +20,22 @@
         self.model_type=model_type
 
         if self.model_type=='onnx':
             self.net = load_onnx.Onnx_session(self.model_path,input_mean=0.0, input_std=1.0,onnx_device=kwargs.get("onnx_device",'cpu'))
         elif self.model_type=='openvino':
             self.net = load_openvino.Openvino(self.model_path,device=kwargs.get("device",'CPU'))
 
-    def get(self,img,face,to_bgr):
+    def get(self,img,face,to_bgr,mask_off=False,eye_min=0):
+        if mask_off and np.argmax(face['mask_sf'])==1:
+            face.race=[]
+            return face.race
+        if eye_min>0 and 'eye_dist' in face.keys() and face.eye_dist<eye_min:
+            face.race=[]
+            return face.race
+            
         if not 'aimg' in face.keys():
             aimg = face_align(img,LMARK_REF_ARC,face.land5,self.out_size)
             face.aimg = aimg
         else:
             aimg = face.aimg
 
         if self.model_type=='onnx':
```

## nsface_cpu/model_zoo/model_detection/__init__.py

```diff
@@ -1,6 +1,4 @@
 from .retinaface_torch import RetinaFace
 from .retinaface_insightface import RetinaFace
 from .scrfd import SCRFD_CV
-from .blazeface import BlazeFace
-from .mask_detector import MaskDetector
-from .liveness import *
+from .blazeface import BlazeFace
```

## nsface_cpu/model_zoo/model_landmark/tddfa.py

```diff
@@ -1,13 +1,13 @@
 import os
 import numpy as np
 import cv2
 import onnxruntime
 
-from ...utils.util_landmark import parse_roi_box_from_bbox, crop_img, recon_ver, calc_pose
+from ...utils.util_landmark import parse_roi_box_from_bbox, crop_img, recon_ver, calc_pose, get_distance_point
 from ...data.constant import PARAM_STD, PARAM_MEAN, U_BASE, W_SHP_BASE, W_EXP_BASE, STD_SIZE
 
 from ..model_common import load_onnx, load_openvino
 
 
 class TDDFA3D_V2:
     def __init__(self,model_type,model_path,**kwargs):
@@ -22,15 +22,15 @@
         self.model_type = model_type
 
         if self.model_type=='onnx':
             self.net = load_onnx.Onnx_session(self.model_path,input_mean=0.0, input_std=1.0,output_sort=True,onnx_device=kwargs.get("onnx_device",'cpu'))
         elif self.model_type=='openvino':
             self.net = load_openvino.Openvino(self.model_path,not_norm=True,device=kwargs.get("device",'CPU'))
 
-    def get(self,img,face):
+    def get(self,img,face,eye_dist=False):
         box = face.bbox
         roi_box = parse_roi_box_from_bbox(box)
 
         img = crop_img(img, roi_box)
         img = cv2.resize(img, dsize=(self.std_size,self.std_size), interpolation=cv2.INTER_LINEAR)
         img = (img - 127.5) / 128.
         # (112,112,3) normalized image
@@ -51,29 +51,32 @@
             y_pred = ver[1][i]
             
             preds_pre.append([x_pred,y_pred])
 
         preds_pre = np.array(preds_pre).astype(np.float32)
         
         land5=[]
-        eye_left = (ver[0][37]+(ver[0][38]-ver[0][37])/2 , ver[1][41]-(ver[1][41]-ver[1][37])/2)
-        eye_right = (ver[0][43]+(ver[0][44]-ver[0][43])/2, ver[1][47]-(ver[1][47]-ver[1][43])/2)
+        eye_left = (ver[0][41]+(ver[0][40]-ver[0][41])/2 , ver[1][38]+(ver[1][40]-ver[1][38])/2)
+        eye_right = (ver[0][43]+(ver[0][44]-ver[0][43])/2, ver[1][43]+(ver[1][47]-ver[1][43])/2)
         nose = (ver[0][30], ver[1][30])
         mouth_left = (ver[0][48]+(ver[0][60]-ver[0][48])/2 , ver[1][60]-(ver[1][48]-ver[1][60])/2)
         mouth_right = (ver[0][64]+(ver[0][54]-ver[0][64])/2 , ver[1][64]-(ver[1][54]-ver[1][64])/2)
         land5.append(eye_left)
         land5.append(eye_right)
         land5.append(nose)
         land5.append(mouth_left)
         land5.append(mouth_right)
         land5 = np.array(land5).astype(np.float32)
         
         face.land = preds_pre
         face.land5 = land5
         face.pose = pose
+
+        if eye_dist:
+            face.eye_dist=get_distance_point(eye_left,eye_right)
         
         return param, roi_box,face.land5
 
     def get_results(self,img_ori, faces):
         param_lst = []
         roi_box_lst = []
```

## nsface_cpu/utils/util_attribute.py

```diff
@@ -1,9 +1,10 @@
 import os
 import numpy as np   
+import cv2
     
     
 def get_pred(sf_gender, sf_age):#,resize):
     age_female = sf_age[:,:101]
     age_male = sf_age[:,101:]
 
     #p_female = np.reshape(sf_gender[:,0],(resize,1))*age_female
@@ -14,8 +15,61 @@
 
     p_age = p_female+p_male
 
     pred_gender = np.argmax(sf_gender)#,axis=1)
     pred_age = np.argmax(p_age)#,axis=1)
 
 
-    return pred_gender, pred_age,sf_gender
+    return pred_gender, pred_age,sf_gender
+
+# liveness
+class CropImage:
+    @staticmethod
+    def _get_new_box(src_w, src_h, bbox, scale):
+        x = bbox[0]
+        y = bbox[1]
+        box_w = bbox[2]
+        box_h = bbox[3]
+
+        scale = min((src_h-1)/box_h, min((src_w-1)/box_w, scale))
+
+        new_width = box_w * scale
+        new_height = box_h * scale
+        center_x, center_y = box_w/2+x, box_h/2+y
+
+        left_top_x = center_x-new_width/2
+        left_top_y = center_y-new_height/2
+        right_bottom_x = center_x+new_width/2
+        right_bottom_y = center_y+new_height/2
+
+        if left_top_x < 0:
+            right_bottom_x -= left_top_x
+            left_top_x = 0
+
+        if left_top_y < 0:
+            right_bottom_y -= left_top_y
+            left_top_y = 0
+
+        if right_bottom_x > src_w-1:
+            left_top_x -= right_bottom_x-src_w+1
+            right_bottom_x = src_w-1
+
+        if right_bottom_y > src_h-1:
+            left_top_y -= right_bottom_y-src_h+1
+            right_bottom_y = src_h-1
+
+        return int(left_top_x), int(left_top_y),\
+               int(right_bottom_x), int(right_bottom_y)
+
+    def crop(self, org_img, bbox, scale, out_w, out_h, crop=True):
+
+        if not crop:
+            dst_img = cv2.resize(org_img, (out_w, out_h))
+        else:
+            src_h, src_w, _ = np.shape(org_img)
+            left_top_x, left_top_y, \
+                right_bottom_x, right_bottom_y = self._get_new_box(src_w, src_h, bbox, scale)
+
+            img = org_img[left_top_y: right_bottom_y+1,
+                          left_top_x: right_bottom_x+1]
+            dst_img = cv2.resize(img, (out_w, out_h))
+        return dst_img
```

## nsface_cpu/utils/util_common.py

```diff
@@ -13,23 +13,24 @@
             face = faces[i]
             x1,y1,x2,y2 = face.bbox.astype(np.int)
             
             cv2.rectangle(dimg,(x1,y1),(x2,y2),color,size)
 
             wtxt = ''
             if ga:
-                if face.gender is not None and face.age is not None:
-                    wtxt+="{} {}".format(face.gender, face.age)
-                else:
+                if len(face.gender)<1:
                     wtxt+="{} {}".format('None','0')
-            if race:
-                if face.race is not None:
-                    wtxt+=" {}".format(face.race)
                 else:
+                    wtxt+="{} {}".format(face.gender, face.age)
+
+            if race:
+                if len(face.race)<1:
                     wtxt+=" {}".format("None")
+                else:
+                    wtxt+=" {}".format(face.race)
 
             if wtxt:
                 cv2.putText(dimg, wtxt,(x1-1, y2+25),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)
 
 
         return dimg
     
@@ -169,21 +170,25 @@
         font_scale=1
         if to_bgr:
             dimg = cv2.cvtColor(dimg,cv2.COLOR_BGR2RGB)
         
         for i in range(len(faces)):
             face = faces[i]
             
-            liveness_lb = np.argmax(face['liveness_sf'])
-            if liveness_lb==0:
-                color = real_color
-                txt = 'real'
+            if len(face['liveness_sf'])<1:
+                continue
             else:
-                color = fake_color
-                txt = 'fake'
-            
-            x1,y1,x2,y2 = face.bbox.astype(np.int)
-            cv2.rectangle(dimg,(x1,y1),(x2,y2),color,2)
+                liveness_lb = np.argmax(face['liveness_sf'])
+                if liveness_lb==0:
+                    color = real_color
+                    txt = 'real'
+                else:
+                    color = fake_color
+                    txt = 'fake'
             
-            cv2.putText(dimg,txt,(x1-1, y2+25),cv2.FONT_HERSHEY_SIMPLEX,font_scale,color,2)
+                x1,y1,x2,y2 = face.bbox.astype(np.int)
+                cv2.rectangle(dimg,(x1,y1),(x2,y2),color,2)
+                
+                cv2.putText(dimg,txt,(x1-1, y2+25),cv2.FONT_HERSHEY_SIMPLEX,font_scale,color,2)
                 
-        return dimg
+        return dimg
+
```

## nsface_cpu/utils/util_detection.py

```diff
@@ -565,59 +565,7 @@
 
             output_detections.append(weighted_detection)
 
         return output_detections
 
 
 
-# liveness
-class CropImage:
-    @staticmethod
-    def _get_new_box(src_w, src_h, bbox, scale):
-        x = bbox[0]
-        y = bbox[1]
-        box_w = bbox[2]
-        box_h = bbox[3]
-
-        scale = min((src_h-1)/box_h, min((src_w-1)/box_w, scale))
-
-        new_width = box_w * scale
-        new_height = box_h * scale
-        center_x, center_y = box_w/2+x, box_h/2+y
-
-        left_top_x = center_x-new_width/2
-        left_top_y = center_y-new_height/2
-        right_bottom_x = center_x+new_width/2
-        right_bottom_y = center_y+new_height/2
-
-        if left_top_x < 0:
-            right_bottom_x -= left_top_x
-            left_top_x = 0
-
-        if left_top_y < 0:
-            right_bottom_y -= left_top_y
-            left_top_y = 0
-
-        if right_bottom_x > src_w-1:
-            left_top_x -= right_bottom_x-src_w+1
-            right_bottom_x = src_w-1
-
-        if right_bottom_y > src_h-1:
-            left_top_y -= right_bottom_y-src_h+1
-            right_bottom_y = src_h-1
-
-        return int(left_top_x), int(left_top_y),\
-               int(right_bottom_x), int(right_bottom_y)
-
-    def crop(self, org_img, bbox, scale, out_w, out_h, crop=True):
-
-        if not crop:
-            dst_img = cv2.resize(org_img, (out_w, out_h))
-        else:
-            src_h, src_w, _ = np.shape(org_img)
-            left_top_x, left_top_y, \
-                right_bottom_x, right_bottom_y = self._get_new_box(src_w, src_h, bbox, scale)
-
-            img = org_img[left_top_y: right_bottom_y+1,
-                          left_top_x: right_bottom_x+1]
-            dst_img = cv2.resize(img, (out_w, out_h))
-        return dst_img
```

## nsface_cpu/utils/util_landmark.py

```diff
@@ -1,13 +1,13 @@
 from math import sqrt
 import pickle
 import os
 import numpy as np
 import cv2
-from math import cos, sin, atan2, asin, sqrt
+from math import cos, sin, atan2, asin, sqrt, dist
 
 from ..data.constant import U_BASE, W_EXP_BASE, W_SHP_BASE
 
 
 def parse_roi_box_from_bbox(bbox):
     left, top, right, bottom = bbox[:4]
     old_size = (right - left + bottom - top) / 2
@@ -470,8 +470,12 @@
     
     land5=[]
     
     for k in dict_to_5[lms_num]:
         land5.append(pip_get_point(lands,dict_to_5[lms_num][k]))
             
             
-    return land5
+    return land5
+
+def get_distance_point(p1,p2):
+    eye_dist = dist(p1,p2)
+    return eye_dist
```

## Comparing `nsface_cpu/model_zoo/model_detection/liveness.py` & `nsface_cpu/model_zoo/model_attribute/liveness.py`

 * *Files 8% similar despite different names*

```diff
@@ -6,15 +6,15 @@
 import os.path as osp
 import cv2
 import time
 import torch
 
 from ..model_common import load_onnx, load_openvino
 from ...data.image import read_torchImage,resize_image_multi
-from ...utils.util_detection import CropImage
+from ...utils.util_attribute import CropImage
 
 def softmax(x):
     f_x = np.exp(x) / np.sum(np.exp(x))
     return f_x
 
 class Liveness:
     def __init__(self, model_type,model_path,**kwargs):
@@ -43,16 +43,23 @@
             if self.load_multi:
                 self.net = load_openvino.Openvino_multi(self.model_path,transform=False,output_sort=True)
                 self.outs_len = len(self.net.output_names)
             else:
                 self.net = load_openvino.Openvino(self.model_path,not_norm=True,torch_image=True,device='CPU')
                 self.outs_len = self.net.outs_len
 
-    def liveness_check(self,img,face): # RGB input original image, (x1,y1,x2,y2) bbox_ori
+    def liveness_check(self,img,face,mask_off=False,eye_min=0): # RGB input original image, (x1,y1,x2,y2) bbox_ori
 
+        if mask_off and np.argmax(face['mask_sf'])==1:
+            face.liveness_sf=[]
+            return []
+        if eye_min>0 and 'eye_dist' in face.keys() and face.eye_dist<eye_min:
+            face.liveness_sf=[]
+            return []
+            
         bbox_ori = face.bbox
         bbox = [int(bbox_ori[0]), int(bbox_ori[1]), int(bbox_ori[2]-bbox_ori[0]+1), int(bbox_ori[3]-bbox_ori[1]+1)]
 
         self.param = {
             "org_img": img,
             "bbox": bbox, # x1 y1 w h
             "scale": self.scale,
```

## Comparing `nsface_cpu/model_zoo/model_detection/mask_detector.py` & `nsface_cpu/model_zoo/model_attribute/mask_detector.py`

 * *Files identical despite different names*

## Comparing `nsface_cpu_python-1.1.59.dist-info/RECORD` & `nsface_cpu_python-1.1.71.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -15,41 +15,42 @@
 nsface_cpu/data/experiments/WFLW/pip_32_16_60_mv3_small_l2_l1_10_1_nb10.py,sha256=t-mRsD9SjzXC6Xv7GtisYaGRXFbWbwi8PSQHoJkC2_g,634
 nsface_cpu/data/experiments/WFLW/pip_32_16_60_r18_l2_l1_10_1_nb10.py,sha256=DdnbcODr8RN3g8HUOw1kXGk9FvUgepKNxHHIUa2xxj0,603
 nsface_cpu/data/experiments/data_300W/__init__.py,sha256=zkAnOVc4xsey5c8gyPbelz--L9DNTcbh-OfjFOL3I7g,52
 nsface_cpu/data/experiments/data_300W/pip_32_16_60_r18_l2_l1_10_1_nb10.py,sha256=dZ8moswknARkTeFJAjY63XzX47XlMDDg3gTqjd9wnYQ,603
 nsface_cpu/face/__init__.py,sha256=F0MyXtPxuvIU1Wed0fBs2q_nj9bOpmeQ2iz8DldTHqo,67
 nsface_cpu/face/blur.py,sha256=4zdD4-nddC9-7jBEq66cMcKsXncVGBCS1YUjKcZbXN0,6642
 nsface_cpu/face/common.py,sha256=7nHL58GsOc38LAvP0jkkmPoVOumi9X3fy193XcoruWE,400
-nsface_cpu/face/get_result.py,sha256=Bakzedpi6pZRD7P1jSO6vD05y5PFzo6hkyS4alH4guU,13915
+nsface_cpu/face/get_result.py,sha256=iinGy2KTjPjM4wAmzDziHzmrPQP7LFBfGhymNkLKxq4,14648
 nsface_cpu/model_zoo/__init__.py,sha256=OvDyNp-899MPBsezKvv13CTgUydIKyOZ8Ee3Qjh7oaM,180
-nsface_cpu/model_zoo/get_models.py,sha256=ewDnL4rU6k4OWJ4cbQdpoo7gPLxWgPLP2cQXDB1LVlI,4519
-nsface_cpu/model_zoo/model_attribute/__init__.py,sha256=BEIhJcTgm7uWOtZAFckZsUcpadbfmi4hX0DgidabmFY,91
-nsface_cpu/model_zoo/model_attribute/arcface_GenderAge.py,sha256=nFiWrWTGNUSgeTriWRaPEri-RW5dnH388RqcQ2vMzeY,1572
-nsface_cpu/model_zoo/model_attribute/arcface_Race.py,sha256=YzmF1LczrsPRbMxDdlXq_kd8g3UgU4lO6Kl-9Yyo78Q,1368
+nsface_cpu/model_zoo/get_models.py,sha256=6uyYjxMbWhIaP_2cO0GlBqwV8qzf0OQbfyS8-To4d4g,4948
+nsface_cpu/model_zoo/model_attribute/__init__.py,sha256=fLYJu6J4hHXGtnedWjmRsPdUMaTLx6wYlgAamvH2X-A,202
+nsface_cpu/model_zoo/model_attribute/arcface_GenderAge.py,sha256=r_liEll0RuuZaLynQZ_Wx3m4mxT1gISw6IVZGhR6siY,2008
+nsface_cpu/model_zoo/model_attribute/arcface_Race.py,sha256=Upf8HSDNE7TYw4izQ_8yfuY8EfwvHsIrBU2xE0_ldys,1647
+nsface_cpu/model_zoo/model_attribute/expression.py,sha256=lLoiI_ULh-nlpv8xzh_yVsBH69gIMYE60QObLwGaK6M,3371
+nsface_cpu/model_zoo/model_attribute/liveness.py,sha256=Z1rzrqP-O8ATuLDwwbIbiZGxbAqS0iknkYmIM3SBNvo,2845
+nsface_cpu/model_zoo/model_attribute/mask_detector.py,sha256=xXVH3jfRZIu744d5FMMQEyzXian_BfKs7GUutreDrvM,2091
 nsface_cpu/model_zoo/model_common/__init__.py,sha256=vCv-IhpbANQvva92Fd0psJdQ5U6mswzgDUChmxdOVac,86
 nsface_cpu/model_zoo/model_common/load_onnx.py,sha256=b0qtgu7CgEudwqzxd7_oKmVlJmiJ6FTicIAIZXruVeI,1533
 nsface_cpu/model_zoo/model_common/load_openvino.py,sha256=Vo0dGa7dp17S-Sw5XGuRCe0-KaBXLv3OZjdAkxeAGes,3350
-nsface_cpu/model_zoo/model_detection/__init__.py,sha256=gP_eIWpPxQ1Kg3rSyAnNq8aItDJSUC0T067r_4dnSRk,212
+nsface_cpu/model_zoo/model_detection/__init__.py,sha256=BIBdFPXuM3zb-ZptMzzUex62Dvb7Mu3x85aWg4DzDpw,148
 nsface_cpu/model_zoo/model_detection/blazeface.py,sha256=mhTK7B0MM5A4A8op0wUU9jCbIwPUHTrcDrWkmGmU4Mk,7337
 nsface_cpu/model_zoo/model_detection/blazeface640.py,sha256=lpV5tAcZUEza6vFcMwkKAKWHUhGHPK01uSRcDXkBVIA,7129
-nsface_cpu/model_zoo/model_detection/liveness.py,sha256=W7bdSoZ-XxGa4ycuHtvWDASqGJmNAcIwHjhAJ6ng6k0,2566
-nsface_cpu/model_zoo/model_detection/mask_detector.py,sha256=xXVH3jfRZIu744d5FMMQEyzXian_BfKs7GUutreDrvM,2091
 nsface_cpu/model_zoo/model_detection/retinaface_insightface.py,sha256=GsYAykEVY2TmJElY53P1oHLAxRZxnDt_JfCtuJBlih8,7347
 nsface_cpu/model_zoo/model_detection/retinaface_torch.py,sha256=MO2H-VrzQtwb37CxxyTYR0zozJEYgKCSl7gvzsCshUk,7405
 nsface_cpu/model_zoo/model_detection/scrfd.py,sha256=PcaoC4Pc9P8vV1taXXXm7Mh7tQSKg_cHF7ObuhXpF44,9751
 nsface_cpu/model_zoo/model_landmark/__init__.py,sha256=6IhuM-t9NhZR94x2S7EOjsp-l6dnUUYz0qDEWpDHivs,29
 nsface_cpu/model_zoo/model_landmark/det2d106.py,sha256=L-rcAPEplkrt6LEbrbfVt7GqNR5X7QDB-o3RmzVio8I,1560
 nsface_cpu/model_zoo/model_landmark/pipnet.py,sha256=_OASCghafE8gEhQtC3apJ3gKPibjOpb26AsgwPMwRuc,11018
-nsface_cpu/model_zoo/model_landmark/tddfa.py,sha256=HQlq1-1BG0OI1KijRgqAHiSBypcKALLszFFpE9fRhzI,5160
+nsface_cpu/model_zoo/model_landmark/tddfa.py,sha256=40fsW1gjNWGJxNcK3T1DrXDBOO7ItEwzsfwLkkjsm6k,5282
 nsface_cpu/model_zoo/model_recognition/__init__.py,sha256=eVI_ENcGFvgR5C9TCQKVsPkp0YDWBgp2R12-XR8XBXo,35
 nsface_cpu/model_zoo/model_recognition/arcface_modify.py,sha256=tv4WjLd6hhwhYPy9rCy_42M1YrO9UAhPVno4vWSCj2E,2897
 nsface_cpu/utils/__init__.py,sha256=nCWNMSBAxrgdDra5eQ-ar3_wiJjWiJEmYJ1oNgqaGio,173
-nsface_cpu/utils/util_attribute.py,sha256=Fzfsuz4LYmkvrrJT0L9X3A2jiboaC5FIFX9uchmP0VY,522
-nsface_cpu/utils/util_common.py,sha256=388mT9WKlnBQR5UXuHUX1x5S05tbfbHmOz0O-BotxgE,6639
-nsface_cpu/utils/util_detection.py,sha256=Ac4c-MqS58OtFlmzvAN7oSERPLhnX4kkOSp0PXAbbZk,22889
-nsface_cpu/utils/util_landmark.py,sha256=SlnPWd2RzUxjEn_21EndRPmGkv8I2-eI1Ct6KyumuT8,16091
+nsface_cpu/utils/util_attribute.py,sha256=gnWlFZ31DJ0fNqRzbfAU3VG0fkZwyDxO5Om9cjRpYjI,2162
+nsface_cpu/utils/util_common.py,sha256=SXCaqXIhE94sYyhGL3nwYbJwow2ITLil-hyj9qFmU5U,6737
+nsface_cpu/utils/util_detection.py,sha256=va4FfMaeZgaU3MDohoV_dD1Yq7loP3X_JzptdMfKF_A,21262
+nsface_cpu/utils/util_landmark.py,sha256=Z0UfCxtwBu2bGzjqQ7jsXFn-LbcAvGY3pVyLNM5YUfg,16176
 nsface_cpu/utils/util_recognition.py,sha256=Ixpffnec3snH9O3bNBwNzBBxOC2oBNoIq7Lpczh4HKs,755
 nsface_cpu/utils/util_warp.py,sha256=V0_QZ4LhG_nBH37UNq3UpbYoM28vGcvk2RagGo_9QTA,3003
-nsface_cpu_python-1.1.59.dist-info/METADATA,sha256=kZw0t-oP5YOEED46tE1d-neuAU-9dSY4YsExqW0QjsI,424
-nsface_cpu_python-1.1.59.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-nsface_cpu_python-1.1.59.dist-info/top_level.txt,sha256=v_l-vx1sgGbTkpyADzIQ5ZhSlP4ByGjZq5FDXTk6JAk,11
-nsface_cpu_python-1.1.59.dist-info/RECORD,,
+nsface_cpu_python-1.1.71.dist-info/METADATA,sha256=K4-NkjnMyGeJM6euFjsCkVldqczHZgEismsfmh8_xbI,424
+nsface_cpu_python-1.1.71.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+nsface_cpu_python-1.1.71.dist-info/top_level.txt,sha256=v_l-vx1sgGbTkpyADzIQ5ZhSlP4ByGjZq5FDXTk6JAk,11
+nsface_cpu_python-1.1.71.dist-info/RECORD,,
```

