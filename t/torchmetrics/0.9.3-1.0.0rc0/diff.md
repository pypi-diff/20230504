# Comparing `tmp/torchmetrics-0.9.3.tar.gz` & `tmp/torchmetrics-1.0.0rc0.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/torchmetrics-0.9.3.tar", last modified: Sat Jul 23 21:26:43 2022, max compression
+gzip compressed data, was "torchmetrics-1.0.0rc0.tar", last modified: Thu May  4 08:32:42 2023, max compression
```

## Comparing `torchmetrics-0.9.3.tar` & `torchmetrics-1.0.0rc0.tar`

### file list

```diff
@@ -1,234 +1,315 @@
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/
--rw-r--r--   0 runner    (1001) docker     (121)    43054 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/CHANGELOG.md
--rw-r--r--   0 runner    (1001) docker     (121)     2568 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/CITATION.cff
--rw-r--r--   0 runner    (1001) docker     (121)    11352 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/LICENSE
--rw-r--r--   0 runner    (1001) docker     (121)      970 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (121)    12528 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (121)    10946 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/README.md
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/requirements/
--rw-r--r--   0 runner    (1001) docker     (121)       57 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/requirements/audio.txt
--rw-r--r--   0 runner    (1001) docker     (121)      183 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/requirements/audio_test.txt
--rw-r--r--   0 runner    (1001) docker     (121)       29 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/requirements/detection.txt
--rw-r--r--   0 runner    (1001) docker     (121)       13 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/requirements/detection_test.txt
--rw-r--r--   0 runner    (1001) docker     (121)      323 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/requirements/devel.txt
--rw-r--r--   0 runner    (1001) docker     (121)      364 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/requirements/docs.txt
--rw-r--r--   0 runner    (1001) docker     (121)      109 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/requirements/image.txt
--rw-r--r--   0 runner    (1001) docker     (121)       35 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/requirements/image_test.txt
--rw-r--r--   0 runner    (1001) docker     (121)       23 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/requirements/integrate.txt
--rw-r--r--   0 runner    (1001) docker     (121)      248 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/requirements/test.txt
--rw-r--r--   0 runner    (1001) docker     (121)       40 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/requirements/text.txt
--rw-r--r--   0 runner    (1001) docker     (121)      148 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/requirements/text_test.txt
--rw-r--r--   0 runner    (1001) docker     (121)       79 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/requirements.txt
--rw-r--r--   0 runner    (1001) docker     (121)      769 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/setup.cfg
--rwxr-xr-x   0 runner    (1001) docker     (121)     4282 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/tm_examples/
--rw-r--r--   0 runner    (1001) docker     (121)     4634 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/tm_examples/bert_score-own_model.py
--rw-r--r--   0 runner    (1001) docker     (121)     2715 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/tm_examples/detection_map.py
--rw-r--r--   0 runner    (1001) docker     (121)     2630 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/tm_examples/rouge_score-own_normalizer_and_tokenizer.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/
--rw-r--r--   0 runner    (1001) docker     (121)     1243 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/__about__.py
--rw-r--r--   0 runner    (1001) docker     (121)     4874 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    12429 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/aggregation.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/audio/
--rw-r--r--   0 runner    (1001) docker     (121)     1168 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/audio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     4321 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/audio/pesq.py
--rw-r--r--   0 runner    (1001) docker     (121)     4062 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/audio/pit.py
--rw-r--r--   0 runner    (1001) docker     (121)     6754 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/audio/sdr.py
--rw-r--r--   0 runner    (1001) docker     (121)     4972 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/audio/snr.py
--rw-r--r--   0 runner    (1001) docker     (121)     4810 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/audio/stoi.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/classification/
--rw-r--r--   0 runner    (1001) docker     (121)     2520 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    12243 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/accuracy.py
--rw-r--r--   0 runner    (1001) docker     (121)     2624 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/auc.py
--rw-r--r--   0 runner    (1001) docker     (121)     7171 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/auroc.py
--rw-r--r--   0 runner    (1001) docker     (121)     5779 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/avg_precision.py
--rw-r--r--   0 runner    (1001) docker     (121)    13267 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/binned_precision_recall.py
--rw-r--r--   0 runner    (1001) docker     (121)     3887 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/calibration_error.py
--rw-r--r--   0 runner    (1001) docker     (121)     4109 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/cohen_kappa.py
--rw-r--r--   0 runner    (1001) docker     (121)     5327 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/confusion_matrix.py
--rw-r--r--   0 runner    (1001) docker     (121)     7818 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/dice.py
--rw-r--r--   0 runner    (1001) docker     (121)    12889 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/f_beta.py
--rw-r--r--   0 runner    (1001) docker     (121)     3478 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/hamming.py
--rw-r--r--   0 runner    (1001) docker     (121)     5206 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/hinge.py
--rw-r--r--   0 runner    (1001) docker     (121)     5439 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/jaccard.py
--rw-r--r--   0 runner    (1001) docker     (121)     4100 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/kl_divergence.py
--rw-r--r--   0 runner    (1001) docker     (121)     3406 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/matthews_corrcoef.py
--rw-r--r--   0 runner    (1001) docker     (121)    14074 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/precision_recall.py
--rw-r--r--   0 runner    (1001) docker     (121)     5851 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/precision_recall_curve.py
--rw-r--r--   0 runner    (1001) docker     (121)     8388 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/ranking.py
--rw-r--r--   0 runner    (1001) docker     (121)     6649 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/roc.py
--rw-r--r--   0 runner    (1001) docker     (121)     7525 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/specificity.py
--rw-r--r--   0 runner    (1001) docker     (121)    11240 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/classification/stat_scores.py
--rw-r--r--   0 runner    (1001) docker     (121)    20398 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/collections.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/detection/
--rw-r--r--   0 runner    (1001) docker     (121)      778 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/detection/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    40453 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/detection/mean_ap.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/functional/
--rw-r--r--   0 runner    (1001) docker     (121)     8158 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/functional/audio/
--rw-r--r--   0 runner    (1001) docker     (121)     1271 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/audio/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     3972 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/audio/pesq.py
--rw-r--r--   0 runner    (1001) docker     (121)     7901 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/audio/pit.py
--rw-r--r--   0 runner    (1001) docker     (121)    11309 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/audio/sdr.py
--rw-r--r--   0 runner    (1001) docker     (121)     3253 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/audio/snr.py
--rw-r--r--   0 runner    (1001) docker     (121)     4596 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/audio/stoi.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/
--rw-r--r--   0 runner    (1001) docker     (121)     2481 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    19009 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/accuracy.py
--rw-r--r--   0 runner    (1001) docker     (121)     4314 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/auc.py
--rw-r--r--   0 runner    (1001) docker     (121)    12200 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/auroc.py
--rw-r--r--   0 runner    (1001) docker     (121)    10436 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/average_precision.py
--rw-r--r--   0 runner    (1001) docker     (121)     8681 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/calibration_error.py
--rw-r--r--   0 runner    (1001) docker     (121)     4119 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/cohen_kappa.py
--rw-r--r--   0 runner    (1001) docker     (121)     7848 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/confusion_matrix.py
--rw-r--r--   0 runner    (1001) docker     (121)    13050 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/dice.py
--rw-r--r--   0 runner    (1001) docker     (121)    17349 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/f_beta.py
--rw-r--r--   0 runner    (1001) docker     (121)     3574 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/hamming.py
--rw-r--r--   0 runner    (1001) docker     (121)     9185 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/hinge.py
--rw-r--r--   0 runner    (1001) docker     (121)     7680 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/jaccard.py
--rw-r--r--   0 runner    (1001) docker     (121)     4338 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/kl_divergence.py
--rw-r--r--   0 runner    (1001) docker     (121)     3020 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/matthews_corrcoef.py
--rw-r--r--   0 runner    (1001) docker     (121)    26362 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/precision_recall.py
--rw-r--r--   0 runner    (1001) docker     (121)    13738 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/precision_recall_curve.py
--rw-r--r--   0 runner    (1001) docker     (121)    10661 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/ranking.py
--rw-r--r--   0 runner    (1001) docker     (121)    11841 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/roc.py
--rw-r--r--   0 runner    (1001) docker     (121)     9787 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/specificity.py
--rw-r--r--   0 runner    (1001) docker     (121)    19215 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/classification/stat_scores.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/functional/image/
--rw-r--r--   0 runner    (1001) docker     (121)     1282 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/image/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     4999 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/image/d_lambda.py
--rw-r--r--   0 runner    (1001) docker     (121)     4605 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/image/ergas.py
--rw-r--r--   0 runner    (1001) docker     (121)     2900 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/image/gradients.py
--rw-r--r--   0 runner    (1001) docker     (121)     4620 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/image/helper.py
--rw-r--r--   0 runner    (1001) docker     (121)     5574 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/image/psnr.py
--rw-r--r--   0 runner    (1001) docker     (121)     4384 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/image/sam.py
--rw-r--r--   0 runner    (1001) docker     (121)    20298 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/image/ssim.py
--rw-r--r--   0 runner    (1001) docker     (121)     6915 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/image/uqi.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/functional/pairwise/
--rw-r--r--   0 runner    (1001) docker     (121)      966 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/pairwise/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     3493 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/pairwise/cosine.py
--rw-r--r--   0 runner    (1001) docker     (121)     3294 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/pairwise/euclidean.py
--rw-r--r--   0 runner    (1001) docker     (121)     2285 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/pairwise/helpers.py
--rw-r--r--   0 runner    (1001) docker     (121)     3181 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/pairwise/linear.py
--rw-r--r--   0 runner    (1001) docker     (121)     3201 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/pairwise/manhattan.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/functional/regression/
--rw-r--r--   0 runner    (1001) docker     (121)     1595 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/regression/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     3387 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/regression/cosine_similarity.py
--rw-r--r--   0 runner    (1001) docker     (121)     5090 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/regression/explained_variance.py
--rw-r--r--   0 runner    (1001) docker     (121)     2580 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/regression/log_mse.py
--rw-r--r--   0 runner    (1001) docker     (121)     2331 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/regression/mae.py
--rw-r--r--   0 runner    (1001) docker     (121)     3033 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/regression/mape.py
--rw-r--r--   0 runner    (1001) docker     (121)     2577 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/regression/mse.py
--rw-r--r--   0 runner    (1001) docker     (121)     3584 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/regression/pearson.py
--rw-r--r--   0 runner    (1001) docker     (121)     6317 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/regression/r2.py
--rw-r--r--   0 runner    (1001) docker     (121)     4518 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/regression/spearman.py
--rw-r--r--   0 runner    (1001) docker     (121)     3314 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/regression/symmetric_mape.py
--rw-r--r--   0 runner    (1001) docker     (121)     6130 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/regression/tweedie_deviance.py
--rw-r--r--   0 runner    (1001) docker     (121)     2766 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/regression/wmape.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/functional/retrieval/
--rw-r--r--   0 runner    (1001) docker     (121)     1445 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/retrieval/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2153 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/retrieval/average_precision.py
--rw-r--r--   0 runner    (1001) docker     (121)     2628 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/retrieval/fall_out.py
--rw-r--r--   0 runner    (1001) docker     (121)     2331 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/retrieval/hit_rate.py
--rw-r--r--   0 runner    (1001) docker     (121)     2742 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/retrieval/ndcg.py
--rw-r--r--   0 runner    (1001) docker     (121)     2688 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/retrieval/precision.py
--rw-r--r--   0 runner    (1001) docker     (121)     3974 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/retrieval/precision_recall_curve.py
--rw-r--r--   0 runner    (1001) docker     (121)     2126 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/retrieval/r_precision.py
--rw-r--r--   0 runner    (1001) docker     (121)     2465 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/retrieval/recall.py
--rw-r--r--   0 runner    (1001) docker     (121)     1994 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/retrieval/reciprocal_rank.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/functional/text/
--rw-r--r--   0 runner    (1001) docker     (121)     1731 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/text/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    28822 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/text/bert.py
--rw-r--r--   0 runner    (1001) docker     (121)     7541 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/text/bleu.py
--rw-r--r--   0 runner    (1001) docker     (121)     2996 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/text/cer.py
--rw-r--r--   0 runner    (1001) docker     (121)    25984 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/text/chrf.py
--rw-r--r--   0 runner    (1001) docker     (121)    17569 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/text/eed.py
--rw-r--r--   0 runner    (1001) docker     (121)    17380 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/text/helper.py
--rw-r--r--   0 runner    (1001) docker     (121)     3046 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/text/mer.py
--rw-r--r--   0 runner    (1001) docker     (121)    21159 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/text/rouge.py
--rw-r--r--   0 runner    (1001) docker     (121)    13141 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/text/sacre_bleu.py
--rw-r--r--   0 runner    (1001) docker     (121)     9911 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/text/squad.py
--rw-r--r--   0 runner    (1001) docker     (121)    23214 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/text/ter.py
--rw-r--r--   0 runner    (1001) docker     (121)     2982 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/text/wer.py
--rw-r--r--   0 runner    (1001) docker     (121)     3475 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/text/wil.py
--rw-r--r--   0 runner    (1001) docker     (121)     3523 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/functional/text/wip.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/image/
--rw-r--r--   0 runner    (1001) docker     (121)     1574 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/image/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     3952 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/image/d_lambda.py
--rw-r--r--   0 runner    (1001) docker     (121)     3753 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/image/ergas.py
--rw-r--r--   0 runner    (1001) docker     (121)    11943 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/image/fid.py
--rw-r--r--   0 runner    (1001) docker     (121)     6713 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/image/inception.py
--rw-r--r--   0 runner    (1001) docker     (121)    11596 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/image/kid.py
--rw-r--r--   0 runner    (1001) docker     (121)     5864 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/image/lpip.py
--rw-r--r--   0 runner    (1001) docker     (121)     5649 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/image/psnr.py
--rw-r--r--   0 runner    (1001) docker     (121)     3548 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/image/sam.py
--rw-r--r--   0 runner    (1001) docker     (121)    10738 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/image/ssim.py
--rw-r--r--   0 runner    (1001) docker     (121)     3552 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/image/uqi.py
--rw-r--r--   0 runner    (1001) docker     (121)    39874 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/metric.py
--rw-r--r--   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/py.typed
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/regression/
--rw-r--r--   0 runner    (1001) docker     (121)     1555 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/regression/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     3230 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/regression/cosine_similarity.py
--rw-r--r--   0 runner    (1001) docker     (121)     4820 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/regression/explained_variance.py
--rw-r--r--   0 runner    (1001) docker     (121)     2618 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/regression/log_mse.py
--rw-r--r--   0 runner    (1001) docker     (121)     2430 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/regression/mae.py
--rw-r--r--   0 runner    (1001) docker     (121)     3027 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/regression/mape.py
--rw-r--r--   0 runner    (1001) docker     (121)     2580 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/regression/mse.py
--rw-r--r--   0 runner    (1001) docker     (121)     5267 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/regression/pearson.py
--rw-r--r--   0 runner    (1001) docker     (121)     5076 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/regression/r2.py
--rw-r--r--   0 runner    (1001) docker     (121)     2913 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/regression/spearman.py
--rw-r--r--   0 runner    (1001) docker     (121)     2842 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/regression/symmetric_mape.py
--rw-r--r--   0 runner    (1001) docker     (121)     3986 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/regression/tweedie_deviance.py
--rw-r--r--   0 runner    (1001) docker     (121)     2706 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/regression/wmape.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/retrieval/
--rw-r--r--   0 runner    (1001) docker     (121)     1420 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/retrieval/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     2783 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/retrieval/average_precision.py
--rw-r--r--   0 runner    (1001) docker     (121)     5780 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/retrieval/base.py
--rw-r--r--   0 runner    (1001) docker     (121)     4951 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/retrieval/fall_out.py
--rw-r--r--   0 runner    (1001) docker     (121)     3499 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/retrieval/hit_rate.py
--rw-r--r--   0 runner    (1001) docker     (121)     3682 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/retrieval/ndcg.py
--rw-r--r--   0 runner    (1001) docker     (121)     3855 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/retrieval/precision.py
--rw-r--r--   0 runner    (1001) docker     (121)    11879 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/retrieval/precision_recall_curve.py
--rw-r--r--   0 runner    (1001) docker     (121)     2776 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/retrieval/r_precision.py
--rw-r--r--   0 runner    (1001) docker     (121)     3466 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/retrieval/recall.py
--rw-r--r--   0 runner    (1001) docker     (121)     2758 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/retrieval/reciprocal_rank.py
--rw-r--r--   0 runner    (1001) docker     (121)     3305 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/setup_tools.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/text/
--rw-r--r--   0 runner    (1001) docker     (121)     1553 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/text/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    10899 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/text/bert.py
--rw-r--r--   0 runner    (1001) docker     (121)     4117 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/text/bleu.py
--rw-r--r--   0 runner    (1001) docker     (121)     3164 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/text/cer.py
--rw-r--r--   0 runner    (1001) docker     (121)     8714 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/text/chrf.py
--rw-r--r--   0 runner    (1001) docker     (121)     4435 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/text/eed.py
--rw-r--r--   0 runner    (1001) docker     (121)     3125 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/text/mer.py
--rw-r--r--   0 runner    (1001) docker     (121)     7428 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/text/rouge.py
--rw-r--r--   0 runner    (1001) docker     (121)     4647 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/text/sacre_bleu.py
--rw-r--r--   0 runner    (1001) docker     (121)     4207 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/text/squad.py
--rw-r--r--   0 runner    (1001) docker     (121)     4838 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/text/ter.py
--rw-r--r--   0 runner    (1001) docker     (121)     3099 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/text/wer.py
--rw-r--r--   0 runner    (1001) docker     (121)     3186 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/text/wil.py
--rw-r--r--   0 runner    (1001) docker     (121)     3225 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/text/wip.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/utilities/
--rw-r--r--   0 runner    (1001) docker     (121)      367 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/utilities/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)    32926 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/utilities/checks.py
--rw-r--r--   0 runner    (1001) docker     (121)     1234 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/utilities/compute.py
--rw-r--r--   0 runner    (1001) docker     (121)     8816 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/utilities/data.py
--rw-r--r--   0 runner    (1001) docker     (121)     5920 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/utilities/distributed.py
--rw-r--r--   0 runner    (1001) docker     (121)     2361 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/utilities/enums.py
--rw-r--r--   0 runner    (1001) docker     (121)      709 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/utilities/exceptions.py
--rw-r--r--   0 runner    (1001) docker     (121)     4919 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/utilities/imports.py
--rw-r--r--   0 runner    (1001) docker     (121)     1583 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/utilities/prints.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics/wrappers/
--rw-r--r--   0 runner    (1001) docker     (121)      953 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/wrappers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (121)     6684 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/wrappers/bootstrapping.py
--rw-r--r--   0 runner    (1001) docker     (121)     3438 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/wrappers/classwise.py
--rw-r--r--   0 runner    (1001) docker     (121)     4054 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/wrappers/minmax.py
--rw-r--r--   0 runner    (1001) docker     (121)     7423 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/wrappers/multioutput.py
--rw-r--r--   0 runner    (1001) docker     (121)     9558 2022-07-23 21:26:38.000000 torchmetrics-0.9.3/torchmetrics/wrappers/tracker.py
-drwxr-xr-x   0 runner    (1001) docker     (121)        0 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics.egg-info/
--rw-r--r--   0 runner    (1001) docker     (121)    12528 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (121)     7831 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (121)        1 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (121)        1 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (121)     1749 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (121)       26 2022-07-23 21:26:43.000000 torchmetrics-0.9.3/torchmetrics.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.616968 torchmetrics-1.0.0rc0/
+-rw-r--r--   0 runner    (1001) docker     (122)    57735 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/CHANGELOG.md
+-rw-r--r--   0 runner    (1001) docker     (122)     2573 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/CITATION.cff
+-rw-r--r--   0 runner    (1001) docker     (122)    11352 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/LICENSE
+-rw-r--r--   0 runner    (1001) docker     (122)      919 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (122)    14423 2023-05-04 08:32:42.616968 torchmetrics-1.0.0rc0/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)    12618 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/README.md
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.580968 torchmetrics-1.0.0rc0/requirements/
+-rw-r--r--   0 runner    (1001) docker     (122)      370 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/audio.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      465 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/audio_test.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      344 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/classification_test.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      305 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/detection.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/detection_test.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      352 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/devel.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      494 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/docs.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      344 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/doctest.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      336 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/image.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      351 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/image_test.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       34 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/integrate.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      280 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/multimodal.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      504 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/nominal_test.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      510 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/test.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      324 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/text.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      440 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/text_test.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      125 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/typing.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      308 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements/visual.txt
+-rw-r--r--   0 runner    (1001) docker     (122)      431 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/requirements.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       38 2023-05-04 08:32:42.620968 torchmetrics-1.0.0rc0/setup.cfg
+-rwxr-xr-x   0 runner    (1001) docker     (122)    10482 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.572968 torchmetrics-1.0.0rc0/src/
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.580968 torchmetrics-1.0.0rc0/src/torchmetrics/
+-rw-r--r--   0 runner    (1001) docker     (122)     1253 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/__about__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8285 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20446 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/aggregation.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.580968 torchmetrics-1.0.0rc0/src/torchmetrics/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)     1417 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3989 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/audio/_deprecated.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7053 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/audio/pesq.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6220 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/audio/pit.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10268 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/audio/sdr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8283 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/audio/snr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6512 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/audio/stoi.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.584968 torchmetrics-1.0.0rc0/src/torchmetrics/classification/
+-rw-r--r--   0 runner    (1001) docker     (122)     6508 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22429 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/accuracy.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24174 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/auroc.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24496 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/average_precision.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16886 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/calibration_error.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13382 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/cohen_kappa.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21540 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/confusion_matrix.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12445 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/dice.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17743 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/exact_match.py
+-rw-r--r--   0 runner    (1001) docker     (122)    50096 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/f_beta.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13732 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/group_fairness.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23104 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/hamming.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15194 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/hinge.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19471 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/jaccard.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16695 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/matthews_corrcoef.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25306 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/precision_fixed_recall.py
+-rw-r--r--   0 runner    (1001) docker     (122)    44737 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/precision_recall.py
+-rw-r--r--   0 runner    (1001) docker     (122)    31828 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/precision_recall_curve.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16414 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/ranking.py
+-rw-r--r--   0 runner    (1001) docker     (122)    25037 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/recall_fixed_precision.py
+-rw-r--r--   0 runner    (1001) docker     (122)    28089 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/roc.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22633 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/specificity.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18488 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/specificity_sensitivity.py
+-rw-r--r--   0 runner    (1001) docker     (122)    24239 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/classification/stat_scores.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27056 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/collections.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.588968 torchmetrics-1.0.0rc0/src/torchmetrics/detection/
+-rw-r--r--   0 runner    (1001) docker     (122)     1495 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2344 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/detection/_deprecated.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7929 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/detection/ciou.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7884 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/detection/diou.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7577 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/detection/giou.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3682 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/detection/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13103 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/detection/iou.py
+-rw-r--r--   0 runner    (1001) docker     (122)    40481 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/detection/mean_ap.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17517 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/detection/panoptic_qualities.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.588968 torchmetrics-1.0.0rc0/src/torchmetrics/functional/
+-rw-r--r--   0 runner    (1001) docker     (122)     9383 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.588968 torchmetrics-1.0.0rc0/src/torchmetrics/functional/audio/
+-rw-r--r--   0 runner    (1001) docker     (122)     1397 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/audio/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4393 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/audio/_deprecated.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4883 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/audio/pesq.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7998 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/audio/pit.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9500 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/audio/sdr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3129 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/audio/snr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4265 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/audio/stoi.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.592968 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/
+-rw-r--r--   0 runner    (1001) docker     (122)     7040 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19949 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/accuracy.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23489 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/auroc.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23096 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/average_precision.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16818 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/calibration_error.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11568 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/cohen_kappa.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27890 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/confusion_matrix.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9361 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/dice.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11726 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/exact_match.py
+-rw-r--r--   0 runner    (1001) docker     (122)    35583 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/f_beta.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16824 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/group_fairness.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20746 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/hamming.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12314 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/hinge.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16462 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/jaccard.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11430 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/matthews_corrcoef.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17938 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/precision_fixed_recall.py
+-rw-r--r--   0 runner    (1001) docker     (122)    35029 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/precision_recall.py
+-rw-r--r--   0 runner    (1001) docker     (122)    44124 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/precision_recall_curve.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11324 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/ranking.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21068 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/recall_fixed_precision.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26597 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/roc.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18560 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/specificity.py
+-rw-r--r--   0 runner    (1001) docker     (122)    22285 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/specificity_sensitivity.py
+-rw-r--r--   0 runner    (1001) docker     (122)    49417 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/stat_scores.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.592968 torchmetrics-1.0.0rc0/src/torchmetrics/functional/detection/
+-rw-r--r--   0 runner    (1001) docker     (122)     1649 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/detection/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2297 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/detection/_deprecated.py
+-rw-r--r--   0 runner    (1001) docker     (122)    19764 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/detection/_panoptic_quality_common.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3148 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/detection/ciou.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3150 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/detection/diou.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3173 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/detection/giou.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2996 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/detection/iou.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7903 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/detection/panoptic_qualities.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.596968 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/
+-rw-r--r--   0 runner    (1001) docker     (122)     1986 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9254 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/_deprecated.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4962 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/d_lambda.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4544 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/ergas.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2899 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/gradients.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6488 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/helper.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15609 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/lpips.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.596968 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/lpips_models/
+-rw-r--r--   0 runner    (1001) docker     (122)     6009 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/lpips_models/alex.pth
+-rw-r--r--   0 runner    (1001) docker     (122)    10811 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/lpips_models/squeeze.pth
+-rw-r--r--   0 runner    (1001) docker     (122)     7289 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/lpips_models/vgg.pth
+-rw-r--r--   0 runner    (1001) docker     (122)     5948 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/psnr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4527 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/psnrb.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4015 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/rase.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5515 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/rmse_sw.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4322 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/sam.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21491 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/ssim.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2799 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/tv.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6820 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/uqi.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.596968 torchmetrics-1.0.0rc0/src/torchmetrics/functional/multimodal/
+-rw-r--r--   0 runner    (1001) docker     (122)      809 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/multimodal/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5914 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/multimodal/clip_score.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.596968 torchmetrics-1.0.0rc0/src/torchmetrics/functional/nominal/
+-rw-r--r--   0 runner    (1001) docker     (122)     1194 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/nominal/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7377 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/nominal/cramers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6924 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/nominal/pearson.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7151 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/nominal/theils_u.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7676 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/nominal/tschuprows.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5731 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/nominal/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.596968 torchmetrics-1.0.0rc0/src/torchmetrics/functional/pairwise/
+-rw-r--r--   0 runner    (1001) docker     (122)     1173 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/pairwise/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3497 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/pairwise/cosine.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3447 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/pairwise/euclidean.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2251 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/pairwise/helpers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3174 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/pairwise/linear.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3194 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/pairwise/manhattan.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4028 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/pairwise/minkowski.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.600968 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/
+-rw-r--r--   0 runner    (1001) docker     (122)     2457 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2877 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/concordance.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3362 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/cosine_similarity.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5445 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/explained_variance.py
+-rw-r--r--   0 runner    (1001) docker     (122)    15067 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/kendall.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4395 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/kl_divergence.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3508 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/log_cosh.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2600 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/log_mse.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2562 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/mae.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3027 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/mape.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3007 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/minkowski.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2598 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/mse.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4367 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/pearson.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6636 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/r2.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5138 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/spearman.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3302 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/symmetric_mape.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6093 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/tweedie_deviance.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1360 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/utils.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2676 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/wmape.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.604968 torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/
+-rw-r--r--   0 runner    (1001) docker     (122)     1598 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5408 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/_deprecated.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2604 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/average_precision.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2670 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/fall_out.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2408 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/hit_rate.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2786 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/ndcg.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2744 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/precision.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4043 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/precision_recall_curve.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2120 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/r_precision.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2503 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/recall.py
+-rw-r--r--   0 runner    (1001) docker     (122)     1982 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/reciprocal_rank.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.604968 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/
+-rw-r--r--   0 runner    (1001) docker     (122)     1957 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13994 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/_deprecated.py
+-rw-r--r--   0 runner    (1001) docker     (122)    20940 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/bert.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7575 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/bleu.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2980 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/cer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    26029 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/chrf.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17582 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/eed.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17031 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/helper.py
+-rw-r--r--   0 runner    (1001) docker     (122)    11800 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/helper_embedding_metric.py
+-rw-r--r--   0 runner    (1001) docker     (122)    27937 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/infolm.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3028 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/mer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5493 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/perplexity.py
+-rw-r--r--   0 runner    (1001) docker     (122)    21939 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/rouge.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13088 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/sacre_bleu.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9892 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/squad.py
+-rw-r--r--   0 runner    (1001) docker     (122)    23216 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/ter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2975 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/wer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3462 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/wil.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3510 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/wip.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.608968 torchmetrics-1.0.0rc0/src/torchmetrics/image/
+-rw-r--r--   0 runner    (1001) docker     (122)     2299 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/image/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8942 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/image/_deprecated.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6111 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/image/d_lambda.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5822 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/image/ergas.py
+-rw-r--r--   0 runner    (1001) docker     (122)    18782 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/image/fid.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9038 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/image/inception.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14543 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/image/kid.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8044 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/image/lpip.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8472 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/image/psnr.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5662 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/image/psnrb.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5063 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/image/rase.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5426 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/image/rmse_sw.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5776 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/image/sam.py
+-rw-r--r--   0 runner    (1001) docker     (122)    17135 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/image/ssim.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5292 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/image/tv.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5919 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/image/uqi.py
+-rw-r--r--   0 runner    (1001) docker     (122)    48424 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/metric.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.608968 torchmetrics-1.0.0rc0/src/torchmetrics/multimodal/
+-rw-r--r--   0 runner    (1001) docker     (122)      796 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/multimodal/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6642 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/multimodal/clip_score.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.608968 torchmetrics-1.0.0rc0/src/torchmetrics/nominal/
+-rw-r--r--   0 runner    (1001) docker     (122)      909 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/nominal/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5928 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/nominal/cramers.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6228 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/nominal/pearson.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5371 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/nominal/theils_u.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5993 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/nominal/tschuprows.py
+-rw-r--r--   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/py.typed
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.612969 torchmetrics-1.0.0rc0/src/torchmetrics/regression/
+-rw-r--r--   0 runner    (1001) docker     (122)     2167 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5312 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/concordance.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5201 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/cosine_similarity.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6882 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/explained_variance.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8184 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/kendall.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6589 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/kl_divergence.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5406 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/log_cosh.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4680 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/log_mse.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4491 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/mae.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5063 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/mape.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4568 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/minkowski.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4618 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/mse.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8352 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/pearson.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7330 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/r2.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5749 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/spearman.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4791 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/symmetric_mape.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5889 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/tweedie_deviance.py
+-rw-r--r--   0 runner    (1001) docker     (122)     4857 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/regression/wmape.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.612969 torchmetrics-1.0.0rc0/src/torchmetrics/retrieval/
+-rw-r--r--   0 runner    (1001) docker     (122)     1558 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/retrieval/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9390 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/retrieval/_deprecated.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5938 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/retrieval/average_precision.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6338 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/retrieval/base.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7461 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/retrieval/fall_out.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5937 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/retrieval/hit_rate.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6062 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/retrieval/ndcg.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6308 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/retrieval/precision.py
+-rw-r--r--   0 runner    (1001) docker     (122)    16598 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/retrieval/precision_recall_curve.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5197 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/retrieval/r_precision.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5903 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/retrieval/recall.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5158 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/retrieval/reciprocal_rank.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.616968 torchmetrics-1.0.0rc0/src/torchmetrics/text/
+-rw-r--r--   0 runner    (1001) docker     (122)     1724 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8333 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/_deprecated.py
+-rw-r--r--   0 runner    (1001) docker     (122)    13939 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/bert.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5855 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/bleu.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5283 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/cer.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10472 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/chrf.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6357 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/eed.py
+-rw-r--r--   0 runner    (1001) docker     (122)    10137 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/infolm.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5226 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/mer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5058 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/perplexity.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9393 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/rouge.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6556 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/sacre_bleu.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6043 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/squad.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6742 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/ter.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5208 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/wer.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5291 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/wil.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5356 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/text/wip.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.616968 torchmetrics-1.0.0rc0/src/torchmetrics/utilities/
+-rw-r--r--   0 runner    (1001) docker     (122)      986 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/utilities/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)    34594 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/utilities/checks.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3737 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/utilities/compute.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9413 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/utilities/data.py
+-rw-r--r--   0 runner    (1001) docker     (122)     5863 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/utilities/distributed.py
+-rw-r--r--   0 runner    (1001) docker     (122)     3644 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/utilities/enums.py
+-rw-r--r--   0 runner    (1001) docker     (122)      830 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/utilities/exceptions.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2978 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/utilities/imports.py
+-rw-r--r--   0 runner    (1001) docker     (122)    12694 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/utilities/plot.py
+-rw-r--r--   0 runner    (1001) docker     (122)     2367 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/utilities/prints.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.616968 torchmetrics-1.0.0rc0/src/torchmetrics/wrappers/
+-rw-r--r--   0 runner    (1001) docker     (122)     1001 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/wrappers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (122)     8494 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/wrappers/bootstrapping.py
+-rw-r--r--   0 runner    (1001) docker     (122)     7154 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/wrappers/classwise.py
+-rw-r--r--   0 runner    (1001) docker     (122)     6027 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/wrappers/minmax.py
+-rw-r--r--   0 runner    (1001) docker     (122)     9090 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/wrappers/multioutput.py
+-rw-r--r--   0 runner    (1001) docker     (122)    14623 2023-05-04 08:32:39.000000 torchmetrics-1.0.0rc0/src/torchmetrics/wrappers/tracker.py
+drwxr-xr-x   0 runner    (1001) docker     (122)        0 2023-05-04 08:32:42.580968 torchmetrics-1.0.0rc0/src/torchmetrics.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (122)    14423 2023-05-04 08:32:42.000000 torchmetrics-1.0.0rc0/src/torchmetrics.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (122)    11955 2023-05-04 08:32:42.000000 torchmetrics-1.0.0rc0/src/torchmetrics.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        1 2023-05-04 08:32:42.000000 torchmetrics-1.0.0rc0/src/torchmetrics.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (122)        1 2023-05-04 08:32:42.000000 torchmetrics-1.0.0rc0/src/torchmetrics.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (122)     2196 2023-05-04 08:32:42.000000 torchmetrics-1.0.0rc0/src/torchmetrics.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (122)       13 2023-05-04 08:32:42.000000 torchmetrics-1.0.0rc0/src/torchmetrics.egg-info/top_level.txt
```

### filetype from file(1)

```diff
@@ -1 +1 @@
-POSIX tar archive (GNU)
+POSIX tar archive
```

### Comparing `torchmetrics-0.9.3/CITATION.cff` & `torchmetrics-1.0.0rc0/CITATION.cff`

 * *Files 8% similar despite different names*

```diff
@@ -27,15 +27,15 @@
   - name: Changsheng Quan
   - name: Maxim Grechkin
   - name: William Falcon
 
 doi: 10.21105/joss.04101
 license: "Apache-2.0"
 url: "https://www.pytorchlightning.ai"
-repository-code: "https://github.com/Lightning-AI/metrics"
+repository-code: "https://github.com/Lightning-AI/torchmetrics"
 date-released: 2022-02-11
 keywords:
   - machine learning
   - deep learning
   - artificial intelligence
   - metrics
   - pytorch
```

### Comparing `torchmetrics-0.9.3/LICENSE` & `torchmetrics-1.0.0rc0/LICENSE`

 * *Files identical despite different names*

### Comparing `torchmetrics-0.9.3/MANIFEST.in` & `torchmetrics-1.0.0rc0/MANIFEST.in`

 * *Files 9% similar despite different names*

```diff
@@ -1,27 +1,26 @@
 # Manifest syntax https://docs.python.org/2/distutils/sourcedist.html
 graft wheelhouse
 
 recursive-exclude __pycache__  *.py[cod] *.orig
+# include also models
+recursive-include src *.pth
 
 # Include the README and CHANGELOG
 include *.md
-recursive-include torchmetrics *.md
+recursive-include src *.md
 
 # Include the license file
 include LICENSE
 
 # Include Citation file
 include *.cff
 
 # Include marker file for PEP 561
-include torchmetrics/py.typed
-
-# Include examples
-recursive-include tm_examples *.py
+recursive-include src *.typed
 
 exclude *.sh
 exclude *.toml
 exclude *.svg
 
 # exclude tests from package
 recursive-exclude tests *
@@ -41,13 +40,11 @@
 exclude *.yml
 exclude *.yaml
 exclude Makefile
 
 prune .devcontainer
 prune .git
 prune .github
-prune .circleci
-prune notebook*
+prune examples*
 prune temp*
 prune test*
-prune benchmark*
-prune integration*
+prune SandBox*
```

### Comparing `torchmetrics-0.9.3/PKG-INFO` & `torchmetrics-1.0.0rc0/PKG-INFO`

 * *Files 9% similar despite different names*

```diff
@@ -1,81 +1,83 @@
 Metadata-Version: 2.1
 Name: torchmetrics
-Version: 0.9.3
+Version: 1.0.0rc0
 Summary: PyTorch native Metrics
-Home-page: https://github.com/Lightning-AI/metrics
-Download-URL: https://github.com/Lightning-AI/metrics/archive/master.zip
+Home-page: https://github.com/Lightning-AI/torchmetrics
+Download-URL: https://github.com/Lightning-AI/torchmetrics/archive/master.zip
 Author: Lightning-AI et al.
 Author-email: name@pytorchlightning.ai
 License: Apache-2.0
-Project-URL: Bug Tracker, https://github.com/Lightning-AI/metrics/issues
+Project-URL: Bug Tracker, https://github.com/Lightning-AI/torchmetrics/issues
 Project-URL: Documentation, https://torchmetrics.rtfd.io/en/latest/
-Project-URL: Source Code, https://github.com/Lightning-AI/metrics
+Project-URL: Source Code, https://github.com/Lightning-AI/torchmetrics
 Keywords: deep learning,machine learning,pytorch,metrics,AI
 Classifier: Environment :: Console
 Classifier: Natural Language :: English
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Developers
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Scientific/Engineering :: Image Recognition
 Classifier: Topic :: Scientific/Engineering :: Information Analysis
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
-Requires-Python: >=3.7
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
+Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 Provides-Extra: audio
 Provides-Extra: detection
-Provides-Extra: docs
 Provides-Extra: image
-Provides-Extra: integrate
+Provides-Extra: multimodal
 Provides-Extra: test
 Provides-Extra: text
+Provides-Extra: typing
+Provides-Extra: visual
 Provides-Extra: all
+Provides-Extra: dev
 License-File: LICENSE
 
 <div align="center">
 
-<img src="https://github.com/Lightning-AI/metrics/raw/v0.9.3/docs/source/_static/images/logo.png" width="400px">
+<img src="https://github.com/Lightning-AI/torchmetrics/raw/v1.0.0.rc0/docs/source/_static/images/logo.png" width="400px">
 
 **Machine learning metrics for distributed, scalable PyTorch applications.**
 
 ______________________________________________________________________
 
 <p align="center">
   <a href="#what-is-torchmetrics">What is Torchmetrics</a> 
   <a href="#implementing-your-own-module-metric">Implementing a metric</a> 
   <a href="#build-in-metrics">Built-in metrics</a> 
-  <a href="https://torchmetrics.readthedocs.io/en/v0.9.3">Docs</a> 
+  <a href="https://torchmetrics.readthedocs.io/en/v1.0.0.rc0">Docs</a> 
   <a href="#community">Community</a> 
   <a href="#license">License</a>
 </p>
 
 ______________________________________________________________________
 
 [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/torchmetrics)](https://pypi.org/project/torchmetrics/)
 [![PyPI Status](https://badge.fury.io/py/torchmetrics.svg)](https://badge.fury.io/py/torchmetrics)
 [![PyPI Status](https://pepy.tech/badge/torchmetrics)](https://pepy.tech/project/torchmetrics)
 [![Conda](https://img.shields.io/conda/v/conda-forge/torchmetrics?label=conda&color=success)](https://anaconda.org/conda-forge/torchmetrics)
 ![Conda](https://img.shields.io/conda/dn/conda-forge/torchmetrics)
-[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/Lightning-AI/metrics/blob/master/LICENSE)
+[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/Lightning-AI/torchmetrics/blob/master/LICENSE)
 
-[![CI testing - complete](https://github.com/Lightning-AI/metrics/actions/workflows/ci_test-full.yml/badge.svg?event=push)](https://github.com/Lightning-AI/metrics/actions/workflows/ci_test-full.yml)
-[![PyTorch & Conda](https://github.com/Lightning-AI/metrics/actions/workflows/ci_test-conda.yml/badge.svg?tag=v0.9.3)](https://github.com/Lightning-AI/metrics/actions/workflows/ci_test-conda.yml)
-[![Build Status](https://dev.azure.com/Lightning-AI/Metrics/_apis/build/status/Lightning-AI.metrics?branchName=refs%2Ftags%2Fv0.9.3)](https://dev.azure.com/Lightning-AI/Metrics/_build/latest?definitionId=2&branchName=refs%2Ftags%2Fv0.9.3)
-[![codecov](https://codecov.io/gh/Lightning-AI/metrics/release/v0.9.3/graph/badge.svg?token=NER6LPI3HS)](https://codecov.io/gh/Lightning-AI/metrics)
+[![CI testing - complete](https://github.com/Lightning-AI/torchmetrics/actions/workflows/ci-tests-full.yml/badge.svg?event=push)](https://github.com/Lightning-AI/torchmetrics/actions/workflows/ci-tests-full.yml)
+[![Build Status](https://dev.azure.com/Lightning-AI/Metrics/_apis/build/status%2FTM.unittests?branchName=refs%2Ftags%2Fv1.0.0.rc0)](https://dev.azure.com/Lightning-AI/Metrics/_build/latest?definitionId=2&branchName=refs%2Ftags%2Fv1.0.0.rc0)
+[![codecov](https://codecov.io/gh/Lightning-AI/torchmetrics/release/v1.0.0.rc0/graph/badge.svg?token=NER6LPI3HS)](https://codecov.io/gh/Lightning-AI/torchmetrics)
+[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/Lightning-AI/torchmetrics/master.svg)](https://results.pre-commit.ci/latest/github/Lightning-AI/torchmetrics/master)
 
-[![Slack](https://img.shields.io/badge/slack-chat-green.svg?logo=slack)](https://www.pytorchlightning.ai/community)
 [![Documentation Status](https://readthedocs.org/projects/torchmetrics/badge/?version=latest)](https://torchmetrics.readthedocs.io/en/latest/?badge=latest)
+[![Discord](https://img.shields.io/discord/1077906959069626439?style=plastic)](https://discord.gg/VptPCZkGNa)
 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5844769.svg)](https://doi.org/10.5281/zenodo.5844769)
 [![JOSS status](https://joss.theoj.org/papers/561d9bb59b400158bc8204e2639dca43/status.svg)](https://joss.theoj.org/papers/561d9bb59b400158bc8204e2639dca43)
-[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/Lightning-AI/metrics/master.svg)](https://results.pre-commit.ci/latest/github/Lightning-AI/metrics/master)
 
 ______________________________________________________________________
 
 </div>
 
 ## Installation
 
@@ -94,45 +96,45 @@
 conda install -c conda-forge torchmetrics
 ```
 
 Pip from source
 
 ```bash
 # with git
-pip install git+https://github.com/Lightning-AI/metrics.git@release/latest
+pip install git+https://github.com/Lightning-AI/torchmetrics.git@release/stable
 ```
 
 Pip from archive
 
 ```bash
-pip install https://github.com/Lightning-AI/metrics/archive/refs/heads/release/latest.zip
+pip install https://github.com/Lightning-AI/torchmetrics/archive/refs/heads/release/stable.zip
 ```
 
 Extra dependencies for specialized metrics:
 
 ```bash
 pip install torchmetrics[audio]
 pip install torchmetrics[image]
 pip install torchmetrics[text]
 pip install torchmetrics[all]  # install all of the above
 ```
 
 Install latest developer version
 
 ```bash
-pip install https://github.com/Lightning-AI/metrics/archive/master.zip
+pip install https://github.com/Lightning-AI/torchmetrics/archive/master.zip
 ```
 
 </details>
 
 ______________________________________________________________________
 
 ## What is TorchMetrics
 
-TorchMetrics is a collection of 80+ PyTorch metrics implementations and an easy-to-use API to create custom metrics. It offers:
+TorchMetrics is a collection of 100+ PyTorch metrics implementations and an easy-to-use API to create custom metrics. It offers:
 
 - A standardized interface to increase reproducibility
 - Reduces boilerplate
 - Automatic accumulation over batches
 - Metrics optimized for distributed-training
 - Automatic synchronization between multiple devices
 
@@ -158,15 +160,15 @@
 ```python
 import torch
 
 # import our library
 import torchmetrics
 
 # initialize metric
-metric = torchmetrics.Accuracy()
+metric = torchmetrics.classification.Accuracy(task="multiclass", num_classes=5)
 
 # move the metric to device you want computations to take place
 device = "cuda" if torch.cuda.is_available() else "cpu"
 metric.to(device)
 
 n_batches = 10
 for i in range(n_batches):
@@ -204,30 +206,29 @@
     os.environ["MASTER_ADDR"] = "localhost"
     os.environ["MASTER_PORT"] = "12355"
 
     # create default process group
     dist.init_process_group("gloo", rank=rank, world_size=world_size)
 
     # initialize model
-    metric = torchmetrics.Accuracy()
+    metric = torchmetrics.classification.Accuracy(task="multiclass", num_classes=5)
 
     # define a model and append your metric to it
     # this allows metric states to be placed on correct accelerators when
     # .to(device) is called on the model
     model = nn.Linear(10, 10)
     model.metric = metric
     model = model.to(rank)
 
     # initialize DDP
     model = DDP(model, device_ids=[rank])
 
     n_epochs = 5
     # this shows iteration over multiple training epochs
     for n in range(n_epochs):
-
         # this will be replaced by a DataLoader with a DistributedSampler
         n_batches = 10
         for i in range(n_batches):
             # simulate a classification problem
             preds = torch.randn(10, 5).softmax(dim=-1)
             target = torch.randint(5, (10,))
 
@@ -298,47 +299,96 @@
 # import our library
 import torchmetrics
 
 # simulate a classification problem
 preds = torch.randn(10, 5).softmax(dim=-1)
 target = torch.randint(5, (10,))
 
-acc = torchmetrics.functional.accuracy(preds, target)
+acc = torchmetrics.functional.classification.multiclass_accuracy(
+    preds, target, num_classes=5
+)
 ```
 
 ### Covered domains and example metrics
 
-We currently have implemented metrics within the following domains:
+In total TorchMetrics contains [100+ metrics](https://torchmetrics.readthedocs.io/en/v1.0.0.rc0all-metrics.html), which
+convers the following domains:
 
 - Audio
 - Classification
 - Detection
 - Information Retrieval
 - Image
+- Multimodal (Image-Text)
+- Nominal
 - Regression
 - Text
 
-In total TorchMetrics contains [80+ metrics](https://torchmetrics.readthedocs.io/en/v0.9.3all-metrics.html)!
+Each domain may require some additional dependencies which can be installed with `pip install torchmetrics[audio]`,
+`pip install torchmetrics['image']` etc.
+
+### Additional features
+
+#### Plotting
+
+Visualization of metrics can be important to help understand what is going on with your machine learning algorithms.
+Torchmetrics have build-in plotting support (install dependencies with `pip install torchmetrics[visual]`) for nearly
+all modular metrics through the `.plot` method. Simply call the method to get a simple visualization of any metric!
+
+```python
+import torch
+from torchmetrics.classification import MulticlassAccuracy, MulticlassConfusionMatrix
+
+num_classes = 3
+
+# this will generate two distributions that comes more similar as iterations increase
+w = torch.randn(num_classes)
+target = lambda it: torch.multinomial((it * w).softmax(dim=-1), 100, replacement=True)
+preds = lambda it: torch.multinomial((it * w).softmax(dim=-1), 100, replacement=True)
+
+acc = MulticlassAccuracy(num_classes=num_classes, average="micro")
+acc_per_class = MulticlassAccuracy(num_classes=num_classes, average=None)
+confmat = MulticlassConfusionMatrix(num_classes=num_classes)
+
+# plot single value
+for i in range(5):
+    acc_per_class.update(preds(i), target(i))
+    confmat.update(preds(i), target(i))
+fig1, ax1 = acc_per_class.plot()
+fig2, ax2 = confmat.plot()
+
+# plot multiple values
+values = []
+for i in range(10):
+    values.append(acc(preds(i), target(i)))
+fig3, ax3 = acc.plot(values)
+```
+
+<p align="center">
+  <img src="https://github.com/Lightning-AI/torchmetrics/raw/v1.0.0.rc0/docs/source/_static/images/plot_example.png" width="1000">
+</p>
+
+For examples of plotting different metrics try running [this example file](examples/plotting.py).
 
 ## Contribute!
 
 The lightning + TorchMetrics team is hard at work adding even more metrics.
 But we're looking for incredible contributors like you to submit new metrics
 and improve existing ones!
 
-Join our [Slack](https://www.pytorchlightning.ai/community) to get help become a contributor!
+Join our [Slack](https://www.pytorchlightning.ai/community) to get help with becoming a contributor!
 
 ## Community
 
 For help or questions, join our huge community on [Slack](https://www.pytorchlightning.ai/community)!
 
 ## Citation
 
 Were excited to continue the strong legacy of open source software and have been inspired
 over the years by Caffe, Theano, Keras, PyTorch, torchbearer, ignite, sklearn and fast.ai.
 
-If you want to cite this framework feel free to use GitHub's built-in citation option to generate a bibtex or APA-Style citation based on [this file](https://github.com/Lightning-AI/metrics/blob/master/CITATION.cff) (but only if you loved it ).
+If you want to cite this framework feel free to use GitHub's built-in citation option to generate a bibtex or APA-Style citation based on [this file](https://github.com/Lightning-AI/torchmetrics/blob/master/CITATION.cff) (but only if you loved it ).
 
 ## License
 
 Please observe the Apache 2.0 license that is listed in this repository.
 In addition, the Lightning framework is Patent Pending.
```

### Comparing `torchmetrics-0.9.3/README.md` & `torchmetrics-1.0.0rc0/README.md`

 * *Files 11% similar despite different names*

```diff
@@ -18,26 +18,25 @@
 ______________________________________________________________________
 
 [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/torchmetrics)](https://pypi.org/project/torchmetrics/)
 [![PyPI Status](https://badge.fury.io/py/torchmetrics.svg)](https://badge.fury.io/py/torchmetrics)
 [![PyPI Status](https://pepy.tech/badge/torchmetrics)](https://pepy.tech/project/torchmetrics)
 [![Conda](https://img.shields.io/conda/v/conda-forge/torchmetrics?label=conda&color=success)](https://anaconda.org/conda-forge/torchmetrics)
 ![Conda](https://img.shields.io/conda/dn/conda-forge/torchmetrics)
-[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/Lightning-AI/metrics/blob/master/LICENSE)
+[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/Lightning-AI/torchmetrics/blob/master/LICENSE)
 
-[![CI testing - complete](https://github.com/Lightning-AI/metrics/actions/workflows/ci_test-full.yml/badge.svg?event=push)](https://github.com/Lightning-AI/metrics/actions/workflows/ci_test-full.yml)
-[![PyTorch & Conda](https://github.com/Lightning-AI/metrics/actions/workflows/ci_test-conda.yml/badge.svg?branch=master&event=push)](https://github.com/Lightning-AI/metrics/actions/workflows/ci_test-conda.yml)
-[![Build Status](https://dev.azure.com/Lightning-AI/Metrics/_apis/build/status/Lightning-AI.metrics?branchName=master)](https://dev.azure.com/Lightning-AI/Metrics/_build/latest?definitionId=3&branchName=master)
-[![codecov](https://codecov.io/gh/Lightning-AI/metrics/branch/master/graph/badge.svg?token=NER6LPI3HS)](https://codecov.io/gh/Lightning-AI/metrics)
+[![CI testing - complete](https://github.com/Lightning-AI/torchmetrics/actions/workflows/ci-tests-full.yml/badge.svg?event=push)](https://github.com/Lightning-AI/torchmetrics/actions/workflows/ci-tests-full.yml)
+[![Build Status](https://dev.azure.com/Lightning-AI/Metrics/_apis/build/status%2FTM.unittests?branchName=master)](https://dev.azure.com/Lightning-AI/Metrics/_build/latest?definitionId=54&branchName=master)
+[![codecov](https://codecov.io/gh/Lightning-AI/torchmetrics/branch/master/graph/badge.svg?token=NER6LPI3HS)](https://codecov.io/gh/Lightning-AI/torchmetrics)
+[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/Lightning-AI/torchmetrics/master.svg)](https://results.pre-commit.ci/latest/github/Lightning-AI/torchmetrics/master)
 
-[![Slack](https://img.shields.io/badge/slack-chat-green.svg?logo=slack)](https://www.pytorchlightning.ai/community)
 [![Documentation Status](https://readthedocs.org/projects/torchmetrics/badge/?version=latest)](https://torchmetrics.readthedocs.io/en/latest/?badge=latest)
+[![Discord](https://img.shields.io/discord/1077906959069626439?style=plastic)](https://discord.gg/VptPCZkGNa)
 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5844769.svg)](https://doi.org/10.5281/zenodo.5844769)
 [![JOSS status](https://joss.theoj.org/papers/561d9bb59b400158bc8204e2639dca43/status.svg)](https://joss.theoj.org/papers/561d9bb59b400158bc8204e2639dca43)
-[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/Lightning-AI/metrics/master.svg)](https://results.pre-commit.ci/latest/github/Lightning-AI/metrics/master)
 
 ______________________________________________________________________
 
 </div>
 
 ## Installation
 
@@ -56,45 +55,45 @@
 conda install -c conda-forge torchmetrics
 ```
 
 Pip from source
 
 ```bash
 # with git
-pip install git+https://github.com/Lightning-AI/metrics.git@release/latest
+pip install git+https://github.com/Lightning-AI/torchmetrics.git@release/stable
 ```
 
 Pip from archive
 
 ```bash
-pip install https://github.com/Lightning-AI/metrics/archive/refs/heads/release/latest.zip
+pip install https://github.com/Lightning-AI/torchmetrics/archive/refs/heads/release/stable.zip
 ```
 
 Extra dependencies for specialized metrics:
 
 ```bash
 pip install torchmetrics[audio]
 pip install torchmetrics[image]
 pip install torchmetrics[text]
 pip install torchmetrics[all]  # install all of the above
 ```
 
 Install latest developer version
 
 ```bash
-pip install https://github.com/Lightning-AI/metrics/archive/master.zip
+pip install https://github.com/Lightning-AI/torchmetrics/archive/master.zip
 ```
 
 </details>
 
 ______________________________________________________________________
 
 ## What is TorchMetrics
 
-TorchMetrics is a collection of 80+ PyTorch metrics implementations and an easy-to-use API to create custom metrics. It offers:
+TorchMetrics is a collection of 100+ PyTorch metrics implementations and an easy-to-use API to create custom metrics. It offers:
 
 - A standardized interface to increase reproducibility
 - Reduces boilerplate
 - Automatic accumulation over batches
 - Metrics optimized for distributed-training
 - Automatic synchronization between multiple devices
 
@@ -120,15 +119,15 @@
 ```python
 import torch
 
 # import our library
 import torchmetrics
 
 # initialize metric
-metric = torchmetrics.Accuracy()
+metric = torchmetrics.classification.Accuracy(task="multiclass", num_classes=5)
 
 # move the metric to device you want computations to take place
 device = "cuda" if torch.cuda.is_available() else "cpu"
 metric.to(device)
 
 n_batches = 10
 for i in range(n_batches):
@@ -166,30 +165,29 @@
     os.environ["MASTER_ADDR"] = "localhost"
     os.environ["MASTER_PORT"] = "12355"
 
     # create default process group
     dist.init_process_group("gloo", rank=rank, world_size=world_size)
 
     # initialize model
-    metric = torchmetrics.Accuracy()
+    metric = torchmetrics.classification.Accuracy(task="multiclass", num_classes=5)
 
     # define a model and append your metric to it
     # this allows metric states to be placed on correct accelerators when
     # .to(device) is called on the model
     model = nn.Linear(10, 10)
     model.metric = metric
     model = model.to(rank)
 
     # initialize DDP
     model = DDP(model, device_ids=[rank])
 
     n_epochs = 5
     # this shows iteration over multiple training epochs
     for n in range(n_epochs):
-
         # this will be replaced by a DataLoader with a DistributedSampler
         n_batches = 10
         for i in range(n_batches):
             # simulate a classification problem
             preds = torch.randn(10, 5).softmax(dim=-1)
             target = torch.randint(5, (10,))
 
@@ -260,47 +258,96 @@
 # import our library
 import torchmetrics
 
 # simulate a classification problem
 preds = torch.randn(10, 5).softmax(dim=-1)
 target = torch.randint(5, (10,))
 
-acc = torchmetrics.functional.accuracy(preds, target)
+acc = torchmetrics.functional.classification.multiclass_accuracy(
+    preds, target, num_classes=5
+)
 ```
 
 ### Covered domains and example metrics
 
-We currently have implemented metrics within the following domains:
+In total TorchMetrics contains [100+ metrics](https://torchmetrics.readthedocs.io/en/stable/all-metrics.html), which
+convers the following domains:
 
 - Audio
 - Classification
 - Detection
 - Information Retrieval
 - Image
+- Multimodal (Image-Text)
+- Nominal
 - Regression
 - Text
 
-In total TorchMetrics contains [80+ metrics](https://torchmetrics.readthedocs.io/en/stable/all-metrics.html)!
+Each domain may require some additional dependencies which can be installed with `pip install torchmetrics[audio]`,
+`pip install torchmetrics['image']` etc.
+
+### Additional features
+
+#### Plotting
+
+Visualization of metrics can be important to help understand what is going on with your machine learning algorithms.
+Torchmetrics have build-in plotting support (install dependencies with `pip install torchmetrics[visual]`) for nearly
+all modular metrics through the `.plot` method. Simply call the method to get a simple visualization of any metric!
+
+```python
+import torch
+from torchmetrics.classification import MulticlassAccuracy, MulticlassConfusionMatrix
+
+num_classes = 3
+
+# this will generate two distributions that comes more similar as iterations increase
+w = torch.randn(num_classes)
+target = lambda it: torch.multinomial((it * w).softmax(dim=-1), 100, replacement=True)
+preds = lambda it: torch.multinomial((it * w).softmax(dim=-1), 100, replacement=True)
+
+acc = MulticlassAccuracy(num_classes=num_classes, average="micro")
+acc_per_class = MulticlassAccuracy(num_classes=num_classes, average=None)
+confmat = MulticlassConfusionMatrix(num_classes=num_classes)
+
+# plot single value
+for i in range(5):
+    acc_per_class.update(preds(i), target(i))
+    confmat.update(preds(i), target(i))
+fig1, ax1 = acc_per_class.plot()
+fig2, ax2 = confmat.plot()
+
+# plot multiple values
+values = []
+for i in range(10):
+    values.append(acc(preds(i), target(i)))
+fig3, ax3 = acc.plot(values)
+```
+
+<p align="center">
+  <img src="docs/source/_static/images/plot_example.png" width="1000">
+</p>
+
+For examples of plotting different metrics try running [this example file](examples/plotting.py).
 
 ## Contribute!
 
 The lightning + TorchMetrics team is hard at work adding even more metrics.
 But we're looking for incredible contributors like you to submit new metrics
 and improve existing ones!
 
-Join our [Slack](https://www.pytorchlightning.ai/community) to get help become a contributor!
+Join our [Slack](https://www.pytorchlightning.ai/community) to get help with becoming a contributor!
 
 ## Community
 
 For help or questions, join our huge community on [Slack](https://www.pytorchlightning.ai/community)!
 
 ## Citation
 
 Were excited to continue the strong legacy of open source software and have been inspired
 over the years by Caffe, Theano, Keras, PyTorch, torchbearer, ignite, sklearn and fast.ai.
 
-If you want to cite this framework feel free to use GitHub's built-in citation option to generate a bibtex or APA-Style citation based on [this file](https://github.com/Lightning-AI/metrics/blob/master/CITATION.cff) (but only if you loved it ).
+If you want to cite this framework feel free to use GitHub's built-in citation option to generate a bibtex or APA-Style citation based on [this file](https://github.com/Lightning-AI/torchmetrics/blob/master/CITATION.cff) (but only if you loved it ).
 
 ## License
 
 Please observe the Apache 2.0 license that is listed in this repository.
 In addition, the Lightning framework is Patent Pending.
```

### Comparing `torchmetrics-0.9.3/tm_examples/bert_score-own_model.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/text/perplexity.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,130 +1,127 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-"""An example of how to use BERTScore with a user's defined/own model and tokenizer.
 
-To run: python bert_score-own_model.py
-"""
+from typing import Any, Dict, Optional, Sequence, Union
 
-from pprint import pprint
-from typing import Dict, List, Union
+from torch import Tensor, tensor
 
-import torch
-import torch.nn as nn
-from torch import Tensor
-from torch.nn import Module
+from torchmetrics.functional.text.perplexity import _perplexity_compute, _perplexity_update
+from torchmetrics.metric import Metric
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
-from torchmetrics.text.bert import BERTScore
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["Perplexity.plot"]
 
-_NUM_LAYERS = 2
-_MODEL_DIM = 4
-_NHEAD = 2
-_MAX_LEN = 6
 
+class Perplexity(Metric):
+    r"""Perplexity measures how well a language model predicts a text sample.
 
-class UserTokenizer:
-    """The `UserTokenizer` class is required to be defined when a non-default model (i.e. not one from
-    `transformers`) is used.
+    It's calculated as the average number of bits per word a model needs to represent the sample.
 
-    The user's defined tokenizer is expected to return either token IDs or token embeddings that are fed into the model.
-    The tokenizer vocabulary should contain some special tokens, such as a `<pad>` token so that a tokenization will run
-    successfully in batches.
-    """
-
-    CLS_TOKEN = "<cls>"
-    SEP_TOKEN = "<sep>"
-    PAD_TOKEN = "<pad>"
-
-    def __init__(self) -> None:
-        self.word2vec = {
-            "hello": 0.5 * torch.ones(1, _MODEL_DIM),
-            "world": -0.5 * torch.ones(1, _MODEL_DIM),
-            self.CLS_TOKEN: torch.zeros(1, _MODEL_DIM),
-            self.SEP_TOKEN: torch.zeros(1, _MODEL_DIM),
-            self.PAD_TOKEN: torch.zeros(1, _MODEL_DIM),
-        }
-
-    def __call__(self, sentences: Union[str, List[str]], max_len: int = _MAX_LEN) -> Dict[str, Tensor]:
-        """The `__call__` method must be defined for this class. To ensure the functionality, the `__call__` method
-        should obey the input/output arguments structure described below.
+    As input to ``forward`` and ``update`` the metric accepts the following input:
 
-        Args:
-            sentences:
-                Input text. `Union[str, List[str]]`
-            max_len:
-                Maximum length of pre-processed text. `int`
+    - ``preds`` (:class:`~torch.Tensor`): Logits or a unnormalized score assigned to each token in a sequence with shape
+        [batch_size, seq_len, vocab_size], which is the output of a language model. Scores will be normalized internally
+        using softmax.
+    - ``target`` (:class:`~torch.Tensor`): Ground truth values with a shape [batch_size, seq_len]
 
-        Return:
-            Python dictionary containing the keys `input_ids` and `attention_mask` with corresponding values.
-        """
-        output_dict: Dict[str, Tensor] = {}
-        if isinstance(sentences, str):
-            sentences = [sentences]
-        # Add special tokens
-        sentences = [" ".join([self.CLS_TOKEN, sentence, self.SEP_TOKEN]) for sentence in sentences]
-        # Tokennize sentence
-        tokenized_sentences = [
-            sentence.lower().split()[:max_len] + [self.PAD_TOKEN] * (max_len - len(sentence.lower().split()))
-            for sentence in sentences
-        ]
-        output_dict["input_ids"] = torch.cat(
-            [torch.cat([self.word2vec[word] for word in sentence]).unsqueeze(0) for sentence in tokenized_sentences]
-        )
-        output_dict["attention_mask"] = torch.cat(
-            [
-                torch.tensor([1 if word != self.PAD_TOKEN else 0 for word in sentence]).unsqueeze(0)
-                for sentence in tokenized_sentences
-            ]
-        ).long()
-
-        return output_dict
-
-
-def get_user_model_encoder(num_layers: int = _NUM_LAYERS, d_model: int = _MODEL_DIM, nhead: int = _NHEAD) -> Module:
-    """Initialize the Transformer encoder."""
-    encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)
-    transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
-    return transformer_encoder
+    As output of ``forward`` and ``compute`` the metric returns the following output:
 
-
-def user_forward_fn(model: Module, batch: Dict[str, Tensor]) -> Tensor:
-    """User forward function used for the computation of model embeddings.
-
-    This function might be arbitrarily complicated inside. However, to ensure functionality, it should obey the
-    input/output argument structure described below.
+    - ``perp`` (:class:`~torch.Tensor`): A tensor with the perplexity score
 
     Args:
-        model:
-        batch:
-
-    Return:
-        The model output.
+        ignore_index: Integer specifying a target class to ignore.
+            If given, this class index does not contribute to the returned score.
+        kwargs:
+            Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+
+    Examples:
+        >>> from torchmetrics.text import Perplexity
+        >>> import torch
+        >>> preds = torch.rand(2, 8, 5, generator=torch.manual_seed(22))
+        >>> target = torch.randint(5, (2, 8), generator=torch.manual_seed(22))
+        >>> target[0, 6:] = -100
+        >>> perp = Perplexity(ignore_index=-100)
+        >>> perp(preds, target)
+        tensor(5.2545)
     """
-    return model(batch["input_ids"])
+    is_differentiable = True
+    higher_is_better = False
+    full_state_update = False
+    total_log_probs: Tensor
+    count: Tensor
+
+    def __init__(
+        self,
+        ignore_index: Optional[int] = None,
+        **kwargs: Dict[str, Any],
+    ) -> None:
+        super().__init__(**kwargs)
+        if ignore_index is not None and not isinstance(ignore_index, int):
+            raise ValueError(f"Argument `ignore_index` expected to either be `None` or an `int` but got {ignore_index}")
+        self.ignore_index = ignore_index
+        self.add_state("total_log_probs", default=tensor(0.0), dist_reduce_fx="sum")
+        self.add_state("count", default=tensor(0.0), dist_reduce_fx="sum")
+
+    def update(self, preds: Tensor, target: Tensor) -> None:
+        """Update state with predictions and targets."""
+        total_log_probs, count = _perplexity_update(preds, target, self.ignore_index)
+        self.total_log_probs += total_log_probs
+        self.count += count
+
+    def compute(self) -> Tensor:
+        """Compute the Perplexity."""
+        return _perplexity_compute(self.total_log_probs, self.count)
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
 
-
-_PREDS = ["hello", "hello world", "world world world"]
-_REFS = ["hello", "hello hello", "hello world hello"]
-
-
-if __name__ == "__main__":
-    tokenizer = UserTokenizer()
-    model = get_user_model_encoder()
-
-    bs = BERTScore(
-        model=model, user_tokenizer=tokenizer, user_forward_fn=user_forward_fn, max_length=_MAX_LEN, return_hash=False
-    )
-    bs.update(_PREDS, _REFS)
-    print(f"Predictions:\n {bs.preds_input_ids}\n {bs.preds_attention_mask}")
-
-    pprint(bs.compute())
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> import torch
+            >>> from torchmetrics.text import Perplexity
+            >>> metric = Perplexity()
+            >>> metric.update(torch.rand(2, 8, 5), torch.randint(5, (2, 8)))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> import torch
+            >>> from torchmetrics.text import Perplexity
+            >>> metric = Perplexity()
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(torch.rand(2, 8, 5), torch.randint(5, (2, 8))))
+            >>> fig_, ax_ = metric.plot(values)
+        """
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/__about__.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/__about__.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-__version__ = "0.9.3"
+__version__ = "1.0.0.rc0"
 __author__ = "Lightning-AI et al."
 __author_email__ = "name@pytorchlightning.ai"
 __license__ = "Apache-2.0"
-__copyright__ = f"Copyright (c) 2020-2022, {__author__}."
-__homepage__ = "https://github.com/Lightning-AI/metrics"
+__copyright__ = f"Copyright (c) 2020-2023, {__author__}."
+__homepage__ = "https://github.com/Lightning-AI/torchmetrics"
 __docs__ = "PyTorch native Metrics"
 __docs_url__ = "https://torchmetrics.readthedocs.io/en/stable/"
 __long_doc__ = """
 Torchmetrics is a metrics API created for easy metric development and usage in both PyTorch and
 [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/stable/). It was originally a part of
 Pytorch Lightning, but got split off so users could take advantage of the large collection of metrics
 implemented without having to install Pytorch Lightning (even though we would love for you to try it out).
-We currently have around 80+ metrics implemented and we continuously are adding more metrics, both within
+We currently have around 100+ metrics implemented and we continuously are adding more metrics, both within
 already covered domains (classification, regression ect.) but also new domains (object detection ect.).
 We make sure that all our metrics are rigorously tested such that you can trust them.
 """
 
 __all__ = [
     "__author__",
     "__author_email__",
```

### Comparing `torchmetrics-0.9.3/torchmetrics/aggregation.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/classification/group_fairness.py`

 * *Files 26% similar despite different names*

```diff
@@ -7,358 +7,307 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import warnings
-from typing import Any, Callable, List, Union
+from typing import Any, Dict, List, Optional, Sequence, Tuple, Union
 
 import torch
 from torch import Tensor
+from typing_extensions import Literal
 
+from torchmetrics.functional.classification.group_fairness import (
+    _binary_groups_stat_scores,
+    _compute_binary_demographic_parity,
+    _compute_binary_equal_opportunity,
+)
+from torchmetrics.functional.classification.stat_scores import _binary_stat_scores_arg_validation
 from torchmetrics.metric import Metric
-from torchmetrics.utilities.data import dim_zero_cat
+from torchmetrics.utilities import rank_zero_warn
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["BinaryFairness.plot"]
 
-class BaseAggregator(Metric):
-    """Base class for aggregation metrics.
 
-    Args:
-        fn: string specifying the reduction function
-        default_value: default tensor value to use for the metric state
-        nan_strategy: options:
-            - ``'error'``: if any `nan` values are encounted will give a RuntimeError
-            - ``'warn'``: if any `nan` values are encounted will give a warning and continue
-            - ``'ignore'``: all `nan` values are silently removed
-            - a float: if a float is provided will impude any `nan` values with this value
+class _AbstractGroupStatScores(Metric):
+    """Create and update states for computing group stats tp, fp, tn and fn."""
 
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+    def _create_states(self, num_groups: int) -> None:
+        default = lambda: torch.zeros(num_groups, dtype=torch.long)
+        self.add_state("tp", default(), dist_reduce_fx="sum")
+        self.add_state("fp", default(), dist_reduce_fx="sum")
+        self.add_state("tn", default(), dist_reduce_fx="sum")
+        self.add_state("fn", default(), dist_reduce_fx="sum")
 
-    Raises:
-        ValueError:
-            If ``nan_strategy`` is not one of ``error``, ``warn``, ``ignore`` or a float
-    """
+    def _update_states(self, group_stats: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]]) -> None:
+        for group, stats in enumerate(group_stats):
+            tp, fp, tn, fn = stats
+            self.tp[group] += tp
+            self.fp[group] += fp
+            self.tn[group] += tn
+            self.fn[group] += fn
 
-    value: Tensor
-    is_differentiable = None
-    higher_is_better = None
-    full_state_update = False
 
-    def __init__(
-        self,
-        fn: Union[Callable, str],
-        default_value: Union[Tensor, List],
-        nan_strategy: Union[str, float] = "error",
-        **kwargs: Any,
-    ):
-        super().__init__(**kwargs)
-        allowed_nan_strategy = ("error", "warn", "ignore")
-        if nan_strategy not in allowed_nan_strategy and not isinstance(nan_strategy, float):
-            raise ValueError(
-                f"Arg `nan_strategy` should either be a float or one of {allowed_nan_strategy}"
-                f" but got {nan_strategy}."
-            )
+class BinaryGroupStatRates(_AbstractGroupStatScores):
+    r"""Computes the true/false positives and true/false negatives rates for binary classification by group.
 
-        self.nan_strategy = nan_strategy
-        self.add_state("value", default=default_value, dist_reduce_fx=fn)
+    Related to `Type I and Type II errors`_.
 
-    def _cast_and_nan_check_input(self, x: Union[float, Tensor]) -> Tensor:
-        """Converts input x to a tensor if not already and afterwards checks for nans that either give an error,
-        warning or just ignored."""
-        if not isinstance(x, Tensor):
-            x = torch.as_tensor(x, dtype=torch.float32, device=self.device)
-
-        nans = torch.isnan(x)
-        if nans.any():
-            if self.nan_strategy == "error":
-                raise RuntimeError("Encounted `nan` values in tensor")
-            if self.nan_strategy == "warn":
-                warnings.warn("Encounted `nan` values in tensor. Will be removed.", UserWarning)
-                x = x[~nans]
-            elif self.nan_strategy == "ignore":
-                x = x[~nans]
-            else:
-                x[nans] = self.nan_strategy
-
-        return x.float()
-
-    def update(self, value: Union[float, Tensor]) -> None:  # type: ignore
-        """Overwrite in child class."""
-        pass
-
-    def compute(self) -> Tensor:
-        """Compute the aggregated value."""
-        return self.value
+    Accepts the following input tensors:
 
+    - ``preds`` (int or float tensor): ``(N, ...)``. If preds is a floating point tensor with values outside
+      [0,1] range we consider the input to be logits and will auto apply sigmoid per element. Addtionally,
+      we convert to int tensor with thresholding using the value in ``threshold``.
+    - ``target`` (int tensor): ``(N, ...)``.
+    - ``groups`` (int tensor): ``(N, ...)``. The group identifiers should be ``0, 1, ..., (num_groups - 1)``.
 
-class MaxMetric(BaseAggregator):
-    """Aggregate a stream of value into their maximum value.
+    The additional dimensions are flatted along the batch dimension.
 
     Args:
-        nan_strategy: options:
-            - ``'error'``: if any `nan` values are encounted will give a RuntimeError
-            - ``'warn'``: if any `nan` values are encounted will give a warning and continue
-            - ``'ignore'``: all `nan` values are silently removed
-            - a float: if a float is provided will impude any `nan` values with this value
-
+        num_groups: The number of groups.
+        threshold: Threshold for transforming probability to binary {0,1} predictions.
+        ignore_index: Specifies a target value that is ignored and does not contribute to the metric calculation
+        validate_args: bool indicating if input arguments and tensors should be validated for correctness.
+            Set to ``False`` for faster computations.
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
-    Raises:
-        ValueError:
-            If ``nan_strategy`` is not one of ``error``, ``warn``, ``ignore`` or a float
-
-    Example:
-        >>> from torchmetrics import MaxMetric
-        >>> metric = MaxMetric()
-        >>> metric.update(1)
-        >>> metric.update(torch.tensor([2, 3]))
-        >>> metric.compute()
-        tensor(3.)
-    """
+    Returns:
+        The metric returns a dict with a group identifier as key and a tensor with the tp, fp, tn and fn rates as value.
 
-    full_state_update = True
+    Example (preds is int tensor):
+        >>> from torchmetrics.classification import BinaryGroupStatRates
+        >>> target = torch.tensor([0, 1, 0, 1, 0, 1])
+        >>> preds = torch.tensor([0, 1, 0, 1, 0, 1])
+        >>> groups = torch.tensor([0, 1, 0, 1, 0, 1])
+        >>> metric = BinaryGroupStatRates(num_groups=2)
+        >>> metric(preds, target, groups)
+        {'group_0': tensor([0., 0., 1., 0.]), 'group_1': tensor([1., 0., 0., 0.])}
+
+    Example (preds is float tensor):
+        >>> from torchmetrics.classification import BinaryGroupStatRates
+        >>> target = torch.tensor([0, 1, 0, 1, 0, 1])
+        >>> preds = torch.tensor([0.11, 0.84, 0.22, 0.73, 0.33, 0.92])
+        >>> groups = torch.tensor([0, 1, 0, 1, 0, 1])
+        >>> metric = BinaryGroupStatRates(num_groups=2)
+        >>> metric(preds, target, groups)
+        {'group_0': tensor([0., 0., 1., 0.]), 'group_1': tensor([1., 0., 0., 0.])}
+    """
+    is_differentiable = False
+    higher_is_better = False
+    full_state_update: bool = False
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
 
     def __init__(
         self,
-        nan_strategy: Union[str, float] = "warn",
+        num_groups: int,
+        threshold: float = 0.5,
+        ignore_index: Optional[int] = None,
+        validate_args: bool = True,
         **kwargs: Any,
-    ):
-        super().__init__(
-            "max",
-            -torch.tensor(float("inf")),
-            nan_strategy,
-            **kwargs,
-        )
-
-    def update(self, value: Union[float, Tensor]) -> None:  # type: ignore
-        """Update state with data.
-
-        Args:
-            value: Either a float or tensor containing data. Additional tensor
-                dimensions will be flattened
-        """
-        value = self._cast_and_nan_check_input(value)
-        if value.numel():  # make sure tensor not empty
-            self.value = torch.max(self.value, torch.max(value))
-
-
-class MinMetric(BaseAggregator):
-    """Aggregate a stream of value into their minimum value.
-
-    Args:
-        nan_strategy: options:
-            - ``'error'``: if any `nan` values are encounted will give a RuntimeError
-            - ``'warn'``: if any `nan` values are encounted will give a warning and continue
-            - ``'ignore'``: all `nan` values are silently removed
-            - a float: if a float is provided will impude any `nan` values with this value
-
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+    ) -> None:
+        super().__init__()
 
-    Raises:
-        ValueError:
-            If ``nan_strategy`` is not one of ``error``, ``warn``, ``ignore`` or a float
-
-    Example:
-        >>> from torchmetrics import MinMetric
-        >>> metric = MinMetric()
-        >>> metric.update(1)
-        >>> metric.update(torch.tensor([2, 3]))
-        >>> metric.compute()
-        tensor(1.)
-    """
+        if validate_args:
+            _binary_stat_scores_arg_validation(threshold, "global", ignore_index)
 
-    full_state_update = True
+        if not isinstance(num_groups, int) and num_groups < 2:
+            raise ValueError(f"Expected argument `num_groups` to be an int larger than 1, but got {num_groups}")
+        self.num_groups = num_groups
+        self.threshold = threshold
+        self.ignore_index = ignore_index
+        self.validate_args = validate_args
 
-    def __init__(
-        self,
-        nan_strategy: Union[str, float] = "warn",
-        **kwargs: Any,
-    ):
-        super().__init__(
-            "min",
-            torch.tensor(float("inf")),
-            nan_strategy,
-            **kwargs,
-        )
+        self._create_states(self.num_groups)
 
-    def update(self, value: Union[float, Tensor]) -> None:  # type: ignore
-        """Update state with data.
+    def update(self, preds: torch.Tensor, target: torch.Tensor, groups: torch.Tensor) -> None:
+        """Update state with predictions, target and group identifiers.
 
         Args:
-            value: Either a float or tensor containing data. Additional tensor
-                dimensions will be flattened
+            preds: Tensor with predictions.
+            target: Tensor with true labels.
+            groups: Tensor with group identifiers. The group identifiers should be ``0, 1, ..., (num_groups - 1)``.
         """
-        value = self._cast_and_nan_check_input(value)
-        if value.numel():  # make sure tensor not empty
-            self.value = torch.min(self.value, torch.min(value))
+        group_stats = _binary_groups_stat_scores(
+            preds, target, groups, self.num_groups, self.threshold, self.ignore_index, self.validate_args
+        )
 
+        self._update_states(group_stats)
 
-class SumMetric(BaseAggregator):
-    """Aggregate a stream of value into their sum.
+    def compute(
+        self,
+    ) -> Dict[str, torch.Tensor]:
+        """Compute tp, fp, tn and fn rates based on inputs passed in to ``update`` previously."""
+        results = torch.stack((self.tp, self.fp, self.tn, self.fn), dim=1)
 
-    Args:
-        nan_strategy: options:
-            - ``'error'``: if any `nan` values are encounted will give a RuntimeError
-            - ``'warn'``: if any `nan` values are encounted will give a warning and continue
-            - ``'ignore'``: all `nan` values are silently removed
-            - a float: if a float is provided will impude any `nan` values with this value
+        return {f"group_{i}": group / group.sum() for i, group in enumerate(results)}
 
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
-    Raises:
-        ValueError:
-            If ``nan_strategy`` is not one of ``error``, ``warn``, ``ignore`` or a float
-
-    Example:
-        >>> from torchmetrics import SumMetric
-        >>> metric = SumMetric()
-        >>> metric.update(1)
-        >>> metric.update(torch.tensor([2, 3]))
-        >>> metric.compute()
-        tensor(6.)
-    """
+class BinaryFairness(_AbstractGroupStatScores):
+    r"""Computes `Demographic parity`_ and `Equal opportunity`_ ratio for binary classification problems.
 
-    def __init__(
-        self,
-        nan_strategy: Union[str, float] = "warn",
-        **kwargs: Any,
-    ):
-        super().__init__(
-            "sum",
-            torch.tensor(0.0),
-            nan_strategy,
-            **kwargs,
-        )
+    Accepts the following input tensors:
 
-    def update(self, value: Union[float, Tensor]) -> None:  # type: ignore
-        """Update state with data.
+    - ``preds`` (int or float tensor): ``(N, ...)``. If preds is a floating point tensor with values outside
+      [0,1] range we consider the input to be logits and will auto apply sigmoid per element. Addtionally,
+      we convert to int tensor with thresholding using the value in ``threshold``.
+    - ``groups`` (int tensor): ``(N, ...)``. The group identifiers should be ``0, 1, ..., (num_groups - 1)``.
+    - ``target`` (int tensor): ``(N, ...)``.
 
-        Args:
-            value: Either a float or tensor containing data. Additional tensor
-                dimensions will be flattened
-        """
-        value = self._cast_and_nan_check_input(value)
-        if value.numel():
-            self.value += value.sum()
+    The additional dimensions are flatted along the batch dimension.
 
+    This class computes the ratio between positivity rates and true positives rates for different groups.
+    If more than two groups are present, the disparity between the lowest and highest group is reported.
+    A disparity between positivity rates indicates a potential violation of demographic parity, and between
+    true positive rates indicates a potential violation of equal opportunity.
 
-class CatMetric(BaseAggregator):
-    """Concatenate a stream of values.
+    The lowest rate is divided by the highest, so a lower value means more discrimination against the numerator.
+    In the results this is also indicated as the key of dict is {metric}_{identifier_low_group}_{identifier_high_group}.
 
     Args:
-        nan_strategy: options:
-            - ``'error'``: if any `nan` values are encounted will give a RuntimeError
-            - ``'warn'``: if any `nan` values are encounted will give a warning and continue
-            - ``'ignore'``: all `nan` values are silently removed
-            - a float: if a float is provided will impude any `nan` values with this value
-
+        num_groups: The number of groups.
+        task: The task to compute. Can be either ``demographic_parity`` or ``equal_oppotunity`` or ``all``.
+        threshold: Threshold for transforming probability to binary {0,1} predictions.
+        ignore_index: Specifies a target value that is ignored and does not contribute to the metric calculation
+        validate_args: bool indicating if input arguments and tensors should be validated for correctness.
+            Set to ``False`` for faster computations.
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
-    Raises:
-        ValueError:
-            If ``nan_strategy`` is not one of ``error``, ``warn``, ``ignore`` or a float
-
-    Example:
-        >>> from torchmetrics import CatMetric
-        >>> metric = CatMetric()
-        >>> metric.update(1)
-        >>> metric.update(torch.tensor([2, 3]))
-        >>> metric.compute()
-        tensor([1., 2., 3.])
+    Returns:
+        The metric returns a dict where the key identifies the metric and groups with the lowest and highest true
+        positives rates as follows: {metric}__{identifier_low_group}_{identifier_high_group}.
+        The value is a tensor with the disparity rate.
+
+    Example (preds is int tensor):
+        >>> from torchmetrics.classification import BinaryFairness
+        >>> target = torch.tensor([0, 1, 0, 1, 0, 1])
+        >>> preds = torch.tensor([0, 1, 0, 1, 0, 1])
+        >>> groups = torch.tensor([0, 1, 0, 1, 0, 1])
+        >>> metric = BinaryFairness(2)
+        >>> metric(preds, target, groups)
+        {'DP_0_1': tensor(0.), 'EO_0_1': tensor(0.)}
+
+    Example (preds is float tensor):
+        >>> from torchmetrics.classification import BinaryFairness
+        >>> target = torch.tensor([0, 1, 0, 1, 0, 1])
+        >>> preds = torch.tensor([0.11, 0.84, 0.22, 0.73, 0.33, 0.92])
+        >>> groups = torch.tensor([0, 1, 0, 1, 0, 1])
+        >>> metric = BinaryFairness(2)
+        >>> metric(preds, target, groups)
+        {'DP_0_1': tensor(0.), 'EO_0_1': tensor(0.)}
     """
+    is_differentiable = False
+    higher_is_better = False
+    full_state_update: bool = False
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
 
     def __init__(
         self,
-        nan_strategy: Union[str, float] = "warn",
+        num_groups: int,
+        task: Literal["demographic_parity", "equal_opportunity", "all"] = "all",
+        threshold: float = 0.5,
+        ignore_index: Optional[int] = None,
+        validate_args: bool = True,
         **kwargs: Any,
-    ):
-        super().__init__("cat", [], nan_strategy, **kwargs)
+    ) -> None:
+        super().__init__()
 
-    def update(self, value: Union[float, Tensor]) -> None:  # type: ignore
-        """Update state with data.
-
-        Args:
-            value: Either a float or tensor containing data. Additional tensor
-                dimensions will be flattened
-        """
-        value = self._cast_and_nan_check_input(value)
-        if value.numel():
-            self.value.append(value)
-
-    def compute(self) -> Tensor:
-        """Compute the aggregated value."""
-        if isinstance(self.value, list) and self.value:
-            return dim_zero_cat(self.value)
-        return self.value
+        if task not in ["demographic_parity", "equal_opportunity", "all"]:
+            raise ValueError(
+                f"Expected argument `task` to either be ``demographic_parity``,"
+                f"``equal_opportunity`` or ``all`` but got {task}."
+            )
 
+        if validate_args:
+            _binary_stat_scores_arg_validation(threshold, "global", ignore_index)
 
-class MeanMetric(BaseAggregator):
-    """Aggregate a stream of value into their mean value.
+        if not isinstance(num_groups, int) and num_groups < 2:
+            raise ValueError(f"Expected argument `num_groups` to be an int larger than 1, but got {num_groups}")
+        self.num_groups = num_groups
+        self.task = task
+        self.threshold = threshold
+        self.ignore_index = ignore_index
+        self.validate_args = validate_args
 
-    Args:
-       nan_strategy: options:
-            - ``'error'``: if any `nan` values are encounted will give a RuntimeError
-            - ``'warn'``: if any `nan` values are encounted will give a warning and continue
-            - ``'ignore'``: all `nan` values are silently removed
-            - a float: if a float is provided will impude any `nan` values with this value
+        self._create_states(self.num_groups)
 
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+    def update(self, preds: torch.Tensor, target: torch.Tensor, groups: Optional[torch.Tensor] = None) -> None:
+        """Update state with predictions, groups, and target.
 
-    Raises:
-        ValueError:
-            If ``nan_strategy`` is not one of ``error``, ``warn``, ``ignore`` or a float
-
-    Example:
-        >>> from torchmetrics import MeanMetric
-        >>> metric = MeanMetric()
-        >>> metric.update(1)
-        >>> metric.update(torch.tensor([2, 3]))
-        >>> metric.compute()
-        tensor(2.)
-    """
+        Args:
+            preds: Tensor with predictions.
+            target: Tensor with true labels.
+            groups: Tensor with group identifiers. The group identifiers should be ``0, 1, ..., (num_groups - 1)``.
+        """
+        if self.task == "demographic_parity":
+            if target is not None:
+                rank_zero_warn("The task demographic_parity does not require a target.", UserWarning)
+            target = torch.zeros(preds.shape)
 
-    def __init__(
-        self,
-        nan_strategy: Union[str, float] = "warn",
-        **kwargs: Any,
-    ):
-        super().__init__(
-            "sum",
-            torch.tensor(0.0),
-            nan_strategy,
-            **kwargs,
+        group_stats = _binary_groups_stat_scores(
+            preds, target, groups, self.num_groups, self.threshold, self.ignore_index, self.validate_args
         )
-        self.add_state("weight", default=torch.tensor(0.0), dist_reduce_fx="sum")
 
-    def update(self, value: Union[float, Tensor], weight: Union[float, Tensor] = 1.0) -> None:  # type: ignore
-        """Update state with data.
+        self._update_states(group_stats)
+
+    def compute(
+        self,
+    ) -> Dict[str, torch.Tensor]:
+        """Compute fairness criteria based on inputs passed in to ``update`` previously."""
+        if self.task == "demographic_parity":
+            return _compute_binary_demographic_parity(self.tp, self.fp, self.tn, self.fn)
+
+        if self.task == "equal_opportunity":
+            return _compute_binary_equal_opportunity(self.tp, self.fp, self.tn, self.fn)
+
+        if self.task == "all":
+            return {
+                **_compute_binary_demographic_parity(self.tp, self.fp, self.tn, self.fn),
+                **_compute_binary_equal_opportunity(self.tp, self.fp, self.tn, self.fn),
+            }
+        return None
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
 
         Args:
-            value: Either a float or tensor containing data. Additional tensor
-                dimensions will be flattened
-            weight: Either a float or tensor containing weights for calculating
-                the average. Shape of weight should be able to broadcast with
-                the shape of `value`. Default to `1.0` corresponding to simple
-                harmonic average.
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure object and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import rand, randint
+            >>> # Example plotting a single value
+            >>> from torchmetrics.classification import BinaryFairness
+            >>> metric = BinaryFairness(2)
+            >>> metric.update(rand(20), randint(2,(20,)), randint(2,(20,)))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import rand, randint, ones
+            >>> # Example plotting multiple values
+            >>> from torchmetrics.classification import BinaryFairness
+            >>> metric = BinaryFairness(2)
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(rand(20), randint(2,(20,)), ones(20).long()))
+            >>> fig_, ax_ = metric.plot(values)
         """
-        value = self._cast_and_nan_check_input(value)
-        weight = self._cast_and_nan_check_input(weight)
-
-        if value.numel() == 0:
-            return
-        # broadcast weight to value shape
-        if hasattr(torch, "broadcast_to"):
-            weight = torch.broadcast_to(weight, value.shape)
-        else:
-            if weight.shape == ():
-                weight = torch.ones_like(value) * weight
-            if weight.shape != value.shape:
-                raise ValueError("Broadcasting not supported on PyTorch <1.8")
-
-        self.value += (value * weight).sum()
-        self.weight += weight.sum()
-
-    def compute(self) -> Tensor:
-        """Compute the aggregated value."""
-        return self.value / self.weight
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/audio/__init__.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/audio/__init__.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,23 +1,32 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from torchmetrics.audio.pit import PermutationInvariantTraining  # noqa: F401
-from torchmetrics.audio.sdr import ScaleInvariantSignalDistortionRatio, SignalDistortionRatio  # noqa: F401
-from torchmetrics.audio.snr import ScaleInvariantSignalNoiseRatio, SignalNoiseRatio  # noqa: F401
+from torchmetrics.functional.audio.pit import permutation_invariant_training, pit_permutate  # noqa: F401
+from torchmetrics.functional.audio.sdr import (  # noqa: F401
+    scale_invariant_signal_distortion_ratio,
+    signal_distortion_ratio,
+)
+from torchmetrics.functional.audio.snr import scale_invariant_signal_noise_ratio, signal_noise_ratio  # noqa: F401
 from torchmetrics.utilities.imports import _PESQ_AVAILABLE, _PYSTOI_AVAILABLE
 
+__all__ = []
+
 if _PESQ_AVAILABLE:
-    from torchmetrics.audio.pesq import PerceptualEvaluationSpeechQuality  # noqa: F401
+    from torchmetrics.functional.audio.pesq import perceptual_evaluation_speech_quality  # noqa: F401
+
+    __all__.append("perceptual_evaluation_speech_quality")
 
 if _PYSTOI_AVAILABLE:
-    from torchmetrics.audio.stoi import ShortTimeObjectiveIntelligibility  # noqa: F401
+    from torchmetrics.functional.audio.stoi import short_time_objective_intelligibility  # noqa: F401
+
+    __all__.append("short_time_objective_intelligibility")
```

### Comparing `torchmetrics-0.9.3/torchmetrics/audio/pesq.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/audio/pesq.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,117 +1,116 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any
+import numpy as np
+import torch
+from torch import Tensor
+
+from torchmetrics.utilities.checks import _check_same_shape
+from torchmetrics.utilities.imports import _MULTIPROCESSING_AVAILABLE, _PESQ_AVAILABLE
+
+if _PESQ_AVAILABLE:
+    import pesq as pesq_backend
+else:
+    pesq_backend = None
+
+
+__doctest_requires__ = {("perceptual_evaluation_speech_quality",): ["pesq"]}
+
+
+def perceptual_evaluation_speech_quality(
+    preds: Tensor,
+    target: Tensor,
+    fs: int,
+    mode: str,
+    keep_same_device: bool = False,
+    n_processes: int = 1,
+) -> Tensor:
+    r"""Calculate `Perceptual Evaluation of Speech Quality`_ (PESQ).
+
+    It's a recognized industry standard for audio quality that takes into considerations characteristics such as: audio
+    sharpness, call volume, background noise, clipping, audio interference ect. PESQ returns a score between -0.5 and
+    4.5 with the higher scores indicating a better quality.
 
-from torch import Tensor, tensor
-
-from torchmetrics.functional.audio.pesq import perceptual_evaluation_speech_quality
-from torchmetrics.metric import Metric
-from torchmetrics.utilities.imports import _PESQ_AVAILABLE
-
-__doctest_requires__ = {"PerceptualEvaluationSpeechQuality": ["pesq"]}
-
-
-class PerceptualEvaluationSpeechQuality(Metric):
-    """Perceptual Evaluation of Speech Quality (PESQ)
-
-    This is a wrapper for the pesq package [1]. Note that input will be moved to `cpu`
-    to perform the metric calculation.
+    This metric is a wrapper for the `pesq package`_. Note that input will be moved to `cpu` to perform the metric
+    calculation.
 
     .. note:: using this metrics requires you to have ``pesq`` install. Either install as ``pip install
         torchmetrics[audio]`` or ``pip install pesq``. Note that ``pesq`` will compile with your currently
         installed version of numpy, meaning that if you upgrade numpy at some point in the future you will
         most likely have to reinstall ``pesq``.
 
-    Forward accepts
-
-    - ``preds``: ``shape [...,time]``
-    - ``target``: ``shape [...,time]``
-
     Args:
+        preds: float tensor with shape ``(...,time)``
+        target: float tensor with shape ``(...,time)``
         fs: sampling frequency, should be 16000 or 8000 (Hz)
         mode: ``'wb'`` (wide-band) or ``'nb'`` (narrow-band)
         keep_same_device: whether to move the pesq value to the device of preds
+        n_processes: integer specifiying the number of processes to run in parallel for the metric calculation.
+            Only applies to batches of data and if ``multiprocessing`` package is installed.
 
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+    Returns:
+        Float tensor with shape ``(...,)`` of PESQ values per sample
 
     Raises:
         ModuleNotFoundError:
-            If ``peqs`` package is not installed
+            If ``pesq`` package is not installed
         ValueError:
             If ``fs`` is not either  ``8000`` or ``16000``
         ValueError:
             If ``mode`` is not either ``"wb"`` or ``"nb"``
+        RuntimeError:
+            If ``preds`` and ``target`` do not have the same shape
 
     Example:
-        >>> from torchmetrics.audio.pesq import PerceptualEvaluationSpeechQuality
-        >>> import torch
+        >>> from torch import randn
+        >>> from torchmetrics.functional.audio.pesq import perceptual_evaluation_speech_quality
         >>> g = torch.manual_seed(1)
-        >>> preds = torch.randn(8000)
-        >>> target = torch.randn(8000)
-        >>> nb_pesq = PerceptualEvaluationSpeechQuality(8000, 'nb')
-        >>> nb_pesq(preds, target)
+        >>> preds = randn(8000)
+        >>> target = randn(8000)
+        >>> perceptual_evaluation_speech_quality(preds, target, 8000, 'nb')
         tensor(2.2076)
-        >>> wb_pesq = PerceptualEvaluationSpeechQuality(16000, 'wb')
-        >>> wb_pesq(preds, target)
+        >>> perceptual_evaluation_speech_quality(preds, target, 16000, 'wb')
         tensor(1.7359)
-
-    References:
-        [1] https://github.com/ludlows/python-pesq
     """
-
-    sum_pesq: Tensor
-    total: Tensor
-    full_state_update: bool = False
-    is_differentiable: bool = False
-    higher_is_better: bool = True
-
-    def __init__(
-        self,
-        fs: int,
-        mode: str,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__(**kwargs)
-        if not _PESQ_AVAILABLE:
-            raise ModuleNotFoundError(
-                "PerceptualEvaluationSpeechQuality metric requires that `pesq` is installed."
-                " Either install as `pip install torchmetrics[audio]` or `pip install pesq`."
-            )
-        if fs not in (8000, 16000):
-            raise ValueError(f"Expected argument `fs` to either be 8000 or 16000 but got {fs}")
-        self.fs = fs
-        if mode not in ("wb", "nb"):
-            raise ValueError(f"Expected argument `mode` to either be 'wb' or 'nb' but got {mode}")
-        self.mode = mode
-
-        self.add_state("sum_pesq", default=tensor(0.0), dist_reduce_fx="sum")
-        self.add_state("total", default=tensor(0), dist_reduce_fx="sum")
-
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
-
-        Args:
-            preds: Predictions from model
-            target: Ground truth values
-        """
-        pesq_batch = perceptual_evaluation_speech_quality(preds, target, self.fs, self.mode, False).to(
-            self.sum_pesq.device
+    if not _PESQ_AVAILABLE:
+        raise ModuleNotFoundError(
+            "PESQ metric requires that pesq is installed."
+            " Either install as `pip install torchmetrics[audio]` or `pip install pesq`."
         )
+    if fs not in (8000, 16000):
+        raise ValueError(f"Expected argument `fs` to either be 8000 or 16000 but got {fs}")
+    if mode not in ("wb", "nb"):
+        raise ValueError(f"Expected argument `mode` to either be 'wb' or 'nb' but got {mode}")
+    _check_same_shape(preds, target)
+
+    if preds.ndim == 1:
+        pesq_val_np = pesq_backend.pesq(fs, target.detach().cpu().numpy(), preds.detach().cpu().numpy(), mode)
+        pesq_val = torch.tensor(pesq_val_np)
+    else:
+        preds_np = preds.reshape(-1, preds.shape[-1]).detach().cpu().numpy()
+        target_np = target.reshape(-1, preds.shape[-1]).detach().cpu().numpy()
+
+        if _MULTIPROCESSING_AVAILABLE and n_processes != 1:
+            pesq_val_np = pesq_backend.pesq_batch(fs, target_np, preds_np, mode, n_processor=n_processes)
+            pesq_val_np = np.array(pesq_val_np)
+        else:
+            pesq_val_np = np.empty(shape=(preds_np.shape[0]))
+            for b in range(preds_np.shape[0]):
+                pesq_val_np[b] = pesq_backend.pesq(fs, target_np[b, :], preds_np[b, :], mode)
+        pesq_val = torch.from_numpy(pesq_val_np)
+        pesq_val = pesq_val.reshape(preds.shape[:-1])
 
-        self.sum_pesq += pesq_batch.sum()
-        self.total += pesq_batch.numel()
+    if keep_same_device:
+        return pesq_val.to(preds.device)
 
-    def compute(self) -> Tensor:
-        """Computes average PESQ."""
-        return self.sum_pesq / self.total
+    return pesq_val
```

### Comparing `torchmetrics-0.9.3/torchmetrics/audio/sdr.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/text/eed.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,187 +1,163 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, Optional
+from typing import Any, List, Optional, Sequence, Tuple, Union
 
-from torch import Tensor, tensor
+from torch import Tensor, stack
+from typing_extensions import Literal
 
-from torchmetrics.functional.audio.sdr import scale_invariant_signal_distortion_ratio, signal_distortion_ratio
+from torchmetrics.functional.text.eed import _eed_compute, _eed_update
 from torchmetrics.metric import Metric
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
-__doctest_requires__ = {"SignalDistortionRatio": ["fast_bss_eval"]}
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["ExtendedEditDistance.plot"]
 
 
-class SignalDistortionRatio(Metric):
-    r"""Signal to Distortion Ratio (SDR) [1,2]
+class ExtendedEditDistance(Metric):
+    """Compute extended edit distance score (`ExtendedEditDistance`_) for strings or list of strings.
 
-    Forward accepts
+    The metric utilises the Levenshtein distance and extends it by adding a jump operation.
 
-    - ``preds``: shape ``[..., time]``
-    - ``target``: shape ``[..., time]``
+    As input to ``forward`` and ``update`` the metric accepts the following input:
 
-    .. note:
-        The metric currently does not seem to work with Pytorch v1.11 and specific GPU hardware.
+    - ``preds`` (:class:`~Sequence`): An iterable of hypothesis corpus
+    - ``target`` (:class:`~Sequence`): An iterable of iterables of reference corpus
 
-    Args:
-        use_cg_iter:
-            If provided, conjugate gradient descent is used to solve for the distortion
-            filter coefficients instead of direct Gaussian elimination, which requires that
-            ``fast-bss-eval`` is installed and pytorch version >= 1.8.
-            This can speed up the computation of the metrics in case the filters
-            are long. Using a value of 10 here has been shown to provide
-            good accuracy in most cases and is sufficient when using this
-            loss to train neural separation networks.
-        filter_length: The length of the distortion filter allowed
-        zero_mean:
-            When set to True, the mean of all signals is subtracted prior to computation of the metrics
-        load_diag:
-            If provided, this small value is added to the diagonal coefficients of the system metrics when solving
-            for the filter coefficients. This can help stabilize the metric in the case where some reference
-            signals may sometimes be zero
+    As output of ``forward`` and ``compute`` the metric returns the following output:
+
+    - ``eed`` (:class:`~torch.Tensor`): A tensor with the extended edit distance score
 
+    Args:
+        language: Language used in sentences. Only supports English (en) and Japanese (ja) for now.
+        return_sentence_level_score: An indication of whether sentence-level EED score is to be returned
+        alpha: optimal jump penalty, penalty for jumps between characters
+        rho: coverage cost, penalty for repetition of characters
+        deletion: penalty for deletion of character
+        insertion: penalty for insertion or substitution of character
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
     Example:
-        >>> from torchmetrics.audio import SignalDistortionRatio
-        >>> import torch
-        >>> g = torch.manual_seed(1)
-        >>> preds = torch.randn(8000)
-        >>> target = torch.randn(8000)
-        >>> sdr = SignalDistortionRatio()
-        >>> sdr(preds, target)
-        tensor(-12.0589)
-        >>> # use with pit
-        >>> from torchmetrics.audio import PermutationInvariantTraining
-        >>> from torchmetrics.functional.audio import signal_distortion_ratio
-        >>> preds = torch.randn(4, 2, 8000)  # [batch, spk, time]
-        >>> target = torch.randn(4, 2, 8000)
-        >>> pit = PermutationInvariantTraining(signal_distortion_ratio, 'max')
-        >>> pit(preds, target)
-        tensor(-11.6051)
-
-    References:
-        [1] Vincent, E., Gribonval, R., & Fevotte, C. (2006). Performance measurement in blind audio source separation.
-        IEEE Transactions on Audio, Speech and Language Processing, 14(4), 14621469.
-
-        [2] Scheibler, R. (2021). SDR -- Medium Rare with Fast Computations.
+        >>> from torchmetrics.text import ExtendedEditDistance
+        >>> preds = ["this is the prediction", "here is an other sample"]
+        >>> target = ["this is the reference", "here is another one"]
+        >>> eed = ExtendedEditDistance()
+        >>> eed(preds=preds, target=target)
+        tensor(0.3078)
     """
 
-    sum_sdr: Tensor
-    total: Tensor
+    higher_is_better: bool = False
+    is_differentiable: bool = False
     full_state_update: bool = False
-    is_differentiable: bool = True
-    higher_is_better: bool = True
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
+
+    sentence_eed: List[Tensor]
 
     def __init__(
         self,
-        use_cg_iter: Optional[int] = None,
-        filter_length: int = 512,
-        zero_mean: bool = False,
-        load_diag: Optional[float] = None,
+        language: Literal["en", "ja"] = "en",
+        return_sentence_level_score: bool = False,
+        alpha: float = 2.0,
+        rho: float = 0.3,
+        deletion: float = 0.2,
+        insertion: float = 1.0,
         **kwargs: Any,
     ) -> None:
         super().__init__(**kwargs)
 
-        self.use_cg_iter = use_cg_iter
-        self.filter_length = filter_length
-        self.zero_mean = zero_mean
-        self.load_diag = load_diag
+        if language not in ("en", "ja"):
+            raise ValueError(f"Expected argument `language` to either be `en` or `ja` but got {language}")
+        self.language: Literal["en", "ja"] = language
+        self.return_sentence_level_score = return_sentence_level_score
+
+        # input validation for parameters
+        for param_name, param in zip(["alpha", "rho", "deletion", "insertion"], [alpha, rho, deletion, insertion]):
+            if not isinstance(param, float) or isinstance(param, float) and param < 0:
+                raise ValueError(f"Parameter `{param_name}` is expected to be a non-negative float.")
+
+        self.alpha = alpha
+        self.rho = rho
+        self.deletion = deletion
+        self.insertion = insertion
 
-        self.add_state("sum_sdr", default=tensor(0.0), dist_reduce_fx="sum")
-        self.add_state("total", default=tensor(0), dist_reduce_fx="sum")
+        self.add_state("sentence_eed", [], dist_reduce_fx="cat")
 
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
-
-        Args:
-            preds: Predictions from model
-            target: Ground truth values
-        """
-        sdr_batch = signal_distortion_ratio(
-            preds, target, self.use_cg_iter, self.filter_length, self.zero_mean, self.load_diag
-        )
-
-        self.sum_sdr += sdr_batch.sum()
-        self.total += sdr_batch.numel()
-
-    def compute(self) -> Tensor:
-        """Computes average SDR."""
-        return self.sum_sdr / self.total
-
-
-class ScaleInvariantSignalDistortionRatio(Metric):
-    """Scale-invariant signal-to-distortion ratio (SI-SDR). The SI-SDR value is in general considered an overall
-    measure of how good a source sound.
-
-    Forward accepts
-
-    - ``preds``: ``shape [...,time]``
-    - ``target``: ``shape [...,time]``
-
-    Args:
-        zero_mean: if to zero mean target and preds or not
-
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
-
-    Raises:
-        TypeError:
-            if target and preds have a different shape
-
-    Returns:
-        average si-sdr value
-
-    Example:
-        >>> import torch
-        >>> from torchmetrics import ScaleInvariantSignalDistortionRatio
-        >>> target = torch.tensor([3.0, -0.5, 2.0, 7.0])
-        >>> preds = torch.tensor([2.5, 0.0, 2.0, 8.0])
-        >>> si_sdr = ScaleInvariantSignalDistortionRatio()
-        >>> si_sdr(preds, target)
-        tensor(18.4030)
-
-    References:
-        [1] Le Roux, Jonathan, et al. "SDR half-baked or well done." IEEE International Conference on Acoustics, Speech
-        and Signal Processing (ICASSP) 2019.
-    """
-
-    is_differentiable = True
-    higher_is_better = True
-    sum_si_sdr: Tensor
-    total: Tensor
-
-    def __init__(
+    def update(
         self,
-        zero_mean: bool = False,
-        **kwargs: Any,
+        preds: Union[str, Sequence[str]],
+        target: Sequence[Union[str, Sequence[str]]],
     ) -> None:
-        super().__init__(**kwargs)
-        self.zero_mean = zero_mean
-
-        self.add_state("sum_si_sdr", default=tensor(0.0), dist_reduce_fx="sum")
-        self.add_state("total", default=tensor(0), dist_reduce_fx="sum")
+        """Update state with predictions and targets."""
+        self.sentence_eed = _eed_update(
+            preds,
+            target,
+            self.language,
+            self.alpha,
+            self.rho,
+            self.deletion,
+            self.insertion,
+            self.sentence_eed,
+        )
 
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
+    def compute(self) -> Union[Tensor, Tuple[Tensor, Tensor]]:
+        """Calculate extended edit distance score."""
+        average = _eed_compute(self.sentence_eed)
+
+        if self.return_sentence_level_score:
+            return average, stack(self.sentence_eed)
+        return average
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
 
         Args:
-            preds: Predictions from model
-            target: Ground truth values
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> from torchmetrics.text import ExtendedEditDistance
+            >>> metric = ExtendedEditDistance()
+            >>> preds = ["this is the prediction", "there is an other sample"]
+            >>> target = ["this is the reference", "there is another one"]
+            >>> metric.update(preds, target)
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> from torchmetrics.text import ExtendedEditDistance
+            >>> metric = ExtendedEditDistance()
+            >>> preds = ["this is the prediction", "there is an other sample"]
+            >>> target = ["this is the reference", "there is another one"]
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(preds, target))
+            >>> fig_, ax_ = metric.plot(values)
         """
-        si_sdr_batch = scale_invariant_signal_distortion_ratio(preds=preds, target=target, zero_mean=self.zero_mean)
-
-        self.sum_si_sdr += si_sdr_batch.sum()
-        self.total += si_sdr_batch.numel()
-
-    def compute(self) -> Tensor:
-        """Computes average SI-SDR."""
-        return self.sum_si_sdr / self.total
+        return self._plot(val, ax)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `torchmetrics-0.9.3/torchmetrics/audio/snr.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/ergas.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,158 +1,124 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any
+from typing import Tuple, Union
 
-from torch import Tensor, tensor
+import torch
+from torch import Tensor
+from typing_extensions import Literal
 
-from torchmetrics.functional.audio.snr import scale_invariant_signal_noise_ratio, signal_noise_ratio
-from torchmetrics.metric import Metric
+from torchmetrics.utilities.checks import _check_same_shape
+from torchmetrics.utilities.distributed import reduce
 
 
-class SignalNoiseRatio(Metric):
-    r"""Signal-to-noise ratio (SNR_):
-
-    .. math::
-        \text{SNR} = \frac{P_{signal}}{P_{noise}}
-
-    where  :math:`P` denotes the power of each signal. The SNR metric compares the level
-    of the desired signal to the level of background noise. Therefore, a high value of
-    SNR means that the audio is clear.
-
-    Forward accepts
-
-    - ``preds``: ``shape [..., time]``
-    - ``target``: ``shape [..., time]``
+def _ergas_update(preds: Tensor, target: Tensor) -> Tuple[Tensor, Tensor]:
+    """Update and returns variables required to compute Erreur Relative Globale Adimensionnelle de Synthse.
 
     Args:
-        zero_mean: if to zero mean target and preds or not
-
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
-
-    Raises:
-        TypeError:
-            if target and preds have a different shape
+        preds: Predicted tensor
+        target: Ground truth tensor
+    """
+    if preds.dtype != target.dtype:
+        raise TypeError(
+            "Expected `preds` and `target` to have the same data type."
+            f" Got preds: {preds.dtype} and target: {target.dtype}."
+        )
+    _check_same_shape(preds, target)
+    if len(preds.shape) != 4:
+        raise ValueError(
+            "Expected `preds` and `target` to have BxCxHxW shape."
+            f" Got preds: {preds.shape} and target: {target.shape}."
+        )
+    return preds, target
+
+
+def _ergas_compute(
+    preds: Tensor,
+    target: Tensor,
+    ratio: Union[int, float] = 4,
+    reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
+) -> Tensor:
+    """Erreur Relative Globale Adimensionnelle de Synthse.
 
-    Returns:
-        average snr value
+    Args:
+        preds: estimated image
+        target: ground truth image
+        ratio: ratio of high resolution to low resolution
+        reduction: a method to reduce metric score over labels.
+
+            - ``'elementwise_mean'``: takes the mean (default)
+            - ``'sum'``: takes the sum
+            - ``'none'`` or ``None``: no reduction will be applied
 
     Example:
-        >>> import torch
-        >>> from torchmetrics import SignalNoiseRatio
-        >>> target = torch.tensor([3.0, -0.5, 2.0, 7.0])
-        >>> preds = torch.tensor([2.5, 0.0, 2.0, 8.0])
-        >>> snr = SignalNoiseRatio()
-        >>> snr(preds, target)
-        tensor(16.1805)
-
-    References:
-        [1] Le Roux, Jonathan, et al. "SDR half-baked or well done." IEEE International Conference on Acoustics, Speech
-        and Signal Processing (ICASSP) 2019.
-
+        >>> preds = torch.rand([16, 1, 16, 16], generator=torch.manual_seed(42))
+        >>> target = preds * 0.75
+        >>> preds, target = _ergas_update(preds, target)
+        >>> torch.round(_ergas_compute(preds, target))
+        tensor(154.)
     """
-    full_state_update: bool = False
-    is_differentiable: bool = True
-    higher_is_better: bool = True
-    sum_snr: Tensor
-    total: Tensor
-
-    def __init__(
-        self,
-        zero_mean: bool = False,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__(**kwargs)
-        self.zero_mean = zero_mean
-
-        self.add_state("sum_snr", default=tensor(0.0), dist_reduce_fx="sum")
-        self.add_state("total", default=tensor(0), dist_reduce_fx="sum")
-
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
-
-        Args:
-            preds: Predictions from model
-            target: Ground truth values
-        """
-        snr_batch = signal_noise_ratio(preds=preds, target=target, zero_mean=self.zero_mean)
-
-        self.sum_snr += snr_batch.sum()
-        self.total += snr_batch.numel()
-
-    def compute(self) -> Tensor:
-        """Computes average SNR."""
-        return self.sum_snr / self.total
-
-
-class ScaleInvariantSignalNoiseRatio(Metric):
-    """Scale-invariant signal-to-noise ratio (SI-SNR).
-
-    Forward accepts
-
-    - ``preds``: ``shape [...,time]``
-    - ``target``: ``shape [...,time]``
+    b, c, h, w = preds.shape
+    preds = preds.reshape(b, c, h * w)
+    target = target.reshape(b, c, h * w)
+
+    diff = preds - target
+    sum_squared_error = torch.sum(diff * diff, dim=2)
+    rmse_per_band = torch.sqrt(sum_squared_error / (h * w))
+    mean_target = torch.mean(target, dim=2)
+
+    ergas_score = 100 * ratio * torch.sqrt(torch.sum((rmse_per_band / mean_target) ** 2, dim=1) / c)
+    return reduce(ergas_score, reduction)
+
+
+def error_relative_global_dimensionless_synthesis(
+    preds: Tensor,
+    target: Tensor,
+    ratio: Union[int, float] = 4,
+    reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
+) -> Tensor:
+    """Erreur Relative Globale Adimensionnelle de Synthse.
 
     Args:
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+        preds: estimated image
+        target: ground truth image
+        ratio: ratio of high resolution to low resolution
+        reduction: a method to reduce metric score over labels.
+
+            - ``'elementwise_mean'``: takes the mean (default)
+            - ``'sum'``: takes the sum
+            - ``'none'`` or ``None``: no reduction will be applied
+
+    Return:
+        Tensor with RelativeG score
 
     Raises:
         TypeError:
-            if target and preds have a different shape
-
-    Returns:
-        average si-snr value
+            If ``preds`` and ``target`` don't have the same data type.
+        ValueError:
+            If ``preds`` and ``target`` don't have ``BxCxHxW shape``.
 
     Example:
-        >>> import torch
-        >>> from torchmetrics import ScaleInvariantSignalNoiseRatio
-        >>> target = torch.tensor([3.0, -0.5, 2.0, 7.0])
-        >>> preds = torch.tensor([2.5, 0.0, 2.0, 8.0])
-        >>> si_snr = ScaleInvariantSignalNoiseRatio()
-        >>> si_snr(preds, target)
-        tensor(15.0918)
+        >>> from torchmetrics.functional.image import error_relative_global_dimensionless_synthesis
+        >>> preds = torch.rand([16, 1, 16, 16], generator=torch.manual_seed(42))
+        >>> target = preds * 0.75
+        >>> ergds = error_relative_global_dimensionless_synthesis(preds, target)
+        >>> torch.round(ergds)
+        tensor(154.)
 
     References:
-        [1] Y. Luo and N. Mesgarani, "TaSNet: Time-Domain Audio Separation Network for Real-Time, Single-Channel Speech
-        Separation," 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp.
-        696-700, doi: 10.1109/ICASSP.2018.8462116.
+        [1] Qian Du; Nicholas H. Younan; Roger King; Vijay P. Shah, "On the Performance Evaluation of
+        Pan-Sharpening Techniques" in IEEE Geoscience and Remote Sensing Letters, vol. 4, no. 4, pp. 518-522,
+        15 October 2007, doi: 10.1109/LGRS.2007.896328.
     """
-
-    is_differentiable = True
-    sum_si_snr: Tensor
-    total: Tensor
-    higher_is_better = True
-
-    def __init__(
-        self,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__(**kwargs)
-
-        self.add_state("sum_si_snr", default=tensor(0.0), dist_reduce_fx="sum")
-        self.add_state("total", default=tensor(0), dist_reduce_fx="sum")
-
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
-
-        Args:
-            preds: Predictions from model
-            target: Ground truth values
-        """
-        si_snr_batch = scale_invariant_signal_noise_ratio(preds=preds, target=target)
-
-        self.sum_si_snr += si_snr_batch.sum()
-        self.total += si_snr_batch.numel()
-
-    def compute(self) -> Tensor:
-        """Computes average SI-SNR."""
-        return self.sum_si_snr / self.total
+    preds, target = _ergas_update(preds, target)
+    return _ergas_compute(preds, target, ratio, reduction)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `torchmetrics-0.9.3/torchmetrics/audio/stoi.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/audio/stoi.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,120 +1,95 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any
+import numpy as np
+import torch
+from torch import Tensor
 
-from torch import Tensor, tensor
-
-from torchmetrics.functional.audio.stoi import short_time_objective_intelligibility
-from torchmetrics.metric import Metric
+from torchmetrics.utilities.checks import _check_same_shape
 from torchmetrics.utilities.imports import _PYSTOI_AVAILABLE
 
-__doctest_requires__ = {"ShortTimeObjectiveIntelligibility": ["pystoi"]}
-
-
-class ShortTimeObjectiveIntelligibility(Metric):
-    r"""STOI (Short-Time Objective Intelligibility, see [2,3]), a wrapper for the pystoi package [1].
-    Note that input will be moved to `cpu` to perform the metric calculation.
-
-    Intelligibility measure which is highly correlated with the intelligibility of degraded speech signals, e.g., due
-    to additive noise, single-/multi-channel noise reduction, binary masking and vocoded speech as in CI simulations.
-    The STOI-measure is intrusive, i.e., a function of the clean and degraded speech signals. STOI may be a good
-    alternative to the speech intelligibility index (SII) or the speech transmission index (STI), when you are
-    interested in the effect of nonlinear processing to noisy speech, e.g., noise reduction, binary masking algorithms,
-    on speech intelligibility. Description taken from `Cees Taal's website <http://www.ceestaal.nl/code/>`_.
+if _PYSTOI_AVAILABLE:
+    from pystoi import stoi as stoi_backend
+else:
+    stoi_backend = None
+    __doctest_skip__ = ["short_time_objective_intelligibility"]
+
+
+def short_time_objective_intelligibility(
+    preds: Tensor, target: Tensor, fs: int, extended: bool = False, keep_same_device: bool = False
+) -> Tensor:
+    r"""Calculate STOI (Short-Time Objective Intelligibility) metric for evaluating speech signals.
+
+    Intelligibility measure which is highly correlated with the intelligibility of degraded speech signals, e.g., due to
+    additive noise, single-/multi-channel noise reduction, binary masking and vocoded speech as in CI simulations. The
+    STOI-measure is intrusive, i.e., a function of the clean and degraded speech signals. STOI may be a good alternative
+    to the speech intelligibility index (SII) or the speech transmission index (STI), when you are interested in
+    the effect of nonlinear processing to noisy speech, e.g., noise reduction, binary masking algorithms, on speech
+    intelligibility. Description taken from  `Cees Taal's website`_ and for further defails see `STOI ref1`_ and
+    `STOI ref2`_.
+
+    This metric is a wrapper for the `pystoi package`_. As the implementation backend implementation only supports
+    calculations on CPU, all input will automatically be moved to CPU to perform the metric calculation before being
+    moved back to the original device.
 
     .. note:: using this metrics requires you to have ``pystoi`` install. Either install as ``pip install
         torchmetrics[audio]`` or ``pip install pystoi``
 
-    Forward accepts
-
-    - ``preds``: ``shape [...,time]``
-    - ``target``: ``shape [...,time]``
-
     Args:
+        preds: float tensor with shape ``(...,time)``
+        target: float tensor with shape ``(...,time)``
         fs: sampling frequency (Hz)
-        extended: whether to use the extended STOI described in [4]
-
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+        extended: whether to use the extended STOI described in `STOI ref3`_.
+        keep_same_device: whether to move the stoi value to the device of preds
 
     Returns:
-        average STOI value
+        stoi value of shape [...]
 
     Raises:
         ModuleNotFoundError:
             If ``pystoi`` package is not installed
+        RuntimeError:
+            If ``preds`` and ``target`` does not have the same shape
 
     Example:
-        >>> from torchmetrics.audio.stoi import ShortTimeObjectiveIntelligibility
         >>> import torch
+        >>> from torchmetrics.functional.audio.stoi import short_time_objective_intelligibility
         >>> g = torch.manual_seed(1)
         >>> preds = torch.randn(8000)
         >>> target = torch.randn(8000)
-        >>> stoi = ShortTimeObjectiveIntelligibility(8000, False)
-        >>> stoi(preds, target)
+        >>> short_time_objective_intelligibility(preds, target, 8000).float()
         tensor(-0.0100)
-
-    References:
-        [1] https://github.com/mpariente/pystoi
-
-        [2] C.H.Taal, R.C.Hendriks, R.Heusdens, J.Jensen 'A Short-Time Objective Intelligibility Measure for
-        Time-Frequency Weighted Noisy Speech', ICASSP 2010, Texas, Dallas.
-
-        [3] C.H.Taal, R.C.Hendriks, R.Heusdens, J.Jensen 'An Algorithm for Intelligibility Prediction of
-        Time-Frequency Weighted Noisy Speech', IEEE Transactions on Audio, Speech, and Language Processing, 2011.
-
-        [4] J. Jensen and C. H. Taal, 'An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated
-        Noise Maskers', IEEE Transactions on Audio, Speech and Language Processing, 2016.
-
     """
-    sum_stoi: Tensor
-    total: Tensor
-    full_state_update: bool = False
-    is_differentiable: bool = False
-    higher_is_better: bool = True
-
-    def __init__(
-        self,
-        fs: int,
-        extended: bool = False,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__(**kwargs)
-        if not _PYSTOI_AVAILABLE:
-            raise ModuleNotFoundError(
-                "STOI metric requires that `pystoi` is installed."
-                " Either install as `pip install torchmetrics[audio]` or `pip install pystoi`."
-            )
-        self.fs = fs
-        self.extended = extended
-
-        self.add_state("sum_stoi", default=tensor(0.0), dist_reduce_fx="sum")
-        self.add_state("total", default=tensor(0), dist_reduce_fx="sum")
-
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
-
-        Args:
-            preds: Predictions from model
-            target: Ground truth values
-        """
-        stoi_batch = short_time_objective_intelligibility(preds, target, self.fs, self.extended, False).to(
-            self.sum_stoi.device
+    if not _PYSTOI_AVAILABLE:
+        raise ModuleNotFoundError(
+            "ShortTimeObjectiveIntelligibility metric requires that `pystoi` is installed."
+            " Either install as `pip install torchmetrics[audio]` or `pip install pystoi`."
         )
+    _check_same_shape(preds, target)
+
+    if len(preds.shape) == 1:
+        stoi_val_np = stoi_backend(target.detach().cpu().numpy(), preds.detach().cpu().numpy(), fs, extended)
+        stoi_val = torch.tensor(stoi_val_np)
+    else:
+        preds_np = preds.reshape(-1, preds.shape[-1]).detach().cpu().numpy()
+        target_np = target.reshape(-1, preds.shape[-1]).detach().cpu().numpy()
+        stoi_val_np = np.empty(shape=(preds_np.shape[0]))
+        for b in range(preds_np.shape[0]):
+            stoi_val_np[b] = stoi_backend(target_np[b, :], preds_np[b, :], fs, extended)
+        stoi_val = torch.from_numpy(stoi_val_np)
+        stoi_val = stoi_val.reshape(preds.shape[:-1])
 
-        self.sum_stoi += stoi_batch.sum()
-        self.total += stoi_batch.numel()
+    if keep_same_device:
+        return stoi_val.to(preds.device)
 
-    def compute(self) -> Tensor:
-        """Computes average STOI."""
-        return self.sum_stoi / self.total
+    return stoi_val
```

### Comparing `torchmetrics-0.9.3/torchmetrics/classification/accuracy.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/classification/dice.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,270 +1,286 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, Optional
+from typing import Any, Callable, Optional, Sequence, Tuple, Union, no_type_check
 
-from torch import Tensor, tensor
+import torch
+from torch import Tensor
+from typing_extensions import Literal
 
-from torchmetrics.functional.classification.accuracy import (
-    _accuracy_compute,
-    _accuracy_update,
-    _check_subset_validity,
-    _mode,
-    _subset_accuracy_compute,
-    _subset_accuracy_update,
-)
-from torchmetrics.utilities.enums import AverageMethod, DataType
+from torchmetrics.functional.classification.dice import _dice_compute
+from torchmetrics.functional.classification.stat_scores import _stat_scores_update
+from torchmetrics.metric import Metric
+from torchmetrics.utilities.enums import AverageMethod, MDMCAverageMethod
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
-from torchmetrics.classification.stat_scores import StatScores  # isort:skip
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["Dice.plot"]
 
 
-class Accuracy(StatScores):
-    r"""
-    Computes Accuracy_:
+class Dice(Metric):
+    r"""Compute `Dice`_.
 
-    .. math::
-        \text{Accuracy} = \frac{1}{N}\sum_i^N 1(y_i = \hat{y}_i)
+    .. math:: \text{Dice} = \frac{\text{2 * TP}}{\text{2 * TP} + \text{FP} + \text{FN}}
 
-    Where :math:`y` is a tensor of target values, and :math:`\hat{y}` is a
-    tensor of predictions.
+    Where :math:`\text{TP}` and :math:`\text{FP}` represent the number of true positives and
+    false positives respecitively.
 
-    For multi-class and multi-dimensional multi-class data with probability or logits predictions, the
-    parameter ``top_k`` generalizes this metric to a Top-K accuracy metric: for each sample the
-    top-K highest probability or logit score items are considered to find the correct label.
+    It is recommend set `ignore_index` to index of background class.
 
-    For multi-label and multi-dimensional multi-class inputs, this metric computes the "global"
-    accuracy by default, which counts all labels or sub-samples separately. This can be
-    changed to subset accuracy (which requires all labels or sub-samples in the sample to
-    be correctly predicted) by setting ``subset_accuracy=True``.
+    The reduction method (how the precision scores are aggregated) is controlled by the
+    ``average`` parameter, and additionally by the ``mdmc_average`` parameter in the
+    multi-dimensional multi-class case.
 
-    Accepts all input types listed in :ref:`pages/classification:input types`.
+    As input to ``forward`` and ``update`` the metric accepts the following input:
+
+    - ``preds`` (:class:`~torch.Tensor`): Predictions from model (probabilities, logits or labels)
+    - ``target`` (:class:`~torch.Tensor`): Ground truth values
+
+    As output to ``forward`` and ``compute`` the metric returns the following output:
+
+    - ``dice`` (:class:`~torch.Tensor`): A tensor containing the dice score.
+
+        - If ``average in ['micro', 'macro', 'weighted', 'samples']``, a one-element tensor will be returned
+        - If ``average in ['none', None]``, the shape will be ``(C,)``, where ``C`` stands  for the number of classes
 
     Args:
         num_classes:
-            Number of classes. Necessary for ``'macro'``, ``'weighted'`` and ``None`` average methods.
+            Number of classes. Necessary for ``'macro'``, and ``None`` average methods.
         threshold:
             Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case
             of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.
+        zero_division:
+            The value to use for the score if denominator equals zero.
         average:
             Defines the reduction that is applied. Should be one of the following:
 
             - ``'micro'`` [default]: Calculate the metric globally, across all samples and classes.
             - ``'macro'``: Calculate the metric for each class separately, and average the
               metrics across classes (with equal weights for each class).
             - ``'weighted'``: Calculate the metric for each class separately, and average the
               metrics across classes, weighting each class by its support (``tp + fn``).
             - ``'none'`` or ``None``: Calculate the metric for each class separately, and return
               the metric for every class.
             - ``'samples'``: Calculate the metric for each sample, and average the metrics
               across samples (with equal weights for each sample).
 
-            .. note:: What is considered a sample in the multi-dimensional multi-class case
-                depends on the value of ``mdmc_average``.
-
-            .. note:: If ``'none'`` and a given class doesn't occur in the ``preds`` or ``target``,
-                the value for the class will be ``nan``.
+            .. note::
+               What is considered a sample in the multi-dimensional multi-class case
+               depends on the value of ``mdmc_average``.
 
         mdmc_average:
             Defines how averaging is done for multi-dimensional multi-class inputs (on top of the
             ``average`` parameter). Should be one of the following:
 
             - ``None`` [default]: Should be left unchanged if your data is not multi-dimensional
               multi-class.
 
             - ``'samplewise'``: In this case, the statistics are computed separately for each
               sample on the ``N`` axis, and then averaged over samples.
               The computation for each sample is done by treating the flattened extra axes ``...``
-              (see :ref:`pages/classification:input types`) as the ``N`` dimension within the sample,
+              as the ``N`` dimension within the sample,
               and computing the metric for the sample based on that.
 
             - ``'global'``: In this case the ``N`` and ``...`` dimensions of the inputs
-              (see :ref:`pages/classification:input types`)
-              are flattened into a new ``N_X`` sample axis, i.e. the inputs are treated as if they
-              were ``(N_X, C)``. From here on the ``average`` parameter applies as usual.
+              are flattened into a new ``N_X`` sample axis, i.e.
+              the inputs are treated as if they were ``(N_X, C)``.
+              From here on the ``average`` parameter applies as usual.
 
         ignore_index:
             Integer specifying a target class to ignore. If given, this class index does not contribute
             to the returned score, regardless of reduction method. If an index is ignored, and ``average=None``
             or ``'none'``, the score for the ignored class will be returned as ``nan``.
 
         top_k:
             Number of the highest probability or logit score predictions considered finding the correct label,
             relevant only for (multi-dimensional) multi-class inputs. The
             default value (``None``) will be interpreted as 1 for these inputs.
-
             Should be left at default (``None``) for all other types of inputs.
 
         multiclass:
             Used only in certain special cases, where you want to treat inputs as a different type
-            than what they appear to be. See the parameter's
-            :ref:`documentation section <pages/classification:using the multiclass parameter>`
-            for a more detailed explanation and examples.
-
-        subset_accuracy:
-            Whether to compute subset accuracy for multi-label and multi-dimensional
-            multi-class inputs (has no effect for other input types).
-
-            - For multi-label inputs, if the parameter is set to ``True``, then all labels for
-              each sample must be correctly predicted for the sample to count as correct. If it
-              is set to ``False``, then all labels are counted separately - this is equivalent to
-              flattening inputs beforehand (i.e. ``preds = preds.flatten()`` and same for ``target``).
-
-            - For multi-dimensional multi-class inputs, if the parameter is set to ``True``, then all
-              sub-sample (on the extra axis) must be correct for the sample to be counted as correct.
-              If it is set to ``False``, then all sub-samples are counter separately - this is equivalent,
-              in the case of label predictions, to flattening the inputs beforehand (i.e.
-              ``preds = preds.flatten()`` and same for ``target``). Note that the ``top_k`` parameter
-              still applies in both cases, if set.
+            than what they appear to be.
 
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
     Raises:
         ValueError:
-            If ``top_k`` is not an ``integer`` larger than ``0``.
+            If ``average`` is none of ``"micro"``, ``"macro"``, ``"samples"``, ``"none"``, ``None``.
         ValueError:
-            If ``average`` is none of ``"micro"``, ``"macro"``, ``"weighted"``, ``"samples"``, ``"none"``, ``None``.
+            If ``mdmc_average`` is not one of ``None``, ``"samplewise"``, ``"global"``.
         ValueError:
-            If two different input modes are provided, eg. using ``multi-label`` with ``multi-class``.
+            If ``average`` is set but ``num_classes`` is not provided.
         ValueError:
-            If ``top_k`` parameter is set for ``multi-label`` inputs.
+            If ``num_classes`` is set and ``ignore_index`` is not in the range ``[0, num_classes)``.
 
     Example:
-        >>> import torch
-        >>> from torchmetrics import Accuracy
-        >>> target = torch.tensor([0, 1, 2, 3])
-        >>> preds = torch.tensor([0, 2, 1, 3])
-        >>> accuracy = Accuracy()
-        >>> accuracy(preds, target)
-        tensor(0.5000)
-
-        >>> target = torch.tensor([0, 1, 2])
-        >>> preds = torch.tensor([[0.1, 0.9, 0], [0.3, 0.1, 0.6], [0.2, 0.5, 0.3]])
-        >>> accuracy = Accuracy(top_k=2)
-        >>> accuracy(preds, target)
-        tensor(0.6667)
-
+        >>> from torch import tensor
+        >>> from torchmetrics.classification import Dice
+        >>> preds  = tensor([2, 0, 2, 1])
+        >>> target = tensor([1, 1, 2, 0])
+        >>> dice = Dice(average='micro')
+        >>> dice(preds, target)
+        tensor(0.2500)
     """
-    is_differentiable = False
-    higher_is_better = True
+    is_differentiable: bool = False
+    higher_is_better: bool = True
     full_state_update: bool = False
-    correct: Tensor
-    total: Tensor
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
+    plot_legend_name: str = "Class"
 
+    @no_type_check
     def __init__(
         self,
-        threshold: float = 0.5,
+        zero_division: int = 0,
         num_classes: Optional[int] = None,
-        average: Optional[str] = "micro",
-        mdmc_average: Optional[str] = None,
+        threshold: float = 0.5,
+        average: Optional[Literal["micro", "macro", "none"]] = "micro",
+        mdmc_average: Optional[str] = "global",
         ignore_index: Optional[int] = None,
         top_k: Optional[int] = None,
         multiclass: Optional[bool] = None,
-        subset_accuracy: bool = False,
         **kwargs: Any,
     ) -> None:
-        allowed_average = ["micro", "macro", "weighted", "samples", "none", None]
+        super().__init__(**kwargs)
+        allowed_average = ("micro", "macro", "samples", "none", None)
         if average not in allowed_average:
             raise ValueError(f"The `average` has to be one of {allowed_average}, got {average}.")
 
         _reduce_options = (AverageMethod.WEIGHTED, AverageMethod.NONE, None)
         if "reduce" not in kwargs:
             kwargs["reduce"] = AverageMethod.MACRO if average in _reduce_options else average
         if "mdmc_reduce" not in kwargs:
             kwargs["mdmc_reduce"] = mdmc_average
 
-        super().__init__(
-            threshold=threshold,
-            top_k=top_k,
-            num_classes=num_classes,
-            multiclass=multiclass,
-            ignore_index=ignore_index,
-            **kwargs,
-        )
-
-        if top_k is not None and (not isinstance(top_k, int) or top_k <= 0):
-            raise ValueError(f"The `top_k` should be an integer larger than 0, got {top_k}")
-
-        self.average = average
+        self.reduce = average
+        self.mdmc_reduce = mdmc_average
+        self.num_classes = num_classes
         self.threshold = threshold
-        self.top_k = top_k
-        self.subset_accuracy = subset_accuracy
-        self.mode: DataType = None  # type: ignore
         self.multiclass = multiclass
         self.ignore_index = ignore_index
+        self.top_k = top_k
 
-        if self.subset_accuracy:
-            self.add_state("correct", default=tensor(0), dist_reduce_fx="sum")
-            self.add_state("total", default=tensor(0), dist_reduce_fx="sum")
-
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets. See
-        :ref:`pages/classification:input types` for more information on input
-        types.
+        if average not in ["micro", "macro", "samples"]:
+            raise ValueError(f"The `reduce` {average} is not valid.")
 
-        Args:
-            preds: Predictions from model (logits, probabilities, or labels)
-            target: Ground truth labels
-        """
-        """ returns the mode of the data (binary, multi label, multi class, multi-dim multi class) """
-        mode = _mode(preds, target, self.threshold, self.top_k, self.num_classes, self.multiclass, self.ignore_index)
+        if mdmc_average not in [None, "samplewise", "global"]:
+            raise ValueError(f"The `mdmc_reduce` {mdmc_average} is not valid.")
 
-        if not self.mode:
-            self.mode = mode
-        elif self.mode != mode:
-            raise ValueError(f"You can not use {mode} inputs with {self.mode} inputs.")
-
-        if self.subset_accuracy and not _check_subset_validity(self.mode):
-            self.subset_accuracy = False
-
-        if self.subset_accuracy:
-            correct, total = _subset_accuracy_update(
-                preds, target, threshold=self.threshold, top_k=self.top_k, ignore_index=self.ignore_index
-            )
-            self.correct += correct
-            self.total += total
-        else:
-            if not self.mode:
-                raise RuntimeError("You have to have determined mode.")
-            tp, fp, tn, fn = _accuracy_update(
-                preds,
-                target,
-                reduce=self.reduce,
-                mdmc_reduce=self.mdmc_reduce,
-                threshold=self.threshold,
-                num_classes=self.num_classes,
-                top_k=self.top_k,
-                multiclass=self.multiclass,
-                ignore_index=self.ignore_index,
-                mode=self.mode,
-            )
-
-            # Update states
-            if self.reduce != "samples" and self.mdmc_reduce != "samplewise":
-                self.tp += tp
-                self.fp += fp
-                self.tn += tn
-                self.fn += fn
+        if average == "macro" and (not num_classes or num_classes < 1):
+            raise ValueError("When you set `average` as 'macro', you have to provide the number of classes.")
+
+        if num_classes and ignore_index is not None and (not ignore_index < num_classes or num_classes == 1):
+            raise ValueError(f"The `ignore_index` {ignore_index} is not valid for inputs with {num_classes} classes")
+
+        default: Callable = lambda: []
+        reduce_fn: Optional[str] = "cat"
+        if mdmc_average != "samplewise" and average != "samples":
+            if average == "micro":
+                zeros_shape = []
+            elif average == "macro":
+                zeros_shape = [num_classes]
             else:
-                self.tp.append(tp)
-                self.fp.append(fp)
-                self.tn.append(tn)
-                self.fn.append(fn)
+                raise ValueError(f'Wrong reduce="{average}"')
+            default = lambda: torch.zeros(zeros_shape, dtype=torch.long)
+            reduce_fn = "sum"
+
+        for s in ("tp", "fp", "tn", "fn"):
+            self.add_state(s, default=default(), dist_reduce_fx=reduce_fn)
+
+        self.average = average
+        self.zero_division = zero_division
 
+    @no_type_check
+    def update(self, preds: Tensor, target: Tensor) -> None:
+        """Update state with predictions and targets."""
+        tp, fp, tn, fn = _stat_scores_update(
+            preds,
+            target,
+            reduce=self.reduce,
+            mdmc_reduce=self.mdmc_reduce,
+            threshold=self.threshold,
+            num_classes=self.num_classes,
+            top_k=self.top_k,
+            multiclass=self.multiclass,
+            ignore_index=self.ignore_index,
+        )
+
+        # Update states
+        if self.reduce != AverageMethod.SAMPLES and self.mdmc_reduce != MDMCAverageMethod.SAMPLEWISE:
+            self.tp += tp
+            self.fp += fp
+            self.tn += tn
+            self.fn += fn
+        else:
+            self.tp.append(tp)
+            self.fp.append(fp)
+            self.tn.append(tn)
+            self.fn.append(fn)
+
+    @no_type_check
+    def _get_final_stats(self) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
+        """Perform concatenation on the stat scores if neccesary, before passing them to a compute function."""
+        tp = torch.cat(self.tp) if isinstance(self.tp, list) else self.tp
+        fp = torch.cat(self.fp) if isinstance(self.fp, list) else self.fp
+        tn = torch.cat(self.tn) if isinstance(self.tn, list) else self.tn
+        fn = torch.cat(self.fn) if isinstance(self.fn, list) else self.fn
+        return tp, fp, tn, fn
+
+    @no_type_check
     def compute(self) -> Tensor:
-        """Computes accuracy based on inputs passed in to ``update`` previously."""
-        if not self.mode:
-            raise RuntimeError("You have to have determined mode.")
-        if self.subset_accuracy:
-            return _subset_accuracy_compute(self.correct, self.total)
-        tp, fp, tn, fn = self._get_final_stats()
-        return _accuracy_compute(tp, fp, tn, fn, self.average, self.mdmc_reduce, self.mode)
+        """Compute metric."""
+        tp, fp, _, fn = self._get_final_stats()
+        return _dice_compute(tp, fp, fn, self.average, self.mdmc_reduce, self.zero_division)
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure object and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> from torch import randint
+            >>> from torchmetrics.classification import Dice
+            >>> metric = Dice()
+            >>> metric.update(randint(2,(10,)), randint(2,(10,)))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> from torch import randint
+            >>> from torchmetrics.classification import Dice
+            >>> metric = Dice()
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(randint(2,(10,)), randint(2,(10,))))
+            >>> fig_, ax_ = metric.plot(values)
+        """
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/classification/auc.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/average_precision.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,77 +1,59 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, List, Optional
+from typing import Optional
 
-from torch import Tensor
+import torch
+from torch import Tensor, tensor
 
-from torchmetrics.functional.classification.auc import _auc_compute, _auc_update
-from torchmetrics.metric import Metric
-from torchmetrics.utilities import rank_zero_warn
-from torchmetrics.utilities.data import dim_zero_cat
+from torchmetrics.utilities.checks import _check_retrieval_functional_inputs
 
 
-class AUC(Metric):
-    r"""
-    Computes Area Under the Curve (AUC) using the trapezoidal rule
+def retrieval_average_precision(preds: Tensor, target: Tensor, top_k: Optional[int] = None) -> Tensor:
+    """Compute average precision (for information retrieval), as explained in `IR Average precision`_.
 
-    Forward accepts two input tensors that should be 1D and have the same number
-    of elements
+    ``preds`` and ``target`` should be of the same shape and live on the same device. If no ``target`` is ``True``,
+    ``0`` is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
+    otherwise an error is raised.
 
     Args:
-        reorder: AUC expects its first input to be sorted. If this is not the case,
-            setting this argument to ``True`` will use a stable sorting algorithm to
-            sort the input in descending order
-
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+        preds: estimated probabilities of each document to be relevant.
+        target: ground truth about each document being relevant or not.
+        top_k: consider only the top k elements (default: ``None``, which considers them all)
+
+    Return:
+        a single-value tensor with the average precision (AP) of the predictions ``preds`` w.r.t. the labels ``target``.
+
+    Raises:
+        ValueError:
+            If ``top_k`` is not ``None`` or an integer larger than 0.
+
+    Example:
+        >>> from torchmetrics.functional.retrieval import retrieval_average_precision
+        >>> preds = tensor([0.2, 0.3, 0.5])
+        >>> target = tensor([True, False, True])
+        >>> retrieval_average_precision(preds, target)
+        tensor(0.8333)
     """
-    is_differentiable: bool = False
-    higher_is_better: Optional[bool] = None
-    full_state_update: bool = False
-    x: List[Tensor]
-    y: List[Tensor]
-
-    def __init__(
-        self,
-        reorder: bool = False,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__(**kwargs)
-
-        self.reorder = reorder
-
-        self.add_state("x", default=[], dist_reduce_fx="cat")
-        self.add_state("y", default=[], dist_reduce_fx="cat")
-
-        rank_zero_warn(
-            "Metric `AUC` will save all targets and predictions in buffer."
-            " For large datasets this may lead to large memory footprint."
-        )
-
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
-
-        Args:
-            preds: Predictions from model (probabilities, or labels)
-            target: Ground truth labels
-        """
-        x, y = _auc_update(preds, target)
-
-        self.x.append(x)
-        self.y.append(y)
-
-    def compute(self) -> Tensor:
-        """Computes AUC based on inputs passed in to ``update`` previously."""
-        x = dim_zero_cat(self.x)
-        y = dim_zero_cat(self.y)
-        return _auc_compute(x, y, reorder=self.reorder)
+    preds, target = _check_retrieval_functional_inputs(preds, target)
+
+    top_k = top_k or preds.shape[-1]
+    if not isinstance(top_k, int) and top_k <= 0:
+        raise ValueError(f"Argument ``top_k`` has to be a positive integer or None, but got {top_k}.")
+
+    target = target[preds.topk(min(top_k, preds.shape[-1]), sorted=True, dim=-1)[1]]
+    if not target.sum():
+        return tensor(0.0, device=preds.device)
+
+    positions = torch.arange(1, len(target) + 1, device=target.device, dtype=torch.float32)[target > 0]
+    return torch.div((torch.arange(len(positions), device=positions.device, dtype=torch.float32) + 1), positions).mean()
```

### Comparing `torchmetrics-0.9.3/torchmetrics/classification/auroc.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/regression/kendall.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,178 +1,205 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, List, Optional
 
-import torch
+from typing import Any, List, Optional, Sequence, Tuple, Union
+
 from torch import Tensor
+from typing_extensions import Literal
 
-from torchmetrics.functional.classification.auroc import _auroc_compute, _auroc_update
+from torchmetrics.functional.regression.kendall import (
+    _kendall_corrcoef_compute,
+    _kendall_corrcoef_update,
+    _MetricVariant,
+    _TestAlternative,
+)
 from torchmetrics.metric import Metric
-from torchmetrics.utilities import rank_zero_warn
 from torchmetrics.utilities.data import dim_zero_cat
-from torchmetrics.utilities.enums import DataType
-from torchmetrics.utilities.imports import _TORCH_LOWER_1_6
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["KendallRankCorrCoef.plot"]
 
-class AUROC(Metric):
-    r"""Compute Area Under the Receiver Operating Characteristic Curve (`ROC AUC`_).
-    Works for both binary, multilabel and multiclass problems. In the case of
-    multiclass, the values will be calculated based on a one-vs-the-rest approach.
 
-    Forward accepts
+class KendallRankCorrCoef(Metric):
+    r"""Compute `Kendall Rank Correlation Coefficient`_.
 
-    - ``preds`` (float tensor): ``(N, ...)`` (binary) or ``(N, C, ...)`` (multiclass) tensor
-      with probabilities, where C is the number of classes.
+    .. math::
+        tau_a = \frac{C - D}{C + D}
 
-    - ``target`` (long tensor): ``(N, ...)`` or ``(N, C, ...)`` with integer labels
+    where :math:`C` represents concordant pairs, :math:`D` stands for discordant pairs.
 
-    For non-binary input, if the ``preds`` and ``target`` tensor have the same
-    size the input will be interpretated as multilabel and if ``preds`` have one
-    dimension more than the ``target`` tensor the input will be interpretated as
-    multiclass.
+    .. math::
+        tau_b = \frac{C - D}{\sqrt{(C + D + T_{preds}) * (C + D + T_{target})}}
 
-    .. note::
-        If either the positive class or negative class is completly missing in the target tensor,
-        the auroc score is meaningless in this case and a score of 0 will be returned together
-        with an warning.
+    where :math:`C` represents concordant pairs, :math:`D` stands for discordant pairs and :math:`T` represents
+    a total number of ties.
 
-    Args:
-        num_classes: integer with number of classes for multi-label and multiclass problems.
+    .. math::
+        tau_c = 2 * \frac{C - D}{n^2 * \frac{m - 1}{m}}
+
+    where :math:`C` represents concordant pairs, :math:`D` stands for discordant pairs, :math:`n` is a total number
+    of observations and :math:`m` is a ``min`` of unique values in ``preds`` and ``target`` sequence.
+
+    Definitions according to Definition according to `The Treatment of Ties in Ranking Problems`_.
+
+    As input to ``forward`` and ``update`` the metric accepts the following input:
+
+    - ``preds`` (:class:`~torch.Tensor`): Sequence of data in float tensor of either shape ``(N,)`` or ``(N,d)``
+    - ``target`` (:class:`~torch.Tensor`): Sequence of data in float tensor of either shape ``(N,)`` or ``(N,d)``
 
-            Should be set to ``None`` for binary problems
-        pos_label: integer determining the positive class. Default is ``None``
-            which for binary problem is translated to 1. For multiclass problems
-            this argument should not be set as we iteratively change it in the
-            range ``[0, num_classes-1]``
-        average:
-            - ``'micro'`` computes metric globally. Only works for multilabel problems
-            - ``'macro'`` computes metric for each class and uniformly averages them
-            - ``'weighted'`` computes metric for each class and does a weighted-average,
-              where each class is weighted by their support (accounts for class imbalance)
-            - ``None`` computes and returns the metric per class
-        max_fpr:
-            If not ``None``, calculates standardized partial AUC over the
-            range ``[0, max_fpr]``. Should be a float between 0 and 1.
+    As output of ``forward`` and ``compute`` the metric returns the following output:
 
+    - ``kendall`` (:class:`~torch.Tensor`): A tensor with the correlation tau statistic,
+      and if it is not None, the p-value of corresponding statistical test.
+
+    Args:
+        variant: Indication of which variant of Kendall's tau to be used
+        t_test: Indication whether to run t-test
+        alternative: Alternative hypothesis for t-test. Possible values:
+            - 'two-sided': the rank correlation is nonzero
+            - 'less': the rank correlation is negative (less than zero)
+            - 'greater':  the rank correlation is positive (greater than zero)
+        num_outputs: Number of outputs in multioutput setting
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
     Raises:
-        ValueError:
-            If ``average`` is none of ``None``, ``"macro"`` or ``"weighted"``.
-        ValueError:
-            If ``max_fpr`` is not a ``float`` in the range ``(0, 1]``.
-        RuntimeError:
-            If ``PyTorch version`` is ``below 1.6`` since ``max_fpr`` requires ``torch.bucketize``
-            which is not available below 1.6.
-        ValueError:
-            If the mode of data (binary, multi-label, multi-class) changes between batches.
-
-    Example (binary case):
-        >>> from torchmetrics import AUROC
-        >>> preds = torch.tensor([0.13, 0.26, 0.08, 0.19, 0.34])
-        >>> target = torch.tensor([0, 0, 1, 1, 1])
-        >>> auroc = AUROC(pos_label=1)
-        >>> auroc(preds, target)
-        tensor(0.5000)
-
-    Example (multiclass case):
-        >>> preds = torch.tensor([[0.90, 0.05, 0.05],
-        ...                       [0.05, 0.90, 0.05],
-        ...                       [0.05, 0.05, 0.90],
-        ...                       [0.85, 0.05, 0.10],
-        ...                       [0.10, 0.10, 0.80]])
-        >>> target = torch.tensor([0, 1, 1, 2, 2])
-        >>> auroc = AUROC(num_classes=3)
-        >>> auroc(preds, target)
-        tensor(0.7778)
+        ValueError: If ``t_test`` is not of a type bool
+        ValueError: If ``t_test=True`` and ``alternative=None``
 
+    Example (single output regression):
+        >>> from torch import tensor
+        >>> from torchmetrics.regression import KendallRankCorrCoef
+        >>> preds = tensor([2.5, 0.0, 2, 8])
+        >>> target = tensor([3, -0.5, 2, 1])
+        >>> kendall = KendallRankCorrCoef()
+        >>> kendall(preds, target)
+        tensor(0.3333)
+
+    Example (multi output regression):
+        >>> from torchmetrics.regression import KendallRankCorrCoef
+        >>> preds = tensor([[2.5, 0.0], [2, 8]])
+        >>> target = tensor([[3, -0.5], [2, 1]])
+        >>> kendall = KendallRankCorrCoef(num_outputs=2)
+        >>> kendall(preds, target)
+        tensor([1., 1.])
+
+    Example (single output regression with t-test):
+        >>> from torchmetrics.regression import KendallRankCorrCoef
+        >>> preds = tensor([2.5, 0.0, 2, 8])
+        >>> target = tensor([3, -0.5, 2, 1])
+        >>> kendall = KendallRankCorrCoef(t_test=True, alternative='two-sided')
+        >>> kendall(preds, target)
+        (tensor(0.3333), tensor(0.4969))
+
+    Example (multi output regression with t-test):
+        >>> from torchmetrics.regression import KendallRankCorrCoef
+        >>> preds = tensor([[2.5, 0.0], [2, 8]])
+        >>> target = tensor([[3, -0.5], [2, 1]])
+        >>> kendall = KendallRankCorrCoef(t_test=True, alternative='two-sided', num_outputs=2)
+        >>> kendall(preds, target)
+        (tensor([1., 1.]), tensor([nan, nan]))
     """
-    is_differentiable: bool = False
-    higher_is_better: bool = True
-    full_state_update: bool = False
+    is_differentiable = False
+    higher_is_better = None
+    full_state_update = True
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
+
     preds: List[Tensor]
     target: List[Tensor]
 
     def __init__(
         self,
-        num_classes: Optional[int] = None,
-        pos_label: Optional[int] = None,
-        average: Optional[str] = "macro",
-        max_fpr: Optional[float] = None,
+        variant: Literal["a", "b", "c"] = "b",
+        t_test: bool = False,
+        alternative: Optional[Literal["two-sided", "less", "greater"]] = "two-sided",
+        num_outputs: int = 1,
         **kwargs: Any,
     ) -> None:
         super().__init__(**kwargs)
+        if not isinstance(t_test, bool):
+            raise ValueError(f"Argument `t_test` is expected to be of a type `bool`, but got {type(t_test)}.")
+        if t_test and alternative is None:
+            raise ValueError("Argument `alternative` is required if `t_test=True` but got `None`.")
+
+        self.variant = _MetricVariant.from_str(str(variant))
+        self.alternative = _TestAlternative.from_str(str(alternative)) if t_test else None
+        self.num_outputs = num_outputs
+
+        self.add_state("preds", [], dist_reduce_fx="cat")
+        self.add_state("target", [], dist_reduce_fx="cat")
+
+    def update(self, preds: Tensor, target: Tensor) -> None:
+        """Update variables required to compute Kendall rank correlation coefficient."""
+        self.preds, self.target = _kendall_corrcoef_update(
+            preds,
+            target,
+            self.preds,
+            self.target,
+            num_outputs=self.num_outputs,
+        )
 
-        self.num_classes = num_classes
-        self.pos_label = pos_label
-        self.average = average
-        self.max_fpr = max_fpr
-
-        allowed_average = (None, "macro", "weighted", "micro")
-        if self.average not in allowed_average:
-            raise ValueError(
-                f"Argument `average` expected to be one of the following: {allowed_average} but got {average}"
-            )
-
-        if self.max_fpr is not None:
-            if not isinstance(max_fpr, float) or not 0 < max_fpr <= 1:
-                raise ValueError(f"`max_fpr` should be a float in range (0, 1], got: {max_fpr}")
-
-            if _TORCH_LOWER_1_6:
-                raise RuntimeError(
-                    "`max_fpr` argument requires `torch.bucketize` which is not available below PyTorch version 1.6"
-                )
-
-        self.mode: DataType = None  # type: ignore
-        self.add_state("preds", default=[], dist_reduce_fx="cat")
-        self.add_state("target", default=[], dist_reduce_fx="cat")
-
-        rank_zero_warn(
-            "Metric `AUROC` will save all targets and predictions in buffer."
-            " For large datasets this may lead to large memory footprint."
+    def compute(self) -> Union[Tensor, Tuple[Tensor, Tensor]]:
+        """Compute Kendall rank correlation coefficient, and optionally p-value of corresponding statistical test."""
+        preds = dim_zero_cat(self.preds)
+        target = dim_zero_cat(self.target)
+        tau, p_value = _kendall_corrcoef_compute(
+            preds, target, self.variant, self.alternative  # type: ignore[arg-type]  # todo
         )
 
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
+        if p_value is not None:
+            return tau, p_value
+        return tau
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
 
         Args:
-            preds: Predictions from model (probabilities, or labels)
-            target: Ground truth labels
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import randn
+            >>> # Example plotting a single value
+            >>> from torchmetrics.regression import KendallRankCorrCoef
+            >>> metric = KendallRankCorrCoef()
+            >>> metric.update(randn(10,), randn(10,))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import randn
+            >>> # Example plotting multiple values
+            >>> from torchmetrics.regression import KendallRankCorrCoef
+            >>> metric = KendallRankCorrCoef()
+            >>> values = []
+            >>> for _ in range(10):
+            ...     values.append(metric(randn(10,), randn(10,)))
+            >>> fig, ax = metric.plot(values)
         """
-        preds, target, mode = _auroc_update(preds, target)
-
-        self.preds.append(preds)
-        self.target.append(target)
-
-        if self.mode and self.mode != mode:
-            raise ValueError(
-                "The mode of data (binary, multi-label, multi-class) should be constant, but changed"
-                f" between batches from {self.mode} to {mode}"
-            )
-        self.mode = mode
-
-    def compute(self) -> Tensor:
-        """Computes AUROC based on inputs passed in to ``update`` previously."""
-        if not self.mode:
-            raise RuntimeError("You have to have determined mode.")
-        preds = dim_zero_cat(self.preds)
-        target = dim_zero_cat(self.target)
-        return _auroc_compute(
-            preds,
-            target,
-            self.mode,
-            self.num_classes,
-            self.pos_label,
-            self.average,
-            self.max_fpr,
-        )
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/classification/avg_precision.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/regression/spearman.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,136 +1,146 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, List, Optional, Union
+from typing import Any, List, Optional, Sequence, Union
 
-import torch
 from torch import Tensor
 
-from torchmetrics.functional.classification.average_precision import (
-    _average_precision_compute,
-    _average_precision_update,
-)
+from torchmetrics.functional.regression.spearman import _spearman_corrcoef_compute, _spearman_corrcoef_update
 from torchmetrics.metric import Metric
 from torchmetrics.utilities import rank_zero_warn
 from torchmetrics.utilities.data import dim_zero_cat
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["SpearmanCorrCoef.plot"]
 
-class AveragePrecision(Metric):
-    """Computes the average precision score, which summarises the precision recall curve into one number. Works for
-    both binary and multiclass problems. In the case of multiclass, the values will be calculated based on a one-
-    vs-the-rest approach.
 
-    Forward accepts
+class SpearmanCorrCoef(Metric):
+    r"""Compute `spearmans rank correlation coefficient`_.
 
-    - ``preds`` (float tensor): ``(N, ...)`` (binary) or ``(N, C, ...)`` (multiclass) tensor
-      with probabilities, where C is the number of classes.
+    .. math:
+        r_s = = \frac{cov(rg_x, rg_y)}{\sigma_{rg_x} * \sigma_{rg_y}}
 
-    - ``target`` (long tensor): ``(N, ...)`` with integer labels
+    where :math:`rg_x` and :math:`rg_y` are the rank associated to the variables :math:`x` and :math:`y`.
+    Spearmans correlations coefficient corresponds to the standard pearsons correlation coefficient calculated
+    on the rank variables.
 
-    Args:
-        num_classes: integer with number of classes. Not nessesary to provide
-            for binary problems.
-        pos_label: integer determining the positive class. Default is ``None``
-            which for binary problem is translated to 1. For multiclass problems
-            this argument should not be set as we iteratively change it in the
-            range ``[0, num_classes-1]``
-        average:
-            defines the reduction that is applied in the case of multiclass and multilabel input.
-            Should be one of the following:
-
-            - ``'macro'`` [default]: Calculate the metric for each class separately, and average the
-              metrics across classes (with equal weights for each class).
-            - ``'micro'``: Calculate the metric globally, across all samples and classes. Cannot be
-              used with multiclass input.
-            - ``'weighted'``: Calculate the metric for each class separately, and average the
-              metrics across classes, weighting each class by its support.
-            - ``'none'`` or ``None``: Calculate the metric for each class separately, and return
-              the metric for every class.
+    As input to ``forward`` and ``update`` the metric accepts the following input:
+
+    - ``preds`` (:class:`~torch.Tensor`): Predictions from model in float tensor with shape ``(N,d)``
+    - ``target`` (:class:`~torch.Tensor`): Ground truth values in float tensor with shape ``(N,d)``
+
+    As output of ``forward`` and ``compute`` the metric returns the following output:
+
+    - ``spearman`` (:class:`~torch.Tensor`): A tensor with the spearman correlation(s)
 
+    Args:
+        num_outputs: Number of outputs in multioutput setting
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
-    Example (binary case):
-        >>> from torchmetrics import AveragePrecision
-        >>> pred = torch.tensor([0, 0.1, 0.8, 0.4])
-        >>> target = torch.tensor([0, 1, 1, 1])
-        >>> average_precision = AveragePrecision(pos_label=1)
-        >>> average_precision(pred, target)
-        tensor(1.)
-
-    Example (multiclass case):
-        >>> pred = torch.tensor([[0.75, 0.05, 0.05, 0.05, 0.05],
-        ...                      [0.05, 0.75, 0.05, 0.05, 0.05],
-        ...                      [0.05, 0.05, 0.75, 0.05, 0.05],
-        ...                      [0.05, 0.05, 0.05, 0.75, 0.05]])
-        >>> target = torch.tensor([0, 1, 3, 2])
-        >>> average_precision = AveragePrecision(num_classes=5, average=None)
-        >>> average_precision(pred, target)
-        [tensor(1.), tensor(1.), tensor(0.2500), tensor(0.2500), tensor(nan)]
+    Example (single output regression):
+        >>> from torch import tensor
+        >>> from torchmetrics.regression import SpearmanCorrCoef
+        >>> target = tensor([3, -0.5, 2, 7])
+        >>> preds = tensor([2.5, 0.0, 2, 8])
+        >>> spearman = SpearmanCorrCoef()
+        >>> spearman(preds, target)
+        tensor(1.0000)
+
+    Example (multi output regression):
+        >>> from torchmetrics.regression import SpearmanCorrCoef
+        >>> target = tensor([[3, -0.5], [2, 7]])
+        >>> preds = tensor([[2.5, 0.0], [2, 8]])
+        >>> spearman = SpearmanCorrCoef(num_outputs=2)
+        >>> spearman(preds, target)
+        tensor([1.0000, 1.0000])
     """
-
     is_differentiable: bool = False
-    higher_is_better: Optional[bool] = None
+    higher_is_better: bool = True
     full_state_update: bool = False
+    plot_lower_bound: float = -1.0
+    plot_upper_bound: float = 1.0
+
     preds: List[Tensor]
     target: List[Tensor]
 
     def __init__(
         self,
-        num_classes: Optional[int] = None,
-        pos_label: Optional[int] = None,
-        average: Optional[str] = "macro",
+        num_outputs: int = 1,
         **kwargs: Any,
     ) -> None:
         super().__init__(**kwargs)
-
-        self.num_classes = num_classes
-        self.pos_label = pos_label
-        allowed_average = ("micro", "macro", "weighted", "none", None)
-        if average not in allowed_average:
-            raise ValueError(f"Expected argument `average` to be one of {allowed_average}" f" but got {average}")
-        self.average = average
-
-        self.add_state("preds", default=[], dist_reduce_fx="cat")
-        self.add_state("target", default=[], dist_reduce_fx="cat")
-
         rank_zero_warn(
-            "Metric `AveragePrecision` will save all targets and predictions in buffer."
-            " For large datasets this may lead to large memory footprint."
+            "Metric `SpearmanCorrcoef` will save all targets and predictions in the buffer."
+            " For large datasets, this may lead to large memory footprint."
         )
+        if not isinstance(num_outputs, int) and num_outputs < 1:
+            raise ValueError("Expected argument `num_outputs` to be an int larger than 0, but got {num_outputs}")
+        self.num_outputs = num_outputs
 
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
+        self.add_state("preds", default=[], dist_reduce_fx="cat")
+        self.add_state("target", default=[], dist_reduce_fx="cat")
 
-        Args:
-            preds: Predictions from model
-            target: Ground truth values
-        """
-        preds, target, num_classes, pos_label = _average_precision_update(
-            preds, target, self.num_classes, self.pos_label, self.average
-        )
+    def update(self, preds: Tensor, target: Tensor) -> None:
+        """Update state with predictions and targets."""
+        preds, target = _spearman_corrcoef_update(preds, target, num_outputs=self.num_outputs)
         self.preds.append(preds)
         self.target.append(target)
-        self.num_classes = num_classes
-        self.pos_label = pos_label
 
-    def compute(self) -> Union[Tensor, List[Tensor]]:
-        """Compute the average precision score.
+    def compute(self) -> Tensor:
+        """Compute Spearman's correlation coefficient."""
+        preds = dim_zero_cat(self.preds)
+        target = dim_zero_cat(self.target)
+        return _spearman_corrcoef_compute(preds, target)
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
 
         Returns:
-            tensor with average precision. If multiclass return list of such tensors, one for each class
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import randn
+            >>> # Example plotting a single value
+            >>> from torchmetrics.regression import SpearmanCorrCoef
+            >>> metric = SpearmanCorrCoef()
+            >>> metric.update(randn(10,), randn(10,))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import randn
+            >>> # Example plotting multiple values
+            >>> from torchmetrics.regression import SpearmanCorrCoef
+            >>> metric = SpearmanCorrCoef()
+            >>> values = []
+            >>> for _ in range(10):
+            ...     values.append(metric(randn(10,), randn(10,)))
+            >>> fig, ax = metric.plot(values)
         """
-        preds = dim_zero_cat(self.preds)
-        target = dim_zero_cat(self.target)
-        if not self.num_classes:
-            raise ValueError(f"`num_classes` bas to be positive number, but got {self.num_classes}")
-        return _average_precision_compute(preds, target, self.num_classes, self.pos_label, self.average)
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/classification/confusion_matrix.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/text/wer.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,134 +1,135 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, Optional
+from typing import Any, List, Optional, Sequence, Union
 
 import torch
-from torch import Tensor
+from torch import Tensor, tensor
 
-from torchmetrics.functional.classification.confusion_matrix import _confusion_matrix_compute, _confusion_matrix_update
+from torchmetrics.functional.text.wer import _wer_compute, _wer_update
 from torchmetrics.metric import Metric
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["WordErrorRate.plot"]
 
-class ConfusionMatrix(Metric):
-    r"""Computes the `confusion matrix`_.
 
-    Works with binary, multiclass, and multilabel data. Accepts probabilities or logits from a model output
-    or integer class values in prediction. Works with multi-dimensional preds and target, but it should be noted that
-    additional dimensions will be flattened.
+class WordErrorRate(Metric):
+    r"""Word error rate (`WordErrorRate`_) is a common metric of the performance of an automatic speech recognition.
 
-    Forward accepts
+    This value indicates the percentage of words that were incorrectly predicted. The lower the value, the
+    better the performance of the ASR system with a WER of 0 being a perfect score. Word error rate can then be
+    computed as:
 
-    - ``preds`` (float or long tensor): ``(N, ...)`` or ``(N, C, ...)`` where C is the number of classes
-    - ``target`` (long tensor): ``(N, ...)``
+    .. math::
+        WER = \frac{S + D + I}{N} = \frac{S + D + I}{S + D + C}
 
-    If preds and target are the same shape and preds is a float tensor, we use the ``self.threshold`` argument
-    to convert into integer labels. This is the case for binary and multi-label probabilities or logits.
+    where:
+    - :math:`S` is the number of substitutions,
+    - :math:`D` is the number of deletions,
+    - :math:`I` is the number of insertions,
+    - :math:`C` is the number of correct words,
+    - :math:`N` is the number of words in the reference (:math:`N=S+D+C`).
 
-    If preds has an extra dimension as in the case of multi-class scores we perform an argmax on ``dim=1``.
+    Compute WER score of transcribed segments against references.
 
-    If working with multilabel data, setting the ``is_multilabel`` argument to ``True`` will make sure that a
-    `confusion matrix gets calculated per label`_.
+    As input to ``forward`` and ``update`` the metric accepts the following input:
 
-    Args:
-        num_classes: Number of classes in the dataset.
-        normalize: Normalization mode for confusion matrix. Choose from:
+    - ``preds`` (:class:`~List`): Transcription(s) to score as a string or list of strings
+    - ``target`` (:class:`~List`): Reference(s) for each speech input as a string or list of strings
 
-            - ``None`` or ``'none'``: no normalization (default)
-            - ``'true'``: normalization over the targets (most commonly used)
-            - ``'pred'``: normalization over the predictions
-            - ``'all'``: normalization over the whole matrix
-
-        threshold:
-            Threshold for transforming probability or logit predictions to binary ``(0,1)`` predictions, in the case
-            of binary or multi-label inputs. Default value of ``0.5`` corresponds to input being probabilities.
+    As output of ``forward`` and ``compute`` the metric returns the following output:
 
-        multilabel: determines if data is multilabel or not.
+    -  ``wer`` (:class:`~torch.Tensor`): A tensor with the Word Error Rate score
 
+    Args:
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
-    Example (binary data):
-        >>> from torchmetrics import ConfusionMatrix
-        >>> target = torch.tensor([1, 1, 0, 0])
-        >>> preds = torch.tensor([0, 1, 0, 0])
-        >>> confmat = ConfusionMatrix(num_classes=2)
-        >>> confmat(preds, target)
-        tensor([[2, 0],
-                [1, 1]])
-
-    Example (multiclass data):
-        >>> target = torch.tensor([2, 1, 0, 0])
-        >>> preds = torch.tensor([2, 1, 0, 1])
-        >>> confmat = ConfusionMatrix(num_classes=3)
-        >>> confmat(preds, target)
-        tensor([[1, 1, 0],
-                [0, 1, 0],
-                [0, 0, 1]])
-
-    Example (multilabel data):
-        >>> target = torch.tensor([[0, 1, 0], [1, 0, 1]])
-        >>> preds = torch.tensor([[0, 0, 1], [1, 0, 1]])
-        >>> confmat = ConfusionMatrix(num_classes=3, multilabel=True)
-        >>> confmat(preds, target)
-        tensor([[[1, 0], [0, 1]],
-                [[1, 0], [1, 0]],
-                [[0, 1], [0, 1]]])
-
+    Examples:
+        >>> from torchmetrics.text import WordErrorRate
+        >>> preds = ["this is the prediction", "there is an other sample"]
+        >>> target = ["this is the reference", "there is another one"]
+        >>> wer = WordErrorRate()
+        >>> wer(preds, target)
+        tensor(0.5000)
     """
     is_differentiable: bool = False
-    higher_is_better: Optional[bool] = None
+    higher_is_better: bool = False
     full_state_update: bool = False
-    confmat: Tensor
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
+
+    errors: Tensor
+    total: Tensor
 
     def __init__(
         self,
-        num_classes: int,
-        normalize: Optional[str] = None,
-        threshold: float = 0.5,
-        multilabel: bool = False,
         **kwargs: Any,
     ) -> None:
         super().__init__(**kwargs)
-        self.num_classes = num_classes
-        self.normalize = normalize
-        self.threshold = threshold
-        self.multilabel = multilabel
-
-        allowed_normalize = ("true", "pred", "all", "none", None)
-        if self.normalize not in allowed_normalize:
-            raise ValueError(f"Argument average needs to one of the following: {allowed_normalize}")
-
-        if multilabel:
-            default = torch.zeros(num_classes, 2, 2, dtype=torch.long)
-        else:
-            default = torch.zeros(num_classes, num_classes, dtype=torch.long)
-        self.add_state("confmat", default=default, dist_reduce_fx="sum")
+        self.add_state("errors", tensor(0, dtype=torch.float), dist_reduce_fx="sum")
+        self.add_state("total", tensor(0, dtype=torch.float), dist_reduce_fx="sum")
 
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
-
-        Args:
-            preds: Predictions from model
-            target: Ground truth values
-        """
-        confmat = _confusion_matrix_update(preds, target, self.num_classes, self.threshold, self.multilabel)
-        self.confmat += confmat
+    def update(self, preds: Union[str, List[str]], target: Union[str, List[str]]) -> None:
+        """Update state with predictions and targets."""
+        errors, total = _wer_update(preds, target)
+        self.errors += errors
+        self.total += total
 
     def compute(self) -> Tensor:
-        """Computes confusion matrix.
+        """Calculate the word error rate."""
+        return _wer_compute(self.errors, self.total)
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
 
         Returns:
-            If ``multilabel=False`` this will be a ``[n_classes, n_classes]`` tensor and if ``multilabel=True``
-            this will be a ``[n_classes, 2, 2]`` tensor.
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> from torchmetrics.text import WordErrorRate
+            >>> metric = WordErrorRate()
+            >>> preds = ["this is the prediction", "there is an other sample"]
+            >>> target = ["this is the reference", "there is another one"]
+            >>> metric.update(preds, target)
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> from torchmetrics.text import WordErrorRate
+            >>> metric = WordErrorRate()
+            >>> preds = ["this is the prediction", "there is an other sample"]
+            >>> target = ["this is the reference", "there is another one"]
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(preds, target))
+            >>> fig_, ax_ = metric.plot(values)
         """
-        return _confusion_matrix_compute(self.confmat, self.normalize)
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/classification/dice.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/dice.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,51 +1,102 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, Optional
+from typing import Optional
 
+import torch
 from torch import Tensor
 
-from torchmetrics.classification.stat_scores import StatScores
-from torchmetrics.functional.classification.dice import _dice_compute
-from torchmetrics.utilities.enums import AverageMethod
+from torchmetrics.functional.classification.stat_scores import _reduce_stat_scores, _stat_scores_update
+from torchmetrics.utilities.checks import _input_squeeze
+from torchmetrics.utilities.enums import AverageMethod, MDMCAverageMethod
 
 
-class Dice(StatScores):
-    r"""Computes `Dice`_:
+def _dice_compute(
+    tp: Tensor,
+    fp: Tensor,
+    fn: Tensor,
+    average: Optional[str],
+    mdmc_average: Optional[str],
+    zero_division: int = 0,
+) -> Tensor:
+    """Compute dice from the stat scores: true positives, false positives, false negatives.
+
+    Args:
+        tp: True positives
+        fp: False positives
+        fn: False negatives
+        average: Defines the reduction that is applied
+        mdmc_average: Defines how averaging is done for multi-dimensional multi-class inputs (on top of the
+            ``average`` parameter)
+        zero_division: The value to use for the score if denominator equals zero.
+    """
+    numerator = 2 * tp
+    denominator = 2 * tp + fp + fn
+
+    if average == AverageMethod.MACRO and mdmc_average != MDMCAverageMethod.SAMPLEWISE:
+        cond = tp + fp + fn == 0
+        numerator = numerator[~cond]
+        denominator = denominator[~cond]
+
+    if average == AverageMethod.NONE and mdmc_average != MDMCAverageMethod.SAMPLEWISE:
+        # a class is not present if there exists no TPs, no FPs, and no FNs
+        meaningless_indeces = torch.nonzero((tp | fn | fp) == 0).cpu()
+        numerator[meaningless_indeces, ...] = -1
+        denominator[meaningless_indeces, ...] = -1
+
+    return _reduce_stat_scores(
+        numerator=numerator,
+        denominator=denominator,
+        weights=None if average != "weighted" else tp + fn,
+        average=average,
+        mdmc_average=mdmc_average,
+        zero_division=zero_division,
+    )
+
+
+def dice(
+    preds: Tensor,
+    target: Tensor,
+    zero_division: int = 0,
+    average: Optional[str] = "micro",
+    mdmc_average: Optional[str] = "global",
+    threshold: float = 0.5,
+    top_k: Optional[int] = None,
+    num_classes: Optional[int] = None,
+    multiclass: Optional[bool] = None,
+    ignore_index: Optional[int] = None,
+) -> Tensor:
+    r"""Compute `Dice`_.
 
     .. math:: \text{Dice} = \frac{\text{2 * TP}}{\text{2 * TP} + \text{FP} + \text{FN}}
 
-    Where :math:`\text{TP}` and :math:`\text{FP}` represent the number of true positives and
-    false positives respecitively.
+    Where :math:`\text{TP}` and :math:`\text{FN}` represent the number of true positives and
+    false negatives respecitively.
 
     It is recommend set `ignore_index` to index of background class.
 
-    The reduction method (how the precision scores are aggregated) is controlled by the
+    The reduction method (how the recall scores are aggregated) is controlled by the
     ``average`` parameter, and additionally by the ``mdmc_average`` parameter in the
-    multi-dimensional multi-class case. Accepts all inputs listed in :ref:`pages/classification:input types`.
+    multi-dimensional multi-class case.
 
     Args:
-        num_classes:
-            Number of classes. Necessary for ``'macro'``, ``'weighted'`` and ``None`` average methods.
-        threshold:
-            Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case
-            of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.
-        zero_division:
-            The value to use for the score if denominator equals zero.
+        preds: Predictions from model (probabilities, logits or labels)
+        target: Ground truth values
+        zero_division: The value to use for the score if denominator equals zero
         average:
             Defines the reduction that is applied. Should be one of the following:
 
             - ``'micro'`` [default]: Calculate the metric globally, across all samples and classes.
             - ``'macro'``: Calculate the metric for each class separately, and average the
               metrics across classes (with equal weights for each class).
             - ``'weighted'``: Calculate the metric for each class separately, and average the
@@ -54,114 +105,104 @@
               the metric for every class.
             - ``'samples'``: Calculate the metric for each sample, and average the metrics
               across samples (with equal weights for each sample).
 
             .. note:: What is considered a sample in the multi-dimensional multi-class case
                 depends on the value of ``mdmc_average``.
 
+            .. note:: If ``'none'`` and a given class doesn't occur in the ``preds`` or ``target``,
+                the value for the class will be ``nan``.
+
         mdmc_average:
             Defines how averaging is done for multi-dimensional multi-class inputs (on top of the
             ``average`` parameter). Should be one of the following:
 
             - ``None`` [default]: Should be left unchanged if your data is not multi-dimensional
               multi-class.
 
             - ``'samplewise'``: In this case, the statistics are computed separately for each
               sample on the ``N`` axis, and then averaged over samples.
               The computation for each sample is done by treating the flattened extra axes ``...``
-              (see :ref:`pages/classification:input types`) as the ``N`` dimension within the sample,
+              as the ``N`` dimension within the sample,
               and computing the metric for the sample based on that.
 
             - ``'global'``: In this case the ``N`` and ``...`` dimensions of the inputs
-              (see :ref:`pages/classification:input types`) are flattened into a new ``N_X`` sample axis, i.e.
-              the inputs are treated as if they were ``(N_X, C)``.
-              From here on the ``average`` parameter applies as usual.
+              are flattened into a new ``N_X`` sample axis, i.e. the inputs are treated as if they
+              were ``(N_X, C)``. From here on the ``average`` parameter applies as usual.
 
         ignore_index:
             Integer specifying a target class to ignore. If given, this class index does not contribute
             to the returned score, regardless of reduction method. If an index is ignored, and ``average=None``
             or ``'none'``, the score for the ignored class will be returned as ``nan``.
 
+        num_classes:
+            Number of classes. Necessary for ``'macro'``, ``'weighted'`` and ``None`` average methods.
+
+        threshold:
+            Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case
+            of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.
         top_k:
             Number of the highest probability or logit score predictions considered finding the correct label,
             relevant only for (multi-dimensional) multi-class inputs. The
             default value (``None``) will be interpreted as 1 for these inputs.
-            Should be left at default (``None``) for all other types of inputs.
 
+            Should be left at default (``None``) for all other types of inputs.
         multiclass:
             Used only in certain special cases, where you want to treat inputs as a different type
-            than what they appear to be. See the parameter's
-            :ref:`documentation section <pages/classification:using the multiclass parameter>`
-            for a more detailed explanation and examples.
+            than what they appear to be.
+
+    Return:
+        The shape of the returned tensor depends on the ``average`` parameter
 
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+        - If ``average in ['micro', 'macro', 'weighted', 'samples']``, a one-element tensor will be returned
+        - If ``average in ['none', None]``, the shape will be ``(C,)``, where ``C`` stands  for the number of classes
 
     Raises:
         ValueError:
-            If ``average`` is none of ``"micro"``, ``"macro"``, ``"weighted"``, ``"samples"``, ``"none"``, ``None``.
+            If ``average`` is not one of ``"micro"``, ``"macro"``, ``"weighted"``, ``"samples"``, ``"none"`` or ``None``
         ValueError:
             If ``mdmc_average`` is not one of ``None``, ``"samplewise"``, ``"global"``.
         ValueError:
             If ``average`` is set but ``num_classes`` is not provided.
         ValueError:
             If ``num_classes`` is set and ``ignore_index`` is not in the range ``[0, num_classes)``.
 
     Example:
-        >>> import torch
-        >>> from torchmetrics import Dice
-        >>> preds  = torch.tensor([2, 0, 2, 1])
+        >>> from torchmetrics.functional.classification import dice
+        >>> preds = torch.tensor([2, 0, 2, 1])
         >>> target = torch.tensor([1, 1, 2, 0])
-        >>> dice = Dice(average='micro')
-        >>> dice(preds, target)
+        >>> dice(preds, target, average='micro')
         tensor(0.2500)
-
     """
-    is_differentiable: bool = False
-    higher_is_better: bool = True
-    full_state_update: bool = False
-
-    def __init__(
-        self,
-        zero_division: int = 0,
-        num_classes: Optional[int] = None,
-        threshold: float = 0.5,
-        average: Optional[str] = "micro",
-        mdmc_average: Optional[str] = "global",
-        ignore_index: Optional[int] = None,
-        top_k: Optional[int] = None,
-        multiclass: Optional[bool] = None,
-        **kwargs: Any,
-    ) -> None:
-        allowed_average = ("micro", "macro", "weighted", "samples", "none", None)
-        if average not in allowed_average:
-            raise ValueError(f"The `average` has to be one of {allowed_average}, got {average}.")
-
-        _reduce_options = (AverageMethod.WEIGHTED, AverageMethod.NONE, None)
-        if "reduce" not in kwargs:
-            kwargs["reduce"] = AverageMethod.MACRO if average in _reduce_options else average
-        if "mdmc_reduce" not in kwargs:
-            kwargs["mdmc_reduce"] = mdmc_average
-
-        super().__init__(
-            threshold=threshold,
-            top_k=top_k,
-            num_classes=num_classes,
-            multiclass=multiclass,
-            ignore_index=ignore_index,
-            **kwargs,
-        )
-
-        self.average = average
-        self.zero_division = zero_division
-
-    def compute(self) -> Tensor:
-        """Computes the dice score based on inputs passed in to ``update`` previously.
-
-        Return:
-            The shape of the returned tensor depends on the ``average`` parameter:
-
-            - If ``average in ['micro', 'macro', 'weighted', 'samples']``, a one-element tensor will be returned
-            - If ``average in ['none', None]``, the shape will be ``(C,)``, where ``C`` stands  for the number
-              of classes
-        """
-        tp, fp, _, fn = self._get_final_stats()
-        return _dice_compute(tp, fp, fn, self.average, self.mdmc_reduce, self.zero_division)
+    allowed_average = ("micro", "macro", "weighted", "samples", "none", None)
+    if average not in allowed_average:
+        raise ValueError(f"The `average` has to be one of {allowed_average}, got {average}.")
+
+    if average in ["macro", "weighted", "none", None] and (not num_classes or num_classes < 1):
+        raise ValueError(f"When you set `average` as {average}, you have to provide the number of classes.")
+
+    allowed_mdmc_average = [None, "samplewise", "global"]
+    if mdmc_average not in allowed_mdmc_average:
+        raise ValueError(f"The `mdmc_average` has to be one of {allowed_mdmc_average}, got {mdmc_average}.")
+
+    if num_classes and ignore_index is not None and (not ignore_index < num_classes or num_classes == 1):
+        raise ValueError(f"The `ignore_index` {ignore_index} is not valid for inputs with {num_classes} classes")
+
+    if top_k is not None and (not isinstance(top_k, int) or top_k <= 0):
+        raise ValueError(f"The `top_k` should be an integer larger than 0, got {top_k}")
+
+    preds, target = _input_squeeze(preds, target)
+    reduce = "macro" if average in ("weighted", "none", None) else average
+
+    tp, fp, _, fn = _stat_scores_update(
+        preds,
+        target,
+        reduce=reduce,
+        mdmc_reduce=mdmc_average,
+        threshold=threshold,
+        num_classes=num_classes,
+        top_k=top_k,
+        multiclass=multiclass,
+        ignore_index=ignore_index,
+    )
+
+    return _dice_compute(tp, fp, fn, average, mdmc_average, zero_division)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/classification/f_beta.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/classification/cohen_kappa.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,275 +1,326 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, Optional
+from typing import Any, Optional, Sequence, Union
 
 from torch import Tensor
+from typing_extensions import Literal
 
-from torchmetrics.classification.stat_scores import StatScores
-from torchmetrics.functional.classification.f_beta import _fbeta_compute
-from torchmetrics.utilities.enums import AverageMethod
+from torchmetrics.classification.confusion_matrix import BinaryConfusionMatrix, MulticlassConfusionMatrix
+from torchmetrics.functional.classification.cohen_kappa import (
+    _binary_cohen_kappa_arg_validation,
+    _cohen_kappa_reduce,
+    _multiclass_cohen_kappa_arg_validation,
+)
+from torchmetrics.metric import Metric
+from torchmetrics.utilities.enums import ClassificationTaskNoMultilabel
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["BinaryCohenKappa.plot", "MulticlassCohenKappa.plot"]
 
-class FBetaScore(StatScores):
-    r"""Computes `F-score`_, specifically:
+
+class BinaryCohenKappa(BinaryConfusionMatrix):
+    r"""Calculate `Cohen's kappa score`_ that measures inter-annotator agreement for binary tasks.
 
     .. math::
-        F_\beta = (1 + \beta^2) * \frac{\text{precision} * \text{recall}}
-        {(\beta^2 * \text{precision}) + \text{recall}}
+        \kappa = (p_o - p_e) / (1 - p_e)
 
-    Where :math:`\beta` is some positive real factor. Works with binary, multiclass, and multilabel data.
-    Accepts logit scores or probabilities from a model output or integer class values in prediction.
-    Works with multi-dimensional preds and target.
+    where :math:`p_o` is the empirical probability of agreement and :math:`p_e` is
+    the expected agreement when both annotators assign labels randomly. Note that
+    :math:`p_e` is estimated using a per-annotator empirical prior over the
+    class labels.
 
-    Forward accepts
+    As input to ``forward`` and ``update`` the metric accepts the following input:
 
-    - ``preds`` (float or long tensor): ``(N, ...)`` or ``(N, C, ...)`` where C is the number of classes
-    - ``target`` (long tensor): ``(N, ...)``
+    - ``preds`` (:class:`~torch.Tensor`): A int or float tensor of shape ``(N, ...)``. If preds is a floating point
+      tensor with values outside [0,1] range we consider the input to be logits and will auto apply sigmoid per element.
+      Addtionally, we convert to int tensor with thresholding using the value in ``threshold``.
+    - ``target`` (:class:`~torch.Tensor`): An int tensor of shape ``(N, ...)``.
 
-    If preds and target are the same shape and preds is a float tensor, we use the ``self.threshold`` argument
-    to convert into integer labels. This is the case for binary and multi-label logits and probabilities.
+    .. note::
+       Additional dimension ``...`` will be flattened into the batch dimension.
 
-    If preds has an extra dimension as in the case of multi-class scores we perform an argmax on ``dim=1``.
+    As output to ``forward`` and ``compute`` the metric returns the following output:
 
-    Args:
-        num_classes: Number of classes. Necessary for ``'macro'``, ``'weighted'`` and ``None`` average methods.
-        beta: Beta coefficient in the F measure.
-        threshold:
-            Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case
-            of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.
-        average:
-            Defines the reduction that is applied. Should be one of the following:
-
-            - ``'micro'`` [default]: Calculate the metric globally, across all samples and classes.
-            - ``'macro'``: Calculate the metric for each class separately, and average the
-              metrics across classes (with equal weights for each class).
-            - ``'weighted'``: Calculate the metric for each class separately, and average the
-              metrics across classes, weighting each class by its support (``tp + fn``).
-            - ``'none'`` or ``None``: Calculate the metric for each class separately, and return
-              the metric for every class.
-            - ``'samples'``: Calculate the metric for each sample, and average the metrics
-              across samples (with equal weights for each sample).
-
-            .. note:: What is considered a sample in the multi-dimensional multi-class case
-                depends on the value of ``mdmc_average``.
-
-            .. note:: If ``'none'`` and a given class doesn't occur in the ``preds`` or ``target``,
-                the value for the class will be ``nan``.
-
-        mdmc_average:
-            Defines how averaging is done for multi-dimensional multi-class inputs (on top of the
-            ``average`` parameter). Should be one of the following:
-
-            - ``None`` [default]: Should be left unchanged if your data is not multi-dimensional
-              multi-class.
-
-            - ``'samplewise'``: In this case, the statistics are computed separately for each
-              sample on the ``N`` axis, and then averaged over samples.
-              The computation for each sample is done by treating the flattened extra axes ``...``
-              (see :ref:`pages/classification:input types`) as the ``N`` dimension within the sample,
-              and computing the metric for the sample based on that.
-
-            - ``'global'``: In this case the ``N`` and ``...`` dimensions of the inputs
-              (see :ref:`pages/classification:input types`)
-              are flattened into a new ``N_X`` sample axis, i.e. the inputs are treated as if they
-              were ``(N_X, C)``. From here on the ``average`` parameter applies as usual.
+    - ``bck`` (:class:`~torch.Tensor`): A tensor containing cohen kappa score
 
+    Args:
+        threshold: Threshold for transforming probability to binary (0,1) predictions
         ignore_index:
-            Integer specifying a target class to ignore. If given, this class index does not contribute
-            to the returned score, regardless of reduction method. If an index is ignored, and ``average=None``
-            or ``'none'``, the score for the ignored class will be returned as ``nan``.
-
-        top_k:
-            Number of the highest probability or logit score predictions considered finding the correct label,
-            relevant only for (multi-dimensional) multi-class inputs. The default value (``None``) will be interpreted
-            as 1 for these inputs.
-
-            Should be left at default (``None``) for all other types of inputs.
-
-        multiclass:
-            Used only in certain special cases, where you want to treat inputs as a different type
-            than what they appear to be. See the parameter's
-            :ref:`documentation section <pages/classification:using the multiclass parameter>`
-            for a more detailed explanation and examples.
+            Specifies a target value that is ignored and does not contribute to the metric calculation
+        weights: Weighting type to calculate the score. Choose from:
 
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+            - ``None`` or ``'none'``: no weighting
+            - ``'linear'``: linear weighting
+            - ``'quadratic'``: quadratic weighting
 
-    Raises:
-        ValueError:
-            If ``average`` is none of ``"micro"``, ``"macro"``, ``"weighted"``, ``"none"``, ``None``.
-
-    Example:
-        >>> import torch
-        >>> from torchmetrics import FBetaScore
-        >>> target = torch.tensor([0, 1, 2, 0, 1, 2])
-        >>> preds = torch.tensor([0, 2, 1, 0, 0, 1])
-        >>> f_beta = FBetaScore(num_classes=3, beta=0.5)
-        >>> f_beta(preds, target)
-        tensor(0.3333)
+        validate_args: bool indicating if input arguments and tensors should be validated for correctness.
+            Set to ``False`` for faster computations.
+        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
+    Example (preds is int tensor):
+        >>> from torch import tensor
+        >>> from torchmetrics.classification import BinaryCohenKappa
+        >>> target = tensor([1, 1, 0, 0])
+        >>> preds = tensor([0, 1, 0, 0])
+        >>> metric = BinaryCohenKappa()
+        >>> metric(preds, target)
+        tensor(0.5000)
+
+    Example (preds is float tensor):
+        >>> from torchmetrics.classification import BinaryCohenKappa
+        >>> target = tensor([1, 1, 0, 0])
+        >>> preds = tensor([0.35, 0.85, 0.48, 0.01])
+        >>> metric = BinaryCohenKappa()
+        >>> metric(preds, target)
+        tensor(0.5000)
     """
+    is_differentiable: bool = False
+    higher_is_better: bool = True
     full_state_update: bool = False
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
 
     def __init__(
         self,
-        num_classes: Optional[int] = None,
-        beta: float = 1.0,
         threshold: float = 0.5,
-        average: Optional[str] = "micro",
-        mdmc_average: Optional[str] = None,
         ignore_index: Optional[int] = None,
-        top_k: Optional[int] = None,
-        multiclass: Optional[bool] = None,
+        weights: Optional[Literal["linear", "quadratic", "none"]] = None,
+        validate_args: bool = True,
         **kwargs: Any,
     ) -> None:
-        self.beta = beta
-        allowed_average = list(AverageMethod)
-        if average not in allowed_average:
-            raise ValueError(f"The `average` has to be one of {allowed_average}, got {average}.")
-
-        _reduce_options = (AverageMethod.WEIGHTED, AverageMethod.NONE, None)
-        if "reduce" not in kwargs:
-            kwargs["reduce"] = AverageMethod.MACRO if average in _reduce_options else average
-        if "mdmc_reduce" not in kwargs:
-            kwargs["mdmc_reduce"] = mdmc_average
-
-        super().__init__(
-            threshold=threshold,
-            top_k=top_k,
-            num_classes=num_classes,
-            multiclass=multiclass,
-            ignore_index=ignore_index,
-            **kwargs,
-        )
-
-        self.average = average
+        super().__init__(threshold, ignore_index, normalize=None, validate_args=False, **kwargs)
+        if validate_args:
+            _binary_cohen_kappa_arg_validation(threshold, ignore_index, weights)
+        self.weights = weights
+        self.validate_args = validate_args
 
     def compute(self) -> Tensor:
-        """Computes f-beta over state."""
-        tp, fp, tn, fn = self._get_final_stats()
-        return _fbeta_compute(tp, fp, tn, fn, self.beta, self.ignore_index, self.average, self.mdmc_reduce)
+        """Compute metric."""
+        return _cohen_kappa_reduce(self.confmat, self.weights)
 
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure object and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import rand, randint
+            >>> # Example plotting a single value
+            >>> from torchmetrics.classification import BinaryCohenKappa
+            >>> metric = BinaryCohenKappa()
+            >>> metric.update(rand(10), randint(2,(10,)))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import rand, randint
+            >>> # Example plotting multiple values
+            >>> from torchmetrics.classification import BinaryCohenKappa
+            >>> metric = BinaryCohenKappa()
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(rand(10), randint(2,(10,))))
+            >>> fig_, ax_ = metric.plot(values)
+        """
+        return self._plot(val, ax)
 
-class F1Score(FBetaScore):
-    """Computes F1 metric.
 
-    F1 metrics correspond to a harmonic mean of the precision and recall scores.
-    Works with binary, multiclass, and multilabel data. Accepts logits or probabilities from a model
-    output or integer class values in prediction. Works with multi-dimensional preds and target.
+class MulticlassCohenKappa(MulticlassConfusionMatrix):
+    r"""Calculate `Cohen's kappa score`_ that measures inter-annotator agreement for multiclass tasks.
 
-    Forward accepts
+    .. math::
+        \kappa = (p_o - p_e) / (1 - p_e)
 
-    - ``preds`` (float or long tensor): ``(N, ...)`` or ``(N, C, ...)`` where C is the number of classes
-    - ``target`` (long tensor): ``(N, ...)``
+    where :math:`p_o` is the empirical probability of agreement and :math:`p_e` is
+    the expected agreement when both annotators assign labels randomly. Note that
+    :math:`p_e` is estimated using a per-annotator empirical prior over the
+    class labels.
 
-    If preds and target are the same shape and preds is a float tensor, we use the ``self.threshold`` argument.
-    This is the case for binary and multi-label logits.
+    As input to ``forward`` and ``update`` the metric accepts the following input:
 
-    If preds has an extra dimension as in the case of multi-class scores we perform an argmax on ``dim=1``.
+    - ``preds`` (:class:`~torch.Tensor`): Either an int tensor of shape ``(N, ...)` or float tensor of shape
+      ``(N, C, ..)``. If preds is a floating point we apply ``torch.argmax`` along the ``C`` dimension to automatically
+      convert probabilities/logits into an int tensor.
+    - ``target`` (:class:`~torch.Tensor`): An int tensor of shape ``(N, ...)``.
 
-    Args:
-        num_classes:
-            Number of classes. Necessary for ``'macro'``, ``'weighted'`` and ``None`` average methods.
-        threshold:
-            Threshold for transforming probability or logit predictions to binary ``(0,1)`` predictions, in the case
-            of binary or multi-label inputs. Default value of ``0.5`` corresponds to input being probabilities.
-        average:
-            Defines the reduction that is applied. Should be one of the following:
-
-            - ``'micro'`` [default]: Calculate the metric globally, across all samples and classes.
-            - ``'macro'``: Calculate the metric for each class separately, and average the
-              metrics across classes (with equal weights for each class).
-            - ``'weighted'``: Calculate the metric for each class separately, and average the
-              metrics across classes, weighting each class by its support (``tp + fn``).
-            - ``'none'`` or ``None``: Calculate the metric for each class separately, and return
-              the metric for every class.
-            - ``'samples'``: Calculate the metric for each sample, and average the metrics
-              across samples (with equal weights for each sample).
-
-            .. note:: What is considered a sample in the multi-dimensional multi-class case
-                depends on the value of ``mdmc_average``.
-
-        mdmc_average:
-            Defines how averaging is done for multi-dimensional multi-class inputs (on top of the
-            ``average`` parameter). Should be one of the following:
-
-            - ``None`` [default]: Should be left unchanged if your data is not multi-dimensional
-              multi-class.
-
-            - ``'samplewise'``: In this case, the statistics are computed separately for each
-              sample on the ``N`` axis, and then averaged over samples.
-              The computation for each sample is done by treating the flattened extra axes ``...``
-              (see :ref:`pages/classification:input types`) as the ``N`` dimension within the sample,
-              and computing the metric for the sample based on that.
-
-            - ``'global'``: In this case the ``N`` and ``...`` dimensions of the inputs
-              (see :ref:`pages/classification:input types`)
-              are flattened into a new ``N_X`` sample axis, i.e. the inputs are treated as if they
-              were ``(N_X, C)``. From here on the ``average`` parameter applies as usual.
+    .. note::
+       Additional dimension ``...`` will be flattened into the batch dimension.
+
+    As output to ``forward`` and ``compute`` the metric returns the following output:
+
+    - ``mcck`` (:class:`~torch.Tensor`): A tensor containing cohen kappa score
 
+    Args:
+        num_classes: Integer specifing the number of classes
         ignore_index:
-            Integer specifying a target class to ignore. If given, this class index does not contribute
-            to the returned score, regardless of reduction method. If an index is ignored, and ``average=None``
-            or ``'none'``, the score for the ignored class will be returned as ``nan``.
-
-        top_k:
-            Number of the highest probability or logit score predictions considered finding the correct label,
-            relevant only for (multi-dimensional) multi-class inputs. The
-            default value (``None``) will be interpreted as 1 for these inputs.
-            Should be left at default (``None``) for all other types of inputs.
-
-        multiclass:
-            Used only in certain special cases, where you want to treat inputs as a different type
-            than what they appear to be. See the parameter's
-            :ref:`documentation section <pages/classification:using the multiclass parameter>`
-            for a more detailed explanation and examples.
+            Specifies a target value that is ignored and does not contribute to the metric calculation
+        weights: Weighting type to calculate the score. Choose from:
 
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+            - ``None`` or ``'none'``: no weighting
+            - ``'linear'``: linear weighting
+            - ``'quadratic'``: quadratic weighting
 
+        validate_args: bool indicating if input arguments and tensors should be validated for correctness.
+            Set to ``False`` for faster computations.
+        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
-    Example:
-        >>> import torch
-        >>> from torchmetrics import F1Score
-        >>> target = torch.tensor([0, 1, 2, 0, 1, 2])
-        >>> preds = torch.tensor([0, 2, 1, 0, 0, 1])
-        >>> f1 = F1Score(num_classes=3)
-        >>> f1(preds, target)
-        tensor(0.3333)
+    Example (pred is integer tensor):
+        >>> from torch import tensor
+        >>> from torchmetrics.classification import MulticlassCohenKappa
+        >>> target = tensor([2, 1, 0, 0])
+        >>> preds = tensor([2, 1, 0, 1])
+        >>> metric = MulticlassCohenKappa(num_classes=3)
+        >>> metric(preds, target)
+        tensor(0.6364)
+
+    Example (pred is float tensor):
+        >>> from torchmetrics.classification import MulticlassCohenKappa
+        >>> target = tensor([2, 1, 0, 0])
+        >>> preds = tensor([[0.16, 0.26, 0.58],
+        ...                 [0.22, 0.61, 0.17],
+        ...                 [0.71, 0.09, 0.20],
+        ...                 [0.05, 0.82, 0.13]])
+        >>> metric = MulticlassCohenKappa(num_classes=3)
+        >>> metric(preds, target)
+        tensor(0.6364)
     """
-
     is_differentiable: bool = False
     higher_is_better: bool = True
     full_state_update: bool = False
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
+    plot_legend_name: str = "Class"
 
     def __init__(
         self,
-        num_classes: Optional[int] = None,
-        threshold: float = 0.5,
-        average: Optional[str] = "micro",
-        mdmc_average: Optional[str] = None,
+        num_classes: int,
         ignore_index: Optional[int] = None,
-        top_k: Optional[int] = None,
-        multiclass: Optional[bool] = None,
+        weights: Optional[Literal["linear", "quadratic", "none"]] = None,
+        validate_args: bool = True,
         **kwargs: Any,
     ) -> None:
-        super().__init__(
-            num_classes=num_classes,
-            beta=1.0,
-            threshold=threshold,
-            average=average,
-            mdmc_average=mdmc_average,
-            ignore_index=ignore_index,
-            top_k=top_k,
-            multiclass=multiclass,
-            **kwargs,
-        )
+        super().__init__(num_classes, ignore_index, normalize=None, validate_args=False, **kwargs)
+        if validate_args:
+            _multiclass_cohen_kappa_arg_validation(num_classes, ignore_index, weights)
+        self.weights = weights
+        self.validate_args = validate_args
+
+    def compute(self) -> Tensor:
+        """Compute metric."""
+        return _cohen_kappa_reduce(self.confmat, self.weights)
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure object and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import randn, randint
+            >>> # Example plotting a single value
+            >>> from torchmetrics.classification import MulticlassCohenKappa
+            >>> metric = MulticlassCohenKappa(num_classes=3)
+            >>> metric.update(randn(20,3).softmax(dim=-1), randint(3, (20,)))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import randn, randint
+            >>> # Example plotting a multiple values
+            >>> from torchmetrics.classification import MulticlassCohenKappa
+            >>> metric = MulticlassCohenKappa(num_classes=3)
+            >>> values = []
+            >>> for _ in range(20):
+            ...     values.append(metric(randn(20,3).softmax(dim=-1), randint(3, (20,))))
+            >>> fig_, ax_ = metric.plot(values)
+        """
+        return self._plot(val, ax)
+
+
+class CohenKappa:
+    r"""Calculate `Cohen's kappa score`_ that measures inter-annotator agreement.
+
+    .. math::
+        \kappa = (p_o - p_e) / (1 - p_e)
+
+    where :math:`p_o` is the empirical probability of agreement and :math:`p_e` is
+    the expected agreement when both annotators assign labels randomly. Note that
+    :math:`p_e` is estimated using a per-annotator empirical prior over the
+    class labels.
+
+    This function is a simple wrapper to get the task specific versions of this metric, which is done by setting the
+    ``task`` argument to either ``'binary'`` or ``'multiclass'``. See the documentation of
+    :mod:`BinaryCohenKappa` and :mod:`MulticlassCohenKappa` for the specific details of
+    each argument influence and examples.
+
+    Legacy Example:
+        >>> from torch import tensor
+        >>> target = tensor([1, 1, 0, 0])
+        >>> preds = tensor([0, 1, 0, 0])
+        >>> cohenkappa = CohenKappa(task="multiclass", num_classes=2)
+        >>> cohenkappa(preds, target)
+        tensor(0.5000)
+    """
+
+    def __new__(
+        cls,
+        task: Literal["binary", "multiclass"],
+        threshold: float = 0.5,
+        num_classes: Optional[int] = None,
+        weights: Optional[Literal["linear", "quadratic", "none"]] = None,
+        ignore_index: Optional[int] = None,
+        validate_args: bool = True,
+        **kwargs: Any,
+    ) -> Metric:
+        """Initialize task metric."""
+        task = ClassificationTaskNoMultilabel.from_str(task)
+        kwargs.update({"weights": weights, "ignore_index": ignore_index, "validate_args": validate_args})
+        if task == ClassificationTaskNoMultilabel.BINARY:
+            return BinaryCohenKappa(threshold, **kwargs)
+        if task == ClassificationTaskNoMultilabel.MULTICLASS:
+            if not isinstance(num_classes, int):
+                raise ValueError(f"`num_classes` is expected to be `int` but `{type(num_classes)} was passed.`")
+            return MulticlassCohenKappa(num_classes, **kwargs)
+        return None
```

### Comparing `torchmetrics-0.9.3/torchmetrics/classification/hamming.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/pairwise/cosine.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,93 +1,89 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any
+from typing import Optional
 
 import torch
-from torch import Tensor, tensor
+from torch import Tensor
+from typing_extensions import Literal
 
-from torchmetrics.functional.classification.hamming import _hamming_distance_compute, _hamming_distance_update
-from torchmetrics.metric import Metric
+from torchmetrics.functional.pairwise.helpers import _check_input, _reduce_distance_matrix
+from torchmetrics.utilities.compute import _safe_matmul
 
 
-class HammingDistance(Metric):
-    r"""Computes the average `Hamming distance`_ (also known as Hamming loss) between targets and predictions:
+def _pairwise_cosine_similarity_update(
+    x: Tensor, y: Optional[Tensor] = None, zero_diagonal: Optional[bool] = None
+) -> Tensor:
+    """Calculate the pairwise cosine similarity matrix.
 
-    .. math::
-        \text{Hamming distance} = \frac{1}{N \cdot L}\sum_i^N \sum_l^L 1(y_{il} \neq \hat{y_{il}})
+    Args:
+        x: tensor of shape ``[N,d]``
+        y: tensor of shape ``[M,d]``
+        zero_diagonal: determines if the diagonal of the distance matrix should be set to zero
+    """
+    x, y, zero_diagonal = _check_input(x, y, zero_diagonal)
 
-    Where :math:`y` is a tensor of target values, :math:`\hat{y}` is a tensor of predictions,
-    and :math:`\bullet_{il}` refers to the :math:`l`-th label of the :math:`i`-th sample of that
-    tensor.
+    norm = torch.norm(x, p=2, dim=1)
+    x = x / norm.unsqueeze(1)
+    norm = torch.norm(y, p=2, dim=1)
+    y = y / norm.unsqueeze(1)
+
+    distance = _safe_matmul(x, y)
+    if zero_diagonal:
+        distance.fill_diagonal_(0)
+    return distance
+
+
+def pairwise_cosine_similarity(
+    x: Tensor,
+    y: Optional[Tensor] = None,
+    reduction: Literal["mean", "sum", "none", None] = None,
+    zero_diagonal: Optional[bool] = None,
+) -> Tensor:
+    r"""Calculate pairwise cosine similarity.
 
-    This is the same as ``1-accuracy`` for binary data, while for all other types of inputs it
-    treats each possible label separately - meaning that, for example, multi-class data is
-    treated as if it were multi-label.
+    .. math::
+        s_{cos}(x,y) = \frac{<x,y>}{||x|| \cdot ||y||}
+                     = \frac{\sum_{d=1}^D x_d \cdot y_d }{\sqrt{\sum_{d=1}^D x_i^2} \cdot \sqrt{\sum_{d=1}^D y_i^2}}
 
-    Accepts all input types listed in :ref:`pages/classification:input types`.
+    If both :math:`x` and :math:`y` are passed in, the calculation will be performed pairwise
+    between the rows of :math:`x` and :math:`y`.
+    If only :math:`x` is passed in, the calculation will be performed between the rows of :math:`x`.
 
     Args:
-        threshold:
-            Threshold for transforming probability or logit predictions to binary ``(0,1)`` predictions, in the case
-            of binary or multi-label inputs. Default value of ``0.5`` corresponds to input being probabilities.
-
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
-
-    Raises:
-        ValueError:
-            If ``threshold`` is not between ``0`` and ``1``.
+        x: Tensor with shape ``[N, d]``
+        y: Tensor with shape ``[M, d]``, optional
+        reduction: reduction to apply along the last dimension. Choose between `'mean'`, `'sum'`
+            (applied along column dimension) or  `'none'`, `None` for no reduction
+        zero_diagonal: if the diagonal of the distance matrix should be set to 0. If only :math:`x` is given
+            this defaults to ``True`` else if :math:`y` is also given it defaults to ``False``
 
-    Example:
-        >>> from torchmetrics import HammingDistance
-        >>> target = torch.tensor([[0, 1], [1, 1]])
-        >>> preds = torch.tensor([[0, 1], [0, 1]])
-        >>> hamming_distance = HammingDistance()
-        >>> hamming_distance(preds, target)
-        tensor(0.2500)
+    Returns:
+        A ``[N,N]`` matrix of distances if only ``x`` is given, else a ``[N,M]`` matrix
 
+    Example:
+        >>> import torch
+        >>> from torchmetrics.functional.pairwise import pairwise_cosine_similarity
+        >>> x = torch.tensor([[2, 3], [3, 5], [5, 8]], dtype=torch.float32)
+        >>> y = torch.tensor([[1, 0], [2, 1]], dtype=torch.float32)
+        >>> pairwise_cosine_similarity(x, y)
+        tensor([[0.5547, 0.8682],
+                [0.5145, 0.8437],
+                [0.5300, 0.8533]])
+        >>> pairwise_cosine_similarity(x)
+        tensor([[0.0000, 0.9989, 0.9996],
+                [0.9989, 0.0000, 0.9998],
+                [0.9996, 0.9998, 0.0000]])
     """
-    is_differentiable: bool = False
-    higher_is_better: bool = False
-    full_state_update: bool = False
-    correct: Tensor
-    total: Tensor
-
-    def __init__(
-        self,
-        threshold: float = 0.5,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__(**kwargs)
-
-        self.add_state("correct", default=tensor(0), dist_reduce_fx="sum")
-        self.add_state("total", default=tensor(0), dist_reduce_fx="sum")
-
-        self.threshold = threshold
-
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
-
-        See :ref:`pages/classification:input types` for more information on input types.
-
-        Args:
-            preds: Predictions from model (probabilities, logits or labels)
-            target: Ground truth labels
-        """
-        correct, total = _hamming_distance_update(preds, target, self.threshold)
-
-        self.correct += correct
-        self.total += total
-
-    def compute(self) -> Tensor:
-        """Computes hamming distance based on inputs passed in to ``update`` previously."""
-        return _hamming_distance_compute(self.correct, self.total)
+    distance = _pairwise_cosine_similarity_update(x, y, zero_diagonal)
+    return _reduce_distance_matrix(distance, reduction)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/classification/hinge.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/retrieval/ndcg.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,124 +1,148 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, Optional, Union
+from typing import Any, Optional, Sequence, Union
 
-from torch import Tensor, tensor
+from torch import Tensor
 
-from torchmetrics.functional.classification.hinge import MulticlassMode, _hinge_compute, _hinge_update
-from torchmetrics.metric import Metric
+from torchmetrics.functional.retrieval.ndcg import retrieval_normalized_dcg
+from torchmetrics.retrieval.base import RetrievalMetric
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["RetrievalNormalizedDCG.plot"]
 
-class HingeLoss(Metric):
-    r"""Computes the mean `Hinge loss`_, typically used for Support Vector Machines (SVMs).
 
-    In the binary case it is defined as:
+class RetrievalNormalizedDCG(RetrievalMetric):
+    """Compute `Normalized Discounted Cumulative Gain`_.
 
-    .. math::
-        \text{Hinge loss} = \max(0, 1 - y \times \hat{y})
+    Works with binary or positive integer target data. Accepts float predictions from a model output.
 
-    Where :math:`y \in {-1, 1}` is the target, and :math:`\hat{y} \in \mathbb{R}` is the prediction.
+    As input to ``forward`` and ``update`` the metric accepts the following input:
 
-    In the multi-class case, when ``multiclass_mode=None`` (default), ``multiclass_mode=MulticlassMode.CRAMMER_SINGER``
-    or ``multiclass_mode="crammer-singer"``, this metric will compute the multi-class hinge loss defined by Crammer and
-    Singer as:
+    - ``preds`` (:class:`~torch.Tensor`): A float tensor of shape ``(N, ...)``
+    - ``target`` (:class:`~torch.Tensor`): A long or bool tensor of shape ``(N, ...)``
+    - ``indexes`` (:class:`~torch.Tensor`): A long tensor of shape ``(N, ...)`` which indicate to which query a
+      prediction belongs
 
-    .. math::
-        \text{Hinge loss} = \max\left(0, 1 - \hat{y}_y + \max_{i \ne y} (\hat{y}_i)\right)
+    As output to ``forward`` and ``compute`` the metric returns the following output:
 
-    Where :math:`y \in {0, ..., \mathrm{C}}` is the target class (where :math:`\mathrm{C}` is the number of classes),
-    and :math:`\hat{y} \in \mathbb{R}^\mathrm{C}` is the predicted output per class.
+    - ``ndcg`` (:class:`~torch.Tensor`): A single-value tensor with the nDCG of the predictions
+      ``preds`` w.r.t. the labels ``target``
 
-    In the multi-class case when ``multiclass_mode=MulticlassMode.ONE_VS_ALL`` or ``multiclass_mode='one-vs-all'``, this
-    metric will use a one-vs-all approach to compute the hinge loss, giving a vector of C outputs where each entry pits
-    that class against all remaining classes.
+    All ``indexes``, ``preds`` and ``target`` must have the same dimension and will be flatten at the beginning,
+    so that for example, a tensor of shape ``(N, M)`` is treated as ``(N * M, )``. Predictions will be first grouped by
+    ``indexes`` and then will be computed as the mean of the metric over each query.
 
-    This metric can optionally output the mean of the squared hinge loss by setting ``squared=True``
-
-    Only accepts inputs with preds shape of (N) (binary) or (N, C) (multi-class) and target shape of (N).
 
     Args:
-        squared:
-            If True, this will compute the squared hinge loss. Otherwise, computes the regular hinge loss (default).
-        multiclass_mode:
-            Which approach to use for multi-class inputs (has no effect in the binary case). ``None`` (default),
-            ``MulticlassMode.CRAMMER_SINGER`` or ``"crammer-singer"``, uses the Crammer Singer multi-class hinge loss.
-            ``MulticlassMode.ONE_VS_ALL`` or ``"one-vs-all"`` computes the hinge loss in a one-vs-all fashion.
+        empty_target_action:
+            Specify what to do with queries that do not have at least a positive ``target``. Choose from:
 
+            - ``'neg'``: those queries count as ``0.0`` (default)
+            - ``'pos'``: those queries count as ``1.0``
+            - ``'skip'``: skip those queries; if all queries are skipped, ``0.0`` is returned
+            - ``'error'``: raise a ``ValueError``
+
+        ignore_index:
+            Ignore predictions where the target is equal to this number.
+        top_k: consider only the top k elements for each query (default: ``None``, which considers them all)
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
-
     Raises:
         ValueError:
-            If ``multiclass_mode`` is not: None, ``MulticlassMode.CRAMMER_SINGER``, ``"crammer-singer"``,
-            ``MulticlassMode.ONE_VS_ALL`` or ``"one-vs-all"``.
-
-    Example (binary case):
-        >>> import torch
-        >>> from torchmetrics import HingeLoss
-        >>> target = torch.tensor([0, 1, 1])
-        >>> preds = torch.tensor([-2.2, 2.4, 0.1])
-        >>> hinge = HingeLoss()
-        >>> hinge(preds, target)
-        tensor(0.3000)
-
-    Example (default / multiclass case):
-        >>> target = torch.tensor([0, 1, 2])
-        >>> preds = torch.tensor([[-1.0, 0.9, 0.2], [0.5, -1.1, 0.8], [2.2, -0.5, 0.3]])
-        >>> hinge = HingeLoss()
-        >>> hinge(preds, target)
-        tensor(2.9000)
-
-    Example (multiclass example, one vs all mode):
-        >>> target = torch.tensor([0, 1, 2])
-        >>> preds = torch.tensor([[-1.0, 0.9, 0.2], [0.5, -1.1, 0.8], [2.2, -0.5, 0.3]])
-        >>> hinge = HingeLoss(multiclass_mode="one-vs-all")
-        >>> hinge(preds, target)
-        tensor([2.2333, 1.5000, 1.2333])
+            If ``empty_target_action`` is not one of ``error``, ``skip``, ``neg`` or ``pos``.
+        ValueError:
+            If ``ignore_index`` is not `None` or an integer.
+        ValueError:
+            If ``top_k`` parameter is not `None` or an integer larger than 0.
 
+    Example:
+        >>> from torch import tensor
+        >>> from torchmetrics.retrieval import RetrievalNormalizedDCG
+        >>> indexes = tensor([0, 0, 0, 1, 1, 1, 1])
+        >>> preds = tensor([0.2, 0.3, 0.5, 0.1, 0.3, 0.5, 0.2])
+        >>> target = tensor([False, False, True, False, True, False, True])
+        >>> ndcg = RetrievalNormalizedDCG()
+        >>> ndcg(preds, target, indexes=indexes)
+        tensor(0.8467)
     """
-    is_differentiable: bool = True
-    higher_is_better: bool = False
+
+    is_differentiable: bool = False
+    higher_is_better: bool = True
     full_state_update: bool = False
-    measure: Tensor
-    total: Tensor
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
 
     def __init__(
         self,
-        squared: bool = False,
-        multiclass_mode: Optional[Union[str, MulticlassMode]] = None,
+        empty_target_action: str = "neg",
+        ignore_index: Optional[int] = None,
+        top_k: Optional[int] = None,
         **kwargs: Any,
     ) -> None:
-        super().__init__(**kwargs)
-
-        self.add_state("measure", default=tensor(0.0), dist_reduce_fx="sum")
-        self.add_state("total", default=tensor(0), dist_reduce_fx="sum")
-
-        if multiclass_mode not in (None, MulticlassMode.CRAMMER_SINGER, MulticlassMode.ONE_VS_ALL):
-            raise ValueError(
-                "The `multiclass_mode` should be either None / 'crammer-singer' / MulticlassMode.CRAMMER_SINGER"
-                "(default) or 'one-vs-all' / MulticlassMode.ONE_VS_ALL,"
-                f" got {multiclass_mode}."
-            )
-
-        self.squared = squared
-        self.multiclass_mode = multiclass_mode
-
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        measure, total = _hinge_update(preds, target, squared=self.squared, multiclass_mode=self.multiclass_mode)
-
-        self.measure = measure + self.measure
-        self.total = total + self.total
-
-    def compute(self) -> Tensor:
-        return _hinge_compute(self.measure, self.total)
+        super().__init__(
+            empty_target_action=empty_target_action,
+            ignore_index=ignore_index,
+            **kwargs,
+        )
+
+        if top_k is not None and not (isinstance(top_k, int) and top_k > 0):
+            raise ValueError("`top_k` has to be a positive integer or None")
+        self.top_k = top_k
+        self.allow_non_binary_target = True
+
+    def _metric(self, preds: Tensor, target: Tensor) -> Tensor:
+        return retrieval_normalized_dcg(preds, target, top_k=self.top_k)
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> import torch
+            >>> from torchmetrics.retrieval import RetrievalNormalizedDCG
+            >>> # Example plotting a single value
+            >>> metric = RetrievalNormalizedDCG()
+            >>> metric.update(torch.rand(10,), torch.randint(2, (10,)), indexes=torch.randint(2,(10,)))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> import torch
+            >>> from torchmetrics.retrieval import RetrievalNormalizedDCG
+            >>> # Example plotting multiple values
+            >>> metric = RetrievalNormalizedDCG()
+            >>> values = []
+            >>> for _ in range(10):
+            ...     values.append(metric(torch.rand(10,), torch.randint(2, (10,)), indexes=torch.randint(2,(10,))))
+            >>> fig, ax = metric.plot(values)
+        """
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/classification/jaccard.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/image/uqi.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,128 +1,154 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, Optional
+from typing import Any, List, Optional, Sequence, Union
 
-import torch
 from torch import Tensor
+from typing_extensions import Literal
 
-from torchmetrics.classification.confusion_matrix import ConfusionMatrix
-from torchmetrics.functional.classification.jaccard import _jaccard_from_confmat
+from torchmetrics.functional.image.uqi import _uqi_compute, _uqi_update
+from torchmetrics.metric import Metric
+from torchmetrics.utilities import rank_zero_warn
+from torchmetrics.utilities.data import dim_zero_cat
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["UniversalImageQualityIndex.plot"]
 
-class JaccardIndex(ConfusionMatrix):
-    r"""Computes Intersection over union, or `Jaccard index`_:
 
-    .. math:: J(A,B) = \frac{|A\cap B|}{|A\cup B|}
+class UniversalImageQualityIndex(Metric):
+    """Compute Universal Image Quality Index (UniversalImageQualityIndex_).
 
-    Where: :math:`A` and :math:`B` are both tensors of the same size, containing integer class values.
-    They may be subject to conversion from input data (see description below). Note that it is different from box IoU.
+    As input to ``forward`` and ``update`` the metric accepts the following input
 
-    Works with binary, multiclass and multi-label data.
-    Accepts probabilities from a model output or integer class values in prediction.
-    Works with multi-dimensional preds and target.
+    - ``preds`` (:class:`~torch.Tensor`): Predictions from model of shape ``(N,C,H,W)``
+    - ``target`` (:class:`~torch.Tensor`): Ground truth values of shape ``(N,C,H,W)``
 
-    Forward accepts
+    As output of `forward` and `compute` the metric returns the following output
 
-    - ``preds`` (float or long tensor): ``(N, ...)`` or ``(N, C, ...)`` where C is the number of classes
-    - ``target`` (long tensor): ``(N, ...)``
-
-    If preds and target are the same shape and preds is a float tensor, we use the ``self.threshold`` argument
-    to convert into integer labels. This is the case for binary and multi-label probabilities.
-
-    If preds has an extra dimension as in the case of multi-class scores we perform an argmax on ``dim=1``.
+    - ``uiqi`` (:class:`~torch.Tensor`): if ``reduction!='none'`` returns float scalar tensor with average UIQI value
+      over sample else returns tensor of shape ``(N,)`` with UIQI values per sample
 
     Args:
-        num_classes: Number of classes in the dataset.
-        average:
-            Defines the reduction that is applied. Should be one of the following:
-
-            - ``'macro'`` [default]: Calculate the metric for each class separately, and average the
-              metrics across classes (with equal weights for each class).
-            - ``'micro'``: Calculate the metric globally, across all samples and classes.
-            - ``'weighted'``: Calculate the metric for each class separately, and average the
-              metrics across classes, weighting each class by its support (``tp + fn``).
-            - ``'none'`` or ``None``: Calculate the metric for each class separately, and return
-              the metric for every class. Note that if a given class doesn't occur in the
-              `preds` or `target`, the value for the class will be ``nan``.
-
-        ignore_index: optional int specifying a target class to ignore. If given, this class index does not contribute
-            to the returned score, regardless of reduction method. Has no effect if given an int that is not in the
-            range [0, num_classes-1]. By default, no index is ignored, and all classes are used.
-        absent_score: score to use for an individual class, if no instances of the class index were present in
-            ``preds`` AND no instances of the class index were present in ``target``. For example, if we have 3 classes,
-            [0, 0] for ``preds``, and [0, 2] for ``target``, then class 1 would be assigned the `absent_score`.
-        threshold: Threshold value for binary or multi-label probabilities.
-        multilabel: determines if data is multilabel or not.
+        kernel_size: size of the gaussian kernel
+        sigma: Standard deviation of the gaussian kernel
+        reduction: a method to reduce metric score over labels.
+
+            - ``'elementwise_mean'``: takes the mean (default)
+            - ``'sum'``: takes the sum
+            - ``'none'`` or ``None``: no reduction will be applied
+
+        data_range: Range of the image. If ``None``, it is determined from the image (max - min)
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
-    Example:
-        >>> from torchmetrics import JaccardIndex
-        >>> target = torch.randint(0, 2, (10, 25, 25))
-        >>> pred = torch.tensor(target)
-        >>> pred[2:5, 7:13, 9:15] = 1 - pred[2:5, 7:13, 9:15]
-        >>> jaccard = JaccardIndex(num_classes=2)
-        >>> jaccard(pred, target)
-        tensor(0.9660)
+    Return:
+        Tensor with UniversalImageQualityIndex score
 
+    Example:
+        >>> import torch
+        >>> from torchmetrics.image import UniversalImageQualityIndex
+        >>> preds = torch.rand([16, 1, 16, 16])
+        >>> target = preds * 0.75
+        >>> uqi = UniversalImageQualityIndex()
+        >>> uqi(preds, target)
+        tensor(0.9216)
     """
-    is_differentiable: bool = False
+
+    is_differentiable: bool = True
     higher_is_better: bool = True
     full_state_update: bool = False
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
+
+    preds: List[Tensor]
+    target: List[Tensor]
 
     def __init__(
         self,
-        num_classes: int,
-        average: Optional[str] = "macro",
-        ignore_index: Optional[int] = None,
-        absent_score: float = 0.0,
-        threshold: float = 0.5,
-        multilabel: bool = False,
+        kernel_size: Sequence[int] = (11, 11),
+        sigma: Sequence[float] = (1.5, 1.5),
+        reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
+        data_range: Optional[float] = None,
         **kwargs: Any,
     ) -> None:
-        kwargs["normalize"] = kwargs.get("normalize")
-
-        super().__init__(
-            num_classes=num_classes,
-            threshold=threshold,
-            multilabel=multilabel,
-            **kwargs,
+        super().__init__(**kwargs)
+        rank_zero_warn(
+            "Metric `UniversalImageQualityIndex` will save all targets and"
+            " predictions in buffer. For large datasets this may lead"
+            " to large memory footprint."
         )
-        self.average = average
-        self.ignore_index = ignore_index
-        self.absent_score = absent_score
 
-    def compute(self) -> Tensor:
-        """Computes intersection over union (IoU)"""
+        self.add_state("preds", default=[], dist_reduce_fx="cat")
+        self.add_state("target", default=[], dist_reduce_fx="cat")
+        self.kernel_size = kernel_size
+        self.sigma = sigma
+        self.data_range = data_range
+        self.reduction = reduction
+
+    def update(self, preds: Tensor, target: Tensor) -> None:
+        """Update state with predictions and targets."""
+        preds, target = _uqi_update(preds, target)
+        self.preds.append(preds)
+        self.target.append(target)
 
-        if self.multilabel:
-            return torch.stack(
-                [
-                    _jaccard_from_confmat(
-                        confmat,
-                        2,
-                        self.average,
-                        self.ignore_index,
-                        self.absent_score,
-                    )[1]
-                    for confmat in self.confmat
-                ]
-            )
-        else:
-            return _jaccard_from_confmat(
-                self.confmat,
-                self.num_classes,
-                self.average,
-                self.ignore_index,
-                self.absent_score,
-            )
+    def compute(self) -> Tensor:
+        """Compute explained variance over state."""
+        preds = dim_zero_cat(self.preds)
+        target = dim_zero_cat(self.target)
+        return _uqi_compute(preds, target, self.kernel_size, self.sigma, self.reduction, self.data_range)
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> import torch
+            >>> from torchmetrics.image import UniversalImageQualityIndex
+            >>> preds = torch.rand([16, 1, 16, 16])
+            >>> target = preds * 0.75
+            >>> metric = UniversalImageQualityIndex()
+            >>> metric.update(preds, target)
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> import torch
+            >>> from torchmetrics.image import UniversalImageQualityIndex
+            >>> preds = torch.rand([16, 1, 16, 16])
+            >>> target = preds * 0.75
+            >>> metric = UniversalImageQualityIndex()
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(preds, target))
+            >>> fig_, ax_ = metric.plot(values)
+        """
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/classification/matthews_corrcoef.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/fall_out.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,95 +1,63 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any
+from typing import Optional
 
 import torch
-from torch import Tensor
+from torch import Tensor, tensor
 
-from torchmetrics.functional.classification.matthews_corrcoef import (
-    _matthews_corrcoef_compute,
-    _matthews_corrcoef_update,
-)
-from torchmetrics.metric import Metric
+from torchmetrics.utilities.checks import _check_retrieval_functional_inputs
 
 
-class MatthewsCorrCoef(Metric):
-    r"""Calculates `Matthews correlation coefficient`_ that measures the general correlation or quality of a classification.
+def retrieval_fall_out(preds: Tensor, target: Tensor, top_k: Optional[int] = None) -> Tensor:
+    """Compute the Fall-out for information retrieval, as explained in `IR Fall-out`_.
 
-    In the binary case it is defined as:
+    Fall-out is the fraction of non-relevant documents retrieved among all the non-relevant documents.
 
-    .. math::
-        MCC = \frac{TP*TN - FP*FN}{\sqrt{(TP+FP)*(TP+FN)*(TN+FP)*(TN+FN)}}
+    ``preds`` and ``target`` should be of the same shape and live on the same device. If no ``target`` is ``True``,
+    ``0`` is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
+    otherwise an error is raised. If you want to measure Fall-out@K, ``top_k`` must be a positive integer.
 
-    where TP, TN, FP and FN are respectively the true postitives, true negatives,
-    false positives and false negatives. Also works in the case of multi-label or
-    multi-class input.
-
-    Note:
-        This metric produces a multi-dimensional output, so it can not be directly logged.
-
-    Forward accepts
-
-    - ``preds`` (float or long tensor): ``(N, ...)`` or ``(N, C, ...)`` where C is the number of classes
-    - ``target`` (long tensor): ``(N, ...)``
+    Args:
+        preds: estimated probabilities of each document to be relevant.
+        target: ground truth about each document being relevant or not.
+        top_k: consider only the top k elements (default: ``None``, which considers them all)
+
+    Returns:
+        A single-value tensor with the fall-out (at ``top_k``) of the predictions ``preds`` w.r.t. the labels ``target``
+
+    Raises:
+        ValueError:
+            If ``top_k`` parameter is not `None` or an integer larger than 0
 
-    If preds and target are the same shape and preds is a float tensor, we use the ``self.threshold`` argument
-    to convert into integer labels. This is the case for binary and multi-label probabilities.
+    Example:
+        >>> from  torchmetrics.functional import retrieval_fall_out
+        >>> preds = tensor([0.2, 0.3, 0.5])
+        >>> target = tensor([True, False, True])
+        >>> retrieval_fall_out(preds, target, top_k=2)
+        tensor(1.)
+    """
+    preds, target = _check_retrieval_functional_inputs(preds, target)
 
-    If preds has an extra dimension as in the case of multi-class scores we perform an argmax on ``dim=1``.
+    top_k = preds.shape[-1] if top_k is None else top_k
 
-    Args:
-        num_classes: Number of classes in the dataset.
-        threshold: Threshold value for binary or multi-label probabilites.
+    if not (isinstance(top_k, int) and top_k > 0):
+        raise ValueError("`top_k` has to be a positive integer or None")
 
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+    target = 1 - target  # we want to compute the probability of getting a non-relevant doc among all non-relevant docs
 
-    Example:
-        >>> from torchmetrics import MatthewsCorrCoef
-        >>> target = torch.tensor([1, 1, 0, 0])
-        >>> preds = torch.tensor([0, 1, 0, 0])
-        >>> matthews_corrcoef = MatthewsCorrCoef(num_classes=2)
-        >>> matthews_corrcoef(preds, target)
-        tensor(0.5774)
+    if not target.sum():
+        return tensor(0.0, device=preds.device)
 
-    """
-    is_differentiable: bool = False
-    higher_is_better: bool = True
-    full_state_update: bool = False
-    confmat: Tensor
-
-    def __init__(
-        self,
-        num_classes: int,
-        threshold: float = 0.5,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__(**kwargs)
-        self.num_classes = num_classes
-        self.threshold = threshold
-
-        self.add_state("confmat", default=torch.zeros(num_classes, num_classes), dist_reduce_fx="sum")
-
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
-
-        Args:
-            preds: Predictions from model
-            target: Ground truth values
-        """
-        confmat = _matthews_corrcoef_update(preds, target, self.num_classes, self.threshold)
-        self.confmat += confmat
-
-    def compute(self) -> Tensor:
-        """Computes matthews correlation coefficient."""
-        return _matthews_corrcoef_compute(self.confmat)
+    relevant = target[torch.argsort(preds, dim=-1, descending=True)][:top_k].sum().float()
+    return relevant / target.sum()
```

### Comparing `torchmetrics-0.9.3/torchmetrics/classification/precision_recall_curve.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/wrappers/minmax.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,139 +1,153 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, List, Optional, Tuple, Union
+
+from typing import Any, Dict, Optional, Sequence, Union
 
 import torch
 from torch import Tensor
 
-from torchmetrics.functional.classification.precision_recall_curve import (
-    _precision_recall_curve_compute,
-    _precision_recall_curve_update,
-)
 from torchmetrics.metric import Metric
-from torchmetrics.utilities import rank_zero_warn
-from torchmetrics.utilities.data import dim_zero_cat
-
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
-class PrecisionRecallCurve(Metric):
-    """Computes precision-recall pairs for different thresholds. Works for both binary and multiclass problems. In
-    the case of multiclass, the values will be calculated based on a one-vs-the-rest approach.
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["MinMaxMetric.plot"]
 
-    Forward accepts
 
-    - ``preds`` (float tensor): ``(N, ...)`` (binary) or ``(N, C, ...)`` (multiclass) tensor
-      with probabilities, where C is the number of classes.
+class MinMaxMetric(Metric):
+    """Wrapper metric that tracks both the minimum and maximum of a scalar/tensor across an experiment.
 
-    - ``target`` (long tensor): ``(N, ...)`` or ``(N, C, ...)`` with integer labels
+    The min/max value will be updated each time ``.compute`` is called.
 
     Args:
-        num_classes: integer with number of classes for multi-label and multiclass problems.
-            Should be set to ``None`` for binary problems
-        pos_label: integer determining the positive class. Default is ``None`` which for binary problem is translated
-            to 1. For multiclass problems this argument should not be set as we iteratively change it in the range
-            ``[0, num_classes-1]``
-
+        base_metric:
+            The metric of which you want to keep track of its maximum and minimum values.
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
-    Example (binary case):
-        >>> from torchmetrics import PrecisionRecallCurve
-        >>> pred = torch.tensor([0, 0.1, 0.8, 0.4])
-        >>> target = torch.tensor([0, 1, 1, 0])
-        >>> pr_curve = PrecisionRecallCurve(pos_label=1)
-        >>> precision, recall, thresholds = pr_curve(pred, target)
-        >>> precision
-        tensor([0.6667, 0.5000, 1.0000, 1.0000])
-        >>> recall
-        tensor([1.0000, 0.5000, 0.5000, 0.0000])
-        >>> thresholds
-        tensor([0.1000, 0.4000, 0.8000])
-
-    Example (multiclass case):
-        >>> pred = torch.tensor([[0.75, 0.05, 0.05, 0.05, 0.05],
-        ...                      [0.05, 0.75, 0.05, 0.05, 0.05],
-        ...                      [0.05, 0.05, 0.75, 0.05, 0.05],
-        ...                      [0.05, 0.05, 0.05, 0.75, 0.05]])
-        >>> target = torch.tensor([0, 1, 3, 2])
-        >>> pr_curve = PrecisionRecallCurve(num_classes=5)
-        >>> precision, recall, thresholds = pr_curve(pred, target)
-        >>> precision
-        [tensor([1., 1.]), tensor([1., 1.]), tensor([0.2500, 0.0000, 1.0000]),
-         tensor([0.2500, 0.0000, 1.0000]), tensor([0., 1.])]
-        >>> recall
-        [tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0., 0.]), tensor([1., 0., 0.]), tensor([nan, 0.])]
-        >>> thresholds
-        [tensor(0.7500), tensor(0.7500), tensor([0.0500, 0.7500]), tensor([0.0500, 0.7500]), tensor(0.0500)]
+    Raises:
+        ValueError
+            If ``base_metric` argument is not a subclasses instance of ``torchmetrics.Metric``
+
+    Example::
+        >>> import torch
+        >>> from torchmetrics.wrappers import MinMaxMetric
+        >>> from torchmetrics.classification import BinaryAccuracy
+        >>> from pprint import pprint
+        >>> base_metric = BinaryAccuracy()
+        >>> minmax_metric = MinMaxMetric(base_metric)
+        >>> preds_1 = torch.Tensor([[0.1, 0.9], [0.2, 0.8]])
+        >>> preds_2 = torch.Tensor([[0.9, 0.1], [0.2, 0.8]])
+        >>> labels = torch.Tensor([[0, 1], [0, 1]]).long()
+        >>> pprint(minmax_metric(preds_1, labels))
+        {'max': tensor(1.), 'min': tensor(1.), 'raw': tensor(1.)}
+        >>> pprint(minmax_metric.compute())
+        {'max': tensor(1.), 'min': tensor(1.), 'raw': tensor(1.)}
+        >>> minmax_metric.update(preds_2, labels)
+        >>> pprint(minmax_metric.compute())
+        {'max': tensor(1.), 'min': tensor(0.7500), 'raw': tensor(0.7500)}
     """
 
-    is_differentiable: bool = False
-    higher_is_better: Optional[bool] = None
-    full_state_update: bool = False
-    preds: List[Tensor]
-    target: List[Tensor]
+    full_state_update: Optional[bool] = True
+    min_val: Tensor
+    max_val: Tensor
 
     def __init__(
         self,
-        num_classes: Optional[int] = None,
-        pos_label: Optional[int] = None,
+        base_metric: Metric,
         **kwargs: Any,
     ) -> None:
         super().__init__(**kwargs)
+        if not isinstance(base_metric, Metric):
+            raise ValueError(
+                f"Expected base metric to be an instance of `torchmetrics.Metric` but received {base_metric}"
+            )
+        self._base_metric = base_metric
+        self.min_val = torch.tensor(float("inf"))
+        self.max_val = torch.tensor(float("-inf"))
+
+    def update(self, *args: Any, **kwargs: Any) -> None:
+        """Update the underlying metric."""
+        self._base_metric.update(*args, **kwargs)
 
-        self.num_classes = num_classes
-        self.pos_label = pos_label
-
-        self.add_state("preds", default=[], dist_reduce_fx="cat")
-        self.add_state("target", default=[], dist_reduce_fx="cat")
+    def compute(self) -> Dict[str, Tensor]:
+        """Compute the underlying metric as well as max and min values for this metric.
 
-        rank_zero_warn(
-            "Metric `PrecisionRecallCurve` will save all targets and predictions in buffer."
-            " For large datasets this may lead to large memory footprint."
-        )
-
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
-
-        Args:
-            preds: Predictions from model
-            target: Ground truth values
+        Returns a dictionary that consists of the computed value (``raw``), as well as the minimum (``min``) and maximum
+        (``max``) values.
         """
-        preds, target, num_classes, pos_label = _precision_recall_curve_update(
-            preds, target, self.num_classes, self.pos_label
-        )
-        self.preds.append(preds)
-        self.target.append(target)
-        self.num_classes = num_classes
-        self.pos_label = pos_label
+        val = self._base_metric.compute()
+        if not self._is_suitable_val(val):
+            raise RuntimeError(
+                f"Returned value from base metric should be a scalar (int, float or tensor of size 1, but got {val}"
+            )
+        self.max_val = val if self.max_val.to(val.device) < val else self.max_val.to(val.device)
+        self.min_val = val if self.min_val.to(val.device) > val else self.min_val.to(val.device)
+        return {"raw": val, "max": self.max_val, "min": self.min_val}
+
+    def reset(self) -> None:
+        """Set ``max_val`` and ``min_val`` to the initialization bounds and resets the base metric."""
+        super().reset()
+        self._base_metric.reset()
+
+    @staticmethod
+    def _is_suitable_val(val: Union[int, float, Tensor]) -> bool:
+        """Check whether min/max is a scalar value."""
+        if isinstance(val, (int, float)):
+            return True
+        if isinstance(val, Tensor):
+            return val.numel() == 1
+        return False
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
 
-    def compute(self) -> Union[Tuple[Tensor, Tensor, Tensor], Tuple[List[Tensor], List[Tensor], List[Tensor]]]:
-        """Compute the precision-recall curve.
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
 
         Returns:
-            3-element tuple containing
+            Figure and Axes object
 
-            precision:
-                tensor where element ``i`` is the precision of predictions with
-                ``score >= thresholds[i]`` and the last element is 1.
-                If multiclass, this is a list of such tensors, one for each class.
-            recall:
-                tensor where element ``i`` is the recall of predictions with
-                ``score >= thresholds[i]`` and the last element is 0.
-                If multiclass, this is a list of such tensors, one for each class.
-            thresholds:
-                Thresholds used for computing precision/recall scores
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> import torch
+            >>> from torchmetrics.wrappers import MinMaxMetric
+            >>> from torchmetrics.classification import BinaryAccuracy
+            >>> metric = MinMaxMetric(BinaryAccuracy())
+            >>> metric.update(torch.randint(2, (20,)), torch.randint(2, (20,)))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> import torch
+            >>> from torchmetrics.wrappers import MinMaxMetric
+            >>> from torchmetrics.classification import BinaryAccuracy
+            >>> metric = MinMaxMetric(BinaryAccuracy())
+            >>> values = [ ]
+            >>> for _ in range(3):
+            ...     values.append(metric(torch.randint(2, (20,)), torch.randint(2, (20,))))
+            >>> fig_, ax_ = metric.plot(values)
         """
-        preds = dim_zero_cat(self.preds)
-        target = dim_zero_cat(self.target)
-        if not self.num_classes:
-            raise ValueError(f"`num_classes` bas to be positive number, but got {self.num_classes}")
-        return _precision_recall_curve_compute(preds, target, self.num_classes, self.pos_label)
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/classification/roc.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/wrappers/classwise.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,158 +1,165 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, List, Optional, Tuple, Union
+from typing import Any, Callable, Dict, List, Optional, Sequence, Union
 
-import torch
 from torch import Tensor
 
-from torchmetrics.functional.classification.roc import _roc_compute, _roc_update
-from torchmetrics.metric import Metric
-from torchmetrics.utilities import rank_zero_warn
-from torchmetrics.utilities.data import dim_zero_cat
+from torchmetrics import Metric
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["ClasswiseWrapper.plot"]
 
-class ROC(Metric):
-    """Computes the Receiver Operating Characteristic (ROC). Works for both binary, multiclass and multilabel
-    problems. In the case of multiclass, the values will be calculated based on a one-vs-the-rest approach.
 
-    Forward accepts
+class ClasswiseWrapper(Metric):
+    """Wrapper metric for altering the output of classification metrics.
 
-    - ``preds`` (float tensor): ``(N, ...)`` (binary) or ``(N, C, ...)`` (multiclass/multilabel) tensor
-      with probabilities, where C is the number of classes/labels.
-
-    - ``target`` (long tensor): ``(N, ...)`` or ``(N, C, ...)`` with integer labels
-
-    .. note::
-        If either the positive class or negative class is completly missing in the target tensor,
-        the roc values are not well-defined in this case and a tensor of zeros will be returned (either fpr
-        or tpr depending on what class is missing) together with a warning.
+    This metric works together with classification metrics that returns multiple values (one value per class) such that
+    label information can be automatically included in the output.
 
     Args:
-        num_classes: integer with number of classes for multi-label and multiclass problems.
-            Should be set to ``None`` for binary problems
-        pos_label: integer determining the positive class. Default is ``None`` which for binary problem is translated
-            to 1. For multiclass problems this argument should not be set as we iteratively change it in the range
-            ``[0,num_classes-1]``
-
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
-
-    Example (binary case):
-        >>> from torchmetrics import ROC
-        >>> pred = torch.tensor([0, 1, 2, 3])
-        >>> target = torch.tensor([0, 1, 1, 1])
-        >>> roc = ROC(pos_label=1)
-        >>> fpr, tpr, thresholds = roc(pred, target)
-        >>> fpr
-        tensor([0., 0., 0., 0., 1.])
-        >>> tpr
-        tensor([0.0000, 0.3333, 0.6667, 1.0000, 1.0000])
-        >>> thresholds
-        tensor([4, 3, 2, 1, 0])
-
-    Example (multiclass case):
-        >>> pred = torch.tensor([[0.75, 0.05, 0.05, 0.05],
-        ...                      [0.05, 0.75, 0.05, 0.05],
-        ...                      [0.05, 0.05, 0.75, 0.05],
-        ...                      [0.05, 0.05, 0.05, 0.75]])
-        >>> target = torch.tensor([0, 1, 3, 2])
-        >>> roc = ROC(num_classes=4)
-        >>> fpr, tpr, thresholds = roc(pred, target)
-        >>> fpr
-        [tensor([0., 0., 1.]), tensor([0., 0., 1.]), tensor([0.0000, 0.3333, 1.0000]), tensor([0.0000, 0.3333, 1.0000])]
-        >>> tpr
-        [tensor([0., 1., 1.]), tensor([0., 1., 1.]), tensor([0., 0., 1.]), tensor([0., 0., 1.])]
-        >>> thresholds
-        [tensor([1.7500, 0.7500, 0.0500]),
-         tensor([1.7500, 0.7500, 0.0500]),
-         tensor([1.7500, 0.7500, 0.0500]),
-         tensor([1.7500, 0.7500, 0.0500])]
-
-    Example (multilabel case):
-        >>> pred = torch.tensor([[0.8191, 0.3680, 0.1138],
-        ...                      [0.3584, 0.7576, 0.1183],
-        ...                      [0.2286, 0.3468, 0.1338],
-        ...                      [0.8603, 0.0745, 0.1837]])
-        >>> target = torch.tensor([[1, 1, 0], [0, 1, 0], [0, 0, 0], [0, 1, 1]])
-        >>> roc = ROC(num_classes=3, pos_label=1)
-        >>> fpr, tpr, thresholds = roc(pred, target)
-        >>> fpr
-        [tensor([0.0000, 0.3333, 0.3333, 0.6667, 1.0000]),
-         tensor([0., 0., 0., 1., 1.]),
-         tensor([0.0000, 0.0000, 0.3333, 0.6667, 1.0000])]
-        >>> tpr
-        [tensor([0., 0., 1., 1., 1.]),
-         tensor([0.0000, 0.3333, 0.6667, 0.6667, 1.0000]),
-         tensor([0., 1., 1., 1., 1.])]
-        >>> thresholds
-        [tensor([1.8603, 0.8603, 0.8191, 0.3584, 0.2286]),
-         tensor([1.7576, 0.7576, 0.3680, 0.3468, 0.0745]),
-         tensor([1.1837, 0.1837, 0.1338, 0.1183, 0.1138])]
+        metric: base metric that should be wrapped. It is assumed that the metric outputs a single
+            tensor that is split along the first dimension.
+        labels: list of strings indicating the different classes.
+
+    Example:
+        >>> import torch
+        >>> _ = torch.manual_seed(42)
+        >>> from torchmetrics.wrappers import ClasswiseWrapper
+        >>> from torchmetrics.classification import MulticlassAccuracy
+        >>> metric = ClasswiseWrapper(MulticlassAccuracy(num_classes=3, average=None))
+        >>> preds = torch.randn(10, 3).softmax(dim=-1)
+        >>> target = torch.randint(3, (10,))
+        >>> metric(preds, target)  # doctest: +NORMALIZE_WHITESPACE
+        {'multiclassaccuracy_0': tensor(0.5000),
+        'multiclassaccuracy_1': tensor(0.7500),
+        'multiclassaccuracy_2': tensor(0.)}
+
+    Example (labels as list of strings):
+        >>> from torchmetrics.wrappers import ClasswiseWrapper
+        >>> from torchmetrics.classification import MulticlassAccuracy
+        >>> metric = ClasswiseWrapper(
+        ...    MulticlassAccuracy(num_classes=3, average=None),
+        ...    labels=["horse", "fish", "dog"]
+        ... )
+        >>> preds = torch.randn(10, 3).softmax(dim=-1)
+        >>> target = torch.randint(3, (10,))
+        >>> metric(preds, target)  # doctest: +NORMALIZE_WHITESPACE
+        {'multiclassaccuracy_horse': tensor(0.3333),
+        'multiclassaccuracy_fish': tensor(0.6667),
+        'multiclassaccuracy_dog': tensor(0.)}
+
+    Example (in metric collection):
+        >>> from torchmetrics import MetricCollection
+        >>> from torchmetrics.wrappers import ClasswiseWrapper
+        >>> from torchmetrics.classification import MulticlassAccuracy, MulticlassRecall
+        >>> labels = ["horse", "fish", "dog"]
+        >>> metric = MetricCollection(
+        ...     {'multiclassaccuracy': ClasswiseWrapper(MulticlassAccuracy(num_classes=3, average=None), labels),
+        ...     'multiclassrecall': ClasswiseWrapper(MulticlassRecall(num_classes=3, average=None), labels)}
+        ... )
+        >>> preds = torch.randn(10, 3).softmax(dim=-1)
+        >>> target = torch.randint(3, (10,))
+        >>> metric(preds, target)  # doctest: +NORMALIZE_WHITESPACE
+        {'multiclassaccuracy_horse': tensor(0.),
+         'multiclassaccuracy_fish': tensor(0.3333),
+         'multiclassaccuracy_dog': tensor(0.4000),
+         'multiclassrecall_horse': tensor(0.),
+         'multiclassrecall_fish': tensor(0.3333),
+         'multiclassrecall_dog': tensor(0.4000)}
     """
 
-    is_differentiable: bool = False
-    higher_is_better: Optional[bool] = None
-    full_state_update: bool = False
-    preds: List[Tensor]
-    target: List[Tensor]
-
-    def __init__(
-        self,
-        num_classes: Optional[int] = None,
-        pos_label: Optional[int] = None,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__(**kwargs)
-
-        self.num_classes = num_classes
-        self.pos_label = pos_label
-
-        self.add_state("preds", default=[], dist_reduce_fx="cat")
-        self.add_state("target", default=[], dist_reduce_fx="cat")
-
-        rank_zero_warn(
-            "Metric `ROC` will save all targets and predictions in buffer."
-            " For large datasets this may lead to large memory footprint."
-        )
-
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
+    def __init__(self, metric: Metric, labels: Optional[List[str]] = None) -> None:
+        super().__init__()
+        if not isinstance(metric, Metric):
+            raise ValueError(f"Expected argument `metric` to be an instance of `torchmetrics.Metric` but got {metric}")
+        if labels is not None and not (isinstance(labels, list) and all(isinstance(lab, str) for lab in labels)):
+            raise ValueError(f"Expected argument `labels` to either be `None` or a list of strings but got {labels}")
+        self.metric = metric
+        self.labels = labels
+        self._update_count = 1
+
+    def _convert(self, x: Tensor) -> Dict[str, Any]:
+        name = self.metric.__class__.__name__.lower()
+        if self.labels is None:
+            return {f"{name}_{i}": val for i, val in enumerate(x)}
+        return {f"{name}_{lab}": val for lab, val in zip(self.labels, x)}
+
+    def forward(self, *args: Any, **kwargs: Any) -> Any:
+        """Calculate on batch and accumulate to global state."""
+        return self._convert(self.metric(*args, **kwargs))
+
+    def update(self, *args: Any, **kwargs: Any) -> None:
+        """Update state."""
+        self.metric.update(*args, **kwargs)
+
+    def compute(self) -> Dict[str, Tensor]:
+        """Compute metric."""
+        return self._convert(self.metric.compute())
+
+    def reset(self) -> None:
+        """Reset metric."""
+        self.metric.reset()
+
+    def _wrap_update(self, update: Callable) -> Callable:
+        """Overwrite to do nothing."""
+        return update
+
+    def _wrap_compute(self, compute: Callable) -> Callable:
+        """Overwrite to do nothing."""
+        return compute
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
 
         Args:
-            preds: Predictions from model
-            target: Ground truth values
-        """
-        preds, target, num_classes, pos_label = _roc_update(preds, target, self.num_classes, self.pos_label)
-        self.preds.append(preds)
-        self.target.append(target)
-        self.num_classes = num_classes
-        self.pos_label = pos_label
-
-    def compute(self) -> Union[Tuple[Tensor, Tensor, Tensor], Tuple[List[Tensor], List[Tensor], List[Tensor]]]:
-        """Compute the receiver operating characteristic.
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
 
         Returns:
-            3-element tuple containing
+            Figure and Axes object
 
-            fpr: tensor with false positive rates.
-                If multiclass, this is a list of such tensors, one for each class.
-            tpr: tensor with true positive rates.
-                If multiclass, this is a list of such tensors, one for each class.
-            thresholds:
-                thresholds used for computing false- and true-positive rates
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> import torch
+            >>> from torchmetrics.wrappers import ClasswiseWrapper
+            >>> from torchmetrics.classification import MulticlassAccuracy
+            >>> metric = ClasswiseWrapper(MulticlassAccuracy(num_classes=3, average=None))
+            >>> metric.update(torch.randint(3, (20,)), torch.randint(3, (20,)))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> import torch
+            >>> from torchmetrics.wrappers import ClasswiseWrapper
+            >>> from torchmetrics.classification import MulticlassAccuracy
+            >>> metric = ClasswiseWrapper(MulticlassAccuracy(num_classes=3, average=None))
+            >>> values = [ ]
+            >>> for _ in range(3):
+            ...     values.append(metric(torch.randint(3, (20,)), torch.randint(3, (20,))))
+            >>> fig_, ax_ = metric.plot(values)
         """
-        preds = dim_zero_cat(self.preds)
-        target = dim_zero_cat(self.target)
-        if not self.num_classes:
-            raise ValueError(f"`num_classes` bas to be positive number, but got {self.num_classes}")
-        return _roc_compute(preds, target, self.num_classes, self.pos_label)
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/collections.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/collections.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,33 +1,37 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+# this is just a bypass for this module name collision with build-in one
+from collections import OrderedDict
 from copy import deepcopy
 from typing import Any, Dict, Hashable, Iterable, List, Optional, Sequence, Tuple, Union
 
 import torch
 from torch import Tensor
 from torch.nn import Module, ModuleDict
 
 from torchmetrics.metric import Metric
 from torchmetrics.utilities import rank_zero_warn
 from torchmetrics.utilities.data import _flatten_dict, allclose
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE, plot_single_or_multi_val
 
-# this is just a bypass for this module name collision with build-in one
-from torchmetrics.utilities.imports import OrderedDict
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["MetricCollection.plot", "MetricCollection.plot_all"]
 
 
 class MetricCollection(ModuleDict):
     """MetricCollection class can be used to chain metrics that have the same call pattern into one single class.
 
     Args:
         metrics: One of the following
@@ -51,14 +55,22 @@
             by checking if they belong to the same **compute group**. All metrics in a compute group share the same
             metric state and are therefore only different in their compute step e.g. accuracy, precision and recall
             can all be computed from the true positives/negatives and false positives/negatives. By default,
             this argument is ``True`` which enables this feature. Set this argument to `False` for disabling
             this behaviour. Can also be set to a list of lists of metrics for setting the compute groups yourself.
 
     .. note::
+        The compute groups feature can significatly speedup the calculation of metrics under the right conditions.
+        First, the feature is only available when calling the ``update`` method and not when calling ``forward`` method
+        due to the internal logic of ``forward`` preventing this. Secondly, since we compute groups share metric
+        states by reference, calling ``.items()``, ``.values()`` etc. on the metric collection will break this
+        reference and a copy of states are instead returned in this case (reference will be reestablished on the next
+        call to ``update``).
+
+    .. note::
         Metric collections can be nested at initilization (see last example) but the output of the collection will
         still be a single flatten dictionary combining the prefix and postfix arguments from the nested collection.
 
     Raises:
         ValueError:
             If one of the elements of ``metrics`` is not an instance of ``pl.metrics.Metric``.
         ValueError:
@@ -69,66 +81,76 @@
             If ``metrics`` is ``dict`` and additional_metrics are passed in.
         ValueError:
             If ``prefix`` is set and it is not a string.
         ValueError:
             If ``postfix`` is set and it is not a string.
 
     Example (input as list):
-        >>> import torch
+        >>> from torch import tensor
         >>> from pprint import pprint
-        >>> from torchmetrics import MetricCollection, Accuracy, Precision, Recall, MeanSquaredError
-        >>> target = torch.tensor([0, 2, 0, 2, 0, 1, 0, 2])
-        >>> preds = torch.tensor([2, 1, 2, 0, 1, 2, 2, 2])
-        >>> metrics = MetricCollection([Accuracy(),
-        ...                             Precision(num_classes=3, average='macro'),
-        ...                             Recall(num_classes=3, average='macro')])
-        >>> metrics(preds, target)
-        {'Accuracy': tensor(0.1250), 'Precision': tensor(0.0667), 'Recall': tensor(0.1111)}
+        >>> from torchmetrics import MetricCollection
+        >>> from torchmetrics.regression import MeanSquaredError
+        >>> from torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, MulticlassRecall
+        >>> target = tensor([0, 2, 0, 2, 0, 1, 0, 2])
+        >>> preds = tensor([2, 1, 2, 0, 1, 2, 2, 2])
+        >>> metrics = MetricCollection([MulticlassAccuracy(num_classes=3, average='micro'),
+        ...                             MulticlassPrecision(num_classes=3, average='macro'),
+        ...                             MulticlassRecall(num_classes=3, average='macro')])
+        >>> metrics(preds, target)  # doctest: +NORMALIZE_WHITESPACE
+        {'MulticlassAccuracy': tensor(0.1250),
+         'MulticlassPrecision': tensor(0.0667),
+         'MulticlassRecall': tensor(0.1111)}
 
     Example (input as arguments):
-        >>> metrics = MetricCollection(Accuracy(), Precision(num_classes=3, average='macro'),
-        ...                            Recall(num_classes=3, average='macro'))
-        >>> metrics(preds, target)
-        {'Accuracy': tensor(0.1250), 'Precision': tensor(0.0667), 'Recall': tensor(0.1111)}
+        >>> metrics = MetricCollection(MulticlassAccuracy(num_classes=3, average='micro'),
+        ...                            MulticlassPrecision(num_classes=3, average='macro'),
+        ...                            MulticlassRecall(num_classes=3, average='macro'))
+        >>> metrics(preds, target)  # doctest: +NORMALIZE_WHITESPACE
+        {'MulticlassAccuracy': tensor(0.1250),
+         'MulticlassPrecision': tensor(0.0667),
+         'MulticlassRecall': tensor(0.1111)}
 
     Example (input as dict):
-        >>> metrics = MetricCollection({'micro_recall': Recall(num_classes=3, average='micro'),
-        ...                             'macro_recall': Recall(num_classes=3, average='macro')})
+        >>> metrics = MetricCollection({'micro_recall': MulticlassRecall(num_classes=3, average='micro'),
+        ...                             'macro_recall': MulticlassRecall(num_classes=3, average='macro')})
         >>> same_metric = metrics.clone()
         >>> pprint(metrics(preds, target))
         {'macro_recall': tensor(0.1111), 'micro_recall': tensor(0.1250)}
         >>> pprint(same_metric(preds, target))
         {'macro_recall': tensor(0.1111), 'micro_recall': tensor(0.1250)}
 
     Example (specification of compute groups):
         >>> metrics = MetricCollection(
-        ...     Accuracy(),
-        ...     Precision(num_classes=3, average='macro'),
+        ...     MulticlassRecall(num_classes=3, average='macro'),
+        ...     MulticlassPrecision(num_classes=3, average='macro'),
         ...     MeanSquaredError(),
-        ...     compute_groups=[['Accuracy', 'Precision'], ['MeanSquaredError']]
+        ...     compute_groups=[['MulticlassRecall', 'MulticlassPrecision'], ['MeanSquaredError']]
         ... )
-        >>> pprint(metrics(preds, target))
-        {'Accuracy': tensor(0.1250), 'MeanSquaredError': tensor(2.3750), 'Precision': tensor(0.0667)}
+        >>> metrics.update(preds, target)
+        >>> pprint(metrics.compute())
+        {'MeanSquaredError': tensor(2.3750), 'MulticlassPrecision': tensor(0.0667), 'MulticlassRecall': tensor(0.1111)}
+        >>> pprint(metrics.compute_groups)
+        {0: ['MulticlassRecall', 'MulticlassPrecision'], 1: ['MeanSquaredError']}
 
     Example (nested metric collections):
         >>> metrics = MetricCollection([
         ...     MetricCollection([
-        ...         Accuracy(num_classes=3, average='macro'),
-        ...         Precision(num_classes=3, average='macro')
+        ...         MulticlassAccuracy(num_classes=3, average='macro'),
+        ...         MulticlassPrecision(num_classes=3, average='macro')
         ...     ], postfix='_macro'),
         ...     MetricCollection([
-        ...         Accuracy(num_classes=3, average='micro'),
-        ...         Precision(num_classes=3, average='micro')
+        ...         MulticlassAccuracy(num_classes=3, average='micro'),
+        ...         MulticlassPrecision(num_classes=3, average='micro')
         ...     ], postfix='_micro'),
         ... ], prefix='valmetrics/')
         >>> pprint(metrics(preds, target))  # doctest: +NORMALIZE_WHITESPACE
-        {'valmetrics/Accuracy_macro': tensor(0.1111),
-        'valmetrics/Accuracy_micro': tensor(0.1250),
-        'valmetrics/Precision_macro': tensor(0.0667),
-        'valmetrics/Precision_micro': tensor(0.1250)}
+        {'valmetrics/MulticlassAccuracy_macro': tensor(0.1111),
+         'valmetrics/MulticlassAccuracy_micro': tensor(0.1250),
+         'valmetrics/MulticlassPrecision_macro': tensor(0.0667),
+         'valmetrics/MulticlassPrecision_micro': tensor(0.1250)}
     """
 
     _groups: Dict[int, List[str]]
 
     def __init__(
         self,
         metrics: Union[Metric, Sequence[Metric], Dict[str, Metric]],
@@ -145,38 +167,35 @@
         self._groups_checked: bool = False
         self._state_is_copy: bool = False
 
         self.add_metrics(metrics, *additional_metrics)
 
     @torch.jit.unused
     def forward(self, *args: Any, **kwargs: Any) -> Dict[str, Any]:
-        """Iteratively call forward for each metric.
+        """Call forward for each metric sequentially.
 
         Positional arguments (args) will be passed to every metric in the collection, while keyword arguments (kwargs)
         will be filtered based on the signature of the individual metric.
         """
         res = {k: m(*args, **m._filter_kwargs(**kwargs)) for k, m in self.items(keep_base=True, copy_state=False)}
         res = _flatten_dict(res)
         return {self._set_name(k): v for k, v in res.items()}
 
     def update(self, *args: Any, **kwargs: Any) -> None:
-        """Iteratively call update for each metric.
+        """Call update for each metric sequentially.
 
         Positional arguments (args) will be passed to every metric in the collection, while keyword arguments (kwargs)
         will be filtered based on the signature of the individual metric.
         """
         # Use compute groups if already initialized and checked
         if self._groups_checked:
             for _, cg in self._groups.items():
                 # only update the first member
                 m0 = getattr(self, cg[0])
                 m0.update(*args, **m0._filter_kwargs(**kwargs))
-                for i in range(1, len(cg)):  # copy over the update count
-                    mi = getattr(self, cg[i])
-                    mi._update_count = m0._update_count
             if self._state_is_copy:
                 # If we have deep copied state inbetween updates, reestablish link
                 self._compute_groups_create_state_ref()
                 self._state_is_copy = False
         else:  # the first update always do per metric to form compute groups
             for _, m in self.items(keep_base=True, copy_state=False):
                 m_kwargs = m._filter_kwargs(**kwargs)
@@ -185,17 +204,18 @@
             if self._enable_compute_groups:
                 self._merge_compute_groups()
                 # create reference between states
                 self._compute_groups_create_state_ref()
                 self._groups_checked = True
 
     def _merge_compute_groups(self) -> None:
-        """Iterates over the collection of metrics, checking if the state of each metric matches another.
+        """Iterate over the collection of metrics, checking if the state of each metric matches another.
 
-        If so, their compute groups will be merged into one
+        If so, their compute groups will be merged into one. The complexity of the method is approximately
+        ``O(number_of_metrics_in_collection ** 2)``, as all metrics need to be compared to all other metrics.
         """
         n_groups = len(self._groups)
         while True:
             for cg_idx1, cg_members1 in deepcopy(self._groups).items():
                 for cg_idx2, cg_members2 in deepcopy(self._groups).items():
                     if cg_idx1 == cg_idx2:
                         continue
@@ -210,16 +230,15 @@
                 # Start over if we merged groups
                 if len(self._groups) != n_groups:
                     break
 
             # Stop when we iterate over everything and do not merge any groups
             if len(self._groups) == n_groups:
                 break
-            else:
-                n_groups = len(self._groups)
+            n_groups = len(self._groups)
 
         # Re-index groups
         temp = deepcopy(self._groups)
         self._groups = {}
         for idx, values in enumerate(temp.values()):
             self._groups[idx] = values
 
@@ -229,15 +248,15 @@
         # empty state
         if len(metric1._defaults) == 0 or len(metric2._defaults) == 0:
             return False
 
         if metric1._defaults.keys() != metric2._defaults.keys():
             return False
 
-        for key in metric1._defaults.keys():
+        for key in metric1._defaults:
             state1 = getattr(metric1, key)
             state2 = getattr(metric2, key)
 
             if type(state1) != type(state2):
                 return False
 
             if isinstance(state1, Tensor) and isinstance(state2, Tensor):
@@ -260,46 +279,48 @@
                 m0 = getattr(self, cg[0])
                 for i in range(1, len(cg)):
                     mi = getattr(self, cg[i])
                     for state in m0._defaults:
                         m0_state = getattr(m0, state)
                         # Determine if we just should set a reference or a full copy
                         setattr(mi, state, deepcopy(m0_state) if copy else m0_state)
+                    setattr(mi, "_update_count", deepcopy(m0._update_count) if copy else m0._update_count)
         self._state_is_copy = copy
 
     def compute(self) -> Dict[str, Any]:
         """Compute the result for each metric in the collection."""
         res = {k: m.compute() for k, m in self.items(keep_base=True, copy_state=False)}
         res = _flatten_dict(res)
         return {self._set_name(k): v for k, v in res.items()}
 
     def reset(self) -> None:
-        """Iteratively call reset for each metric."""
+        """Call reset for each metric sequentially."""
         for _, m in self.items(keep_base=True, copy_state=False):
             m.reset()
         if self._enable_compute_groups and self._groups_checked:
             # reset state reference
             self._compute_groups_create_state_ref()
 
     def clone(self, prefix: Optional[str] = None, postfix: Optional[str] = None) -> "MetricCollection":
-        """Make a copy of the metric collection
+        """Make a copy of the metric collection.
+
         Args:
             prefix: a string to append in front of the metric keys
-            postfix: a string to append after the keys of the output dict
+            postfix: a string to append after the keys of the output dict.
 
         """
         mc = deepcopy(self)
         if prefix:
             mc.prefix = self._check_arg(prefix, "prefix")
         if postfix:
             mc.postfix = self._check_arg(postfix, "postfix")
         return mc
 
     def persistent(self, mode: bool = True) -> None:
-        """Method for post-init to change if metric states should be saved to its state_dict."""
+        """Change if metric states should be saved to its state_dict after initialization."""
         for _, m in self.items(keep_base=True, copy_state=False):
             m.persistent(mode)
 
     def add_metrics(
         self, metrics: Union[Metric, Sequence[Metric], Dict[str, Metric]], *additional_metrics: Metric
     ) -> None:
         """Add new metrics to Metric Collection."""
@@ -365,15 +386,15 @@
     def _init_compute_groups(self) -> None:
         """Initialize compute groups.
 
         If user provided a list, we check that all metrics in the list are also in the collection. If set to `True` we
         simply initialize each metric in the collection as its own group
         """
         if isinstance(self._enable_compute_groups, list):
-            self._groups = {i: k for i, k in enumerate(self._enable_compute_groups)}
+            self._groups = dict(enumerate(self._enable_compute_groups))
             for v in self._groups.values():
                 for metric in v:
                     if metric not in self:
                         raise ValueError(
                             f"Input {metric} in `compute_groups` argument does not match a metric in the collection."
                             f" Please make sure that {self._enable_compute_groups} matches {self.keys(keep_base=True)}"
                         )
@@ -386,25 +407,26 @@
     def compute_groups(self) -> Dict[int, List[str]]:
         """Return a dict with the current compute groups in the collection."""
         return self._groups
 
     def _set_name(self, base: str) -> str:
         """Adjust name of metric with both prefix and postfix."""
         name = base if self.prefix is None else self.prefix + base
-        name = name if self.postfix is None else name + self.postfix
-        return name
+        return name if self.postfix is None else name + self.postfix
 
     def _to_renamed_ordered_dict(self) -> OrderedDict:
         od = OrderedDict()
         for k, v in self._modules.items():
             od[self._set_name(k)] = v
         return od
 
+    # TODO: redefine this as native python dict
     def keys(self, keep_base: bool = False) -> Iterable[Hashable]:
         r"""Return an iterable of the ModuleDict key.
+
         Args:
             keep_base: Whether to add prefix/postfix on the items collection.
         """
         if keep_base:
             return self._modules.keys()
         return self._to_renamed_ordered_dict().keys()
 
@@ -445,13 +467,111 @@
     @staticmethod
     def _check_arg(arg: Optional[str], name: str) -> Optional[str]:
         if arg is None or isinstance(arg, str):
             return arg
         raise ValueError(f"Expected input `{name}` to be a string, but got {type(arg)}")
 
     def __repr__(self) -> str:
+        """Return the representation of the metric collection including all metrics in the collection."""
         repr_str = super().__repr__()[:-2]
         if self.prefix:
             repr_str += f",\n  prefix={self.prefix}{',' if self.postfix else ''}"
         if self.postfix:
             repr_str += f"{',' if not self.prefix else ''}\n  postfix={self.postfix}"
         return repr_str + "\n)"
+
+    def set_dtype(self, dst_type: Union[str, torch.dtype]) -> "MetricCollection":
+        """Transfer all metric state to specific dtype. Special version of standard `type` method.
+
+        Arguments:
+            dst_type (type or string): the desired type.
+        """
+        for _, m in self.items(keep_base=True, copy_state=False):
+            m.set_dtype(dst_type)
+        return self
+
+    def plot(
+        self,
+        val: Optional[Union[Dict, Sequence[Dict]]] = None,
+        ax: Optional[Union[_AX_TYPE, Sequence[_AX_TYPE]]] = None,
+        together: bool = False,
+    ) -> Sequence[_PLOT_OUT_TYPE]:
+        """Plot a single or multiple values from the metric.
+
+        The plot method has two modes of operation. If argument `together` is set to `False` (default), the `.plot`
+        method of each metric will be called individually and the result will be list of figures. If `together` is set
+        to `True`, the values of all metrics will instead be plotted in the same figure.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: Either a single instance of matplotlib axis object or an sequence of matplotlib axis objects. If
+                provided, will add the plots to the provided axis objects. If not provided, will create a new. If
+                argument `together` is set to `True`, a single object is expected. If `together` is set to `False`,
+                the number of axis objects needs to be the same lenght as the number of metrics in the collection.
+            together: If `True`, will plot all metrics in the same axis. If `False`, will plot each metric in a separate
+
+        Returns:
+            Either instal tupel of Figure and Axes object or an sequence of tuples with Figure and Axes object for each
+            metric in the collection.
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+            ValueError:
+                If `together` is not an bool
+            ValueError:
+                If `ax` is not an instance of matplotlib axis object or a sequence of matplotlib axis objects
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> import torch
+            >>> from torchmetrics import MetricCollection
+            >>> from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall
+            >>> metrics = MetricCollection([BinaryAccuracy(), BinaryPrecision(), BinaryRecall()])
+            >>> metrics.update(torch.rand(10), torch.randint(2, (10,)))
+            >>> fig_ax_ = metrics.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> import torch
+            >>> from torchmetrics import MetricCollection
+            >>> from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall
+            >>> metrics = MetricCollection([BinaryAccuracy(), BinaryPrecision(), BinaryRecall()])
+            >>> values = []
+            >>> for _ in range(10):
+            ...     values.append(metrics(torch.rand(10), torch.randint(2, (10,))))
+            >>> fig_, ax_ = metrics.plot(values, together=True)
+        """
+        if not isinstance(together, bool):
+            raise ValueError(f"Expected argument `together` to be a boolean, but got {type(together)}")
+        if ax is not None:
+            if together and not isinstance(ax, _AX_TYPE):
+                raise ValueError(
+                    f"Expected argument `ax` to be a matplotlib axis object, but got {type(ax)} when `together=True`"
+                )
+            if (
+                not together
+                and not isinstance(ax, Sequence)
+                and not all(isinstance(a, _AX_TYPE) for a in ax)
+                and len(ax) != len(self)
+            ):
+                raise ValueError(
+                    f"Expected argument `ax` to be a sequence of matplotlib axis objects with the same length as the "
+                    f"number of metrics in the collection, but got {type(ax)} with len {len(ax)} when `together=False`"
+                )
+
+        val = val or self.compute()
+        if together:
+            return plot_single_or_multi_val(val, ax=ax)
+        fig_axs = []
+        for i, (k, m) in enumerate(self.items(keep_base=True, copy_state=False)):
+            if isinstance(val, dict):
+                f, a = m.plot(val[k], ax=ax[i] if ax is not None else ax)
+            elif isinstance(val, Sequence):
+                f, a = m.plot([v[k] for v in val], ax=ax[i] if ax is not None else ax)
+            fig_axs.append((f, a))
+        return fig_axs
```

### Comparing `torchmetrics-0.9.3/torchmetrics/detection/__init__.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/utilities/__init__.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,17 +1,18 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from torchmetrics.utilities.imports import _TORCHVISION_GREATER_EQUAL_0_8
-
-if _TORCHVISION_GREATER_EQUAL_0_8:
-    from torchmetrics.detection.mean_ap import MeanAveragePrecision  # noqa: F401
+from torchmetrics.utilities.checks import check_forward_full_state_property  # noqa: F401
+from torchmetrics.utilities.data import apply_to_collection  # noqa: F401
+from torchmetrics.utilities.distributed import class_reduce, reduce  # noqa: F401
+from torchmetrics.utilities.prints import _future_warning  # noqa: F401
+from torchmetrics.utilities.prints import rank_zero_debug, rank_zero_info, rank_zero_warn
```

### Comparing `torchmetrics-0.9.3/torchmetrics/detection/mean_ap.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/detection/mean_ap.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -14,91 +14,95 @@
 import logging
 from typing import Any, Dict, List, Optional, Sequence, Tuple, Union
 
 import numpy as np
 import torch
 from torch import IntTensor, Tensor
 
+from torchmetrics.detection.helpers import _fix_empty_tensors, _input_validator
 from torchmetrics.metric import Metric
-from torchmetrics.utilities.imports import _PYCOCOTOOLS_AVAILABLE, _TORCHVISION_GREATER_EQUAL_0_8
+from torchmetrics.utilities.data import _cumsum
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE, _PYCOCOTOOLS_AVAILABLE, _TORCHVISION_GREATER_EQUAL_0_8
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
+
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["MeanAveragePrecision.plot"]
 
 if _TORCHVISION_GREATER_EQUAL_0_8:
     from torchvision.ops import box_area, box_convert, box_iou
 else:
     box_convert = box_iou = box_area = None
-    __doctest_skip__ = ["MeanAveragePrecision"]
+    __doctest_skip__ = ["MeanAveragePrecision.plot", "MeanAveragePrecision"]
 
 if _PYCOCOTOOLS_AVAILABLE:
     import pycocotools.mask as mask_utils
 else:
     mask_utils = None
-    __doctest_skip__ = ["MeanAveragePrecision"]
+    __doctest_skip__ = ["MeanAveragePrecision.plot", "MeanAveragePrecision"]
 
 
 log = logging.getLogger(__name__)
 
 
 def compute_area(input: List[Any], iou_type: str = "bbox") -> Tensor:
     """Compute area of input depending on the specified iou_type.
 
-    Default output for empty input is torch.Tensor([])
+    Default output for empty input is :class:`~torch.Tensor`
     """
     if len(input) == 0:
-
-        return torch.Tensor([])
+        return Tensor([])
 
     if iou_type == "bbox":
         return box_area(torch.stack(input))
-    elif iou_type == "segm":
-
+    if iou_type == "segm":
         input = [{"size": i[0], "counts": i[1]} for i in input]
         area = torch.tensor(mask_utils.area(input).astype("float"))
-
         return area
-    else:
-        raise Exception(f"IOU type {iou_type} is not supported")
+
+    raise Exception(f"IOU type {iou_type} is not supported")
 
 
 def compute_iou(
     det: List[Any],
     gt: List[Any],
     iou_type: str = "bbox",
 ) -> Tensor:
     """Compute IOU between detections and ground-truth using the specified iou_type."""
-
     if iou_type == "bbox":
         return box_iou(torch.stack(det), torch.stack(gt))
-    elif iou_type == "segm":
+    if iou_type == "segm":
         return _segm_iou(det, gt)
-    else:
-        raise Exception(f"IOU type {iou_type} is not supported")
+    raise Exception(f"IOU type {iou_type} is not supported")
 
 
 class BaseMetricResults(dict):
     """Base metric class, that allows fields for pre-defined metrics."""
 
     def __getattr__(self, key: str) -> Tensor:
+        """Get a specific metric attribute."""
         # Using this you get the correct error message, an AttributeError instead of a KeyError
         if key in self:
             return self[key]
         raise AttributeError(f"No such attribute: {key}")
 
     def __setattr__(self, key: str, value: Tensor) -> None:
+        """Set a specific metric attribute."""
         self[key] = value
 
     def __delattr__(self, key: str) -> None:
+        """Delete a specific metric attribute."""
         if key in self:
             del self[key]
         raise AttributeError(f"No such attribute: {key}")
 
 
 class MAPMetricResults(BaseMetricResults):
     """Class to wrap the final mAP results."""
 
-    __slots__ = ("map", "map_50", "map_75", "map_small", "map_medium", "map_large")
+    __slots__ = ("map", "map_50", "map_75", "map_small", "map_medium", "map_large", "classes")
 
 
 class MARMetricResults(BaseMetricResults):
     """Class to wrap the final mAR results."""
 
     __slots__ = ("mar_1", "mar_10", "mar_100", "mar_small", "mar_medium", "mar_large")
 
@@ -120,99 +124,94 @@
         "mar_medium",
         "mar_large",
         "map_per_class",
         "mar_100_per_class",
     )
 
 
-def _segm_iou(det: List[Tuple[np.ndarray, np.ndarray]], gt: List[Tuple[np.ndarray, np.ndarray]]) -> torch.Tensor:
-    """
-    Compute IOU between detections and ground-truths using mask-IOU. Based on pycocotools toolkit for mask_utils
+def _segm_iou(det: List[Tuple[np.ndarray, np.ndarray]], gt: List[Tuple[np.ndarray, np.ndarray]]) -> Tensor:
+    """Compute IOU between detections and ground-truths using mask-IOU.
+
+    Implementation is based on pycocotools toolkit for mask_utils.
+
     Args:
        det: A list of detection masks as ``[(RLE_SIZE, RLE_COUNTS)]``, where ``RLE_SIZE`` is (width, height) dimension
            of the input and RLE_COUNTS is its RLE representation;
 
        gt: A list of ground-truth masks as ``[(RLE_SIZE, RLE_COUNTS)]``, where ``RLE_SIZE`` is (width, height) dimension
            of the input and RLE_COUNTS is its RLE representation;
 
     """
-
     det_coco_format = [{"size": i[0], "counts": i[1]} for i in det]
     gt_coco_format = [{"size": i[0], "counts": i[1]} for i in gt]
 
     return torch.tensor(mask_utils.iou(det_coco_format, gt_coco_format, [False for _ in gt]))
 
 
-def _input_validator(
-    preds: Sequence[Dict[str, Tensor]], targets: Sequence[Dict[str, Tensor]], iou_type: str = "bbox"
-) -> None:
-    """Ensure the correct input format of `preds` and `targets`"""
-    if not isinstance(preds, Sequence):
-        raise ValueError("Expected argument `preds` to be of type Sequence")
-    if not isinstance(targets, Sequence):
-        raise ValueError("Expected argument `target` to be of type Sequence")
-    if len(preds) != len(targets):
-        raise ValueError("Expected argument `preds` and `target` to have the same length")
-    iou_attribute = "boxes" if iou_type == "bbox" else "masks"
-
-    for k in [iou_attribute, "scores", "labels"]:
-        if any(k not in p for p in preds):
-            raise ValueError(f"Expected all dicts in `preds` to contain the `{k}` key")
-
-    for k in [iou_attribute, "labels"]:
-        if any(k not in p for p in targets):
-            raise ValueError(f"Expected all dicts in `target` to contain the `{k}` key")
-
-    if any(type(pred[iou_attribute]) is not Tensor for pred in preds):
-        raise ValueError(f"Expected all {iou_attribute} in `preds` to be of type Tensor")
-    if any(type(pred["scores"]) is not Tensor for pred in preds):
-        raise ValueError("Expected all scores in `preds` to be of type Tensor")
-    if any(type(pred["labels"]) is not Tensor for pred in preds):
-        raise ValueError("Expected all labels in `preds` to be of type Tensor")
-    if any(type(target[iou_attribute]) is not Tensor for target in targets):
-        raise ValueError(f"Expected all {iou_attribute} in `target` to be of type Tensor")
-    if any(type(target["labels"]) is not Tensor for target in targets):
-        raise ValueError("Expected all labels in `target` to be of type Tensor")
-
-    for i, item in enumerate(targets):
-        if item[iou_attribute].size(0) != item["labels"].size(0):
-            raise ValueError(
-                f"Input {iou_attribute} and labels of sample {i} in targets have a"
-                f" different length (expected {item[iou_attribute].size(0)} labels, got {item['labels'].size(0)})"
-            )
-    for i, item in enumerate(preds):
-        if not (item[iou_attribute].size(0) == item["labels"].size(0) == item["scores"].size(0)):
-            raise ValueError(
-                f"Input {iou_attribute}, labels and scores of sample {i} in predictions have a"
-                f" different length (expected {item[iou_attribute].size(0)} labels and scores,"
-                f" got {item['labels'].size(0)} labels and {item['scores'].size(0)})"
-            )
-
+class MeanAveragePrecision(Metric):
+    r"""Compute the `Mean-Average-Precision (mAP) and Mean-Average-Recall (mAR)`_ for object detection predictions.
 
-def _fix_empty_tensors(boxes: Tensor) -> Tensor:
-    """Empty tensors can cause problems in DDP mode, this methods corrects them."""
+    .. math::
+        \text{mAP} = \frac{1}{n} \sum_{i=1}^{n} AP_i
 
-    if boxes.numel() == 0 and boxes.ndim == 1:
-        return boxes.unsqueeze(0)
-    return boxes
+    where :math:`AP_i` is the average precision for class :math:`i` and :math:`n` is the number of classes. The average
+    precision is defined as the area under the precision-recall curve. If argument `class_metrics` is set to ``True``,
+    the metric will also return the mAP/mAR per class.
+
+    As input to ``forward`` and ``update`` the metric accepts the following input:
+
+    - ``preds`` (:class:`~List`): A list consisting of dictionaries each containing the key-values
+      (each dictionary corresponds to a single image). Parameters that should be provided per dict
+
+        - boxes: (:class:`~torch.FloatTensor`) of shape ``(num_boxes, 4)`` containing ``num_boxes`` detection
+          boxes of the format specified in the constructor.
+          By default, this method expects ``(xmin, ymin, xmax, ymax)`` in absolute image coordinates.
+        - scores: :class:`~torch.FloatTensor` of shape ``(num_boxes)`` containing detection scores for the boxes.
+        - labels: :class:`~torch.IntTensor` of shape ``(num_boxes)`` containing 0-indexed detection classes for
+          the boxes.
+        - masks: :class:`~torch.bool` of shape ``(num_boxes, image_height, image_width)`` containing boolean masks.
+          Only required when `iou_type="segm"`.
+
+    - ``target`` (:class:`~List`) A list consisting of dictionaries each containing the key-values
+      (each dictionary corresponds to a single image). Parameters that should be provided per dict:
+
+        - boxes: :class:`~torch.FloatTensor` of shape ``(num_boxes, 4)`` containing ``num_boxes`` ground truth
+          boxes of the format specified in the constructor.
+          By default, this method expects ``(xmin, ymin, xmax, ymax)`` in absolute image coordinates.
+        - labels: :class:`~torch.IntTensor` of shape ``(num_boxes)`` containing 0-indexed ground truth
+          classes for the boxes.
+        - masks: :class:`~torch.bool` of shape ``(num_boxes, image_height, image_width)`` containing boolean masks.
+          Only required when `iou_type="segm"`.
+
+    As output of ``forward`` and ``compute`` the metric returns the following output:
+
+    - ``map_dict``: A dictionary containing the following key-values:
+
+        - map: (:class:`~torch.Tensor`)
+        - map_small: (:class:`~torch.Tensor`)
+        - map_medium:(:class:`~torch.Tensor`)
+        - map_large: (:class:`~torch.Tensor`)
+        - mar_1: (:class:`~torch.Tensor`)
+        - mar_10: (:class:`~torch.Tensor`)
+        - mar_100: (:class:`~torch.Tensor`)
+        - mar_small: (:class:`~torch.Tensor`)
+        - mar_medium: (:class:`~torch.Tensor`)
+        - mar_large: (:class:`~torch.Tensor`)
+        - map_50: (:class:`~torch.Tensor`) (-1 if 0.5 not in the list of iou thresholds)
+        - map_75: (:class:`~torch.Tensor`) (-1 if 0.75 not in the list of iou thresholds)
+        - map_per_class: (:class:`~torch.Tensor`) (-1 if class metrics are disabled)
+        - mar_100_per_class: (:class:`~torch.Tensor`) (-1 if class metrics are disabled)
+        - classes (:class:`~torch.Tensor`)
 
+    For an example on how to use this metric check the `torchmetrics mAP example`_.
 
-class MeanAveragePrecision(Metric):
-    r"""
-    Computes the `Mean-Average-Precision (mAP) and Mean-Average-Recall (mAR)
-    <https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173>`_
-    for object detection predictions.
-    Optionally, the mAP and mAR values can be calculated per class.
-
-    Predicted boxes and targets have to be in Pascal VOC format
-    (xmin-top left, ymin-top left, xmax-bottom right, ymax-bottom right).
-    See the :meth:`update` method for more information about the input format to this metric.
-
-    For an example on how to use this metric check the `torchmetrics examples
-    <https://github.com/Lightning-AI/metrics/blob/master/tm_examples/detection_map.py>`_
+    .. note::
+        ``map`` score is calculated with @[ IoU=self.iou_thresholds | area=all | max_dets=max_detection_thresholds ].
+        Caution: If the initialization parameters are changed, dictionary keys for mAR can change as well.
+        The default properties are also accessible via fields and will raise an ``AttributeError`` if not available.
 
     .. note::
         This metric is following the mAP implementation of
         `pycocotools <https://github.com/cocodataset/cocoapi/tree/master/PythonAPI/pycocotools>`_,
         a standard implementation for the mAP metric for object detection.
 
     .. note::
@@ -222,75 +221,94 @@
         ``pip install torchmetrics[detection]``.
 
     Args:
         box_format:
             Input format of given boxes. Supported formats are ``[`xyxy`, `xywh`, `cxcywh`]``.
         iou_type:
             Type of input (either masks or bounding-boxes) used for computing IOU.
-            Supported IOU types are ``["bboxes", "segm"]``.
+            Supported IOU types are ``["bbox", "segm"]``.
             If using ``"segm"``, masks should be provided (see :meth:`update`).
         iou_thresholds:
             IoU thresholds for evaluation. If set to ``None`` it corresponds to the stepped range ``[0.5,...,0.95]``
             with step ``0.05``. Else provide a list of floats.
         rec_thresholds:
             Recall thresholds for evaluation. If set to ``None`` it corresponds to the stepped range ``[0,...,1]``
             with step ``0.01``. Else provide a list of floats.
         max_detection_thresholds:
             Thresholds on max detections per image. If set to `None` will use thresholds ``[1, 10, 100]``.
             Else, please provide a list of ints.
         class_metrics:
             Option to enable per-class metrics for mAP and mAR_100. Has a performance impact.
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
+    Raises:
+        ModuleNotFoundError:
+            If ``torchvision`` is not installed or version installed is lower than 0.8.0
+        ModuleNotFoundError:
+            If ``iou_type`` is equal to ``seqm`` and ``pycocotools`` is not installed
+        ValueError:
+            If ``class_metrics`` is not a boolean
+        ValueError:
+            If ``preds`` is not of type (:class:`~List[Dict[str, Tensor]]`)
+        ValueError:
+            If ``target`` is not of type ``List[Dict[str, Tensor]]``
+        ValueError:
+            If ``preds`` and ``target`` are not of the same length
+        ValueError:
+            If any of ``preds.boxes``, ``preds.scores`` and ``preds.labels`` are not of the same length
+        ValueError:
+            If any of ``target.boxes`` and ``target.labels`` are not of the same length
+        ValueError:
+            If any box is not type float and of length 4
+        ValueError:
+            If any class is not type int and of length 1
+        ValueError:
+            If any score is not type float and of length 1
+
     Example:
-        >>> import torch
-        >>> from torchmetrics.detection.mean_ap import MeanAveragePrecision
+        >>> from torch import tensor
+        >>> from torchmetrics.detection import MeanAveragePrecision
         >>> preds = [
         ...   dict(
-        ...     boxes=torch.tensor([[258.0, 41.0, 606.0, 285.0]]),
-        ...     scores=torch.tensor([0.536]),
-        ...     labels=torch.tensor([0]),
+        ...     boxes=tensor([[258.0, 41.0, 606.0, 285.0]]),
+        ...     scores=tensor([0.536]),
+        ...     labels=tensor([0]),
         ...   )
         ... ]
         >>> target = [
         ...   dict(
-        ...     boxes=torch.tensor([[214.0, 41.0, 562.0, 285.0]]),
-        ...     labels=torch.tensor([0]),
+        ...     boxes=tensor([[214.0, 41.0, 562.0, 285.0]]),
+        ...     labels=tensor([0]),
         ...   )
         ... ]
         >>> metric = MeanAveragePrecision()
         >>> metric.update(preds, target)
         >>> from pprint import pprint
         >>> pprint(metric.compute())
-        {'map': tensor(0.6000),
+        {'classes': tensor(0, dtype=torch.int32),
+         'map': tensor(0.6000),
          'map_50': tensor(1.),
          'map_75': tensor(1.),
          'map_large': tensor(0.6000),
          'map_medium': tensor(-1.),
          'map_per_class': tensor(-1.),
          'map_small': tensor(-1.),
          'mar_1': tensor(0.6000),
          'mar_10': tensor(0.6000),
          'mar_100': tensor(0.6000),
          'mar_100_per_class': tensor(-1.),
          'mar_large': tensor(0.6000),
          'mar_medium': tensor(-1.),
          'mar_small': tensor(-1.)}
-
-    Raises:
-        ModuleNotFoundError:
-            If ``torchvision`` is not installed or version installed is lower than 0.8.0
-        ModuleNotFoundError:
-            If ``iou_type`` is equal to ``seqm`` and ``pycocotools`` is not installed
-        ValueError:
-            If ``class_metrics`` is not a boolean
     """
     is_differentiable: bool = False
-    higher_is_better: Optional[bool] = None
+    higher_is_better: Optional[bool] = True
     full_state_update: bool = True
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
 
     detections: List[Tensor]
     detection_scores: List[Tensor]
     detection_labels: List[Tensor]
     groundtruths: List[Tensor]
     groundtruth_labels: List[Tensor]
 
@@ -299,15 +317,15 @@
         box_format: str = "xyxy",
         iou_type: str = "bbox",
         iou_thresholds: Optional[List[float]] = None,
         rec_thresholds: Optional[List[float]] = None,
         max_detection_thresholds: Optional[List[int]] = None,
         class_metrics: bool = False,
         **kwargs: Any,
-    ) -> None:  # type: ignore
+    ) -> None:
         super().__init__(**kwargs)
 
         if not _TORCHVISION_GREATER_EQUAL_0_8:
             raise ModuleNotFoundError(
                 "`MeanAveragePrecision` metric requires that `torchvision` version 0.8.0 or newer is installed."
                 " Please install with `pip install torchvision>=0.8` or `pip install torchmetrics[detection]`."
             )
@@ -323,141 +341,90 @@
         self.max_detection_thresholds = max_det_thr.tolist()
         if iou_type not in allowed_iou_types:
             raise ValueError(f"Expected argument `iou_type` to be one of {allowed_iou_types} but got {iou_type}")
         if iou_type == "segm" and not _PYCOCOTOOLS_AVAILABLE:
             raise ModuleNotFoundError("When `iou_type` is set to 'segm', pycocotools need to be installed")
         self.iou_type = iou_type
         self.bbox_area_ranges = {
-            "all": (0**2, int(1e5**2)),
-            "small": (0**2, 32**2),
-            "medium": (32**2, 96**2),
-            "large": (96**2, int(1e5**2)),
+            "all": (float(0**2), float(1e5**2)),
+            "small": (float(0**2), float(32**2)),
+            "medium": (float(32**2), float(96**2)),
+            "large": (float(96**2), float(1e5**2)),
         }
 
         if not isinstance(class_metrics, bool):
             raise ValueError("Expected argument `class_metrics` to be a boolean")
 
         self.class_metrics = class_metrics
         self.add_state("detections", default=[], dist_reduce_fx=None)
         self.add_state("detection_scores", default=[], dist_reduce_fx=None)
         self.add_state("detection_labels", default=[], dist_reduce_fx=None)
         self.add_state("groundtruths", default=[], dist_reduce_fx=None)
         self.add_state("groundtruth_labels", default=[], dist_reduce_fx=None)
 
-    def update(self, preds: List[Dict[str, Tensor]], target: List[Dict[str, Tensor]]) -> None:  # type: ignore
-        """Add detections and ground truth to the metric.
-
-        Args:
-            preds: A list consisting of dictionaries each containing the key-values
-                (each dictionary corresponds to a single image):
-
-                - ``boxes``: ``torch.FloatTensor`` of shape ``[num_boxes, 4]`` containing ``num_boxes`` detection boxes
-                  of the format specified in the constructor. By default, this method expects
-                  ``[xmin, ymin, xmax, ymax]`` in absolute image coordinates.
-                - ``scores``: ``torch.FloatTensor`` of shape ``[num_boxes]`` containing detection scores for the boxes.
-                - ``labels``: ``torch.IntTensor`` of shape ``[num_boxes]`` containing 0-indexed detection classes
-                  for the boxes.
-                - ``masks``: ``torch.bool`` of shape ``[num_boxes, image_height, image_width]`` containing boolean
-                  masks. Only required when `iou_type="segm"`.
-
-            target: A list consisting of dictionaries each containing the key-values
-                (each dictionary corresponds to a single image):
-
-                - ``boxes``: ``torch.FloatTensor`` of shape ``[num_boxes, 4]`` containing ``num_boxes``
-                  ground truth boxes of the format specified in the constructor. By default, this method expects
-                  ``[xmin, ymin, xmax, ymax]`` in absolute image coordinates.
-                - ``labels``: ``torch.IntTensor`` of shape ``[num_boxes]`` containing 0-indexed ground truth
-                   classes for the boxes.
-                - ``masks``: ``torch.bool`` of shape ``[num_boxes, image_height, image_width]`` containing boolean
-                  masks. Only required when `iou_type="segm"`.
-
-        Raises:
-            ValueError:
-                If ``preds`` is not of type ``List[Dict[str, Tensor]]``
-            ValueError:
-                If ``target`` is not of type ``List[Dict[str, Tensor]]``
-            ValueError:
-                If ``preds`` and ``target`` are not of the same length
-            ValueError:
-                If any of ``preds.boxes``, ``preds.scores`` and ``preds.labels`` are not of the same length
-            ValueError:
-                If any of ``target.boxes`` and ``target.labels`` are not of the same length
-            ValueError:
-                If any box is not type float and of length 4
-            ValueError:
-                If any class is not type int and of length 1
-            ValueError:
-                If any score is not type float and of length 1
-        """
+    def update(self, preds: List[Dict[str, Tensor]], target: List[Dict[str, Tensor]]) -> None:
+        """Update state with predictions and targets."""
         _input_validator(preds, target, iou_type=self.iou_type)
 
         for item in preds:
-
             detections = self._get_safe_item_values(item)
 
             self.detections.append(detections)
             self.detection_labels.append(item["labels"])
             self.detection_scores.append(item["scores"])
 
         for item in target:
             groundtruths = self._get_safe_item_values(item)
             self.groundtruths.append(groundtruths)
             self.groundtruth_labels.append(item["labels"])
 
     def _move_list_states_to_cpu(self) -> None:
         """Move list states to cpu to save GPU memory."""
-
-        for key in self._defaults.keys():
+        for key in self._defaults:
             current_val = getattr(self, key)
             current_to_cpu = []
             if isinstance(current_val, Sequence):
                 for cur_v in current_val:
-                    # Cannot handle RLE as torch.Tensor
+                    # Cannot handle RLE as Tensor
                     if not isinstance(cur_v, tuple):
                         cur_v = cur_v.to("cpu")
                     current_to_cpu.append(cur_v)
             setattr(self, key, current_to_cpu)
 
     def _get_safe_item_values(self, item: Dict[str, Any]) -> Union[Tensor, Tuple]:
-
         if self.iou_type == "bbox":
             boxes = _fix_empty_tensors(item["boxes"])
             if boxes.numel() > 0:
                 boxes = box_convert(boxes, in_fmt=self.box_format, out_fmt="xyxy")
             return boxes
-        elif self.iou_type == "segm":
+        if self.iou_type == "segm":
             masks = []
-
             for i in item["masks"].cpu().numpy():
                 rle = mask_utils.encode(np.asfortranarray(i))
                 masks.append((tuple(rle["size"]), rle["counts"]))
-
             return tuple(masks)
-        else:
-            raise Exception(f"IOU type {self.iou_type} is not supported")
+        raise Exception(f"IOU type {self.iou_type} is not supported")
 
     def _get_classes(self) -> List:
-        """Returns a list of unique classes found in ground truth and detection data."""
+        """Return a list of unique classes found in ground truth and detection data."""
         if len(self.detection_labels) > 0 or len(self.groundtruth_labels) > 0:
             return torch.cat(self.detection_labels + self.groundtruth_labels).unique().tolist()
         return []
 
     def _compute_iou(self, idx: int, class_id: int, max_det: int) -> Tensor:
-        """Computes the Intersection over Union (IoU) for ground truth and detection bounding boxes for the given
-        image and class.
+        """Compute the Intersection over Union (IoU) between bounding boxes for the given image and class.
 
         Args:
             idx:
                 Image Id, equivalent to the index of supplied samples
             class_id:
                 Class Id of the supplied ground truth and detection labels
             max_det:
                 Maximum number of evaluated detection bounding boxes
         """
-
         # if self.iou_type == "bbox":
         gt = self.groundtruths[idx]
         det = self.detections[idx]
 
         gt_label_mask = (self.groundtruth_labels[idx] == class_id).nonzero().squeeze(1)
         det_label_mask = (self.detection_labels[idx] == class_id).nonzero().squeeze(1)
 
@@ -476,21 +443,20 @@
         inds = torch.argsort(scores_filtered, descending=True)
 
         # TODO Fix (only for masks is necessary)
         det = [det[i] for i in inds]
         if len(det) > max_det:
             det = det[:max_det]
 
-        ious = compute_iou(det, gt, self.iou_type).to(self.device)
-        return ious
+        return compute_iou(det, gt, self.iou_type).to(self.device)
 
     def __evaluate_image_gt_no_preds(
         self, gt: Tensor, gt_label_mask: Tensor, area_range: Tuple[int, int], nb_iou_thrs: int
     ) -> Dict[str, Any]:
-        """Some GT but no predictions."""
+        """Evaluate images with a ground truth but no predictions."""
         # GTs
         gt = [gt[i] for i in gt_label_mask]
         nb_gt = len(gt)
         areas = compute_area(gt, iou_type=self.iou_type).to(self.device)
         ignore_area = (areas < area_range[0]) | (areas > area_range[1])
         gt_ignore, _ = torch.sort(ignore_area.to(torch.uint8))
         gt_ignore = gt_ignore.to(torch.bool)
@@ -498,23 +464,23 @@
         # Detections
         nb_det = 0
         det_ignore = torch.zeros((nb_iou_thrs, nb_det), dtype=torch.bool, device=self.device)
 
         return {
             "dtMatches": torch.zeros((nb_iou_thrs, nb_det), dtype=torch.bool, device=self.device),
             "gtMatches": torch.zeros((nb_iou_thrs, nb_gt), dtype=torch.bool, device=self.device),
-            "dtScores": torch.zeros(nb_det, dtype=torch.bool, device=self.device),
+            "dtScores": torch.zeros(nb_det, dtype=torch.float32, device=self.device),
             "gtIgnore": gt_ignore,
             "dtIgnore": det_ignore,
         }
 
     def __evaluate_image_preds_no_gt(
         self, det: Tensor, idx: int, det_label_mask: Tensor, max_det: int, area_range: Tuple[int, int], nb_iou_thrs: int
     ) -> Dict[str, Any]:
-        """Some predictions but no GT."""
+        """Evaluate images with a prediction but no ground truth."""
         # GTs
         nb_gt = 0
 
         gt_ignore = torch.zeros(nb_gt, dtype=torch.bool, device=self.device)
 
         # Detections
 
@@ -553,15 +519,14 @@
             area_range:
                 List of lower and upper bounding box area threshold.
             max_det:
                 Maximum number of evaluated detection bounding boxes.
             ious:
                 IoU results for image and class.
         """
-
         gt = self.groundtruths[idx]
         det = self.detections[idx]
         gt_label_mask = (self.groundtruth_labels[idx] == class_id).nonzero().squeeze(1)
         det_label_mask = (self.detection_labels[idx] == class_id).nonzero().squeeze(1)
 
         # No Gt and No predictions --> ignore image
         if len(gt_label_mask) == 0 and len(det_label_mask) == 0:
@@ -707,16 +672,15 @@
             prec = results["recall"]
             if iou_threshold is not None:
                 thr = self.iou_thresholds.index(iou_threshold)
                 prec = prec[thr, :, :, area_inds, mdet_inds]
             else:
                 prec = prec[:, :, area_inds, mdet_inds]
 
-        mean_prec = torch.tensor([-1.0]) if len(prec[prec > -1]) == 0 else torch.mean(prec[prec > -1])
-        return mean_prec
+        return torch.tensor([-1.0]) if len(prec[prec > -1]) == 0 else torch.mean(prec[prec > -1])
 
     def _calculate(self, class_ids: List) -> Tuple[MAPMetricResults, MARMetricResults]:
         """Calculate the precision and recall for all supplied classes to calculate mAP/mAR.
 
         Args:
             class_ids:
                 List of label class Ids.
@@ -776,18 +740,18 @@
 
         Args:
             precisions:
                 Precision values for different thresholds
             recalls:
                 Recall values for different thresholds
         """
-        results = dict(precision=precisions, recall=recalls)
+        results = {"precision": precisions, "recall": recalls}
         map_metrics = MAPMetricResults()
-        map_metrics.map = self._summarize(results, True)
         last_max_det_thr = self.max_detection_thresholds[-1]
+        map_metrics.map = self._summarize(results, True, max_dets=last_max_det_thr)
         if 0.5 in self.iou_thresholds:
             map_metrics.map_50 = self._summarize(results, True, iou_threshold=0.5, max_dets=last_max_det_thr)
         else:
             map_metrics.map_50 = torch.tensor([-1])
         if 0.75 in self.iou_thresholds:
             map_metrics.map_75 = self._summarize(results, True, iou_threshold=0.75, max_dets=last_max_det_thr)
         else:
@@ -843,71 +807,44 @@
         gt_ignore = torch.cat([e["gtIgnore"] for e in img_eval_cls_bbox])
         npig = torch.count_nonzero(gt_ignore == False)  # noqa: E712
         if npig == 0:
             return recall, precision, scores
         tps = torch.logical_and(det_matches, torch.logical_not(det_ignore))
         fps = torch.logical_and(torch.logical_not(det_matches), torch.logical_not(det_ignore))
 
-        tp_sum = torch.cumsum(tps, axis=1, dtype=torch.float)
-        fp_sum = torch.cumsum(fps, axis=1, dtype=torch.float)
+        tp_sum = _cumsum(tps, dim=1, dtype=torch.float)
+        fp_sum = _cumsum(fps, dim=1, dtype=torch.float)
         for idx, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):
             nd = len(tp)
             rc = tp / npig
             pr = tp / (fp + tp + torch.finfo(torch.float64).eps)
             prec = torch.zeros((nb_rec_thrs,))
             score = torch.zeros((nb_rec_thrs,))
 
             recall[idx, idx_cls, idx_bbox_area, idx_max_det_thrs] = rc[-1] if nd else 0
 
             # Remove zigzags for AUC
             diff_zero = torch.zeros((1,), device=pr.device)
             diff = torch.ones((1,), device=pr.device)
             while not torch.all(diff == 0):
-
                 diff = torch.clamp(torch.cat(((pr[1:] - pr[:-1]), diff_zero), 0), min=0)
                 pr += diff
 
             inds = torch.searchsorted(rc, rec_thresholds.to(rc.device), right=False)
             num_inds = inds.argmax() if inds.max() >= nd else nb_rec_thrs
             inds = inds[:num_inds]
             prec[:num_inds] = pr[inds]
             score[:num_inds] = det_scores_sorted[inds]
             precision[idx, :, idx_cls, idx_bbox_area, idx_max_det_thrs] = prec
             scores[idx, :, idx_cls, idx_bbox_area, idx_max_det_thrs] = score
 
         return recall, precision, scores
 
     def compute(self) -> dict:
-        """Compute the `Mean-Average-Precision (mAP) and Mean-Average-Recall (mAR)` scores.
-
-        Note:
-            ``map`` score is calculated with @[ IoU=self.iou_thresholds | area=all | max_dets=max_detection_thresholds ]
-
-            Caution: If the initialization parameters are changed, dictionary keys for mAR can change as well.
-            The default properties are also accessible via fields and will raise an ``AttributeError`` if not available.
-
-        Returns:
-            dict containing
-
-            - map: ``torch.Tensor``
-            - map_small: ``torch.Tensor``
-            - map_medium: ``torch.Tensor``
-            - map_large: ``torch.Tensor``
-            - mar_1: ``torch.Tensor``
-            - mar_10: ``torch.Tensor``
-            - mar_100: ``torch.Tensor``
-            - mar_small: ``torch.Tensor``
-            - mar_medium: ``torch.Tensor``
-            - mar_large: ``torch.Tensor``
-            - map_50: ``torch.Tensor`` (-1 if 0.5 not in the list of iou thresholds)
-            - map_75: ``torch.Tensor`` (-1 if 0.75 not in the list of iou thresholds)
-            - map_per_class: ``torch.Tensor`` (-1 if class metrics are disabled)
-            - mar_100_per_class: ``torch.Tensor`` (-1 if class metrics are disabled)
-        """
-
+        """Compute metric."""
         classes = self._get_classes()
         precisions, recalls = self._calculate(classes)
         map_val, mar_val = self._summarize_results(precisions, recalls)
 
         # if class mode is enabled, evaluate metrics per class
         map_per_class_values: Tensor = torch.tensor([-1.0])
         mar_max_dets_per_class_values: Tensor = torch.tensor([-1.0])
@@ -926,9 +863,67 @@
             mar_max_dets_per_class_values = torch.tensor(mar_max_dets_per_class_list, dtype=torch.float)
 
         metrics = COCOMetricResults()
         metrics.update(map_val)
         metrics.update(mar_val)
         metrics.map_per_class = map_per_class_values
         metrics[f"mar_{self.max_detection_thresholds[-1]}_per_class"] = mar_max_dets_per_class_values
-
+        metrics.classes = torch.tensor(classes, dtype=torch.int)
         return metrics
+
+    def plot(
+        self, val: Optional[Union[Dict[str, Tensor], Sequence[Dict[str, Tensor]]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure object and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import tensor
+            >>> from torchmetrics.detection.mean_ap import MeanAveragePrecision
+            >>> preds = [dict(
+            ...     boxes=tensor([[258.0, 41.0, 606.0, 285.0]]),
+            ...     scores=tensor([0.536]),
+            ...     labels=tensor([0]),
+            ... )]
+            >>> target = [dict(
+            ...     boxes=tensor([[214.0, 41.0, 562.0, 285.0]]),
+            ...     labels=tensor([0]),
+            ... )]
+            >>> metric = MeanAveragePrecision()
+            >>> metric.update(preds, target)
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> import torch
+            >>> from torchmetrics.detection.mean_ap import MeanAveragePrecision
+            >>> preds = lambda: [dict(
+            ...     boxes=torch.tensor([[258.0, 41.0, 606.0, 285.0]]) + torch.randint(10, (1,4)),
+            ...     scores=torch.tensor([0.536]) + 0.1*torch.rand(1),
+            ...     labels=torch.tensor([0]),
+            ... )]
+            >>> target = [dict(
+            ...     boxes=torch.tensor([[214.0, 41.0, 562.0, 285.0]]),
+            ...     labels=torch.tensor([0]),
+            ... )]
+            >>> metric = MeanAveragePrecision()
+            >>> vals = []
+            >>> for _ in range(20):
+            ...     vals.append(metric(preds(), target))
+            >>> fig_, ax_ = metric.plot(vals)
+        """
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/audio/__init__.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/multimodal/__init__.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,26 +1,21 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from torchmetrics.functional.audio.pit import permutation_invariant_training, pit_permutate  # noqa: F401
-from torchmetrics.functional.audio.sdr import (  # noqa: F401
-    scale_invariant_signal_distortion_ratio,
-    signal_distortion_ratio,
-)
-from torchmetrics.functional.audio.snr import scale_invariant_signal_noise_ratio, signal_noise_ratio  # noqa: F401
-from torchmetrics.utilities.imports import _PESQ_AVAILABLE, _PYSTOI_AVAILABLE
+from torchmetrics.utilities.imports import _TRANSFORMERS_AVAILABLE
 
-if _PESQ_AVAILABLE:
-    from torchmetrics.functional.audio.pesq import perceptual_evaluation_speech_quality  # noqa: F401
+__all__ = []
 
-if _PYSTOI_AVAILABLE:
-    from torchmetrics.functional.audio.stoi import short_time_objective_intelligibility  # noqa: F401
+if _TRANSFORMERS_AVAILABLE:
+    from torchmetrics.functional.multimodal.clip_score import clip_score  # noqa: F401
+
+    __all__.append("clip_score")
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/audio/pit.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/audio/pit.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -13,28 +13,30 @@
 # limitations under the License.
 from itertools import permutations
 from typing import Any, Callable, Tuple, Union
 from warnings import warn
 
 import torch
 from torch import Tensor
+from typing_extensions import Literal
 
 from torchmetrics.utilities.imports import _SCIPY_AVAILABLE
 
 # _ps_dict: cache of permutations
 # it's necessary to cache it, otherwise it will consume a large amount of time
 _ps_dict: dict = {}  # _ps_dict[str(spk_num)+str(device)] = permutations
 
 
 def _find_best_perm_by_linear_sum_assignment(
     metric_mtx: Tensor,
     eval_func: Union[torch.min, torch.max],
 ) -> Tuple[Tensor, Tensor]:
-    """Solves the linear sum assignment problem using scipy, and returns the best metric values and the
-    corresponding permutations.
+    """Solves the linear sum assignment problem.
+
+    This implementation uses scipy and input is therefore transferred to cpu during calculations.
 
     Args:
         metric_mtx: the metric matrix, shape [batch_size, spk_num, spk_num]
         eval_func: the function to reduce the metric values of different the permutations
 
     Returns:
         best_metric: shape ``[batch]``
@@ -49,16 +51,18 @@
     return best_metric, best_perm  # shape [batch], shape [batch, spk]
 
 
 def _find_best_perm_by_exhaustive_method(
     metric_mtx: Tensor,
     eval_func: Union[torch.min, torch.max],
 ) -> Tuple[Tensor, Tensor]:
-    """Solves the linear sum assignment problem using exhaustive method, i.e. exhaustively calculates the metric
-    values of all possible permutations, and returns the best metric values and the corresponding permutations.
+    """Solves the linear sum assignment problem using exhaustive method.
+
+    This is done by exhaustively calculating the metric values of all possible permutations, and returns the best metric
+    values and the corresponding permutations.
 
     Args:
         metric_mtx: the metric matrix, shape ``[batch_size, spk_num, spk_num]``
         eval_func: the function to reduce the metric values of different the permutations
 
     Returns:
         best_metric: shape ``[batch]``
@@ -89,33 +93,34 @@
     best_metric, best_indexes = eval_func(metric_of_ps, dim=1)
     best_indexes = best_indexes.detach()
     best_perm = ps.T[best_indexes, :]
     return best_metric, best_perm  # shape [batch], shape [batch, spk]
 
 
 def permutation_invariant_training(
-    preds: Tensor, target: Tensor, metric_func: Callable, eval_func: str = "max", **kwargs: Any
+    preds: Tensor, target: Tensor, metric_func: Callable, eval_func: Literal["max", "min"] = "max", **kwargs: Any
 ) -> Tuple[Tensor, Tensor]:
-    """Permutation invariant training (PIT). The ``permutation_invariant_training`` implements the famous
-    Permutation Invariant Training method.
+    """Calculate `Permutation invariant training`_ (PIT).
 
-    [1] in speech separation field in order to calculate audio metrics in a permutation invariant way.
+    This metric can evaluate models for speaker independent multi-talker speech separation in a permutation
+    invariant way.
 
     Args:
-        preds: shape ``[batch, spk, ...]``
-        target: shape ``[batch, spk, ...]``
+        preds: float tensor with shape ``(batch_size,num_speakers,...)``
+        target: float tensor with shape ``(batch_size,num_speakers,...)``
         metric_func: a metric function accept a batch of target and estimate,
-            i.e. ``metric_func(preds[:, i, ...], target[:, j, ...])``, and returns a batch of metric tensors ``[batch]``
+            i.e. ``metric_func(preds[:, i, ...], target[:, j, ...])``, and returns a batch of metric
+            tensors ``(batch,)``
         eval_func: the function to find the best permutation, can be ``'min'`` or ``'max'``,
             i.e. the smaller the better or the larger the better.
         kwargs: Additional args for metric_func
 
     Returns:
-        best_metric of shape ``[batch]``
-        best_perm of shape ``[batch]``
+        Tuple of two float tensors. First tensor with shape ``(batch,)`` contains the best metric value for each sample
+        and second tensor with shape ``(batch,)`` contains the best permutation.
 
     Example:
         >>> from torchmetrics.functional.audio import scale_invariant_signal_distortion_ratio
         >>> # [batch, spk, time]
         >>> preds = torch.tensor([[[-0.0579,  0.3560, -0.9604], [-0.1719,  0.3205,  0.2951]]])
         >>> target = torch.tensor([[[ 1.0958, -0.1648,  0.5228], [-0.4100,  1.1942, -0.5103]]])
         >>> best_metric, best_perm = permutation_invariant_training(
@@ -123,17 +128,14 @@
         >>> best_metric
         tensor([-5.1091])
         >>> best_perm
         tensor([[0, 1]])
         >>> pit_permutate(preds, best_perm)
         tensor([[[-0.0579,  0.3560, -0.9604],
                  [-0.1719,  0.3205,  0.2951]]])
-
-    Reference:
-        [1]	`Permutation Invariant Training of Deep Models`_
     """
     if preds.shape[0:2] != target.shape[0:2]:
         raise RuntimeError(
             "Predictions and targets are expected to have the same shape at the batch and speaker dimensions"
         )
     if eval_func not in ["max", "min"]:
         raise ValueError(f'eval_func can only be "max" or "min" but got {eval_func}')
@@ -173,9 +175,8 @@
     Args:
         preds: the estimates you want to permutate, shape [batch, spk, ...]
         perm: the permutation returned from permutation_invariant_training, shape [batch, spk]
 
     Returns:
         Tensor: the permutated version of estimate
     """
-    preds_pmted = torch.stack([torch.index_select(pred, 0, p) for pred, p in zip(preds, perm)])
-    return preds_pmted
+    return torch.stack([torch.index_select(pred, 0, p) for pred, p in zip(preds, perm)])
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/audio/sdr.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/audio/sdr.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -15,47 +15,38 @@
 import math
 import warnings
 from typing import Optional, Tuple
 
 import torch
 from torch import Tensor
 
-from torchmetrics.utilities.checks import _check_same_shape
-from torchmetrics.utilities.imports import _FAST_BSS_EVAL_AVAILABLE, _TORCH_GREATER_EQUAL_1_8
-
 # import or def the norm/solve function
-if _TORCH_GREATER_EQUAL_1_8:
-    from torch.linalg import norm
+from torch.linalg import norm
 
-    solve = torch.linalg.solve
-else:
-    from torch import norm
-    from torch import solve as _solve
-    from torch.nn.functional import pad
-
-    def solve(A: Tensor, b: Tensor) -> Tensor:
-        return _solve(b[..., None], A)[0][..., 0]
+from torchmetrics.utilities.checks import _check_same_shape
+from torchmetrics.utilities.imports import _FAST_BSS_EVAL_AVAILABLE
 
+solve = torch.linalg.solve
 
-if _FAST_BSS_EVAL_AVAILABLE and _TORCH_GREATER_EQUAL_1_8:
+if _FAST_BSS_EVAL_AVAILABLE:
     from fast_bss_eval.torch.cgd import toeplitz_conjugate_gradient
 else:
     toeplitz_conjugate_gradient = None
 
 
 def _symmetric_toeplitz(vector: Tensor) -> Tensor:
     """Construct a symmetric Toeplitz matrix using one vector.
 
     Args:
         vector: shape [..., L]
 
     Example:
+        >>> from torch import tensor
         >>> from torchmetrics.functional.audio.sdr import _symmetric_toeplitz
-        >>> import torch
-        >>> v = torch.tensor([0, 1, 2, 3, 4])
+        >>> v = tensor([0, 1, 2, 3, 4])
         >>> _symmetric_toeplitz(v)
         tensor([[0, 1, 2, 3, 4],
                 [1, 0, 1, 2, 3],
                 [2, 1, 0, 1, 2],
                 [3, 2, 1, 0, 1],
                 [4, 3, 2, 1, 0]])
 
@@ -66,18 +57,19 @@
     v_len = vector.shape[-1]
     return torch.as_strided(
         vec_exp, size=vec_exp.shape[:-1] + (v_len, v_len), stride=vec_exp.stride()[:-1] + (1, 1)
     ).flip(dims=(-1,))
 
 
 def _compute_autocorr_crosscorr(target: Tensor, preds: Tensor, corr_len: int) -> Tuple[Tensor, Tensor]:
-    r"""Compute the auto correlation of `target` and the cross correlation of `target` and `preds` using the fast
-    Fourier transform (FFT). Let's denotes the symmetric Toeplitz matric of the auto correlation of `target` as
-    `R`, the cross correlation as 'b', then solving the equation `Rh=b` could have `h` as the coordinate of
-    `preds` in the column space of the `corr_len` shifts of `target`.
+    r"""Compute the auto correlation of `target` and the cross correlation of `target` and `preds`.
+
+    This calculation is done using the fast Fourier transform (FFT). Let's denotes the symmetric Toeplitz matric of the
+    auto correlation of `target` as `R`, the cross correlation as 'b', then solving the equation `Rh=b` could have `h`
+    as the coordinate of `preds` in the column space of the `corr_len` shifts of `target`.
 
     Args:
         target: the target (reference) signal of shape [..., time]
         preds: the preds (estimated) signal of shape [..., time]
         corr_len: the length of the auto correlation and cross correlation
 
     Returns:
@@ -85,56 +77,40 @@
         the cross correlation of `target` and `preds` of shape [..., corr_len]
     """
     # the valid length for the signal after convolution
     n_fft = 2 ** math.ceil(math.log2(preds.shape[-1] + target.shape[-1] - 1))
 
     # computes the auto correlation of `target`
     # r_0 is the first row of the symmetric Toeplitz matric
-    if _TORCH_GREATER_EQUAL_1_8:
-        t_fft = torch.fft.rfft(target, n=n_fft, dim=-1)
-        r_0 = torch.fft.irfft(t_fft.real**2 + t_fft.imag**2, n=n_fft)[..., :corr_len]
-    else:
-        t_pad = pad(target, (0, n_fft - target.shape[-1]), "constant", 0)
-        t_fft = torch.rfft(t_pad, signal_ndim=1)
-        real = t_fft[..., 0] ** 2 + t_fft[..., 1] ** 2
-        imag = torch.zeros(real.shape, dtype=real.dtype, device=real.device)
-        result = torch.stack([real, imag], len(real.shape))
-        r_0 = torch.irfft(result, signal_ndim=1, signal_sizes=[n_fft])[..., :corr_len]
+    t_fft = torch.fft.rfft(target, n=n_fft, dim=-1)
+    r_0 = torch.fft.irfft(t_fft.real**2 + t_fft.imag**2, n=n_fft)[..., :corr_len]
 
     # computes the cross-correlation of `target` and `preds`
-    if _TORCH_GREATER_EQUAL_1_8:
-        p_fft = torch.fft.rfft(preds, n=n_fft, dim=-1)
-        b = torch.fft.irfft(t_fft.conj() * p_fft, n=n_fft, dim=-1)[..., :corr_len]
-    else:
-        p_pad = pad(preds, (0, n_fft - preds.shape[-1]), "constant", 0)
-        p_fft = torch.rfft(p_pad, signal_ndim=1)
-        real = t_fft[..., 0] * p_fft[..., 0] + t_fft[..., 1] * p_fft[..., 1]
-        imag = t_fft[..., 0] * p_fft[..., 1] - t_fft[..., 1] * p_fft[..., 0]
-        result = torch.stack([real, imag], len(real.shape))
-        b = torch.irfft(result, signal_ndim=1, signal_sizes=[n_fft])[..., :corr_len]
+    p_fft = torch.fft.rfft(preds, n=n_fft, dim=-1)
+    b = torch.fft.irfft(t_fft.conj() * p_fft, n=n_fft, dim=-1)[..., :corr_len]
 
     return r_0, b
 
 
 def signal_distortion_ratio(
     preds: Tensor,
     target: Tensor,
     use_cg_iter: Optional[int] = None,
     filter_length: int = 512,
     zero_mean: bool = False,
     load_diag: Optional[float] = None,
 ) -> Tensor:
-    r"""Signal to Distortion Ratio (SDR) [1,2]
+    r"""Calculate Signal to Distortion Ratio (SDR) metric. See `SDR ref1`_ and `SDR ref2`_ for details on the metric.
 
     .. note:
         The metric currently does not seem to work with Pytorch v1.11 and specific GPU hardware.
 
     Args:
-        preds: shape ``[..., time]``
-        target: shape ``[..., time]``
+        preds: float tensor with shape ``(...,time)``
+        target: float tensor with shape ``(...,time)``
         use_cg_iter:
             If provided, conjugate gradient descent is used to solve for the distortion
             filter coefficients instead of direct Gaussian elimination, which requires that
             ``fast-bss-eval`` is installed and pytorch version >= 1.8.
             This can speed up the computation of the metrics in case the filters
             are long. Using a value of 10 here has been shown to provide
             good accuracy in most cases and is sufficient when using this
@@ -143,19 +119,23 @@
         zero_mean: When set to True, the mean of all signals is subtracted prior to computation of the metrics
         load_diag:
             If provided, this small value is added to the diagonal coefficients of
             the system metrics when solving for the filter coefficients.
             This can help stabilize the metric in the case where some reference signals may sometimes be zero
 
     Returns:
-        sdr value of shape ``[...]``
+        Float tensor with shape ``(...,)`` of SDR values per sample
+
+    Raises:
+        RuntimeError:
+            If ``preds`` and ``target`` does not have the same shape
 
     Example:
-        >>> from torchmetrics.functional.audio import signal_distortion_ratio
         >>> import torch
+        >>> from torchmetrics.functional.audio import signal_distortion_ratio
         >>> g = torch.manual_seed(1)
         >>> preds = torch.randn(8000)
         >>> target = torch.randn(8000)
         >>> signal_distortion_ratio(preds, target)
         tensor(-12.0589)
         >>> # use with permutation_invariant_training
         >>> from torchmetrics.functional.audio import permutation_invariant_training
@@ -165,20 +145,14 @@
         >>> best_metric
         tensor([-11.6375, -11.4358, -11.7148, -11.6325])
         >>> best_perm
         tensor([[1, 0],
                 [0, 1],
                 [1, 0],
                 [0, 1]])
-
-    References:
-        [1] Vincent, E., Gribonval, R., & Fevotte, C. (2006). Performance measurement in blind audio source separation.
-        IEEE Transactions on Audio, Speech and Language Processing, 14(4), 14621469.
-
-        [2] Scheibler, R. (2021). SDR -- Medium Rare with Fast Computations.
     """
     _check_same_shape(preds, target)
 
     # use double precision
     preds_dtype = preds.dtype
     preds = preds.double()
     target = target.double()
@@ -195,73 +169,65 @@
     # compute auto-correlation and cross-correlation
     r_0, b = _compute_autocorr_crosscorr(target, preds, corr_len=filter_length)
 
     if load_diag is not None:
         # the diagonal factor of the Toeplitz matrix is the first coefficient of r_0
         r_0[..., 0] += load_diag
 
-    if use_cg_iter is not None and _FAST_BSS_EVAL_AVAILABLE and _TORCH_GREATER_EQUAL_1_8:
+    if use_cg_iter is not None and _FAST_BSS_EVAL_AVAILABLE:
         # use preconditioned conjugate gradient
         sol = toeplitz_conjugate_gradient(r_0, b, n_iter=use_cg_iter)
     else:
-        if use_cg_iter is not None:
-            if not _FAST_BSS_EVAL_AVAILABLE:
-                warnings.warn(
-                    "The `use_cg_iter` parameter of `SDR` requires that `fast-bss-eval` is installed. "
-                    "To make this this warning disappear, you could install `fast-bss-eval` using "
-                    "`pip install fast-bss-eval` or set `use_cg_iter=None`. For this time, the solver "
-                    "provided by Pytorch is used.",
-                    UserWarning,
-                )
-            elif not _TORCH_GREATER_EQUAL_1_8:
-                warnings.warn(
-                    "The `use_cg_iter` parameter of `SDR` requires a Pytorch version >= 1.8. "
-                    "To make this this warning disappear, you could change to Pytorch v1.8+ or set `use_cg_iter=None`. "
-                    "For this time, the solver provided by Pytorch is used.",
-                    UserWarning,
-                )
+        if use_cg_iter is not None and not _FAST_BSS_EVAL_AVAILABLE:
+            warnings.warn(
+                "The `use_cg_iter` parameter of `SDR` requires that `fast-bss-eval` is installed. "
+                "To make this this warning disappear, you could install `fast-bss-eval` using "
+                "`pip install fast-bss-eval` or set `use_cg_iter=None`. For this time, the solver "
+                "provided by Pytorch is used.",
+                UserWarning,
+            )
         # regular matrix solver
         r = _symmetric_toeplitz(r_0)  # the auto-correlation of the L shifts of `target`
         sol = solve(r, b)
 
     # compute the coherence
     coh = torch.einsum("...l,...l->...", b, sol)
 
     # transform to decibels
     ratio = coh / (1 - coh)
     val = 10.0 * torch.log10(ratio)
 
     if preds_dtype == torch.float64:
         return val
-    else:
-        return val.float()
+    return val.float()
 
 
 def scale_invariant_signal_distortion_ratio(preds: Tensor, target: Tensor, zero_mean: bool = False) -> Tensor:
-    """Calculates Scale-invariant signal-to-distortion ratio (SI-SDR) metric. The SI-SDR value is in general
-    considered an overall measure of how good a source sound.
+    """`Scale-invariant signal-to-distortion ratio`_ (SI-SDR).
+
+    The SI-SDR value is in general considered an overall measure of how good a source sound.
 
     Args:
-        preds: shape ``[...,time]``
-        target: shape ``[...,time]``
+        preds: float tensor with shape ``(...,time)``
+        target: float tensor with shape ``(...,time)``
         zero_mean: If to zero mean target and preds or not
 
     Returns:
-        si-sdr value of shape [...]
+        Float tensor with shape ``(...,)`` of SDR values per sample
+
+    Raises:
+        RuntimeError:
+            If ``preds`` and ``target`` does not have the same shape
 
     Example:
         >>> from torchmetrics.functional.audio import scale_invariant_signal_distortion_ratio
         >>> target = torch.tensor([3.0, -0.5, 2.0, 7.0])
         >>> preds = torch.tensor([2.5, 0.0, 2.0, 8.0])
         >>> scale_invariant_signal_distortion_ratio(preds, target)
         tensor(18.4030)
-
-    References:
-        [1] Le Roux, Jonathan, et al. "SDR half-baked or well done." IEEE International Conference on Acoustics, Speech
-        and Signal Processing (ICASSP) 2019.
     """
     _check_same_shape(preds, target)
     eps = torch.finfo(preds.dtype).eps
 
     if zero_mean:
         target = target - torch.mean(target, dim=-1, keepdim=True)
         preds = preds - torch.mean(preds, dim=-1, keepdim=True)
@@ -270,10 +236,8 @@
         torch.sum(target**2, dim=-1, keepdim=True) + eps
     )
     target_scaled = alpha * target
 
     noise = target_scaled - preds
 
     val = (torch.sum(target_scaled**2, dim=-1) + eps) / (torch.sum(noise**2, dim=-1) + eps)
-    val = 10 * torch.log10(val)
-
-    return val
+    return 10 * torch.log10(val)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/audio/snr.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/audio/snr.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -16,75 +16,70 @@
 from torch import Tensor
 
 from torchmetrics.functional.audio.sdr import scale_invariant_signal_distortion_ratio
 from torchmetrics.utilities.checks import _check_same_shape
 
 
 def signal_noise_ratio(preds: Tensor, target: Tensor, zero_mean: bool = False) -> Tensor:
-    r"""Signal-to-noise ratio (SNR_):
+    r"""Calculate `Signal-to-noise ratio`_ (SNR_) meric for evaluating quality of audio.
 
     .. math::
         \text{SNR} = \frac{P_{signal}}{P_{noise}}
 
-    where  :math:`P` denotes the power of each signal. The SNR metric compares the level
-    of the desired signal to the level of background noise. Therefore, a high value of
-    SNR means that the audio is clear.
+    where  :math:`P` denotes the power of each signal. The SNR metric compares the level of the desired signal to
+    the level of background noise. Therefore, a high value of SNR means that the audio is clear.
 
     Args:
-        preds: shape ``[...,time]``
-        target: shape ``[...,time]``
+        preds: float tensor with shape ``(...,time)``
+        target: float tensor with shape ``(...,time)``
         zero_mean: if to zero mean target and preds or not
 
     Returns:
-        snr value of shape [...]
+        Float tensor with shape ``(...,)`` of SNR values per sample
+
+    Raises:
+        RuntimeError:
+            If ``preds`` and ``target`` does not have the same shape
 
     Example:
         >>> from torchmetrics.functional.audio import signal_noise_ratio
         >>> target = torch.tensor([3.0, -0.5, 2.0, 7.0])
         >>> preds = torch.tensor([2.5, 0.0, 2.0, 8.0])
         >>> signal_noise_ratio(preds, target)
         tensor(16.1805)
-
-    References:
-        [1] Le Roux, Jonathan, et al. "SDR half-baked or well done." IEEE International Conference on Acoustics, Speech
-        and Signal Processing (ICASSP) 2019.
-
     """
     _check_same_shape(preds, target)
     eps = torch.finfo(preds.dtype).eps
 
     if zero_mean:
         target = target - torch.mean(target, dim=-1, keepdim=True)
         preds = preds - torch.mean(preds, dim=-1, keepdim=True)
 
     noise = target - preds
 
     snr_value = (torch.sum(target**2, dim=-1) + eps) / (torch.sum(noise**2, dim=-1) + eps)
-    snr_value = 10 * torch.log10(snr_value)
-
-    return snr_value
+    return 10 * torch.log10(snr_value)
 
 
 def scale_invariant_signal_noise_ratio(preds: Tensor, target: Tensor) -> Tensor:
-    """Scale-invariant signal-to-noise ratio (SI-SNR).
+    """`Scale-invariant signal-to-noise ratio`_ (SI-SNR).
 
     Args:
-        preds: shape ``[...,time]``
-        target: shape ``[...,time]``
+        preds: float tensor with shape ``(...,time)``
+        target: float tensor with shape ``(...,time)``
 
     Returns:
-        si-snr value of shape [...]
+         Float tensor with shape ``(...,)`` of SI-SNR values per sample
+
+    Raises:
+        RuntimeError:
+            If ``preds`` and ``target`` does not have the same shape
 
     Example:
         >>> import torch
         >>> from torchmetrics.functional.audio import scale_invariant_signal_noise_ratio
         >>> target = torch.tensor([3.0, -0.5, 2.0, 7.0])
         >>> preds = torch.tensor([2.5, 0.0, 2.0, 8.0])
         >>> scale_invariant_signal_noise_ratio(preds, target)
         tensor(15.0918)
-
-    References:
-        [1] Y. Luo and N. Mesgarani, "TaSNet: Time-Domain Audio Separation Network for Real-Time, Single-Channel Speech
-        Separation," 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp.
-        696-700, doi: 10.1109/ICASSP.2018.8462116.
     """
     return scale_invariant_signal_distortion_ratio(preds=preds, target=target, zero_mean=True)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/classification/__init__.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/__init__.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,37 +1,49 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from torchmetrics.functional.classification.accuracy import accuracy  # noqa: F401
-from torchmetrics.functional.classification.auc import auc  # noqa: F401
-from torchmetrics.functional.classification.auroc import auroc  # noqa: F401
-from torchmetrics.functional.classification.average_precision import average_precision  # noqa: F401
-from torchmetrics.functional.classification.calibration_error import calibration_error  # noqa: F401
-from torchmetrics.functional.classification.cohen_kappa import cohen_kappa  # noqa: F401
-from torchmetrics.functional.classification.confusion_matrix import confusion_matrix  # noqa: F401
-from torchmetrics.functional.classification.dice import dice, dice_score  # noqa: F401
-from torchmetrics.functional.classification.f_beta import f1_score, fbeta_score  # noqa: F401
-from torchmetrics.functional.classification.hamming import hamming_distance  # noqa: F401
-from torchmetrics.functional.classification.hinge import hinge_loss  # noqa: F401
-from torchmetrics.functional.classification.jaccard import jaccard_index  # noqa: F401
-from torchmetrics.functional.classification.kl_divergence import kl_divergence  # noqa: F401
-from torchmetrics.functional.classification.matthews_corrcoef import matthews_corrcoef  # noqa: F401
-from torchmetrics.functional.classification.precision_recall import precision, precision_recall, recall  # noqa: F401
-from torchmetrics.functional.classification.precision_recall_curve import precision_recall_curve  # noqa: F401
-from torchmetrics.functional.classification.ranking import (  # noqa: F401
-    coverage_error,
-    label_ranking_average_precision,
-    label_ranking_loss,
-)
-from torchmetrics.functional.classification.roc import roc  # noqa: F401
-from torchmetrics.functional.classification.specificity import specificity  # noqa: F401
-from torchmetrics.functional.classification.stat_scores import stat_scores  # noqa: F401
+
+from torchmetrics.functional.text.bleu import bleu_score
+from torchmetrics.functional.text.cer import char_error_rate
+from torchmetrics.functional.text.chrf import chrf_score
+from torchmetrics.functional.text.eed import extended_edit_distance
+from torchmetrics.functional.text.mer import match_error_rate
+from torchmetrics.functional.text.perplexity import perplexity
+from torchmetrics.functional.text.rouge import rouge_score
+from torchmetrics.functional.text.sacre_bleu import sacre_bleu_score
+from torchmetrics.functional.text.squad import squad
+from torchmetrics.functional.text.ter import translation_edit_rate
+from torchmetrics.functional.text.wer import word_error_rate
+from torchmetrics.functional.text.wil import word_information_lost
+from torchmetrics.functional.text.wip import word_information_preserved
+from torchmetrics.utilities.imports import _TRANSFORMERS_AVAILABLE
+
+if _TRANSFORMERS_AVAILABLE:
+    from torchmetrics.functional.text.bert import bert_score  # noqa: F401
+    from torchmetrics.functional.text.infolm import infolm  # noqa: F401
+
+
+__all__ = [
+    "bleu_score",
+    "char_error_rate",
+    "chrf_score",
+    "extended_edit_distance",
+    "match_error_rate",
+    "perplexity",
+    "rouge_score",
+    "sacre_bleu_score",
+    "squad",
+    "translation_edit_rate",
+    "word_error_rate",
+    "word_information_lost",
+    "word_information_preserved",
+]
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/classification/accuracy.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/hamming.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,420 +1,426 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Optional, Tuple
+from typing import Optional
 
 import torch
-from torch import Tensor, tensor
+from torch import Tensor
+from typing_extensions import Literal
 
-from torchmetrics.functional.classification.stat_scores import _reduce_stat_scores, _stat_scores_update
-from torchmetrics.utilities.checks import _check_classification_inputs, _input_format_classification, _input_squeeze
-from torchmetrics.utilities.enums import AverageMethod, DataType, MDMCAverageMethod
+from torchmetrics.functional.classification.stat_scores import (
+    _binary_stat_scores_arg_validation,
+    _binary_stat_scores_format,
+    _binary_stat_scores_tensor_validation,
+    _binary_stat_scores_update,
+    _multiclass_stat_scores_arg_validation,
+    _multiclass_stat_scores_format,
+    _multiclass_stat_scores_tensor_validation,
+    _multiclass_stat_scores_update,
+    _multilabel_stat_scores_arg_validation,
+    _multilabel_stat_scores_format,
+    _multilabel_stat_scores_tensor_validation,
+    _multilabel_stat_scores_update,
+)
+from torchmetrics.utilities.compute import _safe_divide
+from torchmetrics.utilities.enums import ClassificationTask
 
 
-def _check_subset_validity(mode: DataType) -> bool:
-    """Checks input mode is valid."""
-    return mode in (DataType.MULTILABEL, DataType.MULTIDIM_MULTICLASS)
+def _hamming_distance_reduce(
+    tp: Tensor,
+    fp: Tensor,
+    tn: Tensor,
+    fn: Tensor,
+    average: Optional[Literal["binary", "micro", "macro", "weighted", "none"]],
+    multidim_average: Literal["global", "samplewise"] = "global",
+    multilabel: bool = False,
+) -> Tensor:
+    """Reduce classification statistics into hamming distance.
+
+    Args:
+        tp: number of true positives
+        fp: number of false positives
+        tn: number of true negatives
+        fn: number of false negatives
+        average:
+            Defines the reduction that is applied over labels. Should be one of the following:
 
+            - ``binary``: for binary reduction
+            - ``micro``: sum score over all classes/labels
+            - ``macro``: salculate score for each class/label and average them
+            - ``weighted``: calculates score for each class/label and computes weighted average using their support
+            - ``"none"`` or ``None``: calculates score for each class/label and applies no reduction
 
-def _mode(
+        multidim_average:
+            Defines how additionally dimensions ``...`` should be handled. Should be one of the following:
+
+            - ``global``: Additional dimensions are flatted along the batch dimension
+            - ``samplewise``: Statistic will be calculated independently for each sample on the ``N`` axis.
+
+        multilabel: If input is multilabel or not
+    """
+    if average == "binary":
+        return 1 - _safe_divide(tp + tn, tp + fp + tn + fn)
+    if average == "micro":
+        tp = tp.sum(dim=0 if multidim_average == "global" else 1)
+        fn = fn.sum(dim=0 if multidim_average == "global" else 1)
+        if multilabel:
+            fp = fp.sum(dim=0 if multidim_average == "global" else 1)
+            tn = tn.sum(dim=0 if multidim_average == "global" else 1)
+            return 1 - _safe_divide(tp + tn, tp + tn + fp + fn)
+        return 1 - _safe_divide(tp, tp + fn)
+
+    score = 1 - _safe_divide(tp + tn, tp + tn + fp + fn) if multilabel else 1 - _safe_divide(tp, tp + fn)
+    if average is None or average == "none":
+        return score
+    weights = tp + fn if average == "weighted" else torch.ones_like(score)
+    return _safe_divide(weights * score, weights.sum(-1, keepdim=True)).sum(-1)
+
+
+def binary_hamming_distance(
     preds: Tensor,
     target: Tensor,
-    threshold: float,
-    top_k: Optional[int],
-    num_classes: Optional[int],
-    multiclass: Optional[bool],
+    threshold: float = 0.5,
+    multidim_average: Literal["global", "samplewise"] = "global",
     ignore_index: Optional[int] = None,
-) -> DataType:
-    """Finds the mode of the input tensors.
+    validate_args: bool = True,
+) -> Tensor:
+    r"""Compute the average `Hamming distance`_ (also known as Hamming loss) for binary tasks.
+
+    .. math::
+        \text{Hamming distance} = \frac{1}{N \cdot L} \sum_i^N \sum_l^L 1(y_{il} \neq \hat{y}_{il})
+
+    Where :math:`y` is a tensor of target values, :math:`\hat{y}` is a tensor of predictions,
+    and :math:`\bullet_{il}` refers to the :math:`l`-th label of the :math:`i`-th sample of that
+    tensor.
+
+    Accepts the following input tensors:
+
+    - ``preds`` (int or float tensor): ``(N, ...)``. If preds is a floating point tensor with values outside
+      [0,1] range we consider the input to be logits and will auto apply sigmoid per element. Addtionally,
+      we convert to int tensor with thresholding using the value in ``threshold``.
+    - ``target`` (int tensor): ``(N, ...)``
 
     Args:
-        preds: Predicted tensor
-        target: Ground truth tensor
-        threshold: Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the
-            case of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.
-        top_k: Number of the highest probability or logit score predictions considered finding the correct label,
-            relevant only for (multi-dimensional) multi-class inputs.
-        num_classes: Number of classes. Necessary for ``'macro'``, ``'weighted'`` and ``None`` average methods.
-        multiclass:
-            Used only in certain special cases, where you want to treat inputs as a different type
-            than what they appear to be.
-
-    Example:
-        >>> target = torch.tensor([0, 1, 2, 3])
-        >>> preds = torch.tensor([0, 2, 1, 3])
-        >>> _mode(preds, target, 0.5, None, None, None)
-        <DataType.MULTICLASS: 'multi-class'>
-    """
+        preds: Tensor with predictions
+        target: Tensor with true labels
+        threshold: Threshold for transforming probability to binary {0,1} predictions
+        multidim_average:
+            Defines how additionally dimensions ``...`` should be handled. Should be one of the following:
+
+            - ``global``: Additional dimensions are flatted along the batch dimension
+            - ``samplewise``: Statistic will be calculated independently for each sample on the ``N`` axis.
+              The statistics in this case are calculated over the additional dimensions.
 
-    mode = _check_classification_inputs(
-        preds,
-        target,
-        threshold=threshold,
-        top_k=top_k,
-        num_classes=num_classes,
-        multiclass=multiclass,
-        ignore_index=ignore_index,
-    )
-    return mode
+        ignore_index:
+            Specifies a target value that is ignored and does not contribute to the metric calculation
+        validate_args: bool indicating if input arguments and tensors should be validated for correctness.
+            Set to ``False`` for faster computations.
+
+    Returns:
+        If ``multidim_average`` is set to ``global``, the metric returns a scalar value. If ``multidim_average``
+        is set to ``samplewise``, the metric returns ``(N,)`` vector consisting of a scalar value per sample.
+
+    Example (preds is int tensor):
+        >>> from torch import tensor
+        >>> from torchmetrics.functional.classification import binary_hamming_distance
+        >>> target = tensor([0, 1, 0, 1, 0, 1])
+        >>> preds = tensor([0, 0, 1, 1, 0, 1])
+        >>> binary_hamming_distance(preds, target)
+        tensor(0.3333)
+
+    Example (preds is float tensor):
+        >>> from torchmetrics.functional.classification import binary_hamming_distance
+        >>> target = tensor([0, 1, 0, 1, 0, 1])
+        >>> preds = tensor([0.11, 0.22, 0.84, 0.73, 0.33, 0.92])
+        >>> binary_hamming_distance(preds, target)
+        tensor(0.3333)
+
+    Example (multidim tensors):
+        >>> from torchmetrics.functional.classification import binary_hamming_distance
+        >>> target = tensor([[[0, 1], [1, 0], [0, 1]], [[1, 1], [0, 0], [1, 0]]])
+        >>> preds = tensor([[[0.59, 0.91], [0.91, 0.99], [0.63, 0.04]],
+        ...                 [[0.38, 0.04], [0.86, 0.780], [0.45, 0.37]]])
+        >>> binary_hamming_distance(preds, target, multidim_average='samplewise')
+        tensor([0.6667, 0.8333])
+    """
+    if validate_args:
+        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index)
+        _binary_stat_scores_tensor_validation(preds, target, multidim_average, ignore_index)
+    preds, target = _binary_stat_scores_format(preds, target, threshold, ignore_index)
+    tp, fp, tn, fn = _binary_stat_scores_update(preds, target, multidim_average)
+    return _hamming_distance_reduce(tp, fp, tn, fn, average="binary", multidim_average=multidim_average)
 
 
-def _accuracy_update(
+def multiclass_hamming_distance(
     preds: Tensor,
     target: Tensor,
-    reduce: Optional[str],
-    mdmc_reduce: Optional[str],
-    threshold: float,
-    num_classes: Optional[int],
-    top_k: Optional[int],
-    multiclass: Optional[bool],
-    ignore_index: Optional[int],
-    mode: DataType,
-) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
-    """Updates and returns stat scores (true positives, false positives, true negatives, false negatives) required
-    to compute accuracy.
+    num_classes: int,
+    average: Optional[Literal["micro", "macro", "weighted", "none"]] = "macro",
+    top_k: int = 1,
+    multidim_average: Literal["global", "samplewise"] = "global",
+    ignore_index: Optional[int] = None,
+    validate_args: bool = True,
+) -> Tensor:
+    r"""Compute the average `Hamming distance`_ (also known as Hamming loss) for multiclass tasks.
 
-    Args:
-        preds: Predicted tensor
-        target: Ground truth tensor
-        reduce: Defines the reduction that is applied.
-        mdmc_reduce: Defines how the multi-dimensional multi-class inputs are handled.
-        threshold: Threshold for transforming probability or logit predictions to binary (0,1) predictions, in
-            the case of binary or multi-label inputs.
-        num_classes: Number of classes. Necessary for ``'macro'``, ``'weighted'`` and ``None`` average methods.
-        top_k: Number of the highest probability or logit score predictions considered finding the correct label,
-            relevant only for (multi-dimensional) multi-class inputs.
-        multiclass: Used only in certain special cases, where you want to treat inputs as a different type
-            than what they appear to be.
-        ignore_index: Integer specifying a target class to ignore. If given, this class index does not contribute
-            to the returned score, regardless of reduction method. If an index is ignored, and ``average=None``
-            or ``'none'``, the score for the ignored class will be returned as ``nan``.
-        mode: Mode of the input tensors.
-    """
+    .. math::
+        \text{Hamming distance} = \frac{1}{N \cdot L} \sum_i^N \sum_l^L 1(y_{il} \neq \hat{y}_{il})
 
-    if mode == DataType.MULTILABEL and top_k:
-        raise ValueError("You can not use the `top_k` parameter to calculate accuracy for multi-label inputs.")
-    preds, target = _input_squeeze(preds, target)
-    tp, fp, tn, fn = _stat_scores_update(
-        preds,
-        target,
-        reduce=reduce,
-        mdmc_reduce=mdmc_reduce,
-        threshold=threshold,
-        num_classes=num_classes,
-        top_k=top_k,
-        multiclass=multiclass,
-        ignore_index=ignore_index,
-        mode=mode,
-    )
-    return tp, fp, tn, fn
+    Where :math:`y` is a tensor of target values, :math:`\hat{y}` is a tensor of predictions,
+    and :math:`\bullet_{il}` refers to the :math:`l`-th label of the :math:`i`-th sample of that
+    tensor.
+
+    Accepts the following input tensors:
+
+    - ``preds``: ``(N, ...)`` (int tensor) or ``(N, C, ..)`` (float tensor). If preds is a floating point
+      we apply ``torch.argmax`` along the ``C`` dimension to automatically convert probabilities/logits into
+      an int tensor.
+    - ``target`` (int tensor): ``(N, ...)``
 
+    Args:
+        preds: Tensor with predictions
+        target: Tensor with true labels
+        num_classes: Integer specifing the number of classes
+        average:
+            Defines the reduction that is applied over labels. Should be one of the following:
 
-def _accuracy_compute(
-    tp: Tensor,
-    fp: Tensor,
-    tn: Tensor,
-    fn: Tensor,
-    average: Optional[str],
-    mdmc_average: Optional[str],
-    mode: DataType,
-) -> Tensor:
-    """Computes accuracy from stat scores: true positives, false positives, true negatives, false negatives.
+            - ``micro``: Sum statistics over all labels
+            - ``macro``: Calculate statistics for each label and average them
+            - ``weighted``: calculates statistics for each label and computes weighted average using their support
+            - ``"none"`` or ``None``: calculates statistic for each label and applies no reduction
 
-    Args:
-        tp: True positives
-        fp: False positives
-        tn: True negatives
-        fn: False negatives
-        average: Defines the reduction that is applied.
-        mdmc_average: Defines how averaging is done for multi-dimensional multi-class inputs (on top of the
-            ``average`` parameter).
-        mode: Mode of the input tensors
-
-    Example:
-        >>> preds = torch.tensor([0, 2, 1, 3])
-        >>> target = torch.tensor([0, 1, 2, 3])
-        >>> threshold = 0.5
-        >>> reduce = average = 'micro'
-        >>> mdmc_average = 'global'
-        >>> mode = _mode(preds, target, threshold, top_k=None, num_classes=None, multiclass=None)
-        >>> tp, fp, tn, fn = _accuracy_update(
-        ...                     preds,
-        ...                     target,
-        ...                     reduce,
-        ...                     mdmc_average,
-        ...                     threshold=0.5,
-        ...                     num_classes=None,
-        ...                     top_k=None,
-        ...                     multiclass=None,
-        ...                     ignore_index=None,
-        ...                     mode=mode)
-        >>> _accuracy_compute(tp, fp, tn, fn, average, mdmc_average, mode)
-        tensor(0.5000)
-
-        >>> target = torch.tensor([0, 1, 2])
-        >>> preds = torch.tensor([[0.1, 0.9, 0], [0.3, 0.1, 0.6], [0.2, 0.5, 0.3]])
-        >>> top_k, threshold = 2, 0.5
-        >>> reduce = average = 'micro'
-        >>> mdmc_average = 'global'
-        >>> mode = _mode(preds, target, threshold, top_k, num_classes=None, multiclass=None)
-        >>> tp, fp, tn, fn = _accuracy_update(preds, target, reduce, mdmc_average, threshold,
-        ...     num_classes=None, top_k=top_k, multiclass=None, ignore_index=None, mode=mode)
-        >>> _accuracy_compute(tp, fp, tn, fn, average, mdmc_average, mode)
-        tensor(0.6667)
-    """
+        top_k:
+            Number of highest probability or logit score predictions considered to find the correct label.
+            Only works when ``preds`` contain probabilities/logits.
+        multidim_average:
+            Defines how additionally dimensions ``...`` should be handled. Should be one of the following:
+
+            - ``global``: Additional dimensions are flatted along the batch dimension
+            - ``samplewise``: Statistic will be calculated independently for each sample on the ``N`` axis.
+              The statistics in this case are calculated over the additional dimensions.
 
-    simple_average = [AverageMethod.MICRO, AverageMethod.SAMPLES]
-    if (mode == DataType.BINARY and average in simple_average) or mode == DataType.MULTILABEL:
-        numerator = tp + tn
-        denominator = tp + tn + fp + fn
-    else:
-        numerator = tp.clone()
-        denominator = tp + fn
-
-    if mdmc_average != MDMCAverageMethod.SAMPLEWISE:
-        if average == AverageMethod.MACRO:
-            cond = tp + fp + fn == 0
-            numerator = numerator[~cond]
-            denominator = denominator[~cond]
-
-        if average == AverageMethod.NONE:
-            # a class is not present if there exists no TPs, no FPs, and no FNs
-            meaningless_indeces = torch.nonzero((tp | fn | fp) == 0).cpu()
-            numerator[meaningless_indeces, ...] = -1
-            denominator[meaningless_indeces, ...] = -1
-
-    return _reduce_stat_scores(
-        numerator=numerator,
-        denominator=denominator,
-        weights=None if average != AverageMethod.WEIGHTED else tp + fn,
-        average=average,
-        mdmc_average=mdmc_average,
+        ignore_index:
+            Specifies a target value that is ignored and does not contribute to the metric calculation
+        validate_args: bool indicating if input arguments and tensors should be validated for correctness.
+            Set to ``False`` for faster computations.
+
+    Returns:
+        The returned shape depends on the ``average`` and ``multidim_average`` arguments:
+
+        - If ``multidim_average`` is set to ``global``:
+
+          - If ``average='micro'/'macro'/'weighted'``, the output will be a scalar tensor
+          - If ``average=None/'none'``, the shape will be ``(C,)``
+
+        - If ``multidim_average`` is set to ``samplewise``:
+
+          - If ``average='micro'/'macro'/'weighted'``, the shape will be ``(N,)``
+          - If ``average=None/'none'``, the shape will be ``(N, C)``
+
+    Example (preds is int tensor):
+        >>> from torch import tensor
+        >>> from torchmetrics.functional.classification import multiclass_hamming_distance
+        >>> target = tensor([2, 1, 0, 0])
+        >>> preds = tensor([2, 1, 0, 1])
+        >>> multiclass_hamming_distance(preds, target, num_classes=3)
+        tensor(0.1667)
+        >>> multiclass_hamming_distance(preds, target, num_classes=3, average=None)
+        tensor([0.5000, 0.0000, 0.0000])
+
+    Example (preds is float tensor):
+        >>> from torchmetrics.functional.classification import multiclass_hamming_distance
+        >>> target = tensor([2, 1, 0, 0])
+        >>> preds = tensor([[0.16, 0.26, 0.58],
+        ...                 [0.22, 0.61, 0.17],
+        ...                 [0.71, 0.09, 0.20],
+        ...                 [0.05, 0.82, 0.13]])
+        >>> multiclass_hamming_distance(preds, target, num_classes=3)
+        tensor(0.1667)
+        >>> multiclass_hamming_distance(preds, target, num_classes=3, average=None)
+        tensor([0.5000, 0.0000, 0.0000])
+
+    Example (multidim tensors):
+        >>> from torchmetrics.functional.classification import multiclass_hamming_distance
+        >>> target = tensor([[[0, 1], [2, 1], [0, 2]], [[1, 1], [2, 0], [1, 2]]])
+        >>> preds = tensor([[[0, 2], [2, 0], [0, 1]], [[2, 2], [2, 1], [1, 0]]])
+        >>> multiclass_hamming_distance(preds, target, num_classes=3, multidim_average='samplewise')
+        tensor([0.5000, 0.7222])
+        >>> multiclass_hamming_distance(preds, target, num_classes=3, multidim_average='samplewise', average=None)
+        tensor([[0.0000, 1.0000, 0.5000],
+                [1.0000, 0.6667, 0.5000]])
+    """
+    if validate_args:
+        _multiclass_stat_scores_arg_validation(num_classes, top_k, average, multidim_average, ignore_index)
+        _multiclass_stat_scores_tensor_validation(preds, target, num_classes, multidim_average, ignore_index)
+    preds, target = _multiclass_stat_scores_format(preds, target, top_k)
+    tp, fp, tn, fn = _multiclass_stat_scores_update(
+        preds, target, num_classes, top_k, average, multidim_average, ignore_index
     )
+    return _hamming_distance_reduce(tp, fp, tn, fn, average=average, multidim_average=multidim_average)
 
 
-def _subset_accuracy_update(
+def multilabel_hamming_distance(
     preds: Tensor,
     target: Tensor,
-    threshold: float,
-    top_k: Optional[int],
+    num_labels: int,
+    threshold: float = 0.5,
+    average: Optional[Literal["micro", "macro", "weighted", "none"]] = "macro",
+    multidim_average: Literal["global", "samplewise"] = "global",
     ignore_index: Optional[int] = None,
-) -> Tuple[Tensor, Tensor]:
-    """Updates and returns variables required to compute subset accuracy.
-
-    Args:
-        preds: Predicted tensor
-        target: Ground truth tensor
-        threshold: Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case
-            of binary or multi-label inputs. Default value of ``0.5`` corresponds to input being probabilities.
-        top_k: Number of the highest probability or logit score predictions considered finding the correct label,
-            relevant only for (multi-dimensional) multi-class inputs.
-    """
-
-    preds, target = _input_squeeze(preds, target)
-    preds, target, mode = _input_format_classification(
-        preds, target, threshold=threshold, top_k=top_k, ignore_index=ignore_index
-    )
-
-    if mode == DataType.MULTILABEL and top_k:
-        raise ValueError("You can not use the `top_k` parameter to calculate accuracy for multi-label inputs.")
+    validate_args: bool = True,
+) -> Tensor:
+    r"""Compute the average `Hamming distance`_ (also known as Hamming loss) for multilabel tasks.
 
-    if mode == DataType.MULTILABEL:
-        correct = (preds == target).all(dim=1).sum()
-        total = tensor(target.shape[0], device=target.device)
-    elif mode == DataType.MULTICLASS:
-        correct = (preds * target).sum()
-        total = target.sum()
-    elif mode == DataType.MULTIDIM_MULTICLASS:
-        sample_correct = (preds * target).sum(dim=(1, 2))
-        correct = (sample_correct == target.shape[2]).sum()
-        total = tensor(target.shape[0], device=target.device)
-    else:
-        correct, total = tensor(0), tensor(0)
+    .. math::
+        \text{Hamming distance} = \frac{1}{N \cdot L} \sum_i^N \sum_l^L 1(y_{il} \neq \hat{y}_{il})
 
-    return correct, total
+    Where :math:`y` is a tensor of target values, :math:`\hat{y}` is a tensor of predictions,
+    and :math:`\bullet_{il}` refers to the :math:`l`-th label of the :math:`i`-th sample of that
+    tensor.
+
+    Accepts the following input tensors:
+
+    - ``preds`` (int or float tensor): ``(N, C, ...)``. If preds is a floating point tensor with values outside
+      [0,1] range we consider the input to be logits and will auto apply sigmoid per element. Addtionally,
+      we convert to int tensor with thresholding using the value in ``threshold``.
+    - ``target`` (int tensor): ``(N, C, ...)``
 
+    Args:
+        preds: Tensor with predictions
+        target: Tensor with true labels
+        num_labels: Integer specifing the number of labels
+        threshold: Threshold for transforming probability to binary (0,1) predictions
+        average:
+            Defines the reduction that is applied over labels. Should be one of the following:
 
-def _subset_accuracy_compute(correct: Tensor, total: Tensor) -> Tensor:
-    """Computes subset accuracy from number of correct observations and total number of observations.
+            - ``micro``: Sum statistics over all labels
+            - ``macro``: Calculate statistics for each label and average them
+            - ``weighted``: calculates statistics for each label and computes weighted average using their support
+            - ``"none"`` or ``None``: calculates statistic for each label and applies no reduction
+
+        multidim_average:
+            Defines how additionally dimensions ``...`` should be handled. Should be one of the following:
+
+            - ``global``: Additional dimensions are flatted along the batch dimension
+            - ``samplewise``: Statistic will be calculated independently for each sample on the ``N`` axis.
+              The statistics in this case are calculated over the additional dimensions.
 
-    Args:
-        correct: Number of correct observations
-        total: Number of observations
+        ignore_index:
+            Specifies a target value that is ignored and does not contribute to the metric calculation
+        validate_args: bool indicating if input arguments and tensors should be validated for correctness.
+            Set to ``False`` for faster computations.
+
+    Returns:
+        The returned shape depends on the ``average`` and ``multidim_average`` arguments:
+
+        - If ``multidim_average`` is set to ``global``:
+
+          - If ``average='micro'/'macro'/'weighted'``, the output will be a scalar tensor
+          - If ``average=None/'none'``, the shape will be ``(C,)``
+
+        - If ``multidim_average`` is set to ``samplewise``:
+
+          - If ``average='micro'/'macro'/'weighted'``, the shape will be ``(N,)``
+          - If ``average=None/'none'``, the shape will be ``(N, C)``
+
+    Example (preds is int tensor):
+        >>> from torch import tensor
+        >>> from torchmetrics.functional.classification import multilabel_hamming_distance
+        >>> target = tensor([[0, 1, 0], [1, 0, 1]])
+        >>> preds = tensor([[0, 0, 1], [1, 0, 1]])
+        >>> multilabel_hamming_distance(preds, target, num_labels=3)
+        tensor(0.3333)
+        >>> multilabel_hamming_distance(preds, target, num_labels=3, average=None)
+        tensor([0.0000, 0.5000, 0.5000])
+
+    Example (preds is float tensor):
+        >>> from torchmetrics.functional.classification import multilabel_hamming_distance
+        >>> target = tensor([[0, 1, 0], [1, 0, 1]])
+        >>> preds = tensor([[0.11, 0.22, 0.84], [0.73, 0.33, 0.92]])
+        >>> multilabel_hamming_distance(preds, target, num_labels=3)
+        tensor(0.3333)
+        >>> multilabel_hamming_distance(preds, target, num_labels=3, average=None)
+        tensor([0.0000, 0.5000, 0.5000])
+
+    Example (multidim tensors):
+        >>> from torchmetrics.functional.classification import multilabel_hamming_distance
+        >>> target = tensor([[[0, 1], [1, 0], [0, 1]], [[1, 1], [0, 0], [1, 0]]])
+        >>> preds = tensor([[[0.59, 0.91], [0.91, 0.99], [0.63, 0.04]],
+        ...                 [[0.38, 0.04], [0.86, 0.780], [0.45, 0.37]]])
+        >>> multilabel_hamming_distance(preds, target, num_labels=3, multidim_average='samplewise')
+        tensor([0.6667, 0.8333])
+        >>> multilabel_hamming_distance(preds, target, num_labels=3, multidim_average='samplewise', average=None)
+        tensor([[0.5000, 0.5000, 1.0000],
+                [1.0000, 1.0000, 0.5000]])
     """
-
-    return correct.float() / total
+    if validate_args:
+        _multilabel_stat_scores_arg_validation(num_labels, threshold, average, multidim_average, ignore_index)
+        _multilabel_stat_scores_tensor_validation(preds, target, num_labels, multidim_average, ignore_index)
+    preds, target = _multilabel_stat_scores_format(preds, target, num_labels, threshold, ignore_index)
+    tp, fp, tn, fn = _multilabel_stat_scores_update(preds, target, multidim_average)
+    return _hamming_distance_reduce(tp, fp, tn, fn, average=average, multidim_average=multidim_average, multilabel=True)
 
 
-def accuracy(
+def hamming_distance(
     preds: Tensor,
     target: Tensor,
-    average: Optional[str] = "micro",
-    mdmc_average: Optional[str] = "global",
+    task: Literal["binary", "multiclass", "multilabel"],
     threshold: float = 0.5,
-    top_k: Optional[int] = None,
-    subset_accuracy: bool = False,
     num_classes: Optional[int] = None,
-    multiclass: Optional[bool] = None,
+    num_labels: Optional[int] = None,
+    average: Optional[Literal["micro", "macro", "weighted", "none"]] = "micro",
+    multidim_average: Optional[Literal["global", "samplewise"]] = "global",
+    top_k: Optional[int] = 1,
     ignore_index: Optional[int] = None,
+    validate_args: bool = True,
 ) -> Tensor:
-    r"""Computes `Accuracy`_
+    r"""Compute the average `Hamming distance`_ (also known as Hamming loss).
 
     .. math::
-        \text{Accuracy} = \frac{1}{N}\sum_i^N 1(y_i = \hat{y}_i)
-
-    Where :math:`y` is a tensor of target values, and :math:`\hat{y}` is a
-    tensor of predictions.
-
-    For multi-class and multi-dimensional multi-class data with probability or logits predictions, the
-    parameter ``top_k`` generalizes this metric to a Top-K accuracy metric: for each sample the
-    top-K highest probability or logits items are considered to find the correct label.
+        \text{Hamming distance} = \frac{1}{N \cdot L} \sum_i^N \sum_l^L 1(y_{il} \neq \hat{y}_{il})
 
-    For multi-label and multi-dimensional multi-class inputs, this metric computes the "global"
-    accuracy by default, which counts all labels or sub-samples separately. This can be
-    changed to subset accuracy (which requires all labels or sub-samples in the sample to
-    be correctly predicted) by setting ``subset_accuracy=True``.
-
-    Accepts all input types listed in :ref:`pages/classification:input types`.
-
-    Args:
-        preds: Predictions from model (probabilities, logits or labels)
-        target: Ground truth labels
-        average:
-            Defines the reduction that is applied. Should be one of the following:
-
-            - ``'micro'`` [default]: Calculate the metric globally, across all samples and classes.
-            - ``'macro'``: Calculate the metric for each class separately, and average the
-              metrics across classes (with equal weights for each class).
-            - ``'weighted'``: Calculate the metric for each class separately, and average the
-              metrics across classes, weighting each class by its support (``tp + fn``).
-            - ``'none'`` or ``None``: Calculate the metric for each class separately, and return
-              the metric for every class.
-            - ``'samples'``: Calculate the metric for each sample, and average the metrics
-              across samples (with equal weights for each sample).
-
-            .. note:: What is considered a sample in the multi-dimensional multi-class case
-                depends on the value of ``mdmc_average``.
-
-            .. note:: If ``'none'`` and a given class doesn't occur in the ``preds`` or ``target``,
-                the value for the class will be ``nan``.
-
-        mdmc_average:
-            Defines how averaging is done for multi-dimensional multi-class inputs (on top of the
-            ``average`` parameter). Should be one of the following:
-
-            - ``None`` [default]: Should be left unchanged if your data is not multi-dimensional multi-class.
-
-            - ``'samplewise'``: In this case, the statistics are computed separately for each
-              sample on the ``N`` axis, and then averaged over samples.
-              The computation for each sample is done by treating the flattened extra axes ``...``
-              (see :ref:`pages/classification:input types`) as the ``N`` dimension within the sample,
-              and computing the metric for the sample based on that.
-
-            - ``'global'``: In this case the ``N`` and ``...`` dimensions of the inputs
-              (see :ref:`pages/classification:input types`)
-              are flattened into a new ``N_X`` sample axis, i.e. the inputs are treated as if they
-              were ``(N_X, C)``. From here on the ``average`` parameter applies as usual.
-
-        num_classes:
-            Number of classes. Necessary for ``'macro'``, ``'weighted'`` and ``None`` average methods.
-
-        threshold:
-            Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case
-            of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.
-        top_k:
-            Number of the highest probability or logit score predictions considered finding the correct label,
-            relevant only for (multi-dimensional) multi-class inputs. The
-            default value (``None``) will be interpreted as 1 for these inputs.
-
-            Should be left at default (``None``) for all other types of inputs.
-        multiclass:
-            Used only in certain special cases, where you want to treat inputs as a different type
-            than what they appear to be. See the parameter's
-            :ref:`documentation section <pages/classification:using the multiclass parameter>`
-            for a more detailed explanation and examples.
-        ignore_index:
-            Integer specifying a target class to ignore. If given, this class index does not contribute
-            to the returned score, regardless of reduction method. If an index is ignored, and ``average=None``
-            or ``'none'``, the score for the ignored class will be returned as ``nan``.
-        subset_accuracy:
-            Whether to compute subset accuracy for multi-label and multi-dimensional
-            multi-class inputs (has no effect for other input types).
-
-            - For multi-label inputs, if the parameter is set to ``True``, then all labels for
-              each sample must be correctly predicted for the sample to count as correct. If it
-              is set to ``False``, then all labels are counted separately - this is equivalent to
-              flattening inputs beforehand (i.e. ``preds = preds.flatten()`` and same for ``target``).
-
-            - For multi-dimensional multi-class inputs, if the parameter is set to ``True``, then all
-              sub-sample (on the extra axis) must be correct for the sample to be counted as correct.
-              If it is set to ``False``, then all sub-samples are counter separately - this is equivalent,
-              in the case of label predictions, to flattening the inputs beforehand (i.e.
-              ``preds = preds.flatten()`` and same for ``target``). Note that the ``top_k`` parameter
-              still applies in both cases, if set.
-
-    Raises:
-        ValueError:
-            If ``top_k`` parameter is set for ``multi-label`` inputs.
-        ValueError:
-            If ``average`` is none of ``"micro"``, ``"macro"``, ``"weighted"``, ``"samples"``, ``"none"``, ``None``.
-        ValueError:
-            If ``mdmc_average`` is not one of ``None``, ``"samplewise"``, ``"global"``.
-        ValueError:
-            If ``average`` is set but ``num_classes`` is not provided.
-        ValueError:
-            If ``num_classes`` is set
-            and ``ignore_index`` is not in the range ``[0, num_classes)``.
-        ValueError:
-            If ``top_k`` is not an ``integer`` larger than ``0``.
-
-    Example:
-        >>> import torch
-        >>> from torchmetrics.functional import accuracy
-        >>> target = torch.tensor([0, 1, 2, 3])
-        >>> preds = torch.tensor([0, 2, 1, 3])
-        >>> accuracy(preds, target)
-        tensor(0.5000)
-
-        >>> target = torch.tensor([0, 1, 2])
-        >>> preds = torch.tensor([[0.1, 0.9, 0], [0.3, 0.1, 0.6], [0.2, 0.5, 0.3]])
-        >>> accuracy(preds, target, top_k=2)
-        tensor(0.6667)
+    Where :math:`y` is a tensor of target values, :math:`\hat{y}` is a tensor of predictions,
+    and :math:`\bullet_{il}` refers to the :math:`l`-th label of the :math:`i`-th sample of that
+    tensor.
+
+    This function is a simple wrapper to get the task specific versions of this metric, which is done by setting the
+    ``task`` argument to either ``'binary'``, ``'multiclass'`` or ``multilabel``. See the documentation of
+    :func:`binary_hamming_distance`, :func:`multiclass_hamming_distance` and :func:`multilabel_hamming_distance` for
+    the specific details of each argument influence and examples.
+
+    Legacy Example:
+        >>> from torch import tensor
+        >>> target = tensor([[0, 1], [1, 1]])
+        >>> preds = tensor([[0, 1], [0, 1]])
+        >>> hamming_distance(preds, target, task="binary")
+        tensor(0.2500)
     """
-    allowed_average = ["micro", "macro", "weighted", "samples", "none", None]
-    if average not in allowed_average:
-        raise ValueError(f"The `average` has to be one of {allowed_average}, got {average}.")
-
-    if average in ["macro", "weighted", "none", None] and (not num_classes or num_classes < 1):
-        raise ValueError(f"When you set `average` as {average}, you have to provide the number of classes.")
-
-    allowed_mdmc_average = [None, "samplewise", "global"]
-    if mdmc_average not in allowed_mdmc_average:
-        raise ValueError(f"The `mdmc_average` has to be one of {allowed_mdmc_average}, got {mdmc_average}.")
-
-    if num_classes and ignore_index is not None and (not ignore_index < num_classes or num_classes == 1):
-        raise ValueError(f"The `ignore_index` {ignore_index} is not valid for inputs with {num_classes} classes")
-
-    if top_k is not None and (not isinstance(top_k, int) or top_k <= 0):
-        raise ValueError(f"The `top_k` should be an integer larger than 0, got {top_k}")
-
-    preds, target = _input_squeeze(preds, target)
-    mode = _mode(preds, target, threshold, top_k, num_classes, multiclass, ignore_index)
-    reduce = "macro" if average in ["weighted", "none", None] else average
-
-    if subset_accuracy and _check_subset_validity(mode):
-        correct, total = _subset_accuracy_update(preds, target, threshold, top_k, ignore_index)
-        return _subset_accuracy_compute(correct, total)
-    tp, fp, tn, fn = _accuracy_update(
-        preds, target, reduce, mdmc_average, threshold, num_classes, top_k, multiclass, ignore_index, mode
-    )
-    return _accuracy_compute(tp, fp, tn, fn, average, mdmc_average, mode)
+    task = ClassificationTask.from_str(task)
+    assert multidim_average is not None  # noqa: S101  # needed for mypy
+    if task == ClassificationTask.BINARY:
+        return binary_hamming_distance(preds, target, threshold, multidim_average, ignore_index, validate_args)
+    if task == ClassificationTask.MULTICLASS:
+        if not isinstance(num_classes, int):
+            raise ValueError(f"`num_classes` is expected to be `int` but `{type(num_classes)} was passed.`")
+        if not isinstance(top_k, int):
+            raise ValueError(f"`top_k` is expected to be `int` but `{type(top_k)} was passed.`")
+        return multiclass_hamming_distance(
+            preds, target, num_classes, average, top_k, multidim_average, ignore_index, validate_args
+        )
+    if task == ClassificationTask.MULTILABEL:
+        if not isinstance(num_labels, int):
+            raise ValueError(f"`num_labels` is expected to be `int` but `{type(num_labels)} was passed.`")
+        return multilabel_hamming_distance(
+            preds, target, num_labels, threshold, average, multidim_average, ignore_index, validate_args
+        )
+    raise ValueError(f"Not handled value: {task}")  # this is for compliant of mypy
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/classification/auroc.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/detection/iou.py`

 * *Files 26% similar despite different names*

```diff
@@ -7,263 +7,302 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import warnings
-from typing import Optional, Sequence, Tuple
+from collections import defaultdict
+from typing import Any, Callable, Dict, List, Optional, Sequence, Union
 
 import torch
-from torch import Tensor, tensor
+from torch import Tensor
 
-from torchmetrics.functional.classification.auc import _auc_compute_without_check
-from torchmetrics.functional.classification.roc import roc
-from torchmetrics.utilities.checks import _input_format_classification
-from torchmetrics.utilities.data import _bincount
-from torchmetrics.utilities.enums import AverageMethod, DataType
-from torchmetrics.utilities.imports import _TORCH_LOWER_1_6
+from torchmetrics.detection.helpers import _fix_empty_tensors, _input_validator
+from torchmetrics.functional.detection.iou import _iou_compute, _iou_update
+from torchmetrics.metric import Metric
+from torchmetrics.utilities.data import dim_zero_cat
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE, _TORCHVISION_GREATER_EQUAL_0_8
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
+if _TORCHVISION_GREATER_EQUAL_0_8:
+    from torchvision.ops import box_convert
+else:
+    box_convert = None
 
-def _auroc_update(preds: Tensor, target: Tensor) -> Tuple[Tensor, Tensor, DataType]:
-    """Updates and returns variables required to compute Area Under the Receiver Operating Characteristic Curve.
-    Validates the inputs and returns the mode of the inputs.
+if not _TORCHVISION_GREATER_EQUAL_0_8:
+    __doctest_skip__ = ["IntersectionOverUnion", "IntersectionOverUnion.plot"]
+elif not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["IntersectionOverUnion.plot"]
 
-    Args:
-        preds: Predicted tensor
-        target: Ground truth tensor
-    """
 
-    # use _input_format_classification for validating the input and get the mode of data
-    _, _, mode = _input_format_classification(preds, target)
+class IntersectionOverUnion(Metric):
+    r"""Computes Intersection Over Union (IoU).
 
-    if mode == "multi class multi dim":
-        n_classes = preds.shape[1]
-        preds = preds.transpose(0, 1).reshape(n_classes, -1).transpose(0, 1)
-        target = target.flatten()
-    if mode == "multi-label" and preds.ndim > 2:
-        n_classes = preds.shape[1]
-        preds = preds.transpose(0, 1).reshape(n_classes, -1).transpose(0, 1)
-        target = target.transpose(0, 1).reshape(n_classes, -1).transpose(0, 1)
-
-    return preds, target, mode
-
-
-def _auroc_compute(
-    preds: Tensor,
-    target: Tensor,
-    mode: DataType,
-    num_classes: Optional[int] = None,
-    pos_label: Optional[int] = None,
-    average: Optional[str] = "macro",
-    max_fpr: Optional[float] = None,
-    sample_weights: Optional[Sequence] = None,
-) -> Tensor:
-    """Computes Area Under the Receiver Operating Characteristic Curve.
+    As input to ``forward`` and ``update`` the metric accepts the following input:
 
-    Args:
-        preds: predictions from model (logits or probabilities)
-        target: Ground truth labels
-        mode: 'multi class multi dim' or 'multi-label' or 'binary'
-        num_classes: integer with number of classes for multi-label and multiclass problems.
-            Should be set to ``None`` for binary problems
-        pos_label: integer determining the positive class.
-            Should be set to ``None`` for binary problems
-        average: Defines the reduction that is applied to the output:
-        max_fpr: If not ``None``, calculates standardized partial AUC over the
-            range ``[0, max_fpr]``. Should be a float between 0 and 1.
-        sample_weights: sample weights for each data point
+    - ``preds`` (:class:`~List`): A list consisting of dictionaries each containing the key-values
+      (each dictionary corresponds to a single image). Parameters that should be provided per dict
 
-    Example:
-        >>> # binary case
-        >>> preds = torch.tensor([0.13, 0.26, 0.08, 0.19, 0.34])
-        >>> target = torch.tensor([0, 0, 1, 1, 1])
-        >>> preds, target, mode = _auroc_update(preds, target)
-        >>> _auroc_compute(preds, target, mode, pos_label=1)
-        tensor(0.5000)
-
-        >>> # multiclass case
-        >>> preds = torch.tensor([[0.90, 0.05, 0.05],
-        ...                       [0.05, 0.90, 0.05],
-        ...                       [0.05, 0.05, 0.90],
-        ...                       [0.85, 0.05, 0.10],
-        ...                       [0.10, 0.10, 0.80]])
-        >>> target = torch.tensor([0, 1, 1, 2, 2])
-        >>> preds, target, mode = _auroc_update(preds, target)
-        >>> _auroc_compute(preds, target, mode, num_classes=3)
-        tensor(0.7778)
-    """
+        - boxes: (:class:`~torch.FloatTensor`) of shape ``(num_boxes, 4)`` containing ``num_boxes`` detection
+          boxes of the format specified in the constructor.
+          By default, this method expects ``(xmin, ymin, xmax, ymax)`` in absolute image coordinates.
+        - scores: :class:`~torch.FloatTensor` of shape ``(num_boxes)`` containing detection scores for the boxes.
+        - labels: :class:`~torch.IntTensor` of shape ``(num_boxes)`` containing 0-indexed detection classes for
+          the boxes.
 
-    # binary mode override num_classes
-    if mode == DataType.BINARY:
-        num_classes = 1
-
-    # check max_fpr parameter
-    if max_fpr is not None:
-        if not isinstance(max_fpr, float) and 0 < max_fpr <= 1:
-            raise ValueError(f"`max_fpr` should be a float in range (0, 1], got: {max_fpr}")
-
-        if _TORCH_LOWER_1_6:
-            raise RuntimeError(
-                "`max_fpr` argument requires `torch.bucketize` which" " is not available below PyTorch version 1.6"
-            )
+    - ``target`` (:class:`~List`) A list consisting of dictionaries each containing the key-values
+      (each dictionary corresponds to a single image). Parameters that should be provided per dict:
 
-        # max_fpr parameter is only support for binary
-        if mode != DataType.BINARY:
-            raise ValueError(
-                "Partial AUC computation not available in multilabel/multiclass setting,"
-                f" 'max_fpr' must be set to `None`, received `{max_fpr}`."
-            )
+        - boxes: :class:`~torch.FloatTensor` of shape ``(num_boxes, 4)`` containing ``num_boxes`` ground truth
+          boxes of the format specified in the constructor.
+          By default, this method expects ``(xmin, ymin, xmax, ymax)`` in absolute image coordinates.
+        - labels: :class:`~torch.IntTensor` of shape ``(num_boxes)`` containing 0-indexed ground truth
+          classes for the boxes.
 
-    # calculate fpr, tpr
-    if mode == DataType.MULTILABEL:
-        if average == AverageMethod.MICRO:
-            fpr, tpr, _ = roc(preds.flatten(), target.flatten(), 1, pos_label, sample_weights)
-        elif num_classes:
-            # for multilabel we iteratively evaluate roc in a binary fashion
-            output = [
-                roc(preds[:, i], target[:, i], num_classes=1, pos_label=1, sample_weights=sample_weights)
-                for i in range(num_classes)
-            ]
-            fpr = [o[0] for o in output]
-            tpr = [o[1] for o in output]
-        else:
-            raise ValueError("Detected input to be `multilabel` but you did not provide `num_classes` argument")
-    else:
-        if mode != DataType.BINARY:
-            if num_classes is None:
-                raise ValueError("Detected input to `multiclass` but you did not provide `num_classes` argument")
-            if average == AverageMethod.WEIGHTED and len(torch.unique(target)) < num_classes:
-                # If one or more classes has 0 observations, we should exclude them, as its weight will be 0
-                target_bool_mat = torch.zeros((len(target), num_classes), dtype=bool, device=target.device)
-                target_bool_mat[torch.arange(len(target)), target.long()] = 1
-                class_observed = target_bool_mat.sum(axis=0) > 0
-                for c in range(num_classes):
-                    if not class_observed[c]:
-                        warnings.warn(f"Class {c} had 0 observations, omitted from AUROC calculation", UserWarning)
-                preds = preds[:, class_observed]
-                target = target_bool_mat[:, class_observed]
-                target = torch.where(target)[1]
-                num_classes = class_observed.sum()
-                if num_classes == 1:
-                    raise ValueError("Found 1 non-empty class in `multiclass` AUROC calculation")
-        fpr, tpr, _ = roc(preds, target, num_classes, pos_label, sample_weights)
-
-    # calculate standard roc auc score
-    if max_fpr is None or max_fpr == 1:
-        if mode == DataType.MULTILABEL and average == AverageMethod.MICRO:
-            pass
-        elif num_classes != 1:
-            # calculate auc scores per class
-            auc_scores = [_auc_compute_without_check(x, y, 1.0) for x, y in zip(fpr, tpr)]
-
-            # calculate average
-            if average == AverageMethod.NONE:
-                return tensor(auc_scores)
-            if average == AverageMethod.MACRO:
-                return torch.mean(torch.stack(auc_scores))
-            if average == AverageMethod.WEIGHTED:
-                if mode == DataType.MULTILABEL:
-                    support = torch.sum(target, dim=0)
-                else:
-                    support = _bincount(target.flatten(), minlength=num_classes)
-                return torch.sum(torch.stack(auc_scores) * support / support.sum())
-
-            allowed_average = (AverageMethod.NONE.value, AverageMethod.MACRO.value, AverageMethod.WEIGHTED.value)
-            raise ValueError(
-                f"Argument `average` expected to be one of the following: {allowed_average} but got {average}"
-            )
+    As output of ``forward`` and ``compute`` the metric returns the following output:
 
-        return _auc_compute_without_check(fpr, tpr, 1.0)
+    - ``iou_dict``: A dictionary containing the following key-values:
 
-    _device = fpr.device if isinstance(fpr, Tensor) else fpr[0].device
-    max_area: Tensor = tensor(max_fpr, device=_device)
-    # Add a single point at max_fpr and interpolate its tpr value
-    stop = torch.bucketize(max_area, fpr, out_int32=True, right=True)
-    weight = (max_area - fpr[stop - 1]) / (fpr[stop] - fpr[stop - 1])
-    interp_tpr: Tensor = torch.lerp(tpr[stop - 1], tpr[stop], weight)
-    tpr = torch.cat([tpr[:stop], interp_tpr.view(1)])
-    fpr = torch.cat([fpr[:stop], max_area.view(1)])
-
-    # Compute partial AUC
-    partial_auc = _auc_compute_without_check(fpr, tpr, 1.0)
-
-    # McClish correction: standardize result to be 0.5 if non-discriminant and 1 if maximal
-    min_area: Tensor = 0.5 * max_area**2
-    return 0.5 * (1 + (partial_auc - min_area) / (max_area - min_area))
-
-
-def auroc(
-    preds: Tensor,
-    target: Tensor,
-    num_classes: Optional[int] = None,
-    pos_label: Optional[int] = None,
-    average: Optional[str] = "macro",
-    max_fpr: Optional[float] = None,
-    sample_weights: Optional[Sequence] = None,
-) -> Tensor:
-    """Compute Area Under the Receiver Operating Characteristic Curve (`ROC AUC`_)
-
-    For non-binary input, if the ``preds`` and ``target`` tensor have the same
-    size the input will be interpretated as multilabel and if ``preds`` have one
-    dimension more than the ``target`` tensor the input will be interpretated as
-    multiclass.
-
-    .. note::
-        If either the positive class or negative class is completly missing in the target tensor,
-        the auroc score is meaningless in this case and a score of 0 will be returned together
-        with a warning.
+        - iou: (:class:`~torch.Tensor`)
+        - iou/cl_{cl}: (:class:`~torch.Tensor`), if argument ``class metrics=True``
 
     Args:
-        preds: predictions from model (logits or probabilities)
-        target: Ground truth labels
-        num_classes: integer with number of classes for multi-label and multiclass problems.
-            Should be set to ``None`` for binary problems
-        pos_label: integer determining the positive class. Default is ``None``
-            which for binary problem is translate to 1. For multiclass problems
-            this argument should not be set as we iteratively change it in the
-            range [0,num_classes-1]
-        average:
-
-            - ``'micro'`` computes metric globally. Only works for multilabel problems
-            - ``'macro'`` computes metric for each class and uniformly averages them
-            - ``'weighted'`` computes metric for each class and does a weighted-average,
-              where each class is weighted by their support (accounts for class imbalance)
-            - ``None`` computes and returns the metric per class
-
-        max_fpr:
-            If not ``None``, calculates standardized partial AUC over the
-            range ``[0, max_fpr]``. Should be a float between 0 and 1.
-        sample_weights: sample weights for each data point
+        box_format:
+            Input format of given boxes. Supported formats are ``[`xyxy`, `xywh`, `cxcywh`]``.
+        iou_thresholds:
+            Optional IoU thresholds for evaluation. If set to `None` the threshold is ignored.
+        class_metrics:
+            Option to enable per-class metrics for IoU. Has a performance impact.
+        respect_labels:
+            Replace IoU values with the `invalid_val` if the labels do not match.
+        kwargs:
+            Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+
+    Example:
+        >>> import torch
+        >>> from torchmetrics.detection import IntersectionOverUnion
+        >>> preds = [
+        ...    {
+        ...        "boxes": torch.tensor([[296.55, 93.96, 314.97, 152.79], [298.55, 98.96, 314.97, 151.79]]),
+        ...        "scores": torch.tensor([0.236, 0.56]),
+        ...        "labels": torch.tensor([4, 5]),
+        ...    }
+        ... ]
+        >>> target = [
+        ...    {
+        ...        "boxes": torch.tensor([[300.00, 100.00, 315.00, 150.00]]),
+        ...        "labels": torch.tensor([5]),
+        ...    }
+        ... ]
+        >>> metric = IntersectionOverUnion()
+        >>> metric(preds, target)
+        {'iou': tensor(0.4307)}
 
     Raises:
-        ValueError:
-            If ``max_fpr`` is not a ``float`` in the range ``(0, 1]``.
-        RuntimeError:
-            If ``PyTorch version`` is below 1.6 since max_fpr requires ``torch.bucketize``
-            which is not available below 1.6.
-        ValueError:
-            If ``max_fpr`` is not set to ``None`` and the mode is ``not binary``
-            since partial AUC computation is not available in multilabel/multiclass.
-        ValueError:
-            If ``average`` is none of ``None``, ``"macro"`` or ``"weighted"``.
-
-    Example (binary case):
-        >>> from torchmetrics.functional import auroc
-        >>> preds = torch.tensor([0.13, 0.26, 0.08, 0.19, 0.34])
-        >>> target = torch.tensor([0, 0, 1, 1, 1])
-        >>> auroc(preds, target, pos_label=1)
-        tensor(0.5000)
-
-    Example (multiclass case):
-        >>> preds = torch.tensor([[0.90, 0.05, 0.05],
-        ...                       [0.05, 0.90, 0.05],
-        ...                       [0.05, 0.05, 0.90],
-        ...                       [0.85, 0.05, 0.10],
-        ...                       [0.10, 0.10, 0.80]])
-        >>> target = torch.tensor([0, 1, 1, 2, 2])
-        >>> auroc(preds, target, num_classes=3)
-        tensor(0.7778)
+        ModuleNotFoundError:
+            If torchvision is not installed with version 0.8.0 or newer.
+
     """
-    preds, target, mode = _auroc_update(preds, target)
-    return _auroc_compute(preds, target, mode, num_classes, pos_label, average, max_fpr, sample_weights)
+    is_differentiable: bool = False
+    higher_is_better: Optional[bool] = True
+    full_state_update: bool = True
+
+    detections: List[Tensor]
+    detection_scores: List[Tensor]
+    detection_labels: List[Tensor]
+    groundtruths: List[Tensor]
+    groundtruth_labels: List[Tensor]
+    results: List[Tensor]
+    labels_eq: List[Tensor]
+    _iou_type: str = "iou"
+    _invalid_val: float = 0.0
+
+    def __init__(
+        self,
+        box_format: str = "xyxy",
+        iou_threshold: Optional[float] = None,
+        class_metrics: bool = False,
+        respect_labels: bool = True,
+        **kwargs: Any,
+    ) -> None:
+        super().__init__(**kwargs)
+
+        if not _TORCHVISION_GREATER_EQUAL_0_8:
+            raise ModuleNotFoundError(
+                f"Metric `{self._iou_type.upper()}` requires that `torchvision` version 0.8.0 or newer is installed."
+                " Please install with `pip install torchvision>=0.8` or `pip install torchmetrics[detection]`."
+            )
+
+        allowed_box_formats = ("xyxy", "xywh", "cxcywh")
+        if box_format not in allowed_box_formats:
+            raise ValueError(f"Expected argument `box_format` to be one of {allowed_box_formats} but got {box_format}")
+
+        self.box_format = box_format
+        self.iou_threshold = iou_threshold
+
+        if not isinstance(class_metrics, bool):
+            raise ValueError("Expected argument `class_metrics` to be a boolean")
+        self.class_metrics = class_metrics
+
+        if not isinstance(respect_labels, bool):
+            raise ValueError("Expected argument `respect_labels` to be a boolean")
+        self.respect_labels = respect_labels
+
+        self.add_state("detections", default=[], dist_reduce_fx=None)
+        self.add_state("detection_scores", default=[], dist_reduce_fx=None)
+        self.add_state("detection_labels", default=[], dist_reduce_fx=None)
+        self.add_state("groundtruths", default=[], dist_reduce_fx=None)
+        self.add_state("groundtruth_labels", default=[], dist_reduce_fx=None)
+        self.add_state("results", default=[], dist_reduce_fx=None)
+        self.add_state("labels_eq", default=[], dist_reduce_fx=None)
+
+    @staticmethod
+    def _iou_update_fn(*args: Any, **kwargs: Any) -> Tensor:
+        return _iou_update(*args, **kwargs)
+
+    @staticmethod
+    def _iou_compute_fn(*args: Any, **kwargs: Any) -> Tensor:
+        return _iou_compute(*args, **kwargs)
+
+    def update(self, preds: List[Dict[str, Tensor]], target: List[Dict[str, Tensor]]) -> None:
+        """Update state with predictions and targets.
+
+        Raises:
+            ValueError:
+                If ``preds`` is not of type List[Dict[str, Tensor]]
+            ValueError:
+                If ``target`` is not of type List[Dict[str, Tensor]]
+            ValueError:
+                If ``preds`` and ``target`` are not of the same length
+            ValueError:
+                If any of ``preds.boxes``, ``preds.scores``
+                and ``preds.labels`` are not of the same length
+            ValueError:
+                If any of ``target.boxes`` and ``target.labels`` are not of the same length
+            ValueError:
+                If any box is not type float and of length 4
+            ValueError:
+                If any class is not type int and of length 1
+            ValueError:
+                If any score is not type float and of length 1
+        """
+        _input_validator(preds, target)
+
+        for p, t in zip(preds, target):
+            det_boxes = self._get_safe_item_values(p["boxes"])
+            self.detections.append(det_boxes)
+            self.detection_labels.append(p["labels"])
+            self.detection_scores.append(p["scores"])
+
+            gt_boxes = self._get_safe_item_values(t["boxes"])
+            self.groundtruths.append(gt_boxes)
+            self.groundtruth_labels.append(t["labels"])
+
+            label_eq = torch.equal(p["labels"], t["labels"])
+            # Workaround to persist state, which only works with tensors
+            self.labels_eq.append(torch.tensor([label_eq], dtype=torch.int, device=self.device))
+
+            ious = self._iou_update_fn(det_boxes, gt_boxes, self.iou_threshold, self._invalid_val)
+            if self.respect_labels and not label_eq:
+                label_diff = p["labels"].unsqueeze(0).T - t["labels"].unsqueeze(0)
+                labels_not_eq = label_diff != 0.0
+                ious[labels_not_eq] = self._invalid_val
+            self.results.append(ious.to(dtype=torch.float, device=self.device))
+
+    def _get_safe_item_values(self, boxes: Tensor) -> Tensor:
+        boxes = _fix_empty_tensors(boxes)
+        if boxes.numel() > 0:
+            boxes = box_convert(boxes, in_fmt=self.box_format, out_fmt="xyxy")
+        return boxes
+
+    def _get_gt_classes(self) -> List:
+        """Returns a list of unique classes found in ground truth and detection data."""
+        if len(self.groundtruth_labels) > 0:
+            return torch.cat(self.groundtruth_labels).unique().tolist()
+        return []
+
+    def compute(self) -> dict:
+        """Computes IoU based on inputs passed in to ``update`` previously."""
+        aggregated_iou = dim_zero_cat(
+            [self._iou_compute_fn(iou, bool(lbl_eq)) for iou, lbl_eq in zip(self.results, self.labels_eq)]
+        )
+        results: Dict[str, Tensor] = {f"{self._iou_type}": aggregated_iou.mean()}
+
+        if self.class_metrics:
+            class_results: Dict[int, List[Tensor]] = defaultdict(list)
+            for iou, label in zip(self.results, self.groundtruth_labels):
+                for cl in self._get_gt_classes():
+                    masked_iou = iou[:, label == cl]
+                    if masked_iou.numel() > 0:
+                        class_results[cl].append(self._iou_compute_fn(masked_iou, False))
+
+            results.update(
+                {f"{self._iou_type}/cl_{cl}": dim_zero_cat(class_results[cl]).mean() for cl in class_results}
+            )
+        return results
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure object and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> import torch
+            >>> from torchmetrics.detection import IntersectionOverUnion
+            >>> preds = [
+            ...    {
+            ...        "boxes": torch.tensor([[296.55, 93.96, 314.97, 152.79], [298.55, 98.96, 314.97, 151.79]]),
+            ...        "scores": torch.tensor([0.236, 0.56]),
+            ...        "labels": torch.tensor([4, 5]),
+            ...    }
+            ... ]
+            >>> target = [
+            ...    {
+            ...        "boxes": torch.tensor([[300.00, 100.00, 315.00, 150.00]]),
+            ...        "labels": torch.tensor([5]),
+            ...    }
+            ... ]
+            >>> metric = IntersectionOverUnion()
+            >>> metric.update(preds, target)
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> import torch
+            >>> from torchmetrics.detection import IntersectionOverUnion
+            >>> preds = [
+            ...    {
+            ...        "boxes": torch.tensor([[296.55, 93.96, 314.97, 152.79], [298.55, 98.96, 314.97, 151.79]]),
+            ...        "scores": torch.tensor([0.236, 0.56]),
+            ...        "labels": torch.tensor([4, 5]),
+            ...    }
+            ... ]
+            >>> target = lambda : [
+            ...    {
+            ...        "boxes": torch.tensor([[300.00, 100.00, 315.00, 150.00]]) + torch.randint(-10, 10, (1, 4)),
+            ...        "labels": torch.tensor([5]),
+            ...    }
+            ... ]
+            >>> metric = IntersectionOverUnion()
+            >>> vals = []
+            >>> for _ in range(20):
+            ...     vals.append(metric(preds, target()))
+            >>> fig_, ax_ = metric.plot(vals)
+        """
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/classification/cohen_kappa.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/psnrb.py`

 * *Files 26% similar despite different names*

```diff
@@ -7,104 +7,127 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Optional
+import math
+from typing import Optional, Tuple, Union
 
 import torch
-from torch import Tensor
+from torch import Tensor, tensor
 
-from torchmetrics.functional.classification.confusion_matrix import _confusion_matrix_compute, _confusion_matrix_update
 
-_cohen_kappa_update = _confusion_matrix_update
+def _compute_bef(x: Tensor, block_size: int = 8) -> Tensor:
+    """Compute block effect.
 
+    Args:
+        x: input image
+        block_size: integer indication the block size
 
-def _cohen_kappa_compute(confmat: Tensor, weights: Optional[str] = None) -> Tensor:
-    """Computes Cohen's kappa based on the weighting type.
+    Returns:
+        Computed block effect
 
-    Args:
-        confmat: Confusion matrix without normalization
-        weights: Weighting type to calculate the score. Choose from:
+    Raises:
+        ValueError:
+            If the image is not a grayscale image
 
-            - ``None`` or ``'none'``: no weighting
-            - ``'linear'``: linear weighting
-            - ``'quadratic'``: quadratic weighting
+    """
+    (
+        _,
+        channels,
+        height,
+        width,
+    ) = x.shape
+    if channels > 1:
+        raise ValueError(f"`psnrb` metric expects grayscale images, but got images with {channels} channels.")
+
+    h = torch.arange(width - 1)
+    h_b = torch.tensor(range(block_size - 1, width - 1, block_size))
+    h_bc = torch.tensor(list(set(h.tolist()).symmetric_difference(h_b.tolist())))
+
+    v = torch.arange(height - 1)
+    v_b = torch.tensor(range(block_size - 1, height - 1, block_size))
+    v_bc = torch.tensor(list(set(v.tolist()).symmetric_difference(v_b.tolist())))
+
+    d_b = (x[:, :, :, h_b] - x[:, :, :, h_b + 1]).pow(2.0).sum()
+    d_bc = (x[:, :, :, h_bc] - x[:, :, :, h_bc + 1]).pow(2.0).sum()
+    d_b += (x[:, :, v_b, :] - x[:, :, v_b + 1, :]).pow(2.0).sum()
+    d_bc += (x[:, :, v_bc, :] - x[:, :, v_bc + 1, :]).pow(2.0).sum()
+
+    n_hb = height * (width / block_size) - 1
+    n_hbc = (height * (width - 1)) - n_hb
+    n_vb = width * (height / block_size) - 1
+    n_vbc = (width * (height - 1)) - n_vb
+    d_b /= n_hb + n_vb
+    d_bc /= n_hbc + n_vbc
+    t = math.log2(block_size) / math.log2(min(height, width)) if d_b > d_bc else 0
+    return t * (d_b - d_bc)
+
+
+def _psnrb_compute(
+    sum_squared_error: Tensor,
+    bef: Tensor,
+    n_obs: Tensor,
+    data_range: Tensor,
+) -> Tensor:
+    """Computes peak signal-to-noise ratio.
 
-    Example:
-        >>> target = torch.tensor([1, 1, 0, 0])
-        >>> preds = torch.tensor([0, 1, 0, 0])
-        >>> confmat = _cohen_kappa_update(preds, target, num_classes=2)
-        >>> _cohen_kappa_compute(confmat)
-        tensor(0.5000)
+    Args:
+        sum_squared_error: Sum of square of errors over all observations
+        bef: block effect
+        n_obs: Number of predictions or observations
+        data_range: the range of the data. If None, it is determined from the data (max - min).
     """
+    sum_squared_error = sum_squared_error / n_obs + bef
+    if data_range > 2:
+        return 10 * torch.log10(data_range**2 / sum_squared_error)
+    return 10 * torch.log10(1.0 / sum_squared_error)
 
-    confmat = _confusion_matrix_compute(confmat)
-    confmat = confmat.float() if not confmat.is_floating_point() else confmat
-    n_classes = confmat.shape[0]
-    sum0 = confmat.sum(dim=0, keepdim=True)
-    sum1 = confmat.sum(dim=1, keepdim=True)
-    expected = sum1 @ sum0 / sum0.sum()  # outer product
-
-    if weights is None:
-        w_mat = torch.ones_like(confmat).flatten()
-        w_mat[:: n_classes + 1] = 0
-        w_mat = w_mat.reshape(n_classes, n_classes)
-    elif weights in ("linear", "quadratic"):
-        w_mat = torch.zeros_like(confmat)
-        w_mat += torch.arange(n_classes, dtype=w_mat.dtype, device=w_mat.device)
-        if weights == "linear":
-            w_mat = torch.abs(w_mat - w_mat.T)
-        else:
-            w_mat = torch.pow(w_mat - w_mat.T, 2.0)
-    else:
-        raise ValueError(
-            f"Received {weights} for argument ``weights`` but should be either" " None, 'linear' or 'quadratic'"
-        )
 
-    k = torch.sum(w_mat * confmat) / torch.sum(w_mat * expected)
-    return 1 - k
+def _psnrb_update(preds: Tensor, target: Tensor, block_size: int = 8) -> Tuple[Tensor, Tensor, Tensor]:
+    """Updates and returns variables required to compute peak signal-to-noise ratio.
 
+    Args:
+        preds: Predicted tensor
+        target: Ground truth tensor
+        block_size: Integer indication the block size
+    """
+    sum_squared_error = torch.sum(torch.pow(preds - target, 2))
+    n_obs = tensor(target.numel(), device=target.device)
+    bef = _compute_bef(preds, block_size=block_size)
+    return sum_squared_error, bef, n_obs
 
-def cohen_kappa(
+
+def peak_signal_noise_ratio_with_blocked_effect(
     preds: Tensor,
     target: Tensor,
-    num_classes: int,
-    weights: Optional[str] = None,
-    threshold: float = 0.5,
+    block_size: int = 8,
 ) -> Tensor:
-    r"""Calculates `Cohen's kappa score`_ that measures inter-annotator agreement.
-
-    It is defined as
+    r"""Computes `Peak Signal to Noise Ratio With Blocked Effect` (PSNRB) metrics.
 
     .. math::
-        \kappa = (p_o - p_e) / (1 - p_e)
+        \text{PSNRB}(I, J) = 10 * \log_{10} \left(\frac{\max(I)^2}{\text{MSE}(I, J)-\text{B}(I, J)}\right)
 
-    where :math:`p_o` is the empirical probability of agreement and :math:`p_e` is
-    the expected agreement when both annotators assign labels randomly. Note that
-    :math:`p_e` is estimated using a per-annotator empirical prior over the
-    class labels.
+    Where :math:`\text{MSE}` denotes the `mean-squared-error`_ function.
 
     Args:
-        preds: (float or long tensor), Either a ``(N, ...)`` tensor with labels or
-            ``(N, C, ...)`` where C is the number of classes, tensor with labels/probabilities
-        target: ``target`` (long tensor), tensor with shape ``(N, ...)`` with ground true labels
-        num_classes: Number of classes in the dataset.
-        weights: Weighting type to calculate the score. Choose from:
-
-            - ``None`` or ``'none'``: no weighting
-            - ``'linear'``: linear weighting
-            - ``'quadratic'``: quadratic weighting
+        preds: estimated signal
+        target: groun truth signal
+        block_size: integer indication the block size
 
-        threshold: Threshold value for binary or multi-label probabilities.
+    Return:
+        Tensor with PSNRB score
 
     Example:
-        >>> from torchmetrics.functional import cohen_kappa
-        >>> target = torch.tensor([1, 1, 0, 0])
-        >>> preds = torch.tensor([0, 1, 0, 0])
-        >>> cohen_kappa(preds, target, num_classes=2)
-        tensor(0.5000)
+        >>> import torch
+        >>> from torchmetrics.functional.image import peak_signal_noise_ratio_with_blocked_effect
+        >>> _ = torch.manual_seed(42)
+        >>> preds = torch.rand(1, 1, 28, 28)
+        >>> target = torch.rand(1, 1, 28, 28)
+        >>> peak_signal_noise_ratio_with_blocked_effect(preds, target)
+        tensor(7.8402)
     """
-    confmat = _cohen_kappa_update(preds, target, num_classes, threshold)
-    return _cohen_kappa_compute(confmat, weights)
+    data_range = target.max() - target.min()
+    sum_squared_error, bef, n_obs = _psnrb_update(preds, target, block_size=block_size)
+    return _psnrb_compute(sum_squared_error, bef, n_obs, data_range)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/classification/f_beta.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/classification/calibration_error.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,354 +1,376 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Optional
+from typing import Any, Optional, Sequence, Union
 
-import torch
 from torch import Tensor
+from typing_extensions import Literal
 
-from torchmetrics.functional.classification.stat_scores import _reduce_stat_scores, _stat_scores_update
-from torchmetrics.utilities.enums import AverageMethod as AvgMethod
-from torchmetrics.utilities.enums import MDMCAverageMethod
-
-
-def _safe_divide(num: Tensor, denom: Tensor) -> Tensor:
-    """prevent zero division."""
-    denom[denom == 0.0] = 1
-    return num / denom
-
-
-def _fbeta_compute(
-    tp: Tensor,
-    fp: Tensor,
-    tn: Tensor,
-    fn: Tensor,
-    beta: float,
-    ignore_index: Optional[int],
-    average: str,
-    mdmc_average: Optional[str],
-) -> Tensor:
-    """Computes f_beta metric from stat scores: true positives, false positives, true negatives, false negatives.
+from torchmetrics.functional.classification.calibration_error import (
+    _binary_calibration_error_arg_validation,
+    _binary_calibration_error_tensor_validation,
+    _binary_calibration_error_update,
+    _binary_confusion_matrix_format,
+    _ce_compute,
+    _multiclass_calibration_error_arg_validation,
+    _multiclass_calibration_error_tensor_validation,
+    _multiclass_calibration_error_update,
+    _multiclass_confusion_matrix_format,
+)
+from torchmetrics.metric import Metric
+from torchmetrics.utilities.data import dim_zero_cat
+from torchmetrics.utilities.enums import ClassificationTaskNoMultilabel
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
+
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["BinaryCalibrationError.plot", "MulticlassCalibrationError.plot"]
+
+
+class BinaryCalibrationError(Metric):
+    r"""`Top-label Calibration Error`_ for binary tasks.
+
+    The expected calibration error can be used to quantify how well a given model is calibrated e.g. how well the
+    predicted output probabilities of the model matches the actual probabilities of the ground truth distribution.
+    Three different norms are implemented, each corresponding to variations on the calibration error metric.
+
+    .. math::
+        \text{ECE} = \sum_i^N b_i \|(p_i - c_i)\|, \text{L1 norm (Expected Calibration Error)}
+
+    .. math::
+        \text{MCE} =  \max_{i} (p_i - c_i), \text{Infinity norm (Maximum Calibration Error)}
+
+    .. math::
+        \text{RMSCE} = \sqrt{\sum_i^N b_i(p_i - c_i)^2}, \text{L2 norm (Root Mean Square Calibration Error)}
+
+    Where :math:`p_i` is the top-1 prediction accuracy in bin :math:`i`, :math:`c_i` is the average confidence of
+    predictions in bin :math:`i`, and :math:`b_i` is the fraction of data points in bin :math:`i`. Bins are constructed
+    in an uniform way in the [0,1] range.
+
+    As input to ``forward`` and ``update`` the metric accepts the following input:
+
+    - ``preds`` (:class:`~torch.Tensor`): A float tensor of shape ``(N, ...)`` containing probabilities or logits for
+      each observation. If preds has values outside [0,1] range we consider the input to be logits and will auto apply
+      sigmoid per element.
+    - ``target`` (:class:`~torch.Tensor`): An int tensor of shape ``(N, ...)`` containing ground truth labels, and
+      therefore only contain {0,1} values (except if `ignore_index` is specified). The value 1 always encodes the
+      positive class.
+
+    As output to ``forward`` and ``compute`` the metric returns the following output:
+
+    - ``bce`` (:class:`~torch.Tensor`): A scalar tensor containing the calibration error
+
+    Additional dimension ``...`` will be flattened into the batch dimension.
 
     Args:
-        tp: True positives
-        fp: False positives
-        tn: True negatives
-        fn: False negatives
-        beta: The parameter `beta` (which determines the weight of recall in the combined score)
-        ignore_index: Integer specifying a target class to ignore. If given, this class index does not contribute
-            to the returned score, regardless of reduction method
-        average: Defines the reduction that is applied
-        mdmc_average: Defines how averaging is done for multi-dimensional multi-class inputs (on top of the
-            ``average`` parameter)
+        n_bins: Number of bins to use when computing the metric.
+        norm: Norm used to compare empirical and expected probability bins.
+        ignore_index:
+            Specifies a target value that is ignored and does not contribute to the metric calculation
+        validate_args: bool indicating if input arguments and tensors should be validated for correctness.
+            Set to ``False`` for faster computations.
+        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
     Example:
-        >>> from torchmetrics.functional.classification.stat_scores import _stat_scores_update
-        >>> target = torch.tensor([0, 1, 2, 0, 1, 2])
-        >>> preds = torch.tensor([0, 2, 1, 0, 0, 1])
-        >>> tp, fp, tn, fn = _stat_scores_update(
-        ...                         preds,
-        ...                         target,
-        ...                         reduce='micro',
-        ...                         num_classes=3,
-        ...                     )
-        >>> _fbeta_compute(tp, fp, tn, fn, beta=0.5, ignore_index=None, average='micro', mdmc_average=None)
-        tensor(0.3333)
+        >>> from torch import tensor
+        >>> from torchmetrics.classification import BinaryCalibrationError
+        >>> preds = tensor([0.25, 0.25, 0.55, 0.75, 0.75])
+        >>> target = tensor([0, 0, 1, 1, 1])
+        >>> metric = BinaryCalibrationError(n_bins=2, norm='l1')
+        >>> metric(preds, target)
+        tensor(0.2900)
+        >>> bce = BinaryCalibrationError(n_bins=2, norm='l2')
+        >>> bce(preds, target)
+        tensor(0.2918)
+        >>> bce = BinaryCalibrationError(n_bins=2, norm='max')
+        >>> bce(preds, target)
+        tensor(0.3167)
     """
-    if average == AvgMethod.MICRO and mdmc_average != MDMCAverageMethod.SAMPLEWISE:
-        mask = tp >= 0
-        precision = _safe_divide(tp[mask].sum().float(), (tp[mask] + fp[mask]).sum())
-        recall = _safe_divide(tp[mask].sum().float(), (tp[mask] + fn[mask]).sum())
-    else:
-        precision = _safe_divide(tp.float(), tp + fp)
-        recall = _safe_divide(tp.float(), tp + fn)
-
-    num = (1 + beta**2) * precision * recall
-    denom = beta**2 * precision + recall
-    denom[denom == 0.0] = 1.0  # avoid division by 0
-
-    # if classes matter and a given class is not present in both the preds and the target,
-    # computing the score for this class is meaningless, thus they should be ignored
-    if average == AvgMethod.NONE and mdmc_average != MDMCAverageMethod.SAMPLEWISE:
-        # a class is not present if there exists no TPs, no FPs, and no FNs
-        meaningless_indeces = torch.nonzero((tp | fn | fp) == 0).cpu()
-        if ignore_index is None:
-            ignore_index = meaningless_indeces
-        else:
-            ignore_index = torch.unique(torch.cat((meaningless_indeces, torch.tensor([[ignore_index]]))))
-
-    if ignore_index is not None:
-        if average not in (AvgMethod.MICRO, AvgMethod.SAMPLES) and mdmc_average == MDMCAverageMethod.SAMPLEWISE:
-            num[..., ignore_index] = -1
-            denom[..., ignore_index] = -1
-        elif average not in (AvgMethod.MICRO, AvgMethod.SAMPLES):
-            num[ignore_index, ...] = -1
-            denom[ignore_index, ...] = -1
-
-    if average == AvgMethod.MACRO and mdmc_average != MDMCAverageMethod.SAMPLEWISE:
-        cond = (tp + fp + fn == 0) | (tp + fp + fn == -3)
-        num = num[~cond]
-        denom = denom[~cond]
-
-    return _reduce_stat_scores(
-        numerator=num,
-        denominator=denom,
-        weights=None if average != AvgMethod.WEIGHTED else tp + fn,
-        average=average,
-        mdmc_average=mdmc_average,
-    )
-
-
-def fbeta_score(
-    preds: Tensor,
-    target: Tensor,
-    beta: float = 1.0,
-    average: Optional[str] = "micro",
-    mdmc_average: Optional[str] = None,
-    ignore_index: Optional[int] = None,
-    num_classes: Optional[int] = None,
-    threshold: float = 0.5,
-    top_k: Optional[int] = None,
-    multiclass: Optional[bool] = None,
-) -> Tensor:
-    r"""
-    Computes f_beta metric.
+    is_differentiable: bool = False
+    higher_is_better: bool = False
+    full_state_update: bool = False
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
+
+    def __init__(
+        self,
+        n_bins: int = 15,
+        norm: Literal["l1", "l2", "max"] = "l1",
+        ignore_index: Optional[int] = None,
+        validate_args: bool = True,
+        **kwargs: Any,
+    ) -> None:
+        super().__init__(**kwargs)
+        if validate_args:
+            _binary_calibration_error_arg_validation(n_bins, norm, ignore_index)
+        self.validate_args = validate_args
+        self.n_bins = n_bins
+        self.norm = norm
+        self.ignore_index = ignore_index
+        self.add_state("confidences", [], dist_reduce_fx="cat")
+        self.add_state("accuracies", [], dist_reduce_fx="cat")
+
+    def update(self, preds: Tensor, target: Tensor) -> None:
+        """Update metric states with predictions and targets."""
+        if self.validate_args:
+            _binary_calibration_error_tensor_validation(preds, target, self.ignore_index)
+        preds, target = _binary_confusion_matrix_format(
+            preds, target, threshold=0.0, ignore_index=self.ignore_index, convert_to_labels=False
+        )
+        confidences, accuracies = _binary_calibration_error_update(preds, target)
+        self.confidences.append(confidences)
+        self.accuracies.append(accuracies)
+
+    def compute(self) -> Tensor:
+        """Compute metric."""
+        confidences = dim_zero_cat(self.confidences)
+        accuracies = dim_zero_cat(self.accuracies)
+        return _ce_compute(confidences, accuracies, self.n_bins, norm=self.norm)
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure object and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import rand, randint
+            >>> # Example plotting a single value
+            >>> from torchmetrics.classification import BinaryCalibrationError
+            >>> metric = BinaryCalibrationError(n_bins=2, norm='l1')
+            >>> metric.update(rand(10), randint(2,(10,)))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import rand, randint
+            >>> # Example plotting multiple values
+            >>> from torchmetrics.classification import BinaryCalibrationError
+            >>> metric = BinaryCalibrationError(n_bins=2, norm='l1')
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(rand(10), randint(2,(10,))))
+            >>> fig_, ax_ = metric.plot(values)
+        """
+        return self._plot(val, ax)
+
+
+class MulticlassCalibrationError(Metric):
+    r"""`Top-label Calibration Error`_ for multiclass tasks.
+
+    The expected calibration error can be used to quantify how well a given model is calibrated e.g. how well the
+    predicted output probabilities of the model matches the actual probabilities of the ground truth distribution.
+    Three different norms are implemented, each corresponding to variations on the calibration error metric.
 
     .. math::
-        F_{\beta} = (1 + \beta^2) * \frac{\text{precision} * \text{recall}}
-        {(\beta^2 * \text{precision}) + \text{recall}}
+        \text{ECE} = \sum_i^N b_i \|(p_i - c_i)\|, \text{L1 norm (Expected Calibration Error)}
 
-    Works with binary, multiclass, and multilabel data.
-    Accepts probabilities or logits from a model output or integer class values in prediction.
-    Works with multi-dimensional preds and target.
+    .. math::
+        \text{MCE} =  \max_{i} (p_i - c_i), \text{Infinity norm (Maximum Calibration Error)}
 
-    If preds and target are the same shape and preds is a float tensor, we use the ``self.threshold`` argument
-    to convert into integer labels. This is the case for binary and multi-label logits or probabilities.
+    .. math::
+        \text{RMSCE} = \sqrt{\sum_i^N b_i(p_i - c_i)^2}, \text{L2 norm (Root Mean Square Calibration Error)}
 
-    If preds has an extra dimension as in the case of multi-class scores we perform an argmax on ``dim=1``.
+    Where :math:`p_i` is the top-1 prediction accuracy in bin :math:`i`, :math:`c_i` is the average confidence of
+    predictions in bin :math:`i`, and :math:`b_i` is the fraction of data points in bin :math:`i`. Bins are constructed
+    in an uniform way in the [0,1] range.
 
-    The reduction method (how the precision scores are aggregated) is controlled by the
-    ``average`` parameter, and additionally by the ``mdmc_average`` parameter in the
-    multi-dimensional multi-class case. Accepts all inputs listed in :ref:`pages/classification:input types`.
+    As input to ``forward`` and ``update`` the metric accepts the following input:
 
-    Args:
-        preds: Predictions from model (probabilities, logits or labels)
-        target: Ground truth values
-        beta: beta coefficient
-        average:
-            Defines the reduction that is applied. Should be one of the following:
-
-                - ``'micro'`` [default]: Calculate the metric globally, across all samples and classes.
-                - ``'macro'``: Calculate the metric for each class separately, and average the
-                  metrics across classes (with equal weights for each class).
-                - ``'weighted'``: Calculate the metric for each class separately, and average the
-                  metrics across classes, weighting each class by its support (``tp + fn``).
-                - ``'none'`` or ``None``: Calculate the metric for each class separately, and return
-                  the metric for every class.
-                - ``'samples'``: Calculate the metric for each sample, and average the metrics
-                  across samples (with equal weights for each sample).
-
-            .. note:: What is considered a sample in the multi-dimensional multi-class case
-                depends on the value of ``mdmc_average``.
-
-            .. note:: If ``'none'`` and a given class doesn't occur in the ``preds`` or ``target``,
-                the value for the class will be ``nan``.
-
-        mdmc_average:
-            Defines how averaging is done for multi-dimensional multi-class inputs (on top of the
-            ``average`` parameter). Should be one of the following:
-
-                - ``None`` [default]: Should be left unchanged if your data is not multi-dimensional
-                  multi-class.
-                - ``'samplewise'``: In this case, the statistics are computed separately for each
-                  sample on the ``N`` axis, and then averaged over samples.
-                  The computation for each sample is done by treating the flattened extra axes ``...``
-                  (see :ref:`pages/classification:input types`) as the ``N`` dimension within the sample,
-                  and computing the metric for the sample based on that.
-                - ``'global'``: In this case the ``N`` and ``...`` dimensions of the inputs
-                  (see :ref:`pages/classification:input types`)
-                  are flattened into a new ``N_X`` sample axis, i.e. the inputs are treated as if they
-                  were ``(N_X, C)``. From here on the ``average`` parameter applies as usual.
+    - ``preds`` (:class:`~torch.Tensor`): A float tensor of shape ``(N, C, ...)`` containing probabilities or logits for
+      each observation. If preds has values outside [0,1] range we consider the input to be logits and will auto apply
+      softmax per sample.
+    - ``target`` (:class:`~torch.Tensor`): An int tensor of shape ``(N, ...)`` containing ground truth labels, and
+      therefore only contain values in the [0, n_classes-1] range (except if `ignore_index` is specified).
 
-        ignore_index:
-            Integer specifying a target class to ignore. If given, this class index does not contribute
-            to the returned score, regardless of reduction method. If an index is ignored, and ``average=None``
-            or ``'none'``, the score for the ignored class will be returned as ``nan``.
-        num_classes:
-            Number of classes. Necessary for ``'macro'``, ``'weighted'`` and ``None`` average methods.
-        threshold:
-            Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case
-            of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.
-        top_k:
-            Number of highest probability or logit score predictions considered to find the correct label,
-            relevant only for (multi-dimensional) multi-class inputs. The
-            default value (``None``) will be interpreted as 1 for these inputs.
-
-            Should be left at default (``None``) for all other types of inputs.
-        multiclass:
-            Used only in certain special cases, where you want to treat inputs as a different type
-            than what they appear to be. See the parameter's
-            :ref:`documentation section <pages/classification:using the multiclass parameter>`
-            for a more detailed explanation and examples.
-
-    Return:
-        The shape of the returned tensor depends on the ``average`` parameter
-
-        - If ``average in ['micro', 'macro', 'weighted', 'samples']``, a one-element tensor will be returned
-        - If ``average in ['none', None]``, the shape will be ``(C,)``, where ``C`` stands  for the number
-          of classes
+    .. note::
+       Additional dimension ``...`` will be flattened into the batch dimension.
 
-    Example:
-        >>> from torchmetrics.functional import fbeta_score
-        >>> target = torch.tensor([0, 1, 2, 0, 1, 2])
-        >>> preds = torch.tensor([0, 2, 1, 0, 0, 1])
-        >>> fbeta_score(preds, target, num_classes=3, beta=0.5)
-        tensor(0.3333)
+    As output to ``forward`` and ``compute`` the metric returns the following output:
 
-    """
-    allowed_average = list(AvgMethod)
-    if average not in allowed_average:
-        raise ValueError(f"The `average` has to be one of {allowed_average}, got {average}.")
-
-    if mdmc_average is not None and MDMCAverageMethod.from_str(mdmc_average) is None:
-        raise ValueError(f"The `mdmc_average` has to be one of {list(MDMCAverageMethod)}, got {mdmc_average}.")
-
-    if average in [AvgMethod.MACRO, AvgMethod.WEIGHTED, AvgMethod.NONE] and (not num_classes or num_classes < 1):
-        raise ValueError(f"When you set `average` as {average}, you have to provide the number of classes.")
-
-    if num_classes and ignore_index is not None and (not 0 <= ignore_index < num_classes or num_classes == 1):
-        raise ValueError(f"The `ignore_index` {ignore_index} is not valid for inputs with {num_classes} classes")
-
-    reduce = AvgMethod.MACRO if average in [AvgMethod.WEIGHTED, AvgMethod.NONE] else average
-    tp, fp, tn, fn = _stat_scores_update(
-        preds,
-        target,
-        reduce=reduce,
-        mdmc_reduce=mdmc_average,
-        threshold=threshold,
-        num_classes=num_classes,
-        top_k=top_k,
-        multiclass=multiclass,
-        ignore_index=ignore_index,
-    )
-
-    return _fbeta_compute(tp, fp, tn, fn, beta, ignore_index, average, mdmc_average)
-
-
-def f1_score(
-    preds: Tensor,
-    target: Tensor,
-    beta: float = 1.0,
-    average: Optional[str] = "micro",
-    mdmc_average: Optional[str] = None,
-    ignore_index: Optional[int] = None,
-    num_classes: Optional[int] = None,
-    threshold: float = 0.5,
-    top_k: Optional[int] = None,
-    multiclass: Optional[bool] = None,
-) -> Tensor:
-    """Computes F1 metric. F1 metrics correspond to a equally weighted average of the precision and recall scores.
-
-    Works with binary, multiclass, and multilabel data.
-    Accepts probabilities or logits from a model output or integer class values in prediction.
-    Works with multi-dimensional preds and target.
-
-    If preds and target are the same shape and preds is a float tensor, we use the ``self.threshold`` argument
-    to convert into integer labels. This is the case for binary and multi-label probabilities or logits.
-
-    If preds has an extra dimension as in the case of multi-class scores we perform an argmax on ``dim=1``.
-
-    The reduction method (how the precision scores are aggregated) is controlled by the
-    ``average`` parameter, and additionally by the ``mdmc_average`` parameter in the
-    multi-dimensional multi-class case. Accepts all inputs listed in :ref:`pages/classification:input types`.
+    - ``mcce`` (:class:`~torch.Tensor`): A scalar tensor containing the calibration error
 
     Args:
-        preds: Predictions from model (probabilities, logits or labels)
-        target: Ground truth values
-        beta: it is ignored
-        average:
-            Defines the reduction that is applied. Should be one of the following:
-
-            - ``'micro'`` [default]: Calculate the metric globally, across all samples and classes.
-            - ``'macro'``: Calculate the metric for each class separately, and average the
-              metrics across classes (with equal weights for each class).
-            - ``'weighted'``: Calculate the metric for each class separately, and average the
-              metrics across classes, weighting each class by its support (``tp + fn``).
-            - ``'none'`` or ``None``: Calculate the metric for each class separately, and return
-              the metric for every class.
-            - ``'samples'``: Calculate the metric for each sample, and average the metrics
-              across samples (with equal weights for each sample).
-
-            .. note:: What is considered a sample in the multi-dimensional multi-class case
-                depends on the value of ``mdmc_average``.
-
-            .. note:: If ``'none'`` and a given class doesn't occur in the ``preds`` or ``target``,
-                the value for the class will be ``nan``.
-
-        mdmc_average:
-            Defines how averaging is done for multi-dimensional multi-class inputs (on top of the
-            ``average`` parameter). Should be one of the following:
-
-            - ``None`` [default]: Should be left unchanged if your data is not multi-dimensional multi-class.
-
-            - ``'samplewise'``: In this case, the statistics are computed separately for each
-              sample on the ``N`` axis, and then averaged over samples.
-              The computation for each sample is done by treating the flattened extra axes ``...``
-              (see :ref:`pages/classification:input types`) as the ``N`` dimension within the sample,
-              and computing the metric for the sample based on that.
-
-            - ``'global'``: In this case the ``N`` and ``...`` dimensions of the inputs
-              (see :ref:`pages/classification:input types`)
-              are flattened into a new ``N_X`` sample axis, i.e. the inputs are treated as if they
-              were ``(N_X, C)``. From here on the ``average`` parameter applies as usual.
-
+        num_classes: Integer specifing the number of classes
+        n_bins: Number of bins to use when computing the metric.
+        norm: Norm used to compare empirical and expected probability bins.
         ignore_index:
-            Integer specifying a target class to ignore. If given, this class index does not contribute
-            to the returned score, regardless of reduction method. If an index is ignored, and ``average=None``
-            or ``'none'``, the score for the ignored class will be returned as ``nan``.
-
-        num_classes:
-            Number of classes. Necessary for ``'macro'``, ``'weighted'`` and ``None`` average methods.
-
-        threshold:
-            Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case
-            of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.
-        top_k:
-            Number of highest probability or logit score predictions considered to find the correct label,
-            relevant only for (multi-dimensional) multi-class inputs. The
-            default value (``None``) will be interpreted as 1 for these inputs.
-
-            Should be left at default (``None``) for all other types of inputs.
-
-        multiclass:
-            Used only in certain special cases, where you want to treat inputs as a different type
-            than what they appear to be. See the parameter's
-            :ref:`documentation section <pages/classification:using the multiclass parameter>`
-            for a more detailed explanation and examples.
-
-    Return:
-        The shape of the returned tensor depends on the ``average`` parameter
-
-        - If ``average in ['micro', 'macro', 'weighted', 'samples']``, a one-element tensor will be returned
-        - If ``average in ['none', None]``, the shape will be ``(C,)``, where ``C`` stands  for the number
-          of classes
+            Specifies a target value that is ignored and does not contribute to the metric calculation
+        validate_args: bool indicating if input arguments and tensors should be validated for correctness.
+            Set to ``False`` for faster computations.
+        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
     Example:
-        >>> from torchmetrics.functional import f1_score
-        >>> target = torch.tensor([0, 1, 2, 0, 1, 2])
-        >>> preds = torch.tensor([0, 2, 1, 0, 0, 1])
-        >>> f1_score(preds, target, num_classes=3)
-        tensor(0.3333)
+        >>> from torch import tensor
+        >>> from torchmetrics.classification import MulticlassCalibrationError
+        >>> preds = tensor([[0.25, 0.20, 0.55],
+        ...                 [0.55, 0.05, 0.40],
+        ...                 [0.10, 0.30, 0.60],
+        ...                 [0.90, 0.05, 0.05]])
+        >>> target = tensor([0, 1, 2, 0])
+        >>> metric = MulticlassCalibrationError(num_classes=3, n_bins=3, norm='l1')
+        >>> metric(preds, target)
+        tensor(0.2000)
+        >>> mcce = MulticlassCalibrationError(num_classes=3, n_bins=3, norm='l2')
+        >>> mcce(preds, target)
+        tensor(0.2082)
+        >>> mcce = MulticlassCalibrationError(num_classes=3, n_bins=3, norm='max')
+        >>> mcce(preds, target)
+        tensor(0.2333)
+    """
+    is_differentiable: bool = False
+    higher_is_better: bool = False
+    full_state_update: bool = False
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
+    plot_legend_name: str = "Class"
+
+    def __init__(
+        self,
+        num_classes: int,
+        n_bins: int = 15,
+        norm: Literal["l1", "l2", "max"] = "l1",
+        ignore_index: Optional[int] = None,
+        validate_args: bool = True,
+        **kwargs: Any,
+    ) -> None:
+        super().__init__(**kwargs)
+        if validate_args:
+            _multiclass_calibration_error_arg_validation(num_classes, n_bins, norm, ignore_index)
+        self.validate_args = validate_args
+        self.num_classes = num_classes
+        self.n_bins = n_bins
+        self.norm = norm
+        self.ignore_index = ignore_index
+        self.add_state("confidences", [], dist_reduce_fx="cat")
+        self.add_state("accuracies", [], dist_reduce_fx="cat")
+
+    def update(self, preds: Tensor, target: Tensor) -> None:
+        """Update metric states with predictions and targets."""
+        if self.validate_args:
+            _multiclass_calibration_error_tensor_validation(preds, target, self.num_classes, self.ignore_index)
+        preds, target = _multiclass_confusion_matrix_format(
+            preds, target, ignore_index=self.ignore_index, convert_to_labels=False
+        )
+        confidences, accuracies = _multiclass_calibration_error_update(preds, target)
+        self.confidences.append(confidences)
+        self.accuracies.append(accuracies)
+
+    def compute(self) -> Tensor:
+        """Compute metric."""
+        confidences = dim_zero_cat(self.confidences)
+        accuracies = dim_zero_cat(self.accuracies)
+        return _ce_compute(confidences, accuracies, self.n_bins, norm=self.norm)
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure object and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import randn, randint
+            >>> # Example plotting a single value
+            >>> from torchmetrics.classification import MulticlassCalibrationError
+            >>> metric = MulticlassCalibrationError(num_classes=3, n_bins=3, norm='l1')
+            >>> metric.update(randn(20,3).softmax(dim=-1), randint(3, (20,)))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import randn, randint
+            >>> # Example plotting a multiple values
+            >>> from torchmetrics.classification import MulticlassCalibrationError
+            >>> metric = MulticlassCalibrationError(num_classes=3, n_bins=3, norm='l1')
+            >>> values = []
+            >>> for _ in range(20):
+            ...     values.append(metric(randn(20,3).softmax(dim=-1), randint(3, (20,))))
+            >>> fig_, ax_ = metric.plot(values)
+        """
+        return self._plot(val, ax)
+
+
+class CalibrationError:
+    r"""`Top-label Calibration Error`_.
+
+    The expected calibration error can be used to quantify how well a given model is calibrated e.g. how well the
+    predicted output probabilities of the model matches the actual probabilities of the ground truth distribution.
+    Three different norms are implemented, each corresponding to variations on the calibration error metric.
+
+    .. math::
+        \text{ECE} = \sum_i^N b_i \|(p_i - c_i)\|, \text{L1 norm (Expected Calibration Error)}
+
+    .. math::
+        \text{MCE} =  \max_{i} (p_i - c_i), \text{Infinity norm (Maximum Calibration Error)}
+
+    .. math::
+        \text{RMSCE} = \sqrt{\sum_i^N b_i(p_i - c_i)^2}, \text{L2 norm (Root Mean Square Calibration Error)}
+
+    Where :math:`p_i` is the top-1 prediction accuracy in bin :math:`i`, :math:`c_i` is the average confidence of
+    predictions in bin :math:`i`, and :math:`b_i` is the fraction of data points in bin :math:`i`. Bins are constructed
+    in an uniform way in the [0,1] range.
+
+    This function is a simple wrapper to get the task specific versions of this metric, which is done by setting the
+    ``task`` argument to either ``'binary'`` or ``'multiclass'``. See the documentation of
+    :mod:`BinaryCalibrationError` and :mod:`MulticlassCalibrationError` for the specific details of
+    each argument influence and examples.
     """
-    return fbeta_score(
-        preds, target, 1.0, average, mdmc_average, ignore_index, num_classes, threshold, top_k, multiclass
-    )
+
+    def __new__(
+        cls,
+        task: Literal["binary", "multiclass"] = None,
+        n_bins: int = 15,
+        norm: Literal["l1", "l2", "max"] = "l1",
+        num_classes: Optional[int] = None,
+        ignore_index: Optional[int] = None,
+        validate_args: bool = True,
+        **kwargs: Any,
+    ) -> Metric:
+        """Initialize task metric."""
+        task = ClassificationTaskNoMultilabel.from_str(task)
+        kwargs.update({"n_bins": n_bins, "norm": norm, "ignore_index": ignore_index, "validate_args": validate_args})
+        if task == ClassificationTaskNoMultilabel.BINARY:
+            return BinaryCalibrationError(**kwargs)
+        if task == ClassificationTaskNoMultilabel.MULTICLASS:
+            if not isinstance(num_classes, int):
+                raise ValueError(f"`num_classes` is expected to be `int` but `{type(num_classes)} was passed.`")
+            return MulticlassCalibrationError(num_classes, **kwargs)
+        return None
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/classification/hamming.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/wer.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,96 +1,84 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Tuple, Union
+
+from typing import List, Tuple, Union
 
 import torch
-from torch import Tensor
+from torch import Tensor, tensor
 
-from torchmetrics.utilities.checks import _input_format_classification
+from torchmetrics.functional.text.helper import _edit_distance
 
 
-def _hamming_distance_update(
-    preds: Tensor,
-    target: Tensor,
-    threshold: float = 0.5,
-) -> Tuple[Tensor, int]:
-    """Returns the number of positions where prediction equals target, and number of predictions.
+def _wer_update(
+    preds: Union[str, List[str]],
+    target: Union[str, List[str]],
+) -> Tuple[Tensor, Tensor]:
+    """Update the wer score with the current set of references and predictions.
 
     Args:
-        preds: Predicted tensor
-        target: Ground truth tensor
-        threshold: Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case
-            of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.
-    """
-
-    preds, target, _ = _input_format_classification(preds, target, threshold=threshold)
+        preds: Transcription(s) to score as a string or list of strings
+        target: Reference(s) for each speech input as a string or list of strings
 
-    correct = (preds == target).sum()
-    total = preds.numel()
-
-    return correct, total
+    Returns:
+        Number of edit operations to get from the reference to the prediction, summed over all samples
+        Number of words overall references
+    """
+    if isinstance(preds, str):
+        preds = [preds]
+    if isinstance(target, str):
+        target = [target]
+    errors = tensor(0, dtype=torch.float)
+    total = tensor(0, dtype=torch.float)
+    for pred, tgt in zip(preds, target):
+        pred_tokens = pred.split()
+        tgt_tokens = tgt.split()
+        errors += _edit_distance(pred_tokens, tgt_tokens)
+        total += len(tgt_tokens)
+    return errors, total
 
 
-def _hamming_distance_compute(correct: Tensor, total: Union[int, Tensor]) -> Tensor:
-    """Computes the Hamming distance.
+def _wer_compute(errors: Tensor, total: Tensor) -> Tensor:
+    """Compute the word error rate.
 
     Args:
-        correct: Number of positions where prediction equals target
-        total: Total number of predictions
+        errors: Number of edit operations to get from the reference to the prediction, summed over all samples
+        total: Number of words overall references
 
-    Example:
-        >>> target = torch.tensor([[0, 1], [1, 1]])
-        >>> preds = torch.tensor([[0, 1], [0, 1]])
-        >>> correct, total = _hamming_distance_update(preds, target)
-        >>> _hamming_distance_compute(correct, total)
-        tensor(0.2500)
+    Returns:
+        Word error rate score
     """
+    return errors / total
 
-    return 1 - correct.float() / total
 
+def word_error_rate(preds: Union[str, List[str]], target: Union[str, List[str]]) -> Tensor:
+    """Word error rate (WordErrorRate_) is a common metric of the performance of an automatic speech recognition system.
 
-def hamming_distance(preds: Tensor, target: Tensor, threshold: float = 0.5) -> Tensor:
-    r"""
-    Computes the average `Hamming distance`_ (also
-    known as Hamming loss) between targets and predictions:
+    This value indicates the percentage of words that were incorrectly predicted. The lower the value, the better the
+    performance of the ASR system with a WER of 0 being a perfect score.
 
-    .. math::
-        \text{Hamming distance} = \frac{1}{N \cdot L} \sum_i^N \sum_l^L 1(y_{il} \neq \hat{y}_{il})
-
-    Where :math:`y` is a tensor of target values, :math:`\hat{y}` is a tensor of predictions,
-    and :math:`\bullet_{il}` refers to the :math:`l`-th label of the :math:`i`-th sample of that
-    tensor.
-
-    This is the same as ``1-accuracy`` for binary data, while for all other types of inputs it
-    treats each possible label separately - meaning that, for example, multi-class data is
-    treated as if it were multi-label.
+    Args:
+        preds: Transcription(s) to score as a string or list of strings
+        target: Reference(s) for each speech input as a string or list of strings
 
-    Accepts all input types listed in :ref:`pages/classification:input types`.
+    Returns:
+        Word error rate score
 
-    Args:
-        preds: Predictions from model (probabilities, logits or labels)
-        target: Ground truth
-        threshold:
-            Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case
-            of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.
-
-    Example:
-        >>> from torchmetrics.functional import hamming_distance
-        >>> target = torch.tensor([[0, 1], [1, 1]])
-        >>> preds = torch.tensor([[0, 1], [0, 1]])
-        >>> hamming_distance(preds, target)
-        tensor(0.2500)
+    Examples:
+        >>> preds = ["this is the prediction", "there is an other sample"]
+        >>> target = ["this is the reference", "there is another one"]
+        >>> word_error_rate(preds=preds, target=target)
+        tensor(0.5000)
     """
-
-    correct, total = _hamming_distance_update(preds, target, threshold)
-    return _hamming_distance_compute(correct, total)
+    errors, total = _wer_update(preds, target)
+    return _wer_compute(errors, total)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/classification/jaccard.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/detection/ciou.py`

 * *Files 26% similar despite different names*

```diff
@@ -7,158 +7,179 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Optional
+from typing import Any, Callable, Optional, Sequence, Union
 
-import torch
 from torch import Tensor
 
-from torchmetrics.functional.classification.confusion_matrix import _confusion_matrix_update
+from torchmetrics.detection.iou import IntersectionOverUnion
+from torchmetrics.functional.detection.ciou import _ciou_compute, _ciou_update
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE, _TORCHVISION_GREATER_EQUAL_0_13
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
+if not _TORCHVISION_GREATER_EQUAL_0_13:
+    __doctest_skip__ = ["CompleteIntersectionOverUnion", "CompleteIntersectionOverUnion.plot"]
+elif not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["CompleteIntersectionOverUnion.plot"]
 
-def _jaccard_from_confmat(
-    confmat: Tensor,
-    num_classes: int,
-    average: Optional[str] = "macro",
-    ignore_index: Optional[int] = None,
-    absent_score: float = 0.0,
-) -> Tensor:
-    """Computes the intersection over union from confusion matrix.
 
-    Args:
-        confmat: Confusion matrix without normalization
-        num_classes: Number of classes for a given prediction and target tensor
-        average:
-            Defines the reduction that is applied. Should be one of the following:
-
-            - ``'macro'`` [default]: Calculate the metric for each class separately, and average the
-              metrics across classes (with equal weights for each class).
-            - ``'micro'``: Calculate the metric globally, across all samples and classes.
-            - ``'weighted'``: Calculate the metric for each class separately, and average the
-              metrics across classes, weighting each class by its support (``tp + fn``).
-            - ``'none'`` or ``None``: Calculate the metric for each class separately, and return
-              the metric for every class. Note that if a given class doesn't occur in the
-              `preds` or `target`, the value for the class will be ``nan``.
-
-        ignore_index: optional int specifying a target class to ignore. If given, this class index does not contribute
-            to the returned score, regardless of reduction method.
-        absent_score: score to use for an individual class, if no instances of the class index were present in `pred`
-            AND no instances of the class index were present in `target`.
-    """
-    allowed_average = ["micro", "macro", "weighted", "none", None]
-    if average not in allowed_average:
-        raise ValueError(f"The `average` has to be one of {allowed_average}, got {average}.")
-
-    # Remove the ignored class index from the scores.
-    if ignore_index is not None and 0 <= ignore_index < num_classes:
-        confmat[ignore_index] = 0.0
-
-    if average == "none" or average is None:
-        intersection = torch.diag(confmat)
-        union = confmat.sum(0) + confmat.sum(1) - intersection
-
-        # If this class is absent in both target AND pred (union == 0), then use the absent_score for this class.
-        scores = intersection.float() / union.float()
-        scores[union == 0] = absent_score
-
-        if ignore_index is not None and 0 <= ignore_index < num_classes:
-            scores = torch.cat(
-                [
-                    scores[:ignore_index],
-                    scores[ignore_index + 1 :],
-                ]
-            )
-        return scores
+class CompleteIntersectionOverUnion(IntersectionOverUnion):
+    r"""Computes Complete Intersection Over Union (CIoU) <https://arxiv.org/abs/2005.03572>`_.
+
+    As input to ``forward`` and ``update`` the metric accepts the following input:
+
+    - ``preds`` (:class:`~List`): A list consisting of dictionaries each containing the key-values
+      (each dictionary corresponds to a single image). Parameters that should be provided per dict:
 
-    if average == "macro":
-        scores = _jaccard_from_confmat(
-            confmat, num_classes, average="none", ignore_index=ignore_index, absent_score=absent_score
-        )
-        return torch.mean(scores)
-
-    if average == "micro":
-        intersection = torch.sum(torch.diag(confmat))
-        union = torch.sum(torch.sum(confmat, dim=1) + torch.sum(confmat, dim=0) - torch.diag(confmat))
-        return intersection.float() / union.float()
-
-    weights = torch.sum(confmat, dim=1).float() / torch.sum(confmat).float()
-    scores = _jaccard_from_confmat(
-        confmat, num_classes, average="none", ignore_index=ignore_index, absent_score=absent_score
-    )
-    return torch.sum(weights * scores)
-
-
-def jaccard_index(
-    preds: Tensor,
-    target: Tensor,
-    num_classes: int,
-    average: Optional[str] = "macro",
-    ignore_index: Optional[int] = None,
-    absent_score: float = 0.0,
-    threshold: float = 0.5,
-) -> Tensor:
-    r"""Computes `Jaccard index`_
-
-    .. math:: J(A,B) = \frac{|A\cap B|}{|A\cup B|}
-
-    Where: :math:`A` and :math:`B` are both tensors of the same size,
-    containing integer class values. They may be subject to conversion from
-    input data (see description below).
+        - boxes: (:class:`~torch.FloatTensor`) of shape ``(num_boxes, 4)`` containing ``num_boxes`` detection
+          boxes of the format specified in the constructor.
+          By default, this method expects ``(xmin, ymin, xmax, ymax)`` in absolute image coordinates.
+        - scores: :class:`~torch.FloatTensor` of shape ``(num_boxes)`` containing detection scores for the boxes.
+        - labels: :class:`~torch.IntTensor` of shape ``(num_boxes)`` containing 0-indexed detection classes for
+          the boxes.
 
-    Note that it is different from box IoU.
+    - ``target`` (:class:`~List`) A list consisting of dictionaries each containing the key-values
+      (each dictionary corresponds to a single image). Parameters that should be provided per dict:
 
-    If preds and target are the same shape and preds is a float tensor, we use the ``self.threshold`` argument
-    to convert into integer labels. This is the case for binary and multi-label probabilities.
+        - boxes: :class:`~torch.FloatTensor` of shape ``(num_boxes, 4)`` containing ``num_boxes`` ground truth
+          boxes of the format specified in the constructor.
+          By default, this method expects ``(xmin, ymin, xmax, ymax)`` in absolute image coordinates.
+        - labels: :class:`~torch.IntTensor` of shape ``(num_boxes)`` containing 0-indexed ground truth
+          classes for the boxes.
 
-    If pred has an extra dimension as in the case of multi-class scores we
-    perform an argmax on ``dim=1``.
+    As output of ``forward`` and ``compute`` the metric returns the following output:
+
+    - ``ciou_dict``: A dictionary containing the following key-values:
+
+        - ciou: (:class:`~torch.Tensor`)
+        - ciou/cl_{cl}: (:class:`~torch.Tensor`), if argument ``class metrics=True``
 
     Args:
-        preds: tensor containing predictions from model (probabilities, or labels) with shape ``[N, d1, d2, ...]``
-        target: tensor containing ground truth labels with shape ``[N, d1, d2, ...]``
-        num_classes: Specify the number of classes
-        average:
-            Defines the reduction that is applied. Should be one of the following:
-
-            - ``'macro'`` [default]: Calculate the metric for each class separately, and average the
-              metrics across classes (with equal weights for each class).
-            - ``'micro'``: Calculate the metric globally, across all samples and classes.
-            - ``'weighted'``: Calculate the metric for each class separately, and average the
-              metrics across classes, weighting each class by its support (``tp + fn``).
-            - ``'none'`` or ``None``: Calculate the metric for each class separately, and return
-              the metric for every class. Note that if a given class doesn't occur in the
-              `preds` or `target`, the value for the class will be ``nan``.
-
-        ignore_index: optional int specifying a target class to ignore. If given,
-            this class index does not contribute to the returned score, regardless
-            of reduction method. Has no effect if given an int that is not in the
-            range ``[0, num_classes-1]``, where num_classes is either given or derived
-            from pred and target. By default, no index is ignored, and all classes are used.
-        absent_score: score to use for an individual class, if no instances of
-            the class index were present in ``preds`` AND no instances of the class
-            index were present in ``target``. For example, if we have 3 classes,
-            [0, 0] for ``preds``, and [0, 2] for ``target``, then class 1 would be
-            assigned the `absent_score`.
-        threshold: Threshold value for binary or multi-label probabilities.
-
-    Return:
-        The shape of the returned tensor depends on the ``average`` parameter
-
-        - If ``average in ['micro', 'macro', 'weighted']``, a one-element tensor will be returned
-        - If ``average in ['none', None]``, the shape will be ``(C,)``, where ``C`` stands  for the number
-          of classes
+        box_format:
+            Input format of given boxes. Supported formats are ``[`xyxy`, `xywh`, `cxcywh`]``.
+        iou_thresholds:
+            Optional IoU thresholds for evaluation. If set to `None` the threshold is ignored.
+        class_metrics:
+            Option to enable per-class metrics for IoU. Has a performance impact.
+        kwargs:
+            Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
     Example:
-        >>> from torchmetrics.functional import jaccard_index
-        >>> target = torch.randint(0, 2, (10, 25, 25))
-        >>> pred = torch.tensor(target)
-        >>> pred[2:5, 7:13, 9:15] = 1 - pred[2:5, 7:13, 9:15]
-        >>> jaccard_index(pred, target, num_classes=2)
-        tensor(0.9660)
+        >>> import torch
+        >>> from torchmetrics.detection import CompleteIntersectionOverUnion
+        >>> preds = [
+        ...    {
+        ...        "boxes": torch.tensor([[296.55, 93.96, 314.97, 152.79], [298.55, 98.96, 314.97, 151.79]]),
+        ...        "scores": torch.tensor([0.236, 0.56]),
+        ...        "labels": torch.tensor([4, 5]),
+        ...    }
+        ... ]
+        >>> target = [
+        ...    {
+        ...        "boxes": torch.tensor([[300.00, 100.00, 315.00, 150.00]]),
+        ...        "labels": torch.tensor([5]),
+        ...    }
+        ... ]
+        >>> metric = CompleteIntersectionOverUnion()
+        >>> metric(preds, target)
+        {'ciou': tensor(-0.5694)}
+
+    Raises:
+        ModuleNotFoundError:
+            If torchvision is not installed with version 0.13.0 or newer.
+
     """
+    _iou_type: str = "ciou"
+    _invalid_val: float = -2.0  # unsure, min val could be just -1.5 as well
+
+    def __init__(
+        self,
+        box_format: str = "xyxy",
+        iou_threshold: Optional[float] = None,
+        class_metrics: bool = False,
+        **kwargs: Any,
+    ) -> None:
+        if not _TORCHVISION_GREATER_EQUAL_0_13:
+            raise ModuleNotFoundError(
+                f"Metric `{self._iou_type.upper()}` requires that `torchvision` version 0.13.0 or newer is installed."
+                " Please install with `pip install torchvision>=0.13` or `pip install torchmetrics[detection]`."
+            )
+        super().__init__(box_format, iou_threshold, class_metrics, **kwargs)
 
-    confmat = _confusion_matrix_update(preds, target, num_classes, threshold)
-    return _jaccard_from_confmat(confmat, num_classes, average, ignore_index, absent_score)
+    @staticmethod
+    def _iou_update_fn(*args: Any, **kwargs: Any) -> Tensor:
+        return _ciou_update(*args, **kwargs)
+
+    @staticmethod
+    def _iou_compute_fn(*args: Any, **kwargs: Any) -> Tensor:
+        return _ciou_compute(*args, **kwargs)
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure object and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting single value
+            >>> import torch
+            >>> from torchmetrics.detection import CompleteIntersectionOverUnion
+            >>> preds = [
+            ...    {
+            ...        "boxes": torch.tensor([[296.55, 93.96, 314.97, 152.79], [298.55, 98.96, 314.97, 151.79]]),
+            ...        "scores": torch.tensor([0.236, 0.56]),
+            ...        "labels": torch.tensor([4, 5]),
+            ...    }
+            ... ]
+            >>> target = [
+            ...    {
+            ...        "boxes": torch.tensor([[300.00, 100.00, 315.00, 150.00]]),
+            ...        "labels": torch.tensor([5]),
+            ...    }
+            ... ]
+            >>> metric = CompleteIntersectionOverUnion()
+            >>> metric.update(preds, target)
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> import torch
+            >>> from torchmetrics.detection import CompleteIntersectionOverUnion
+            >>> preds = [
+            ...    {
+            ...        "boxes": torch.tensor([[296.55, 93.96, 314.97, 152.79], [298.55, 98.96, 314.97, 151.79]]),
+            ...        "scores": torch.tensor([0.236, 0.56]),
+            ...        "labels": torch.tensor([4, 5]),
+            ...    }
+            ... ]
+            >>> target = lambda : [
+            ...    {
+            ...        "boxes": torch.tensor([[300.00, 100.00, 315.00, 150.00]]) + torch.randint(-10, 10, (1, 4)),
+            ...        "labels": torch.tensor([5]),
+            ...    }
+            ... ]
+            >>> metric = CompleteIntersectionOverUnion()
+            >>> vals = []
+            >>> for _ in range(20):
+            ...     vals.append(metric(preds, target()))
+            >>> fig_, ax_ = metric.plot(vals)
+        """
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/classification/kl_divergence.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/kl_divergence.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,58 +1,59 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import Tuple
+from typing import Tuple, Union
 
 import torch
 from torch import Tensor
 from typing_extensions import Literal
 
 from torchmetrics.utilities.checks import _check_same_shape
 from torchmetrics.utilities.compute import _safe_xlogy
 
 
 def _kld_update(p: Tensor, q: Tensor, log_prob: bool) -> Tuple[Tensor, int]:
-    """Updates and returns KL divergence scores for each observation and the total number of observations. Checks
-    same shape and 2D nature of the input tensors else raises ValueError.
+    """Update and returns KL divergence scores for each observation and the total number of observations.
 
     Args:
         p: data distribution with shape ``[N, d]``
         q: prior or approximate distribution with shape ``[N, d]``
         log_prob: bool indicating if input is log-probabilities or probabilities. If given as probabilities,
             will normalize to make sure the distributes sum to 1
     """
     _check_same_shape(p, q)
     if p.ndim != 2 or q.ndim != 2:
         raise ValueError(f"Expected both p and q distribution to be 2D but got {p.ndim} and {q.ndim} respectively")
 
     total = p.shape[0]
     if log_prob:
-        measures = torch.sum(p.exp() * (p - q), axis=-1)
+        measures = torch.sum(p.exp() * (p - q), axis=-1)  # type: ignore[call-overload]
     else:
-        p = p / p.sum(axis=-1, keepdim=True)
-        q = q / q.sum(axis=-1, keepdim=True)
-        measures = _safe_xlogy(p, p / q).sum(axis=-1)
+        p = p / p.sum(axis=-1, keepdim=True)  # type: ignore[call-overload]
+        q = q / q.sum(axis=-1, keepdim=True)  # type: ignore[call-overload]
+        measures = _safe_xlogy(p, p / q).sum(axis=-1)  # type: ignore[call-overload]
 
     return measures, total
 
 
-def _kld_compute(measures: Tensor, total: Tensor, reduction: Literal["mean", "sum", "none", None] = "mean") -> Tensor:
-    """Computes the KL divergenece based on the type of reduction.
+def _kld_compute(
+    measures: Tensor, total: Union[int, Tensor], reduction: Literal["mean", "sum", "none", None] = "mean"
+) -> Tensor:
+    """Compute the KL divergenece based on the type of reduction.
 
     Args:
         measures: Tensor of KL divergence scores for each observation
         total: Number of observations
         reduction:
             Determines how to reduce over the ``N``/batch dimension:
 
@@ -63,28 +64,27 @@
     Example:
         >>> p = torch.tensor([[0.36, 0.48, 0.16]])
         >>> q = torch.tensor([[1/3, 1/3, 1/3]])
         >>> measures, total = _kld_update(p, q, log_prob=False)
         >>> _kld_compute(measures, total)
         tensor(0.0853)
     """
-
     if reduction == "sum":
         return measures.sum()
     if reduction == "mean":
         return measures.sum() / total
     if reduction is None or reduction == "none":
         return measures
     return measures / total
 
 
 def kl_divergence(
     p: Tensor, q: Tensor, log_prob: bool = False, reduction: Literal["mean", "sum", "none", None] = "mean"
 ) -> Tensor:
-    r"""Computes `KL divergence`_
+    r"""Compute `KL divergence`_.
 
     .. math::
         D_{KL}(P||Q) = \sum_{x\in\mathcal{X}} P(x) \log\frac{P(x)}{Q{x}}
 
     Where :math:`P` and :math:`Q` are probability distributions where :math:`P` usually represents a distribution
     over data and :math:`Q` is often a prior or approximation of :math:`P`. It should be noted that the KL divergence
     is a non-symetrical metric i.e. :math:`D_{KL}(P||Q) \neq D_{KL}(Q||P)`.
@@ -98,15 +98,15 @@
             Determines how to reduce over the ``N``/batch dimension:
 
             - ``'mean'`` [default]: Averages score across samples
             - ``'sum'``: Sum score across samples
             - ``'none'`` or ``None``: Returns score per sample
 
     Example:
-        >>> import torch
-        >>> p = torch.tensor([[0.36, 0.48, 0.16]])
-        >>> q = torch.tensor([[1/3, 1/3, 1/3]])
+        >>> from torch import tensor
+        >>> p = tensor([[0.36, 0.48, 0.16]])
+        >>> q = tensor([[1/3, 1/3, 1/3]])
         >>> kl_divergence(p, q)
         tensor(0.0853)
     """
     measures, total = _kld_update(p, q, log_prob)
     return _kld_compute(measures, total, reduction)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/classification/precision_recall.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/auroc.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,552 +1,473 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Optional, Tuple
+from typing import List, Optional, Tuple, Union
 
 import torch
-from torch import Tensor
+from torch import Tensor, tensor
+from typing_extensions import Literal
 
-from torchmetrics.functional.classification.stat_scores import _reduce_stat_scores, _stat_scores_update
-from torchmetrics.utilities.enums import AverageMethod, MDMCAverageMethod
-
-
-def _precision_compute(
-    tp: Tensor,
-    fp: Tensor,
-    fn: Tensor,
-    average: Optional[str],
-    mdmc_average: Optional[str],
+from torchmetrics.functional.classification.precision_recall_curve import (
+    _binary_precision_recall_curve_arg_validation,
+    _binary_precision_recall_curve_format,
+    _binary_precision_recall_curve_tensor_validation,
+    _binary_precision_recall_curve_update,
+    _multiclass_precision_recall_curve_arg_validation,
+    _multiclass_precision_recall_curve_format,
+    _multiclass_precision_recall_curve_tensor_validation,
+    _multiclass_precision_recall_curve_update,
+    _multilabel_precision_recall_curve_arg_validation,
+    _multilabel_precision_recall_curve_format,
+    _multilabel_precision_recall_curve_tensor_validation,
+    _multilabel_precision_recall_curve_update,
+)
+from torchmetrics.functional.classification.roc import (
+    _binary_roc_compute,
+    _multiclass_roc_compute,
+    _multilabel_roc_compute,
+)
+from torchmetrics.utilities.compute import _auc_compute_without_check, _safe_divide
+from torchmetrics.utilities.data import _bincount
+from torchmetrics.utilities.enums import ClassificationTask
+from torchmetrics.utilities.prints import rank_zero_warn
+
+
+def _reduce_auroc(
+    fpr: Union[Tensor, List[Tensor]],
+    tpr: Union[Tensor, List[Tensor]],
+    average: Optional[Literal["macro", "weighted", "none"]] = "macro",
+    weights: Optional[Tensor] = None,
 ) -> Tensor:
-    """Computes precision from the stat scores: true positives, false positives, true negatives, false negatives.
-
-    Args:
-        tp: True positives
-        fp: False positives
-        fn: False negatives
-        average: Defines the reduction that is applied
-        mdmc_average: Defines how averaging is done for multi-dimensional multi-class inputs (on top of the
-            ``average`` parameter)
-
-    Example:
-        >>> from torchmetrics.functional.classification.stat_scores import _stat_scores_update
-        >>> preds  = torch.tensor([2, 0, 2, 1])
-        >>> target = torch.tensor([1, 1, 2, 0])
-        >>> tp, fp, tn, fn = _stat_scores_update( preds, target, reduce='macro', num_classes=3)
-        >>> _precision_compute(tp, fp, fn, average='macro', mdmc_average=None)
-        tensor(0.1667)
-        >>> tp, fp, tn, fn = _stat_scores_update(preds, target, reduce='micro')
-        >>> _precision_compute(tp, fp, fn, average='micro', mdmc_average=None)
-        tensor(0.2500)
-    """
+    """Reduce multiple average precision score into one number."""
+    if isinstance(fpr, Tensor) and isinstance(tpr, Tensor):
+        res = _auc_compute_without_check(fpr, tpr, 1.0, axis=1)
+    else:
+        res = torch.stack([_auc_compute_without_check(x, y, 1.0) for x, y in zip(fpr, tpr)])
+    if average is None or average == "none":
+        return res
+    if torch.isnan(res).any():
+        rank_zero_warn(
+            f"Average precision score for one or more classes was `nan`. Ignoring these classes in {average}-average",
+            UserWarning,
+        )
+    idx = ~torch.isnan(res)
+    if average == "macro":
+        return res[idx].mean()
+    if average == "weighted" and weights is not None:
+        weights = _safe_divide(weights[idx], weights[idx].sum())
+        return (res[idx] * weights).sum()
+    raise ValueError("Received an incompatible combinations of inputs to make reduction.")
+
+
+def _binary_auroc_arg_validation(
+    max_fpr: Optional[float] = None,
+    thresholds: Optional[Union[int, List[float], Tensor]] = None,
+    ignore_index: Optional[int] = None,
+) -> None:
+    _binary_precision_recall_curve_arg_validation(thresholds, ignore_index)
+    if max_fpr is not None and not isinstance(max_fpr, float) and 0 < max_fpr <= 1:
+        raise ValueError(f"Arguments `max_fpr` should be a float in range (0, 1], but got: {max_fpr}")
 
-    numerator = tp.clone()
-    denominator = tp + fp
 
-    if average == AverageMethod.MACRO and mdmc_average != MDMCAverageMethod.SAMPLEWISE:
-        cond = tp + fp + fn == 0
-        numerator = numerator[~cond]
-        denominator = denominator[~cond]
-
-    if average == AverageMethod.NONE and mdmc_average != MDMCAverageMethod.SAMPLEWISE:
-        # a class is not present if there exists no TPs, no FPs, and no FNs
-        meaningless_indeces = torch.nonzero((tp | fn | fp) == 0).cpu()
-        numerator[meaningless_indeces, ...] = -1
-        denominator[meaningless_indeces, ...] = -1
-
-    return _reduce_stat_scores(
-        numerator=numerator,
-        denominator=denominator,
-        weights=None if average != "weighted" else tp + fn,
-        average=average,
-        mdmc_average=mdmc_average,
-    )
+def _binary_auroc_compute(
+    state: Union[Tensor, Tuple[Tensor, Tensor]],
+    thresholds: Optional[Tensor],
+    max_fpr: Optional[float] = None,
+    pos_label: int = 1,
+) -> Tensor:
+    fpr, tpr, _ = _binary_roc_compute(state, thresholds, pos_label)
+    if max_fpr is None or max_fpr == 1:
+        return _auc_compute_without_check(fpr, tpr, 1.0)
+
+    _device = fpr.device if isinstance(fpr, Tensor) else fpr[0].device
+    max_area: Tensor = tensor(max_fpr, device=_device)
+    # Add a single point at max_fpr and interpolate its tpr value
+    stop = torch.bucketize(max_area, fpr, out_int32=True, right=True)
+    weight = (max_area - fpr[stop - 1]) / (fpr[stop] - fpr[stop - 1])
+    interp_tpr: Tensor = torch.lerp(tpr[stop - 1], tpr[stop], weight)
+    tpr = torch.cat([tpr[:stop], interp_tpr.view(1)])
+    fpr = torch.cat([fpr[:stop], max_area.view(1)])
+
+    # Compute partial AUC
+    partial_auc = _auc_compute_without_check(fpr, tpr, 1.0)
+
+    # McClish correction: standardize result to be 0.5 if non-discriminant and 1 if maximal
+    min_area: Tensor = 0.5 * max_area**2
+    return 0.5 * (1 + (partial_auc - min_area) / (max_area - min_area))
 
 
-def precision(
+def binary_auroc(
     preds: Tensor,
     target: Tensor,
-    average: Optional[str] = "micro",
-    mdmc_average: Optional[str] = None,
+    max_fpr: Optional[float] = None,
+    thresholds: Optional[Union[int, List[float], Tensor]] = None,
     ignore_index: Optional[int] = None,
-    num_classes: Optional[int] = None,
-    threshold: float = 0.5,
-    top_k: Optional[int] = None,
-    multiclass: Optional[bool] = None,
+    validate_args: bool = True,
 ) -> Tensor:
-    r"""Computes `Precision`_
-
-    .. math:: \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
-
-    Where :math:`\text{TP}` and :math:`\text{FP}` represent the number of true positives and
-    false positives respecitively. With the use of ``top_k`` parameter, this metric can
-    generalize to Precision@K.
+    r"""Compute Area Under the Receiver Operating Characteristic Curve (`ROC AUC`_) for binary tasks.
 
-    The reduction method (how the precision scores are aggregated) is controlled by the
-    ``average`` parameter, and additionally by the ``mdmc_average`` parameter in the
-    multi-dimensional multi-class case. Accepts all inputs listed in :ref:`pages/classification:input types`.
+    The AUROC score summarizes the ROC curve into an single number that describes the performance of a model for
+    multiple thresholds at the same time. Notably, an AUROC score of 1 is a perfect score and an AUROC score of 0.5
+    corresponds to random guessing.
+
+    Accepts the following input tensors:
+
+    - ``preds`` (float tensor): ``(N, ...)``. Preds should be a tensor containing probabilities or logits for each
+      observation. If preds has values outside [0,1] range we consider the input to be logits and will auto apply
+      sigmoid per element.
+    - ``target`` (int tensor): ``(N, ...)``. Target should be a tensor containing ground truth labels, and therefore
+      only contain {0,1} values (except if `ignore_index` is specified). The value 1 always encodes the positive class.
+
+    Additional dimension ``...`` will be flattened into the batch dimension.
+
+    The implementation both supports calculating the metric in a non-binned but accurate version and a binned version
+    that is less accurate but more memory efficient. Setting the `thresholds` argument to `None` will activate the
+    non-binned  version that uses memory of size :math:`\mathcal{O}(n_{samples})` whereas setting the `thresholds`
+    argument to either an integer, list or a 1d tensor will use a binned version that uses memory of
+    size :math:`\mathcal{O}(n_{thresholds})` (constant memory).
 
     Args:
-        preds: Predictions from model (probabilities, logits or labels)
-        target: Ground truth values
-        average:
-            Defines the reduction that is applied. Should be one of the following:
-
-            - ``'micro'`` [default]: Calculate the metric globally, across all samples and classes.
-            - ``'macro'``: Calculate the metric for each class separately, and average the
-              metrics across classes (with equal weights for each class).
-            - ``'weighted'``: Calculate the metric for each class separately, and average the
-              metrics across classes, weighting each class by its support (``tp + fn``).
-            - ``'none'`` or ``None``: Calculate the metric for each class separately, and return
-              the metric for every class.
-            - ``'samples'``: Calculate the metric for each sample, and average the metrics
-              across samples (with equal weights for each sample).
-
-            .. note:: What is considered a sample in the multi-dimensional multi-class case
-                depends on the value of ``mdmc_average``.
-
-            .. note:: If ``'none'`` and a given class doesn't occur in the ``preds`` or ``target``,
-                the value for the class will be ``nan``.
-
-        mdmc_average:
-            Defines how averaging is done for multi-dimensional multi-class inputs (on top of the
-            ``average`` parameter). Should be one of the following:
-
-            - ``None`` [default]: Should be left unchanged if your data is not multi-dimensional multi-class.
-
-            - ``'samplewise'``: In this case, the statistics are computed separately for each
-              sample on the ``N`` axis, and then averaged over samples.
-              The computation for each sample is done by treating the flattened extra axes ``...``
-              (see :ref:`pages/classification:input types`) as the ``N`` dimension within the sample,
-              and computing the metric for the sample based on that.
-
-            - ``'global'``: In this case the ``N`` and ``...`` dimensions of the inputs
-              (see :ref:`pages/classification:input types`)
-              are flattened into a new ``N_X`` sample axis, i.e. the inputs are treated as if they
-              were ``(N_X, C)``. From here on the ``average`` parameter applies as usual.
+        preds: Tensor with predictions
+        target: Tensor with true labels
+        max_fpr: If not ``None``, calculates standardized partial AUC over the range ``[0, max_fpr]``.
+        thresholds:
+            Can be one of:
+
+            - If set to `None`, will use a non-binned approach where thresholds are dynamically calculated from
+              all the data. Most accurate but also most memory consuming approach.
+            - If set to an `int` (larger than 1), will use that number of thresholds linearly spaced from
+              0 to 1 as bins for the calculation.
+            - If set to an `list` of floats, will use the indicated thresholds in the list as bins for the calculation
+            - If set to an 1d `tensor` of floats, will use the indicated thresholds in the tensor as
+              bins for the calculation.
 
         ignore_index:
-            Integer specifying a target class to ignore. If given, this class index does not contribute
-            to the returned score, regardless of reduction method. If an index is ignored, and ``average=None``
-            or ``'none'``, the score for the ignored class will be returned as ``nan``.
-
-        num_classes:
-            Number of classes. Necessary for ``'macro'``, ``'weighted'`` and ``None`` average methods.
-
-        threshold:
-            Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case
-            of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.
-        top_k:
-            Number of highest probability or logit score predictions considered to find the correct label,
-            relevant only for (multi-dimensional) multi-class inputs. The
-            default value (``None``) will be interpreted as 1 for these inputs.
-
-            Should be left at default (``None``) for all other types of inputs.
-        multiclass:
-            Used only in certain special cases, where you want to treat inputs as a different type
-            than what they appear to be. See the parameter's
-            :ref:`documentation section <pages/classification:using the multiclass parameter>`
-            for a more detailed explanation and examples.
-
-    Return:
-        The shape of the returned tensor depends on the ``average`` parameter
-
-        - If ``average in ['micro', 'macro', 'weighted', 'samples']``, a one-element tensor will be returned
-        - If ``average in ['none', None]``, the shape will be ``(C,)``, where ``C`` stands  for the number of classes
-
-    Raises:
-        ValueError:
-            If ``average`` is not one of ``"micro"``, ``"macro"``, ``"weighted"``, ``"samples"``, ``"none"`` or ``None``
-        ValueError:
-            If ``mdmc_average`` is not one of ``None``, ``"samplewise"``, ``"global"``.
-        ValueError:
-            If ``average`` is set but ``num_classes`` is not provided.
-        ValueError:
-            If ``num_classes`` is set and ``ignore_index`` is not in the range ``[0, num_classes)``.
+            Specifies a target value that is ignored and does not contribute to the metric calculation
+        validate_args: bool indicating if input arguments and tensors should be validated for correctness.
+            Set to ``False`` for faster computations.
 
-    Example:
-        >>> from torchmetrics.functional import precision
-        >>> preds  = torch.tensor([2, 0, 2, 1])
-        >>> target = torch.tensor([1, 1, 2, 0])
-        >>> precision(preds, target, average='macro', num_classes=3)
-        tensor(0.1667)
-        >>> precision(preds, target, average='micro')
-        tensor(0.2500)
+    Returns:
+        A single scalar with the auroc score
 
+    Example:
+        >>> from torchmetrics.functional.classification import binary_auroc
+        >>> preds = torch.tensor([0, 0.5, 0.7, 0.8])
+        >>> target = torch.tensor([0, 1, 1, 0])
+        >>> binary_auroc(preds, target, thresholds=None)
+        tensor(0.5000)
+        >>> binary_auroc(preds, target, thresholds=5)
+        tensor(0.5000)
     """
-    allowed_average = ["micro", "macro", "weighted", "samples", "none", None]
-    if average not in allowed_average:
-        raise ValueError(f"The `average` has to be one of {allowed_average}, got {average}.")
+    if validate_args:
+        _binary_auroc_arg_validation(max_fpr, thresholds, ignore_index)
+        _binary_precision_recall_curve_tensor_validation(preds, target, ignore_index)
+    preds, target, thresholds = _binary_precision_recall_curve_format(preds, target, thresholds, ignore_index)
+    state = _binary_precision_recall_curve_update(preds, target, thresholds)
+    return _binary_auroc_compute(state, thresholds, max_fpr)
 
-    allowed_mdmc_average = [None, "samplewise", "global"]
-    if mdmc_average not in allowed_mdmc_average:
-        raise ValueError(f"The `mdmc_average` has to be one of {allowed_mdmc_average}, got {mdmc_average}.")
-
-    if average in ["macro", "weighted", "none", None] and (not num_classes or num_classes < 1):
-        raise ValueError(f"When you set `average` as {average}, you have to provide the number of classes.")
-
-    if num_classes and ignore_index is not None and (not 0 <= ignore_index < num_classes or num_classes == 1):
-        raise ValueError(f"The `ignore_index` {ignore_index} is not valid for inputs with {num_classes} classes")
-
-    reduce = "macro" if average in ["weighted", "none", None] else average
-    tp, fp, _, fn = _stat_scores_update(
-        preds,
-        target,
-        reduce=reduce,
-        mdmc_reduce=mdmc_average,
-        threshold=threshold,
-        num_classes=num_classes,
-        top_k=top_k,
-        multiclass=multiclass,
-        ignore_index=ignore_index,
-    )
 
-    return _precision_compute(tp, fp, fn, average, mdmc_average)
+def _multiclass_auroc_arg_validation(
+    num_classes: int,
+    average: Optional[Literal["macro", "weighted", "none"]] = "macro",
+    thresholds: Optional[Union[int, List[float], Tensor]] = None,
+    ignore_index: Optional[int] = None,
+) -> None:
+    _multiclass_precision_recall_curve_arg_validation(num_classes, thresholds, ignore_index)
+    allowed_average = ("macro", "weighted", "none", None)
+    if average not in allowed_average:
+        raise ValueError(f"Expected argument `average` to be one of {allowed_average} but got {average}")
 
 
-def _recall_compute(
-    tp: Tensor,
-    fp: Tensor,
-    fn: Tensor,
-    average: Optional[str],
-    mdmc_average: Optional[str],
+def _multiclass_auroc_compute(
+    state: Union[Tensor, Tuple[Tensor, Tensor]],
+    num_classes: int,
+    average: Optional[Literal["macro", "weighted", "none"]] = "macro",
+    thresholds: Optional[Tensor] = None,
 ) -> Tensor:
-    """Computes precision from the stat scores: true positives, false positives, true negatives, false negatives.
-
-    Args:
-        tp: True positives
-        fp: False positives
-        fn: False negatives
-        average: Defines the reduction that is applied
-        mdmc_average: Defines how averaging is done for multi-dimensional multi-class inputs (on top of the
-            ``average`` parameter)
-
-    Example:
-        >>> from torchmetrics.functional.classification.stat_scores import _stat_scores_update
-        >>> preds  = torch.tensor([2, 0, 2, 1])
-        >>> target = torch.tensor([1, 1, 2, 0])
-        >>> tp, fp, tn, fn = _stat_scores_update(preds, target, reduce='macro', num_classes=3)
-        >>> _recall_compute(tp, fp, fn, average='macro', mdmc_average=None)
-        tensor(0.3333)
-        >>> tp, fp, tn, fn = _stat_scores_update(preds, target, reduce='micro')
-        >>> _recall_compute(tp, fp, fn, average='micro', mdmc_average=None)
-        tensor(0.2500)
-    """
-    numerator = tp.clone()
-    denominator = tp + fn
-
-    if average == AverageMethod.MACRO and mdmc_average != MDMCAverageMethod.SAMPLEWISE:
-        cond = tp + fp + fn == 0
-        numerator = numerator[~cond]
-        denominator = denominator[~cond]
-
-    if average == AverageMethod.NONE and mdmc_average != MDMCAverageMethod.SAMPLEWISE:
-        # a class is not present if there exists no TPs, no FPs, and no FNs
-        meaningless_indeces = ((tp | fn | fp) == 0).nonzero().cpu()
-        numerator[meaningless_indeces, ...] = -1
-        denominator[meaningless_indeces, ...] = -1
-
-    return _reduce_stat_scores(
-        numerator=numerator,
-        denominator=denominator,
-        weights=None if average != AverageMethod.WEIGHTED else tp + fn,
-        average=average,
-        mdmc_average=mdmc_average,
+    fpr, tpr, _ = _multiclass_roc_compute(state, num_classes, thresholds)
+    return _reduce_auroc(
+        fpr,
+        tpr,
+        average,
+        weights=_bincount(state[1], minlength=num_classes).float() if thresholds is None else state[0][:, 1, :].sum(-1),
     )
 
 
-def recall(
+def multiclass_auroc(
     preds: Tensor,
     target: Tensor,
-    average: Optional[str] = "micro",
-    mdmc_average: Optional[str] = None,
+    num_classes: int,
+    average: Optional[Literal["macro", "weighted", "none"]] = "macro",
+    thresholds: Optional[Union[int, List[float], Tensor]] = None,
     ignore_index: Optional[int] = None,
-    num_classes: Optional[int] = None,
-    threshold: float = 0.5,
-    top_k: Optional[int] = None,
-    multiclass: Optional[bool] = None,
+    validate_args: bool = True,
 ) -> Tensor:
-    r"""Computes `Recall`_
-
-    .. math:: \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
+    r"""Compute Area Under the Receiver Operating Characteristic Curve (`ROC AUC`_) for multiclass tasks.
 
-    Where :math:`\text{TP}` and :math:`\text{FN}` represent the number of true positives and
-    false negatives respecitively. With the use of ``top_k`` parameter, this metric can
-    generalize to Recall@K.
-
-    The reduction method (how the recall scores are aggregated) is controlled by the
-    ``average`` parameter, and additionally by the ``mdmc_average`` parameter in the
-    multi-dimensional multi-class case. Accepts all inputs listed in :ref:`pages/classification:input types`.
+    The AUROC score summarizes the ROC curve into an single number that describes the performance of a model for
+    multiple thresholds at the same time. Notably, an AUROC score of 1 is a perfect score and an AUROC score of 0.5
+    corresponds to random guessing.
+
+    Accepts the following input tensors:
+
+    - ``preds`` (float tensor): ``(N, C, ...)``. Preds should be a tensor containing probabilities or logits for each
+      observation. If preds has values outside [0,1] range we consider the input to be logits and will auto apply
+      softmax per sample.
+    - ``target`` (int tensor): ``(N, ...)``. Target should be a tensor containing ground truth labels, and therefore
+      only contain values in the [0, n_classes-1] range (except if `ignore_index` is specified).
+
+    Additional dimension ``...`` will be flattened into the batch dimension.
+
+    The implementation both supports calculating the metric in a non-binned but accurate version and a binned version
+    that is less accurate but more memory efficient. Setting the `thresholds` argument to `None` will activate the
+    non-binned  version that uses memory of size :math:`\mathcal{O}(n_{samples})` whereas setting the `thresholds`
+    argument to either an integer, list or a 1d tensor will use a binned version that uses memory of
+    size :math:`\mathcal{O}(n_{thresholds} \times n_{classes})` (constant memory).
 
     Args:
-        preds: Predictions from model (probabilities, logits or labels)
-        target: Ground truth values
+        preds: Tensor with predictions
+        target: Tensor with true labels
+        num_classes: Integer specifing the number of classes
         average:
-            Defines the reduction that is applied. Should be one of the following:
+            Defines the reduction that is applied over classes. Should be one of the following:
 
-            - ``'micro'`` [default]: Calculate the metric globally, across all samples and classes.
-            - ``'macro'``: Calculate the metric for each class separately, and average the
-              metrics across classes (with equal weights for each class).
-            - ``'weighted'``: Calculate the metric for each class separately, and average the
-              metrics across classes, weighting each class by its support (``tp + fn``).
-            - ``'none'`` or ``None``: Calculate the metric for each class separately, and return
-              the metric for every class.
-            - ``'samples'``: Calculate the metric for each sample, and average the metrics
-              across samples (with equal weights for each sample).
-
-            .. note:: What is considered a sample in the multi-dimensional multi-class case
-                depends on the value of ``mdmc_average``.
-
-            .. note:: If ``'none'`` and a given class doesn't occur in the ``preds`` or ``target``,
-                the value for the class will be ``nan``.
-
-        mdmc_average:
-            Defines how averaging is done for multi-dimensional multi-class inputs (on top of the
-            ``average`` parameter). Should be one of the following:
-
-            - ``None`` [default]: Should be left unchanged if your data is not multi-dimensional
-              multi-class.
-
-            - ``'samplewise'``: In this case, the statistics are computed separately for each
-              sample on the ``N`` axis, and then averaged over samples.
-              The computation for each sample is done by treating the flattened extra axes ``...``
-              (see :ref:`pages/classification:input types`) as the ``N`` dimension within the sample,
-              and computing the metric for the sample based on that.
-
-            - ``'global'``: In this case the ``N`` and ``...`` dimensions of the inputs
-              (see :ref:`pages/classification:input types`)
-              are flattened into a new ``N_X`` sample axis, i.e. the inputs are treated as if they
-              were ``(N_X, C)``. From here on the ``average`` parameter applies as usual.
+            - ``macro``: Calculate score for each class and average them
+            - ``weighted``: calculates score for each class and computes weighted average using their support
+            - ``"none"`` or ``None``: calculates score for each class and applies no reduction
+        thresholds:
+            Can be one of:
+
+            - If set to `None`, will use a non-binned approach where thresholds are dynamically calculated from
+              all the data. Most accurate but also most memory consuming approach.
+            - If set to an `int` (larger than 1), will use that number of thresholds linearly spaced from
+              0 to 1 as bins for the calculation.
+            - If set to an `list` of floats, will use the indicated thresholds in the list as bins for the calculation
+            - If set to an 1d `tensor` of floats, will use the indicated thresholds in the tensor as
+              bins for the calculation.
 
         ignore_index:
-            Integer specifying a target class to ignore. If given, this class index does not contribute
-            to the returned score, regardless of reduction method. If an index is ignored, and ``average=None``
-            or ``'none'``, the score for the ignored class will be returned as ``nan``.
-
-        num_classes:
-            Number of classes. Necessary for ``'macro'``, ``'weighted'`` and ``None`` average methods.
-
-        threshold:
-            Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case
-            of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.
-        top_k:
-            Number of the highest probability or logit score predictions considered finding the correct label,
-            relevant only for (multi-dimensional) multi-class inputs. The
-            default value (``None``) will be interpreted as 1 for these inputs.
-
-            Should be left at default (``None``) for all other types of inputs.
-        multiclass:
-            Used only in certain special cases, where you want to treat inputs as a different type
-            than what they appear to be. See the parameter's
-            :ref:`documentation section <pages/classification:using the multiclass parameter>`
-            for a more detailed explanation and examples.
-
-    Return:
-        The shape of the returned tensor depends on the ``average`` parameter
-
-        - If ``average in ['micro', 'macro', 'weighted', 'samples']``, a one-element tensor will be returned
-        - If ``average in ['none', None]``, the shape will be ``(C,)``, where ``C`` stands  for the number of classes
-
-    Raises:
-        ValueError:
-            If ``average`` is not one of ``"micro"``, ``"macro"``, ``"weighted"``, ``"samples"``, ``"none"`` or ``None``
-        ValueError:
-            If ``mdmc_average`` is not one of ``None``, ``"samplewise"``, ``"global"``.
-        ValueError:
-            If ``average`` is set but ``num_classes`` is not provided.
-        ValueError:
-            If ``num_classes`` is set and ``ignore_index`` is not in the range ``[0, num_classes)``.
+            Specifies a target value that is ignored and does not contribute to the metric calculation
+        validate_args: bool indicating if input arguments and tensors should be validated for correctness.
+            Set to ``False`` for faster computations.
+
+    Returns:
+        If `average=None|"none"` then a 1d tensor of shape (n_classes, ) will be returned with auroc score per class.
+        If `average="macro"|"weighted"` then a single scalar is returned.
 
     Example:
-        >>> from torchmetrics.functional import recall
-        >>> preds  = torch.tensor([2, 0, 2, 1])
-        >>> target = torch.tensor([1, 1, 2, 0])
-        >>> recall(preds, target, average='macro', num_classes=3)
-        tensor(0.3333)
-        >>> recall(preds, target, average='micro')
-        tensor(0.2500)
-
+        >>> from torchmetrics.functional.classification import multiclass_auroc
+        >>> preds = torch.tensor([[0.75, 0.05, 0.05, 0.05, 0.05],
+        ...                       [0.05, 0.75, 0.05, 0.05, 0.05],
+        ...                       [0.05, 0.05, 0.75, 0.05, 0.05],
+        ...                       [0.05, 0.05, 0.05, 0.75, 0.05]])
+        >>> target = torch.tensor([0, 1, 3, 2])
+        >>> multiclass_auroc(preds, target, num_classes=5, average="macro", thresholds=None)
+        tensor(0.5333)
+        >>> multiclass_auroc(preds, target, num_classes=5, average=None, thresholds=None)
+        tensor([1.0000, 1.0000, 0.3333, 0.3333, 0.0000])
+        >>> multiclass_auroc(preds, target, num_classes=5, average="macro", thresholds=5)
+        tensor(0.5333)
+        >>> multiclass_auroc(preds, target, num_classes=5, average=None, thresholds=5)
+        tensor([1.0000, 1.0000, 0.3333, 0.3333, 0.0000])
     """
-    allowed_average = ("micro", "macro", "weighted", "samples", "none", None)
+    if validate_args:
+        _multiclass_auroc_arg_validation(num_classes, average, thresholds, ignore_index)
+        _multiclass_precision_recall_curve_tensor_validation(preds, target, num_classes, ignore_index)
+    preds, target, thresholds = _multiclass_precision_recall_curve_format(
+        preds, target, num_classes, thresholds, ignore_index
+    )
+    state = _multiclass_precision_recall_curve_update(preds, target, num_classes, thresholds)
+    return _multiclass_auroc_compute(state, num_classes, average, thresholds)
+
+
+def _multilabel_auroc_arg_validation(
+    num_labels: int,
+    average: Optional[Literal["micro", "macro", "weighted", "none"]],
+    thresholds: Optional[Union[int, List[float], Tensor]] = None,
+    ignore_index: Optional[int] = None,
+) -> None:
+    _multilabel_precision_recall_curve_arg_validation(num_labels, thresholds, ignore_index)
+    allowed_average = ("micro", "macro", "weighted", "none", None)
     if average not in allowed_average:
-        raise ValueError(f"The `average` has to be one of {allowed_average}, got {average}.")
+        raise ValueError(f"Expected argument `average` to be one of {allowed_average} but got {average}")
 
-    allowed_mdmc_average = (None, "samplewise", "global")
-    if mdmc_average not in allowed_mdmc_average:
-        raise ValueError(f"The `mdmc_average` has to be one of {allowed_mdmc_average}, got {mdmc_average}.")
-
-    if average in ["macro", "weighted", "none", None] and (not num_classes or num_classes < 1):
-        raise ValueError(f"When you set `average` as {average}, you have to provide the number of classes.")
-
-    if num_classes and ignore_index is not None and (not 0 <= ignore_index < num_classes or num_classes == 1):
-        raise ValueError(f"The `ignore_index` {ignore_index} is not valid for inputs with {num_classes} classes")
-
-    reduce = "macro" if average in ["weighted", "none", None] else average
-    tp, fp, _, fn = _stat_scores_update(
-        preds,
-        target,
-        reduce=reduce,
-        mdmc_reduce=mdmc_average,
-        threshold=threshold,
-        num_classes=num_classes,
-        top_k=top_k,
-        multiclass=multiclass,
-        ignore_index=ignore_index,
-    )
 
-    return _recall_compute(tp, fp, fn, average, mdmc_average)
+def _multilabel_auroc_compute(
+    state: Union[Tensor, Tuple[Tensor, Tensor]],
+    num_labels: int,
+    average: Optional[Literal["micro", "macro", "weighted", "none"]],
+    thresholds: Optional[Tensor],
+    ignore_index: Optional[int] = None,
+) -> Tensor:
+    if average == "micro":
+        if isinstance(state, Tensor) and thresholds is not None:
+            return _binary_auroc_compute(state.sum(1), thresholds, max_fpr=None)
+
+        preds = state[0].flatten()
+        target = state[1].flatten()
+        if ignore_index is not None:
+            idx = target == ignore_index
+            preds = preds[~idx]
+            target = target[~idx]
+        return _binary_auroc_compute((preds, target), thresholds, max_fpr=None)
+
+    fpr, tpr, _ = _multilabel_roc_compute(state, num_labels, thresholds, ignore_index)
+    return _reduce_auroc(
+        fpr,
+        tpr,
+        average,
+        weights=(state[1] == 1).sum(dim=0).float() if thresholds is None else state[0][:, 1, :].sum(-1),
+    )
 
 
-def precision_recall(
+def multilabel_auroc(
     preds: Tensor,
     target: Tensor,
-    average: Optional[str] = "micro",
-    mdmc_average: Optional[str] = None,
+    num_labels: int,
+    average: Optional[Literal["micro", "macro", "weighted", "none"]] = "macro",
+    thresholds: Optional[Union[int, List[float], Tensor]] = None,
     ignore_index: Optional[int] = None,
-    num_classes: Optional[int] = None,
-    threshold: float = 0.5,
-    top_k: Optional[int] = None,
-    multiclass: Optional[bool] = None,
-) -> Tuple[Tensor, Tensor]:
-    r"""Computes `Precision`_
-
-    .. math:: \text{Precision} = \frac{\text{TP}}{\text{TP} + \text{FP}}
-
-    .. math:: \text{Recall} = \frac{\text{TP}}{\text{TP} + \text{FN}}
-
-    Where :math:`\text{TP}`m :math:`\text{FN}` and :math:`\text{FP}` represent the number
-    of true positives, false negatives and false positives respecitively. With the use of
-    ``top_k`` parameter, this metric can generalize to Recall@K and Precision@K.
-
-    The reduction method (how the recall scores are aggregated) is controlled by the
-    ``average`` parameter, and additionally by the ``mdmc_average`` parameter in the
-    multi-dimensional multi-class case. Accepts all inputs listed in :ref:`pages/classification:input types`.
+    validate_args: bool = True,
+) -> Tensor:
+    r"""Compute Area Under the Receiver Operating Characteristic Curve (`ROC AUC`_) for multilabel tasks.
+
+    The AUROC score summarizes the ROC curve into an single number that describes the performance of a model for
+    multiple thresholds at the same time. Notably, an AUROC score of 1 is a perfect score and an AUROC score of 0.5
+    corresponds to random guessing.
+
+    Accepts the following input tensors:
+
+    - ``preds`` (float tensor): ``(N, C, ...)``. Preds should be a tensor containing probabilities or logits for each
+      observation. If preds has values outside [0,1] range we consider the input to be logits and will auto apply
+      sigmoid per element.
+    - ``target`` (int tensor): ``(N, C, ...)``. Target should be a tensor containing ground truth labels, and therefore
+      only contain {0,1} values (except if `ignore_index` is specified).
+
+    Additional dimension ``...`` will be flattened into the batch dimension.
+
+    The implementation both supports calculating the metric in a non-binned but accurate version and a binned version
+    that is less accurate but more memory efficient. Setting the `thresholds` argument to `None` will activate the
+    non-binned  version that uses memory of size :math:`\mathcal{O}(n_{samples})` whereas setting the `thresholds`
+    argument to either an integer, list or a 1d tensor will use a binned version that uses memory of
+    size :math:`\mathcal{O}(n_{thresholds} \times n_{labels})` (constant memory).
 
     Args:
-        preds: Predictions from model (probabilities, logits or labels)
-        target: Ground truth values
+        preds: Tensor with predictions
+        target: Tensor with true labels
+        num_labels: Integer specifing the number of labels
         average:
-            Defines the reduction that is applied. Should be one of the following:
+            Defines the reduction that is applied over labels. Should be one of the following:
 
-            - ``'micro'`` [default]: Calculate the metric globally, across all samples and classes.
-            - ``'macro'``: Calculate the metric for each class separately, and average the
-              metrics across classes (with equal weights for each class).
-            - ``'weighted'``: Calculate the metric for each class separately, and average the
-              metrics across classes, weighting each class by its support (``tp + fn``).
-            - ``'none'`` or ``None``: Calculate the metric for each class separately, and return
-              the metric for every class.
-            - ``'samples'``: Calculate the metric for each sample, and average the metrics
-              across samples (with equal weights for each sample).
-
-            .. note:: What is considered a sample in the multi-dimensional multi-class case
-                depends on the value of ``mdmc_average``.
-
-            .. note:: If ``'none'`` and a given class doesn't occur in the ``preds`` or ``target``,
-                the value for the class will be ``nan``.
-
-        mdmc_average:
-            Defines how averaging is done for multi-dimensional multi-class inputs (on top of the
-            ``average`` parameter). Should be one of the following:
-
-            - ``None`` [default]: Should be left unchanged if your data is not multi-dimensional multi-class.
-
-            - ``'samplewise'``: In this case, the statistics are computed separately for each
-              sample on the ``N`` axis, and then averaged over samples.
-              The computation for each sample is done by treating the flattened extra axes ``...``
-              (see :ref:`pages/classification:input types`) as the ``N`` dimension within the sample,
-              and computing the metric for the sample based on that.
-
-            - ``'global'``: In this case the ``N`` and ``...`` dimensions of the inputs
-              (see :ref:`pages/classification:input types`)
-              are flattened into a new ``N_X`` sample axis, i.e. the inputs are treated as if they
-              were ``(N_X, C)``. From here on the ``average`` parameter applies as usual.
+            - ``micro``: Sum score over all labels
+            - ``macro``: Calculate score for each label and average them
+            - ``weighted``: calculates score for each label and computes weighted average using their support
+            - ``"none"`` or ``None``: calculates score for each label and applies no reduction
+        thresholds:
+            Can be one of:
+
+            - If set to `None`, will use a non-binned approach where thresholds are dynamically calculated from
+              all the data. Most accurate but also most memory consuming approach.
+            - If set to an `int` (larger than 1), will use that number of thresholds linearly spaced from
+              0 to 1 as bins for the calculation.
+            - If set to an `list` of floats, will use the indicated thresholds in the list as bins for the calculation
+            - If set to an 1d `tensor` of floats, will use the indicated thresholds in the tensor as
+              bins for the calculation.
 
         ignore_index:
-            Integer specifying a target class to ignore. If given, this class index does not contribute
-            to the returned score, regardless of reduction method. If an index is ignored, and ``average=None``
-            or ``'none'``, the score for the ignored class will be returned as ``nan``.
-        num_classes:
-            Number of classes. Necessary for ``'macro'``, ``'weighted'`` and ``None`` average methods.
-        threshold:
-            Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case
-            of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.
-        top_k:
-            Number of highest probability or logit score predictions considered to find the correct label,
-            relevant only for (multi-dimensional) multi-class inputs. The
-            default value (``None``) will be interpreted as 1 for these inputs.
-
-            Should be left at default (``None``) for all other types of inputs.
-        multiclass:
-            Used only in certain special cases, where you want to treat inputs as a different type
-            than what they appear to be. See the parameter's
-            :ref:`documentation section <pages/classification:using the multiclass parameter>`
-            for a more detailed explanation and examples.
-
-    Return:
-        The function returns a tuple with two elements: precision and recall. Their shape
-        depends on the ``average`` parameter
-
-        - If ``average in ['micro', 'macro', 'weighted', 'samples']``, they are a single element tensor
-        - If ``average in ['none', None]``, they are a tensor of shape ``(C, )``, where ``C`` stands for
-          the number of classes
-
-    Raises:
-        ValueError:
-            If ``average`` is not one of ``"micro"``, ``"macro"``, ``"weighted"``, ``"samples"``, ``"none"`` or ``None``
-        ValueError:
-            If ``mdmc_average`` is not one of ``None``, ``"samplewise"``, ``"global"``.
-        ValueError:
-            If ``average`` is set but ``num_classes`` is not provided.
-        ValueError:
-            If ``num_classes`` is set and ``ignore_index`` is not in the range ``[0, num_classes)``.
+            Specifies a target value that is ignored and does not contribute to the metric calculation
+        validate_args: bool indicating if input arguments and tensors should be validated for correctness.
+            Set to ``False`` for faster computations.
+
+    Returns:
+        If `average=None|"none"` then a 1d tensor of shape (n_classes, ) will be returned with auroc score per class.
+        If `average="micro|macro"|"weighted"` then a single scalar is returned.
 
     Example:
-        >>> from torchmetrics.functional import precision_recall
-        >>> preds  = torch.tensor([2, 0, 2, 1])
-        >>> target = torch.tensor([1, 1, 2, 0])
-        >>> precision_recall(preds, target, average='macro', num_classes=3)
-        (tensor(0.1667), tensor(0.3333))
-        >>> precision_recall(preds, target, average='micro')
-        (tensor(0.2500), tensor(0.2500))
-
+        >>> from torchmetrics.functional.classification import multilabel_auroc
+        >>> preds = torch.tensor([[0.75, 0.05, 0.35],
+        ...                       [0.45, 0.75, 0.05],
+        ...                       [0.05, 0.55, 0.75],
+        ...                       [0.05, 0.65, 0.05]])
+        >>> target = torch.tensor([[1, 0, 1],
+        ...                        [0, 0, 0],
+        ...                        [0, 1, 1],
+        ...                        [1, 1, 1]])
+        >>> multilabel_auroc(preds, target, num_labels=3, average="macro", thresholds=None)
+        tensor(0.6528)
+        >>> multilabel_auroc(preds, target, num_labels=3, average=None, thresholds=None)
+        tensor([0.6250, 0.5000, 0.8333])
+        >>> multilabel_auroc(preds, target, num_labels=3, average="macro", thresholds=5)
+        tensor(0.6528)
+        >>> multilabel_auroc(preds, target, num_labels=3, average=None, thresholds=5)
+        tensor([0.6250, 0.5000, 0.8333])
     """
-    allowed_average = ("micro", "macro", "weighted", "samples", "none", None)
-    if average not in allowed_average:
-        raise ValueError(f"The `average` has to be one of {allowed_average}, got {average}.")
-
-    allowed_mdmc_average = (None, "samplewise", "global")
-    if mdmc_average not in allowed_mdmc_average:
-        raise ValueError(f"The `mdmc_average` has to be one of {allowed_mdmc_average}, got {mdmc_average}.")
-
-    if average in ["macro", "weighted", "none", None] and (not num_classes or num_classes < 1):
-        raise ValueError(f"When you set `average` as {average}, you have to provide the number of classes.")
-
-    if num_classes and ignore_index is not None and (not 0 <= ignore_index < num_classes or num_classes == 1):
-        raise ValueError(f"The `ignore_index` {ignore_index} is not valid for inputs with {num_classes} classes")
-
-    reduce = "macro" if average in ["weighted", "none", None] else average
-    tp, fp, _, fn = _stat_scores_update(
-        preds,
-        target,
-        reduce=reduce,
-        mdmc_reduce=mdmc_average,
-        threshold=threshold,
-        num_classes=num_classes,
-        top_k=top_k,
-        multiclass=multiclass,
-        ignore_index=ignore_index,
+    if validate_args:
+        _multilabel_auroc_arg_validation(num_labels, average, thresholds, ignore_index)
+        _multilabel_precision_recall_curve_tensor_validation(preds, target, num_labels, ignore_index)
+    preds, target, thresholds = _multilabel_precision_recall_curve_format(
+        preds, target, num_labels, thresholds, ignore_index
     )
+    state = _multilabel_precision_recall_curve_update(preds, target, num_labels, thresholds)
+    return _multilabel_auroc_compute(state, num_labels, average, thresholds, ignore_index)
 
-    precision_ = _precision_compute(tp, fp, fn, average, mdmc_average)
-    recall_ = _recall_compute(tp, fp, fn, average, mdmc_average)
 
-    return precision_, recall_
+def auroc(
+    preds: Tensor,
+    target: Tensor,
+    task: Literal["binary", "multiclass", "multilabel"],
+    thresholds: Optional[Union[int, List[float], Tensor]] = None,
+    num_classes: Optional[int] = None,
+    num_labels: Optional[int] = None,
+    average: Optional[Literal["macro", "weighted", "none"]] = "macro",
+    max_fpr: Optional[float] = None,
+    ignore_index: Optional[int] = None,
+    validate_args: bool = True,
+) -> Optional[Tensor]:
+    r"""Compute Area Under the Receiver Operating Characteristic Curve (`ROC AUC`_).
+
+    The AUROC score summarizes the ROC curve into an single number that describes the performance of a model for
+    multiple thresholds at the same time. Notably, an AUROC score of 1 is a perfect score and an AUROC score of 0.5
+    corresponds to random guessing.
+
+    This function is a simple wrapper to get the task specific versions of this metric, which is done by setting the
+    ``task`` argument to either ``'binary'``, ``'multiclass'`` or ``multilabel``. See the documentation of
+    :func:`binary_auroc`, :func:`multiclass_auroc` and :func:`multilabel_auroc` for the specific details of
+    each argument influence and examples.
+
+    Legacy Example:
+        >>> preds = torch.tensor([0.13, 0.26, 0.08, 0.19, 0.34])
+        >>> target = torch.tensor([0, 0, 1, 1, 1])
+        >>> auroc(preds, target, task='binary')
+        tensor(0.5000)
+
+        >>> preds = torch.tensor([[0.90, 0.05, 0.05],
+        ...                       [0.05, 0.90, 0.05],
+        ...                       [0.05, 0.05, 0.90],
+        ...                       [0.85, 0.05, 0.10],
+        ...                       [0.10, 0.10, 0.80]])
+        >>> target = torch.tensor([0, 1, 1, 2, 2])
+        >>> auroc(preds, target, task='multiclass', num_classes=3)
+        tensor(0.7778)
+    """
+    task = ClassificationTask.from_str(task)
+    if task == ClassificationTask.BINARY:
+        return binary_auroc(preds, target, max_fpr, thresholds, ignore_index, validate_args)
+    if task == ClassificationTask.MULTICLASS:
+        if not isinstance(num_classes, int):
+            raise ValueError(f"`num_classes` is expected to be `int` but `{type(num_classes)} was passed.`")
+        return multiclass_auroc(preds, target, num_classes, average, thresholds, ignore_index, validate_args)
+    if task == ClassificationTask.MULTILABEL:
+        if not isinstance(num_labels, int):
+            raise ValueError(f"`num_labels` is expected to be `int` but `{type(num_labels)} was passed.`")
+        return multilabel_auroc(preds, target, num_labels, average, thresholds, ignore_index, validate_args)
+    return None
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/classification/precision_recall_curve.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/kendall.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,331 +1,407 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import List, Optional, Sequence, Tuple, Union
+from typing import List, Optional, Tuple, Union
 
 import torch
-from torch import Tensor, tensor
-from torch.nn import functional as F
+from torch import Tensor
+from typing_extensions import Literal
 
-from torchmetrics.utilities import rank_zero_warn
+from torchmetrics.functional.regression.utils import _check_data_shape_to_num_outputs
+from torchmetrics.utilities.checks import _check_same_shape
+from torchmetrics.utilities.data import _bincount, _cumsum, dim_zero_cat
+from torchmetrics.utilities.enums import EnumStr
 
 
-def _binary_clf_curve(
-    preds: Tensor,
-    target: Tensor,
-    sample_weights: Optional[Sequence] = None,
-    pos_label: int = 1,
-) -> Tuple[Tensor, Tensor, Tensor]:
-    """adapted from https://github.com/scikit-learn/scikit- learn/blob/master/sklearn/metrics/_ranking.py."""
-    if sample_weights is not None and not isinstance(sample_weights, Tensor):
-        sample_weights = tensor(sample_weights, device=preds.device, dtype=torch.float)
-
-    # remove class dimension if necessary
-    if preds.ndim > target.ndim:
-        preds = preds[:, 0]
-    desc_score_indices = torch.argsort(preds, descending=True)
+class _MetricVariant(EnumStr):
+    """Enumerate for metric variants."""
 
-    preds = preds[desc_score_indices]
-    target = target[desc_score_indices]
+    A = "a"
+    B = "b"
+    C = "c"
 
-    if sample_weights is not None:
-        weight = sample_weights[desc_score_indices]
-    else:
-        weight = 1.0
+    @staticmethod
+    def _name() -> str:
+        return "variant"
 
-    # pred typically has many tied values. Here we extract
-    # the indices associated with the distinct values. We also
-    # concatenate a value for the end of the curve.
-    distinct_value_indices = torch.where(preds[1:] - preds[:-1])[0]
-    threshold_idxs = F.pad(distinct_value_indices, [0, 1], value=target.size(0) - 1)
-    target = (target == pos_label).to(torch.long)
-    tps = torch.cumsum(target * weight, dim=0)[threshold_idxs]
-
-    if sample_weights is not None:
-        # express fps as a cumsum to ensure fps is increasing even in
-        # the presence of floating point errors
-        fps = torch.cumsum((1 - target) * weight, dim=0)[threshold_idxs]
-    else:
-        fps = 1 + threshold_idxs - tps
 
-    return fps, tps, preds[threshold_idxs]
+class _TestAlternative(EnumStr):
+    """Enumerate for test alternative options."""
 
+    TWO_SIDED = "two-sided"
+    LESS = "less"
+    GREATER = "greater"
 
-def _precision_recall_curve_update(
-    preds: Tensor,
-    target: Tensor,
-    num_classes: Optional[int] = None,
-    pos_label: Optional[int] = None,
-) -> Tuple[Tensor, Tensor, int, Optional[int]]:
-    """Updates and returns variables required to compute the precision-recall pairs for different thresholds.
+    @staticmethod
+    def _name() -> str:
+        return "alternative"
 
-    Args:
-        preds: Predicted tensor
-        target: Ground truth tensor
-        num_classes: integer with number of classes for multi-label and multiclass problems.
-            Should be set to ``None`` for binary problems.
-        pos_label: integer determining the positive class. Default is ``None``
-            which for binary problem is translated to 1. For multiclass problems
-            this argument should not be set as we iteratively change it in the
-            range [0,num_classes-1]
-    """
 
-    if len(preds.shape) == len(target.shape):
-        if pos_label is None:
-            pos_label = 1
-        if num_classes is not None and num_classes != 1:
-            # multilabel problem
-            if num_classes != preds.shape[1]:
-                raise ValueError(
-                    f"Argument `num_classes` was set to {num_classes} in"
-                    f" metric `precision_recall_curve` but detected {preds.shape[1]}"
-                    " number of classes from predictions"
-                )
-            preds = preds.transpose(0, 1).reshape(num_classes, -1).transpose(0, 1)
-            target = target.transpose(0, 1).reshape(num_classes, -1).transpose(0, 1)
-        else:
-            # binary problem
-            preds = preds.flatten()
-            target = target.flatten()
-            num_classes = 1
-
-    # multi class problem
-    elif len(preds.shape) == len(target.shape) + 1:
-        if pos_label is not None:
-            rank_zero_warn(
-                "Argument `pos_label` should be `None` when running"
-                f" multiclass precision recall curve. Got {pos_label}"
-            )
-        if num_classes != preds.shape[1]:
-            raise ValueError(
-                f"Argument `num_classes` was set to {num_classes} in"
-                f" metric `precision_recall_curve` but detected {preds.shape[1]}"
-                " number of classes from predictions"
-            )
-        preds = preds.transpose(0, 1).reshape(num_classes, -1).transpose(0, 1)
-        target = target.flatten()
+def _sort_on_first_sequence(x: Tensor, y: Tensor) -> Tuple[Tensor, Tensor]:
+    """Sort sequences in an ascent order according to the sequence ``x``."""
+    # We need to clone `y` tensor not to change an object in memory
+    y = torch.clone(y)
+    x, y = x.T, y.T
+    x, perm = x.sort()
+    for i in range(x.shape[0]):
+        y[i] = y[i][perm[i]]
+    return x.T, y.T
 
-    else:
-        raise ValueError("preds and target must have same number of dimensions, or one additional dimension for preds")
 
-    return preds, target, num_classes, pos_label
+def _concordant_element_sum(x: Tensor, y: Tensor, i: int) -> Tensor:
+    """Count a total number of concordant pairs in a single sequence."""
+    return torch.logical_and(x[i] < x[(i + 1) :], y[i] < y[(i + 1) :]).sum(0).unsqueeze(0)
 
 
-def _precision_recall_curve_compute_single_class(
-    preds: Tensor,
-    target: Tensor,
-    pos_label: int,
-    sample_weights: Optional[Sequence] = None,
-) -> Tuple[Tensor, Tensor, Tensor]:
-    """Computes precision-recall pairs for single class inputs.
+def _count_concordant_pairs(preds: Tensor, target: Tensor) -> Tensor:
+    """Count a total number of concordant pairs in given sequences."""
+    return torch.cat([_concordant_element_sum(preds, target, i) for i in range(preds.shape[0])]).sum(0)
 
-    Args:
-        preds: Predicted tensor
-        target: Ground truth tensor
-        pos_label: integer determining the positive class.
-        sample_weights: sample weights for each data point
-    """
 
-    fps, tps, thresholds = _binary_clf_curve(
-        preds=preds, target=target, sample_weights=sample_weights, pos_label=pos_label
+def _discordant_element_sum(x: Tensor, y: Tensor, i: int) -> Tensor:
+    """Count a total number of discordant pairs in a single sequences."""
+    return (
+        torch.logical_or(
+            torch.logical_and(x[i] > x[(i + 1) :], y[i] < y[(i + 1) :]),
+            torch.logical_and(x[i] < x[(i + 1) :], y[i] > y[(i + 1) :]),
+        )
+        .sum(0)
+        .unsqueeze(0)
     )
-    precision = tps / (tps + fps)
-    recall = tps / tps[-1]
 
-    # stop when full recall attained and reverse the outputs so recall is decreasing
-    last_ind = torch.where(tps == tps[-1])[0][0]
-    sl = slice(0, last_ind.item() + 1)
 
-    # need to call reversed explicitly, since including that to slice would
-    # introduce negative strides that are not yet supported in pytorch
-    precision = torch.cat([reversed(precision[sl]), torch.ones(1, dtype=precision.dtype, device=precision.device)])
+def _count_discordant_pairs(preds: Tensor, target: Tensor) -> Tensor:
+    """Count a total number of discordant pairs in given sequences."""
+    return torch.cat([_discordant_element_sum(preds, target, i) for i in range(preds.shape[0])]).sum(0)
+
+
+def _convert_sequence_to_dense_rank(x: Tensor, sort: bool = False) -> Tensor:
+    """Convert a sequence to the rank tensor."""
+    # Sort if a sequence has not been sorted before
+    if sort:
+        x = x.sort(dim=0).values
+    _ones = torch.zeros(1, x.shape[1], dtype=torch.int32, device=x.device)
+    return _cumsum(torch.cat([_ones, (x[1:] != x[:-1]).int()], dim=0), dim=0)
+
+
+def _get_ties(x: Tensor) -> Tuple[Tensor, Tensor, Tensor]:
+    """Get a total number of ties and staistics for p-value calculation for  a given sequence."""
+    ties = torch.zeros(x.shape[1], dtype=x.dtype, device=x.device)
+    ties_p1 = torch.zeros(x.shape[1], dtype=x.dtype, device=x.device)
+    ties_p2 = torch.zeros(x.shape[1], dtype=x.dtype, device=x.device)
+    for dim in range(x.shape[1]):
+        n_ties = _bincount(x[:, dim])
+        n_ties = n_ties[n_ties > 1]
+        ties[dim] = (n_ties * (n_ties - 1) // 2).sum()
+        ties_p1[dim] = (n_ties * (n_ties - 1.0) * (n_ties - 2)).sum()
+        ties_p2[dim] = (n_ties * (n_ties - 1.0) * (2 * n_ties + 5)).sum()
+
+    return ties, ties_p1, ties_p2
+
+
+def _get_metric_metadata(
+    preds: Tensor, target: Tensor, variant: _MetricVariant
+) -> Tuple[
+    Tensor,
+    Tensor,
+    Optional[Tensor],
+    Optional[Tensor],
+    Optional[Tensor],
+    Optional[Tensor],
+    Optional[Tensor],
+    Optional[Tensor],
+    Tensor,
+]:
+    """Obtain statistics to calculate metric value."""
+    preds, target = _sort_on_first_sequence(preds, target)
+
+    concordant_pairs = _count_concordant_pairs(preds, target)
+    discordant_pairs = _count_discordant_pairs(preds, target)
+
+    n_total = torch.tensor(preds.shape[0], device=preds.device)
+    preds_ties = target_ties = None
+    preds_ties_p1 = preds_ties_p2 = target_ties_p1 = target_ties_p2 = None
+    if variant != _MetricVariant.A:
+        preds = _convert_sequence_to_dense_rank(preds)
+        target = _convert_sequence_to_dense_rank(target, sort=True)
+        preds_ties, preds_ties_p1, preds_ties_p2 = _get_ties(preds)
+        target_ties, target_ties_p1, target_ties_p2 = _get_ties(target)
+    return (
+        concordant_pairs,
+        discordant_pairs,
+        preds_ties,
+        preds_ties_p1,
+        preds_ties_p2,
+        target_ties,
+        target_ties_p1,
+        target_ties_p2,
+        n_total,
+    )
 
-    recall = torch.cat([reversed(recall[sl]), torch.zeros(1, dtype=recall.dtype, device=recall.device)])
 
-    thresholds = reversed(thresholds[sl]).detach().clone()  # type: ignore
+def _calculate_tau(
+    preds: Tensor,
+    target: Tensor,
+    concordant_pairs: Tensor,
+    discordant_pairs: Tensor,
+    con_min_dis_pairs: Tensor,
+    n_total: Tensor,
+    preds_ties: Optional[Tensor],
+    target_ties: Optional[Tensor],
+    variant: _MetricVariant,
+) -> Tensor:
+    """Calculate Kendall's tau from metric metadata."""
+    if variant == _MetricVariant.A:
+        return con_min_dis_pairs / (concordant_pairs + discordant_pairs)
+    if variant == _MetricVariant.B:
+        total_combinations: Tensor = n_total * (n_total - 1) // 2
+        denominator = (total_combinations - preds_ties) * (total_combinations - target_ties)
+        return con_min_dis_pairs / torch.sqrt(denominator)
+
+    preds_unique = torch.tensor([len(p.unique()) for p in preds.T], dtype=preds.dtype, device=preds.device)
+    target_unique = torch.tensor([len(t.unique()) for t in target.T], dtype=target.dtype, device=target.device)
+    min_classes = torch.minimum(preds_unique, target_unique)
+    return 2 * con_min_dis_pairs / ((min_classes - 1) / min_classes * n_total**2)
 
-    return precision, recall, thresholds
+
+def _get_p_value_for_t_value_from_dist(t_value: Tensor) -> Tensor:
+    """Obtain p-value for a given Tensor of t-values. Handle ``nan`` which cannot be passed into torch distributions.
+
+    When t-value is ``nan``, a resulted p-value should be alson ``nan``.
+    """
+    device = t_value
+    normal_dist = torch.distributions.normal.Normal(torch.tensor([0.0]).to(device), torch.tensor([1.0]).to(device))
+
+    is_nan = t_value.isnan()
+    t_value = t_value.nan_to_num()
+    p_value = normal_dist.cdf(t_value)
+    return p_value.where(~is_nan, torch.tensor(float("nan"), dtype=p_value.dtype, device=p_value.device))
+
+
+def _calculate_p_value(
+    con_min_dis_pairs: Tensor,
+    n_total: Tensor,
+    preds_ties: Optional[Tensor],
+    preds_ties_p1: Optional[Tensor],
+    preds_ties_p2: Optional[Tensor],
+    target_ties: Optional[Tensor],
+    target_ties_p1: Optional[Tensor],
+    target_ties_p2: Optional[Tensor],
+    variant: _MetricVariant,
+    alternative: Optional[_TestAlternative],
+) -> Tensor:
+    """Calculate p-value for Kendall's tau from metric metadata."""
+    t_value_denominator_base = n_total * (n_total - 1) * (2 * n_total + 5)
+    if variant == _MetricVariant.A:
+        t_value = 3 * con_min_dis_pairs / torch.sqrt(t_value_denominator_base / 2)
+    else:
+        m = n_total * (n_total - 1)
+        t_value_denominator: Tensor = (t_value_denominator_base - preds_ties_p2 - target_ties_p2) / 18
+        t_value_denominator += (2 * preds_ties * target_ties) / m  # type: ignore
+        t_value_denominator += preds_ties_p1 * target_ties_p1 / (9 * m * (n_total - 2))  # type: ignore
+        t_value = con_min_dis_pairs / torch.sqrt(t_value_denominator)
+
+    if alternative == _TestAlternative.TWO_SIDED:
+        t_value = torch.abs(t_value)
+    if alternative in [_TestAlternative.TWO_SIDED, _TestAlternative.GREATER]:
+        t_value *= -1
+    p_value = _get_p_value_for_t_value_from_dist(t_value)
+    if alternative == _TestAlternative.TWO_SIDED:
+        p_value *= 2
+    return p_value
 
 
-def _precision_recall_curve_compute_multi_class(
+def _kendall_corrcoef_update(
     preds: Tensor,
     target: Tensor,
-    num_classes: int,
-    sample_weights: Optional[Sequence] = None,
-) -> Tuple[List[Tensor], List[Tensor], List[Tensor]]:
-    """Computes precision-recall pairs for multi class inputs.
+    concat_preds: List[Tensor] = [],
+    concat_target: List[Tensor] = [],
+    num_outputs: int = 1,
+) -> Tuple[List[Tensor], List[Tensor]]:
+    """Update variables required to compute Kendall rank correlation coefficient.
 
     Args:
-        preds: Predicted tensor
-        target: Ground truth tensor
-        num_classes: integer with number of classes for multi-label and multiclass problems.
-            Should be set to ``None`` for binary problems.
-        sample_weights: sample weights for each data point
+        preds: Sequence of data
+        target: Sequence of data
+        concat_preds: List of batches of preds sequence to be concatenated
+        concat_target: List of batches of target sequence to be concatenated
+        num_outputs: Number of outputs in multioutput setting
+
+    Raises:
+        RuntimeError: If ``preds`` and ``target`` do not have the same shape
     """
+    # Data checking
+    _check_same_shape(preds, target)
+    _check_data_shape_to_num_outputs(preds, target, num_outputs)
 
-    # Recursively call per class
-    precision, recall, thresholds = [], [], []
-    for cls in range(num_classes):
-        preds_cls = preds[:, cls]
-
-        prc_args = dict(
-            preds=preds_cls,
-            target=target,
-            num_classes=1,
-            pos_label=cls,
-            sample_weights=sample_weights,
-        )
-        if target.ndim > 1:
-            prc_args.update(
-                dict(
-                    target=target[:, cls],
-                    pos_label=1,
-                )
-            )
-        res = precision_recall_curve(**prc_args)
-        precision.append(res[0])
-        recall.append(res[1])
-        thresholds.append(res[2])
+    if num_outputs == 1:
+        preds = preds.unsqueeze(1)
+        target = target.unsqueeze(1)
 
-    return precision, recall, thresholds
+    concat_preds.append(preds)
+    concat_target.append(target)
 
+    return concat_preds, concat_target
 
-def _precision_recall_curve_compute(
+
+def _kendall_corrcoef_compute(
     preds: Tensor,
     target: Tensor,
-    num_classes: int,
-    pos_label: Optional[int] = None,
-    sample_weights: Optional[Sequence] = None,
-) -> Union[Tuple[Tensor, Tensor, Tensor], Tuple[List[Tensor], List[Tensor], List[Tensor]]]:
-    """Computes precision-recall pairs based on the number of classes.
+    variant: _MetricVariant,
+    alternative: Optional[_TestAlternative] = None,
+) -> Tuple[Tensor, Optional[Tensor]]:
+    """Compute Kendall rank correlation coefficient, and optionally p-value of corresponding statistical test.
 
     Args:
-        preds: Predicted tensor
-        target: Ground truth tensor
-        num_classes: integer with number of classes for multi-label and multiclass problems.
-            Should be set to ``None`` for binary problems.
-        pos_label: integer determining the positive class. Default is ``None``
-            which for binary problem is translated to 1. For multiclass problems
-            this argument should not be set as we iteratively change it in the
-            range ``[0,num_classes-1]``
-        sample_weights: sample weights for each data point
-
-    Example:
-        >>> # binary case
-        >>> preds = torch.tensor([0, 1, 2, 3])
-        >>> target = torch.tensor([0, 1, 1, 0])
-        >>> pos_label = 1
-        >>> preds, target, num_classes, pos_label = _precision_recall_curve_update(preds, target, pos_label=pos_label)
-        >>> precision, recall, thresholds = _precision_recall_curve_compute(preds, target, num_classes, pos_label)
-        >>> precision
-        tensor([0.6667, 0.5000, 0.0000, 1.0000])
-        >>> recall
-        tensor([1.0000, 0.5000, 0.0000, 0.0000])
-        >>> thresholds
-        tensor([1, 2, 3])
-
-        >>> # multiclass case
-        >>> preds = torch.tensor([[0.75, 0.05, 0.05, 0.05, 0.05],
-        ...                      [0.05, 0.75, 0.05, 0.05, 0.05],
-        ...                      [0.05, 0.05, 0.75, 0.05, 0.05],
-        ...                      [0.05, 0.05, 0.05, 0.75, 0.05]])
-        >>> target = torch.tensor([0, 1, 3, 2])
-        >>> num_classes = 5
-        >>> preds, target, num_classes, pos_label = _precision_recall_curve_update(preds, target, num_classes)
-        >>> precision, recall, thresholds = _precision_recall_curve_compute(preds, target, num_classes)
-        >>> precision
-        [tensor([1., 1.]), tensor([1., 1.]), tensor([0.2500, 0.0000, 1.0000]),
-         tensor([0.2500, 0.0000, 1.0000]), tensor([0., 1.])]
-        >>> recall
-        [tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0., 0.]), tensor([1., 0., 0.]), tensor([nan, 0.])]
-        >>> thresholds
-        [tensor([0.7500]), tensor([0.7500]), tensor([0.0500, 0.7500]), tensor([0.0500, 0.7500]), tensor([0.0500])]
+        Args:
+        preds: Sequence of data
+        target: Sequence of data
+        variant: Indication of which variant of Kendall's tau to be used
+        alternative: Alternative hypothesis for for t-test. Possible values:
+            - 'two-sided': the rank correlation is nonzero
+            - 'less': the rank correlation is negative (less than zero)
+            - 'greater':  the rank correlation is positive (greater than zero)
     """
+    (
+        concordant_pairs,
+        discordant_pairs,
+        preds_ties,
+        preds_ties_p1,
+        preds_ties_p2,
+        target_ties,
+        target_ties_p1,
+        target_ties_p2,
+        n_total,
+    ) = _get_metric_metadata(preds, target, variant)
+    con_min_dis_pairs = concordant_pairs - discordant_pairs
 
-    with torch.no_grad():
-        if num_classes == 1:
-            if pos_label is None:
-                pos_label = 1
-            return _precision_recall_curve_compute_single_class(preds, target, pos_label, sample_weights)
-        return _precision_recall_curve_compute_multi_class(preds, target, num_classes, sample_weights)
+    tau = _calculate_tau(
+        preds, target, concordant_pairs, discordant_pairs, con_min_dis_pairs, n_total, preds_ties, target_ties, variant
+    )
+    p_value = (
+        _calculate_p_value(
+            con_min_dis_pairs,
+            n_total,
+            preds_ties,
+            preds_ties_p1,
+            preds_ties_p2,
+            target_ties,
+            target_ties_p1,
+            target_ties_p2,
+            variant,
+            alternative,
+        )
+        if alternative
+        else None
+    )
+
+    # Squeeze tensor if num_outputs=1
+    if tau.shape[0] == 1:
+        tau = tau.squeeze()
+        p_value = p_value.squeeze() if p_value is not None else None
+
+    return tau.clamp(-1, 1), p_value
 
 
-def precision_recall_curve(
+def kendall_rank_corrcoef(
     preds: Tensor,
     target: Tensor,
-    num_classes: Optional[int] = None,
-    pos_label: Optional[int] = None,
-    sample_weights: Optional[Sequence] = None,
-) -> Union[Tuple[Tensor, Tensor, Tensor], Tuple[List[Tensor], List[Tensor], List[Tensor]]]:
-    """Computes precision-recall pairs for different thresholds.
+    variant: Literal["a", "b", "c"] = "b",
+    t_test: bool = False,
+    alternative: Optional[Literal["two-sided", "less", "greater"]] = "two-sided",
+) -> Union[Tensor, Tuple[Tensor, Tensor]]:
+    r"""Compute `Kendall Rank Correlation Coefficient`_.
+
+    .. math::
+        tau_a = \frac{C - D}{C + D}
+
+    where :math:`C` represents concordant pairs, :math:`D` stands for discordant pairs.
+
+    .. math::
+        tau_b = \frac{C - D}{\sqrt{(C + D + T_{preds}) * (C + D + T_{target})}}
+
+    where :math:`C` represents concordant pairs, :math:`D` stands for discordant pairs and :math:`T` represents
+    a total number of ties.
+
+    .. math::
+        tau_c = 2 * \frac{C - D}{n^2 * \frac{m - 1}{m}}
+
+    where :math:`C` represents concordant pairs, :math:`D` stands for discordant pairs, :math:`n` is a total number
+    of observations and :math:`m` is a ``min`` of unique values in ``preds`` and ``target`` sequence.
+
+    Definitions according to Definition according to `The Treatment of Ties in Ranking Problems`_.
 
     Args:
-        preds: predictions from model (probabilities)
-        target: ground truth labels
-        num_classes: integer with number of classes for multi-label and multiclass problems.
-            Should be set to ``None`` for binary problems.
-        pos_label: integer determining the positive class. Default is ``None`` which for binary problem is translated
-            to 1. For multiclass problems this argument should not be set as we iteratively change it in the
-            range ``[0, num_classes-1]``
-        sample_weights: sample weights for each data point
-
-    Returns:
-        3-element tuple containing
-
-        precision:
-            tensor where element ``i`` is the precision of predictions with
-            ``score >= thresholds[i]`` and the last element is 1.
-            If multiclass, this is a list of such tensors, one for each class.
-        recall:
-            tensor where element ``i`` is the recall of predictions with
-            ``score >= thresholds[i]`` and the last element is 0.
-            If multiclass, this is a list of such tensors, one for each class.
-        thresholds:
-            Thresholds used for computing precision/recall scores
+        preds: Sequence of data of either shape ``(N,)`` or ``(N,d)``
+        target: Sequence of data of either shape ``(N,)`` or ``(N,d)``
+        variant: Indication of which variant of Kendall's tau to be used
+        t_test: Indication whether to run t-test
+        alternative: Alternative hypothesis for t-test. Possible values:
+            - 'two-sided': the rank correlation is nonzero
+            - 'less': the rank correlation is negative (less than zero)
+            - 'greater':  the rank correlation is positive (greater than zero)
+
+    Return:
+        Correlation tau statistic
+        (Optional) p-value of corresponding statistical test (asymptotic)
 
     Raises:
-        ValueError:
-            If ``preds`` and ``target`` don't have the same number of dimensions,
-            or one additional dimension for ``preds``.
-        ValueError:
-            If the number of classes deduced from ``preds`` is not the same as the ``num_classes`` provided.
-
-    Example (binary case):
-        >>> from torchmetrics.functional import precision_recall_curve
-        >>> pred = torch.tensor([0, 1, 2, 3])
-        >>> target = torch.tensor([0, 1, 1, 0])
-        >>> precision, recall, thresholds = precision_recall_curve(pred, target, pos_label=1)
-        >>> precision
-        tensor([0.6667, 0.5000, 0.0000, 1.0000])
-        >>> recall
-        tensor([1.0000, 0.5000, 0.0000, 0.0000])
-        >>> thresholds
-        tensor([1, 2, 3])
-
-    Example (multiclass case):
-        >>> pred = torch.tensor([[0.75, 0.05, 0.05, 0.05, 0.05],
-        ...                      [0.05, 0.75, 0.05, 0.05, 0.05],
-        ...                      [0.05, 0.05, 0.75, 0.05, 0.05],
-        ...                      [0.05, 0.05, 0.05, 0.75, 0.05]])
-        >>> target = torch.tensor([0, 1, 3, 2])
-        >>> precision, recall, thresholds = precision_recall_curve(pred, target, num_classes=5)
-        >>> precision
-        [tensor([1., 1.]), tensor([1., 1.]), tensor([0.2500, 0.0000, 1.0000]),
-         tensor([0.2500, 0.0000, 1.0000]), tensor([0., 1.])]
-        >>> recall
-        [tensor([1., 0.]), tensor([1., 0.]), tensor([1., 0., 0.]), tensor([1., 0., 0.]), tensor([nan, 0.])]
-        >>> thresholds
-        [tensor([0.7500]), tensor([0.7500]), tensor([0.0500, 0.7500]), tensor([0.0500, 0.7500]), tensor([0.0500])]
+        ValueError: If ``t_test`` is not of a type bool
+        ValueError: If ``t_test=True`` and ``alternative=None``
+
+    Example (single output regression):
+        >>> from torchmetrics.functional.regression import kendall_rank_corrcoef
+        >>> preds = torch.tensor([2.5, 0.0, 2, 8])
+        >>> target = torch.tensor([3, -0.5, 2, 1])
+        >>> kendall_rank_corrcoef(preds, target)
+        tensor(0.3333)
+
+    Example (multi output regression):
+        >>> from torchmetrics.functional.regression import kendall_rank_corrcoef
+        >>> preds = torch.tensor([[2.5, 0.0], [2, 8]])
+        >>> target = torch.tensor([[3, -0.5], [2, 1]])
+        >>> kendall_rank_corrcoef(preds, target)
+        tensor([1., 1.])
+
+    Example (single output regression with t-test)
+        >>> from torchmetrics.functional.regression import kendall_rank_corrcoef
+        >>> preds = torch.tensor([2.5, 0.0, 2, 8])
+        >>> target = torch.tensor([3, -0.5, 2, 1])
+        >>> kendall_rank_corrcoef(preds, target, t_test=True, alternative='two-sided')
+        (tensor(0.3333), tensor(0.4969))
+
+    Example (multi output regression with t-test):
+        >>> from torchmetrics.functional.regression import kendall_rank_corrcoef
+        >>> preds = torch.tensor([[2.5, 0.0], [2, 8]])
+        >>> target = torch.tensor([[3, -0.5], [2, 1]])
+        >>> kendall_rank_corrcoef(preds, target, t_test=True, alternative='two-sided')
+            (tensor([1., 1.]), tensor([nan, nan]))
     """
-    preds, target, num_classes, pos_label = _precision_recall_curve_update(preds, target, num_classes, pos_label)
-    return _precision_recall_curve_compute(preds, target, num_classes, pos_label, sample_weights)
+    if not isinstance(t_test, bool):
+        raise ValueError(f"Argument `t_test` is expected to be of a type `bool`, but got {type(t_test)}.")
+    if t_test and alternative is None:
+        raise ValueError("Argument `alternative` is required if `t_test=True` but got `None`.")
+
+    _variant = _MetricVariant.from_str(str(variant))
+    _alternative = _TestAlternative.from_str(str(alternative)) if t_test else None
+
+    _preds, _target = _kendall_corrcoef_update(
+        preds, target, [], [], num_outputs=1 if preds.ndim == 1 else preds.shape[-1]
+    )
+    tau, p_value = _kendall_corrcoef_compute(
+        dim_zero_cat(_preds), dim_zero_cat(_target), _variant, _alternative  # type: ignore[arg-type]  # todo
+    )
+
+    if p_value is not None:
+        return tau, p_value
+    return tau
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/classification/stat_scores.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/classification/accuracy.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,442 +1,433 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import List, Optional, Tuple, Union
+from typing import Optional
 
 import torch
-from torch import Tensor, tensor
+from torch import Tensor
+from typing_extensions import Literal
 
-from torchmetrics.utilities.checks import _input_format_classification
-from torchmetrics.utilities.enums import AverageMethod, DataType, MDMCAverageMethod
-
-
-def _del_column(data: Tensor, idx: int) -> Tensor:
-    """Delete the column at index."""
-    return torch.cat([data[:, :idx], data[:, (idx + 1) :]], 1)
-
-
-def _drop_negative_ignored_indices(
-    preds: Tensor, target: Tensor, ignore_index: int, mode: DataType
-) -> Tuple[Tensor, Tensor]:
-    """Remove negative ignored indices.
+from torchmetrics.functional.classification.stat_scores import (
+    _binary_stat_scores_arg_validation,
+    _binary_stat_scores_format,
+    _binary_stat_scores_tensor_validation,
+    _binary_stat_scores_update,
+    _multiclass_stat_scores_arg_validation,
+    _multiclass_stat_scores_format,
+    _multiclass_stat_scores_tensor_validation,
+    _multiclass_stat_scores_update,
+    _multilabel_stat_scores_arg_validation,
+    _multilabel_stat_scores_format,
+    _multilabel_stat_scores_tensor_validation,
+    _multilabel_stat_scores_update,
+)
+from torchmetrics.utilities.compute import _safe_divide
+from torchmetrics.utilities.enums import ClassificationTask
+
+
+def _accuracy_reduce(
+    tp: Tensor,
+    fp: Tensor,
+    tn: Tensor,
+    fn: Tensor,
+    average: Optional[Literal["binary", "micro", "macro", "weighted", "none"]],
+    multidim_average: Literal["global", "samplewise"] = "global",
+    multilabel: bool = False,
+) -> Tensor:
+    """Reduce classification statistics into accuracy score.
 
     Args:
-        preds: Predicted tensor
-        target: Ground truth tensor
-        ignore_index: Specify a class (label) to ignore. If given, this class index does not contribute
-            to the returned score, regardless of reduction method. If an index is ignored, and
-            ``reduce='macro'``, the class statistics for the ignored class will all be returned
-            as ``-1``.
-        mode: Mode of the input tensors
-
-    Return:
-        Tensors of preds and target without negative ignore target values.
-    """
-    if mode == mode.MULTIDIM_MULTICLASS and preds.dtype == torch.float:
-        # In case or multi-dimensional multi-class with logits
-        n_dims = len(preds.shape)
-        num_classes = preds.shape[1]
-        # move class dim to last so that we can flatten the additional dimensions into N: [N, C, ...] -> [N, ..., C]
-        preds = preds.transpose(1, n_dims - 1)
+        tp: number of true positives
+        fp: number of false positives
+        tn: number of true negatives
+        fn: number of false negatives
+        average:
+            Defines the reduction that is applied over labels. Should be one of the following:
+
+            - ``binary``: for binary reduction
+            - ``micro``: sum score over all classes/labels
+            - ``macro``: salculate score for each class/label and average them
+            - ``weighted``: calculates score for each class/label and computes weighted average using their support
+            - ``"none"`` or ``None``: calculates score for each class/label and applies no reduction
+
+        multidim_average:
+            Defines how additionally dimensions ``...`` should be handled. Should be one of the following:
 
-        # flatten: [N, ..., C] -> [N', C]
-        preds = preds.reshape(-1, num_classes)
-        target = target.reshape(-1)
+            - ``global``: Additional dimensions are flatted along the batch dimension
+            - ``samplewise``: Statistic will be calculated independently for each sample on the ``N`` axis.
 
-    if mode in [mode.MULTICLASS, mode.MULTIDIM_MULTICLASS]:
-        preds = preds[target != ignore_index]
-        target = target[target != ignore_index]
+        multilabel: If input is multilabel or not
 
-    return preds, target
+    Returns:
+        Accuracy score
+    """
+    if average == "binary":
+        return _safe_divide(tp + tn, tp + tn + fp + fn)
+    if average == "micro":
+        tp = tp.sum(dim=0 if multidim_average == "global" else 1)
+        fn = fn.sum(dim=0 if multidim_average == "global" else 1)
+        if multilabel:
+            fp = fp.sum(dim=0 if multidim_average == "global" else 1)
+            tn = tn.sum(dim=0 if multidim_average == "global" else 1)
+            return _safe_divide(tp + tn, tp + tn + fp + fn)
+        return _safe_divide(tp, tp + fn)
+
+    score = _safe_divide(tp + tn, tp + tn + fp + fn) if multilabel else _safe_divide(tp, tp + fn)
+    if average is None or average == "none":
+        return score
+    weights = tp + fn if average == "weighted" else torch.ones_like(score)
+    return _safe_divide(weights * score, weights.sum(-1, keepdim=True)).sum(-1)
 
 
-def _stat_scores(
+def binary_accuracy(
     preds: Tensor,
     target: Tensor,
-    reduce: Optional[str] = "micro",
-) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
-    """Calculate the number of tp, fp, tn, fn.
+    threshold: float = 0.5,
+    multidim_average: Literal["global", "samplewise"] = "global",
+    ignore_index: Optional[int] = None,
+    validate_args: bool = True,
+) -> Tensor:
+    r"""Compute `Accuracy`_ for binary tasks.
 
-    Args:
-        preds: An ``(N, C)`` or ``(N, C, X)`` tensor of predictions (0 or 1)
-        target: An ``(N, C)`` or ``(N, C, X)`` tensor of true labels (0 or 1)
-        reduce: One of ``'micro'``, ``'macro'``, ``'samples'``
-
-    Return:
-        Returns a list of 4 tensors; tp, fp, tn, fn.
-        The shape of the returned tensors depends on the shape of the inputs
-        and the ``reduce`` parameter:
-
-        If inputs are of the shape ``(N, C)``, then:
-
-        - If ``reduce='micro'``, the returned tensors are 1 element tensors
-        - If ``reduce='macro'``, the returned tensors are ``(C,)`` tensors
-        - If ``reduce='samples'``, the returned tensors are ``(N,)`` tensors
-
-        If inputs are of the shape ``(N, C, X)``, then:
-
-        - If ``reduce='micro'``, the returned tensors are ``(N,)`` tensors
-        - If ``reduce='macro'``, the returned tensors are ``(N,C)`` tensors
-        - If ``reduce='samples'``, the returned tensors are ``(N,X)`` tensors
-    """
-    dim: Union[int, List[int]] = 1  # for "samples"
-    if reduce == "micro":
-        dim = [0, 1] if preds.ndim == 2 else [1, 2]
-    elif reduce == "macro":
-        dim = 0 if preds.ndim == 2 else 2
+    .. math::
+        \text{Accuracy} = \frac{1}{N}\sum_i^N 1(y_i = \hat{y}_i)
 
-    true_pred, false_pred = target == preds, target != preds
-    pos_pred, neg_pred = preds == 1, preds == 0
+    Where :math:`y` is a tensor of target values, and :math:`\hat{y}` is a
+    tensor of predictions.
 
-    tp = (true_pred * pos_pred).sum(dim=dim)
-    fp = (false_pred * pos_pred).sum(dim=dim)
+    Accepts the following input tensors:
 
-    tn = (true_pred * neg_pred).sum(dim=dim)
-    fn = (false_pred * neg_pred).sum(dim=dim)
+    - ``preds`` (int or float tensor): ``(N, ...)``. If preds is a floating point tensor with values outside
+      [0,1] range we consider the input to be logits and will auto apply sigmoid per element. Addtionally,
+      we convert to int tensor with thresholding using the value in ``threshold``.
+    - ``target`` (int tensor): ``(N, ...)``
 
-    return tp.long(), fp.long(), tn.long(), fn.long()
+    Args:
+        preds: Tensor with predictions
+        target: Tensor with true labels
+        threshold: Threshold for transforming probability to binary {0,1} predictions
+        multidim_average:
+            Defines how additionally dimensions ``...`` should be handled. Should be one of the following:
+
+            - ``global``: Additional dimensions are flatted along the batch dimension
+            - ``samplewise``: Statistic will be calculated independently for each sample on the ``N`` axis.
+              The statistics in this case are calculated over the additional dimensions.
+
+        ignore_index:
+            Specifies a target value that is ignored and does not contribute to the metric calculation
+        validate_args: bool indicating if input arguments and tensors should be validated for correctness.
+            Set to ``False`` for faster computations.
+
+    Returns:
+        If ``multidim_average`` is set to ``global``, the metric returns a scalar value. If ``multidim_average``
+        is set to ``samplewise``, the metric returns ``(N,)`` vector consisting of a scalar value per sample.
+
+    Example (preds is int tensor):
+        >>> from torch import tensor
+        >>> from torchmetrics.functional.classification import binary_accuracy
+        >>> target = tensor([0, 1, 0, 1, 0, 1])
+        >>> preds = tensor([0, 0, 1, 1, 0, 1])
+        >>> binary_accuracy(preds, target)
+        tensor(0.6667)
+
+    Example (preds is float tensor):
+        >>> from torchmetrics.functional.classification import binary_accuracy
+        >>> target = tensor([0, 1, 0, 1, 0, 1])
+        >>> preds = tensor([0.11, 0.22, 0.84, 0.73, 0.33, 0.92])
+        >>> binary_accuracy(preds, target)
+        tensor(0.6667)
+
+    Example (multidim tensors):
+        >>> from torchmetrics.functional.classification import binary_accuracy
+        >>> target = tensor([[[0, 1], [1, 0], [0, 1]], [[1, 1], [0, 0], [1, 0]]])
+        >>> preds = tensor([[[0.59, 0.91], [0.91, 0.99], [0.63, 0.04]],
+        ...                 [[0.38, 0.04], [0.86, 0.780], [0.45, 0.37]]])
+        >>> binary_accuracy(preds, target, multidim_average='samplewise')
+        tensor([0.3333, 0.1667])
+    """
+    if validate_args:
+        _binary_stat_scores_arg_validation(threshold, multidim_average, ignore_index)
+        _binary_stat_scores_tensor_validation(preds, target, multidim_average, ignore_index)
+    preds, target = _binary_stat_scores_format(preds, target, threshold, ignore_index)
+    tp, fp, tn, fn = _binary_stat_scores_update(preds, target, multidim_average)
+    return _accuracy_reduce(tp, fp, tn, fn, average="binary", multidim_average=multidim_average)
 
 
-def _stat_scores_update(
+def multiclass_accuracy(
     preds: Tensor,
     target: Tensor,
-    reduce: Optional[str] = "micro",
-    mdmc_reduce: Optional[str] = None,
-    num_classes: Optional[int] = None,
-    top_k: Optional[int] = None,
-    threshold: float = 0.5,
-    multiclass: Optional[bool] = None,
+    num_classes: int,
+    average: Optional[Literal["micro", "macro", "weighted", "none"]] = "macro",
+    top_k: int = 1,
+    multidim_average: Literal["global", "samplewise"] = "global",
     ignore_index: Optional[int] = None,
-    mode: DataType = None,
-) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
-    """Updates and returns the number of true positives, false positives, true negatives, false negatives. Raises
-    ValueError if:
-
-        - The `ignore_index` is not valid
-        - When `ignore_index` is used with binary data
-        - When inputs are multi-dimensional multi-class, and the ``mdmc_reduce`` parameter is not set
-
-    Args:
-        preds: Predicted tensor
-        target: Ground truth tensor
-        reduce: Defines the reduction that is applied
-        mdmc_reduce: Defines how the multi-dimensional multi-class inputs are handled
-        num_classes: Number of classes. Necessary for (multi-dimensional) multi-class or multi-label data.
-        top_k: Number of the highest probability or logit score predictions considered finding the correct label,
-            relevant only for (multi-dimensional) multi-class inputs
-        threshold: Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case
-            of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities
-        multiclass: Used only in certain special cases, where you want to treat inputs as a different type
-            than what they appear to be
-        ignore_index: Specify a class (label) to ignore. If given, this class index does not contribute
-            to the returned score, regardless of reduction method. If an index is ignored, and
-            ``reduce='macro'``, the class statistics for the ignored class will all be returned
-            as ``-1``.
-        mode: Mode of the input tensors
-    """
+    validate_args: bool = True,
+) -> Tensor:
+    r"""Compute `Accuracy`_ for multiclass tasks.
 
-    _negative_index_dropped = False
+    .. math::
+        \text{Accuracy} = \frac{1}{N}\sum_i^N 1(y_i = \hat{y}_i)
 
-    if ignore_index is not None and ignore_index < 0 and mode is not None:
-        preds, target = _drop_negative_ignored_indices(preds, target, ignore_index, mode)
-        _negative_index_dropped = True
-
-    preds, target, _ = _input_format_classification(
-        preds,
-        target,
-        threshold=threshold,
-        num_classes=num_classes,
-        multiclass=multiclass,
-        top_k=top_k,
-        ignore_index=ignore_index,
-    )
+    Where :math:`y` is a tensor of target values, and :math:`\hat{y}` is a
+    tensor of predictions.
 
-    if ignore_index is not None and ignore_index >= preds.shape[1]:
-        raise ValueError(f"The `ignore_index` {ignore_index} is not valid for inputs with {preds.shape[1]} classes")
+    Accepts the following input tensors:
 
-    if ignore_index is not None and preds.shape[1] == 1:
-        raise ValueError("You can not use `ignore_index` with binary data.")
-
-    if preds.ndim == 3:
-        if not mdmc_reduce:
-            raise ValueError(
-                "When your inputs are multi-dimensional multi-class, you have to set the `mdmc_reduce` parameter"
-            )
-        if mdmc_reduce == "global":
-            preds = torch.transpose(preds, 1, 2).reshape(-1, preds.shape[1])
-            target = torch.transpose(target, 1, 2).reshape(-1, target.shape[1])
-
-    # Delete what is in ignore_index, if applicable (and classes don't matter):
-    if ignore_index is not None and reduce != "macro" and not _negative_index_dropped:
-        preds = _del_column(preds, ignore_index)
-        target = _del_column(target, ignore_index)
-
-    tp, fp, tn, fn = _stat_scores(preds, target, reduce=reduce)
-
-    # Take care of ignore_index
-    if ignore_index is not None and reduce == "macro" and not _negative_index_dropped:
-        tp[..., ignore_index] = -1
-        fp[..., ignore_index] = -1
-        tn[..., ignore_index] = -1
-        fn[..., ignore_index] = -1
-
-    return tp, fp, tn, fn
-
-
-def _stat_scores_compute(tp: Tensor, fp: Tensor, tn: Tensor, fn: Tensor) -> Tensor:
-    """Computes the number of true positives, false positives, true negatives, false negatives. Concatenates the
-    input tensors along with the support into one output.
+    - ``preds``: ``(N, ...)`` (int tensor) or ``(N, C, ..)`` (float tensor). If preds is a floating point
+      we apply ``torch.argmax`` along the ``C`` dimension to automatically convert probabilities/logits into
+      an int tensor.
+    - ``target`` (int tensor): ``(N, ...)``
 
     Args:
-        tp: True positives
-        fp: False positives
-        tn: True negatives
-        fn: False negatives
-
-    Example:
-        >>> preds  = torch.tensor([1, 0, 2, 1])
-        >>> target = torch.tensor([1, 1, 2, 0])
-        >>> tp, fp, tn, fn = _stat_scores_update(preds, target, reduce='macro', num_classes=3)
-        >>> _stat_scores_compute(tp, fp, tn, fn)
-        tensor([[0, 1, 2, 1, 1],
-                [1, 1, 1, 1, 2],
-                [1, 0, 3, 0, 1]])
-        >>> tp, fp, tn, fn = _stat_scores_update(preds, target, reduce='micro')
-        >>> _stat_scores_compute(tp, fp, tn, fn)
-        tensor([2, 2, 6, 2, 4])
-    """
-    stats = [
-        tp.unsqueeze(-1),
-        fp.unsqueeze(-1),
-        tn.unsqueeze(-1),
-        fn.unsqueeze(-1),
-        tp.unsqueeze(-1) + fn.unsqueeze(-1),  # support
-    ]
-    outputs: Tensor = torch.cat(stats, -1)
-    outputs = torch.where(outputs < 0, tensor(-1, device=outputs.device), outputs)
-
-    return outputs
-
-
-def _reduce_stat_scores(
-    numerator: Tensor,
-    denominator: Tensor,
-    weights: Optional[Tensor],
-    average: Optional[str],
-    mdmc_average: Optional[str],
-    zero_division: int = 0,
-) -> Tensor:
-    """Reduces scores of type ``numerator/denominator`` or.
+        preds: Tensor with predictions
+        target: Tensor with true labels
+        num_classes: Integer specifing the number of classes
+        average:
+            Defines the reduction that is applied over labels. Should be one of the following:
+
+            - ``micro``: Sum statistics over all labels
+            - ``macro``: Calculate statistics for each label and average them
+            - ``weighted``: calculates statistics for each label and computes weighted average using their support
+            - ``"none"`` or ``None``: calculates statistic for each label and applies no reduction
 
-    ``weights * (numerator/denominator)``, if ``average='weighted'``.
+        top_k:
+            Number of highest probability or logit score predictions considered to find the correct label.
+            Only works when ``preds`` contain probabilities/logits.
+        multidim_average:
+            Defines how additionally dimensions ``...`` should be handled. Should be one of the following:
+
+            - ``global``: Additional dimensions are flatted along the batch dimension
+            - ``samplewise``: Statistic will be calculated independently for each sample on the ``N`` axis.
+              The statistics in this case are calculated over the additional dimensions.
 
-    Args:
-        numerator: A tensor with numerator numbers.
-        denominator: A tensor with denominator numbers. If a denominator is
-            negative, the class will be ignored (if averaging), or its score
-            will be returned as ``nan`` (if ``average=None``).
-            If the denominator is zero, then ``zero_division`` score will be
-            used for those elements.
-        weights: A tensor of weights to be used if ``average='weighted'``.
-        average: The method to average the scores
-        mdmc_average: The method to average the scores if inputs were multi-dimensional multi-class (MDMC)
-        zero_division: The value to use for the score if denominator equals zero.
+        ignore_index:
+            Specifies a target value that is ignored and does not contribute to the metric calculation
+        validate_args: bool indicating if input arguments and tensors should be validated for correctness.
+            Set to ``False`` for faster computations.
+
+    Returns:
+        The returned shape depends on the ``average`` and ``multidim_average`` arguments:
+
+        - If ``multidim_average`` is set to ``global``:
+
+          - If ``average='micro'/'macro'/'weighted'``, the output will be a scalar tensor
+          - If ``average=None/'none'``, the shape will be ``(C,)``
+
+        - If ``multidim_average`` is set to ``samplewise``:
+
+          - If ``average='micro'/'macro'/'weighted'``, the shape will be ``(N,)``
+          - If ``average=None/'none'``, the shape will be ``(N, C)``
+
+    Example (preds is int tensor):
+        >>> from torch import tensor
+        >>> from torchmetrics.functional.classification import multiclass_accuracy
+        >>> target = tensor([2, 1, 0, 0])
+        >>> preds = tensor([2, 1, 0, 1])
+        >>> multiclass_accuracy(preds, target, num_classes=3)
+        tensor(0.8333)
+        >>> multiclass_accuracy(preds, target, num_classes=3, average=None)
+        tensor([0.5000, 1.0000, 1.0000])
+
+    Example (preds is float tensor):
+        >>> from torchmetrics.functional.classification import multiclass_accuracy
+        >>> target = tensor([2, 1, 0, 0])
+        >>> preds = tensor([[0.16, 0.26, 0.58],
+        ...                 [0.22, 0.61, 0.17],
+        ...                 [0.71, 0.09, 0.20],
+        ...                 [0.05, 0.82, 0.13]])
+        >>> multiclass_accuracy(preds, target, num_classes=3)
+        tensor(0.8333)
+        >>> multiclass_accuracy(preds, target, num_classes=3, average=None)
+        tensor([0.5000, 1.0000, 1.0000])
+
+    Example (multidim tensors):
+        >>> from torchmetrics.functional.classification import multiclass_accuracy
+        >>> target = tensor([[[0, 1], [2, 1], [0, 2]], [[1, 1], [2, 0], [1, 2]]])
+        >>> preds = tensor([[[0, 2], [2, 0], [0, 1]], [[2, 2], [2, 1], [1, 0]]])
+        >>> multiclass_accuracy(preds, target, num_classes=3, multidim_average='samplewise')
+        tensor([0.5000, 0.2778])
+        >>> multiclass_accuracy(preds, target, num_classes=3, multidim_average='samplewise', average=None)
+        tensor([[1.0000, 0.0000, 0.5000],
+                [0.0000, 0.3333, 0.5000]])
     """
-    numerator, denominator = numerator.float(), denominator.float()
-    zero_div_mask = denominator == 0
-    ignore_mask = denominator < 0
-
-    if weights is None:
-        weights = torch.ones_like(denominator)
-    else:
-        weights = weights.float()
-
-    numerator = torch.where(
-        zero_div_mask, tensor(zero_division, dtype=numerator.dtype, device=numerator.device), numerator
+    if validate_args:
+        _multiclass_stat_scores_arg_validation(num_classes, top_k, average, multidim_average, ignore_index)
+        _multiclass_stat_scores_tensor_validation(preds, target, num_classes, multidim_average, ignore_index)
+    preds, target = _multiclass_stat_scores_format(preds, target, top_k)
+    tp, fp, tn, fn = _multiclass_stat_scores_update(
+        preds, target, num_classes, top_k, average, multidim_average, ignore_index
     )
-    denominator = torch.where(
-        zero_div_mask | ignore_mask, tensor(1.0, dtype=denominator.dtype, device=denominator.device), denominator
-    )
-    weights = torch.where(ignore_mask, tensor(0.0, dtype=weights.dtype, device=weights.device), weights)
+    return _accuracy_reduce(tp, fp, tn, fn, average=average, multidim_average=multidim_average)
 
-    if average not in (AverageMethod.MICRO, AverageMethod.NONE, None):
-        weights = weights / weights.sum(dim=-1, keepdim=True)
 
-    scores = weights * (numerator / denominator)
+def multilabel_accuracy(
+    preds: Tensor,
+    target: Tensor,
+    num_labels: int,
+    threshold: float = 0.5,
+    average: Optional[Literal["micro", "macro", "weighted", "none"]] = "macro",
+    multidim_average: Literal["global", "samplewise"] = "global",
+    ignore_index: Optional[int] = None,
+    validate_args: bool = True,
+) -> Tensor:
+    r"""Compute `Accuracy`_ for multilabel tasks.
 
-    # This is in case where sum(weights) = 0, which happens if we ignore the only present class with average='weighted'
-    scores = torch.where(torch.isnan(scores), tensor(zero_division, dtype=scores.dtype, device=scores.device), scores)
+    .. math::
+        \text{Accuracy} = \frac{1}{N}\sum_i^N 1(y_i = \hat{y}_i)
 
-    if mdmc_average == MDMCAverageMethod.SAMPLEWISE:
-        scores = scores.mean(dim=0)
-        ignore_mask = ignore_mask.sum(dim=0).bool()
+    Where :math:`y` is a tensor of target values, and :math:`\hat{y}` is a
+    tensor of predictions.
 
-    if average in (AverageMethod.NONE, None):
-        scores = torch.where(ignore_mask, tensor(float("nan"), device=scores.device), scores)
-    else:
-        scores = scores.sum()
+    Accepts the following input tensors:
 
-    return scores
+    - ``preds`` (int or float tensor): ``(N, C, ...)``. If preds is a floating point tensor with values outside
+      [0,1] range we consider the input to be logits and will auto apply sigmoid per element. Addtionally,
+      we convert to int tensor with thresholding using the value in ``threshold``.
+    - ``target`` (int tensor): ``(N, C, ...)``
+
+    Args:
+        preds: Tensor with predictions
+        target: Tensor with true labels
+        num_labels: Integer specifing the number of labels
+        threshold: Threshold for transforming probability to binary (0,1) predictions
+        average:
+            Defines the reduction that is applied over labels. Should be one of the following:
+
+            - ``micro``: Sum statistics over all labels
+            - ``macro``: Calculate statistics for each label and average them
+            - ``weighted``: calculates statistics for each label and computes weighted average using their support
+            - ``"none"`` or ``None``: calculates statistic for each label and applies no reduction
+
+        multidim_average:
+            Defines how additionally dimensions ``...`` should be handled. Should be one of the following:
+
+            - ``global``: Additional dimensions are flatted along the batch dimension
+            - ``samplewise``: Statistic will be calculated independently for each sample on the ``N`` axis.
+              The statistics in this case are calculated over the additional dimensions.
+
+        ignore_index:
+            Specifies a target value that is ignored and does not contribute to the metric calculation
+        validate_args: bool indicating if input arguments and tensors should be validated for correctness.
+            Set to ``False`` for faster computations.
+
+    Returns:
+        The returned shape depends on the ``average`` and ``multidim_average`` arguments:
+
+        - If ``multidim_average`` is set to ``global``:
+
+          - If ``average='micro'/'macro'/'weighted'``, the output will be a scalar tensor
+          - If ``average=None/'none'``, the shape will be ``(C,)``
+
+        - If ``multidim_average`` is set to ``samplewise``:
+
+          - If ``average='micro'/'macro'/'weighted'``, the shape will be ``(N,)``
+          - If ``average=None/'none'``, the shape will be ``(N, C)``
+
+    Example (preds is int tensor):
+        >>> from torch import tensor
+        >>> from torchmetrics.functional.classification import multilabel_accuracy
+        >>> target = tensor([[0, 1, 0], [1, 0, 1]])
+        >>> preds = tensor([[0, 0, 1], [1, 0, 1]])
+        >>> multilabel_accuracy(preds, target, num_labels=3)
+        tensor(0.6667)
+        >>> multilabel_accuracy(preds, target, num_labels=3, average=None)
+        tensor([1.0000, 0.5000, 0.5000])
+
+    Example (preds is float tensor):
+        >>> from torchmetrics.functional.classification import multilabel_accuracy
+        >>> target = tensor([[0, 1, 0], [1, 0, 1]])
+        >>> preds = tensor([[0.11, 0.22, 0.84], [0.73, 0.33, 0.92]])
+        >>> multilabel_accuracy(preds, target, num_labels=3)
+        tensor(0.6667)
+        >>> multilabel_accuracy(preds, target, num_labels=3, average=None)
+        tensor([1.0000, 0.5000, 0.5000])
+
+    Example (multidim tensors):
+        >>> from torchmetrics.functional.classification import multilabel_accuracy
+        >>> target = tensor([[[0, 1], [1, 0], [0, 1]], [[1, 1], [0, 0], [1, 0]]])
+        >>> preds = tensor([[[0.59, 0.91], [0.91, 0.99], [0.63, 0.04]],
+        ...                 [[0.38, 0.04], [0.86, 0.780], [0.45, 0.37]]])
+        >>> multilabel_accuracy(preds, target, num_labels=3, multidim_average='samplewise')
+        tensor([0.3333, 0.1667])
+        >>> multilabel_accuracy(preds, target, num_labels=3, multidim_average='samplewise', average=None)
+        tensor([[0.5000, 0.5000, 0.0000],
+                [0.0000, 0.0000, 0.5000]])
+    """
+    if validate_args:
+        _multilabel_stat_scores_arg_validation(num_labels, threshold, average, multidim_average, ignore_index)
+        _multilabel_stat_scores_tensor_validation(preds, target, num_labels, multidim_average, ignore_index)
+    preds, target = _multilabel_stat_scores_format(preds, target, num_labels, threshold, ignore_index)
+    tp, fp, tn, fn = _multilabel_stat_scores_update(preds, target, multidim_average)
+    return _accuracy_reduce(tp, fp, tn, fn, average=average, multidim_average=multidim_average, multilabel=True)
 
 
-def stat_scores(
+def accuracy(
     preds: Tensor,
     target: Tensor,
-    reduce: str = "micro",
-    mdmc_reduce: Optional[str] = None,
-    num_classes: Optional[int] = None,
-    top_k: Optional[int] = None,
+    task: Literal["binary", "multiclass", "multilabel"],
     threshold: float = 0.5,
-    multiclass: Optional[bool] = None,
+    num_classes: Optional[int] = None,
+    num_labels: Optional[int] = None,
+    average: Literal["micro", "macro", "weighted", "none"] = "micro",
+    multidim_average: Literal["global", "samplewise"] = "global",
+    top_k: Optional[int] = 1,
     ignore_index: Optional[int] = None,
+    validate_args: bool = True,
 ) -> Tensor:
-    r"""Computes the number of true positives, false positives, true negatives, false negatives.
-    Related to `Type I and Type II errors`_ and the `confusion matrix`_.
+    r"""Compute `Accuracy`_.
 
-    The reduction method (how the statistics are aggregated) is controlled by the
-    ``reduce`` parameter, and additionally by the ``mdmc_reduce`` parameter in the
-    multi-dimensional multi-class case. Accepts all inputs listed in :ref:`pages/classification:input types`.
+    .. math::
+        \text{Accuracy} = \frac{1}{N}\sum_i^N 1(y_i = \hat{y}_i)
 
-    Args:
-        preds: Predictions from model (probabilities, logits or labels)
-        target: Ground truth values
-        threshold:
-            Threshold for transforming probability or logit predictions to binary (0,1) predictions, in the case
-            of binary or multi-label inputs. Default value of 0.5 corresponds to input being probabilities.
-        top_k:
-            Number of highest probability or logit score predictions considered to find the correct label,
-            relevant only for (multi-dimensional) multi-class inputs. The
-            default value (``None``) will be interpreted as 1 for these inputs.
-
-            Should be left at default (``None``) for all other types of inputs.
-        reduce:
-            Defines the reduction that is applied. Should be one of the following:
-
-            - ``'micro'`` [default]: Counts the statistics by summing over all [sample, class]
-              combinations (globally). Each statistic is represented by a single integer.
-            - ``'macro'``: Counts the statistics for each class separately (over all samples).
-              Each statistic is represented by a ``(C,)`` tensor. Requires ``num_classes``
-              to be set.
-            - ``'samples'``: Counts the statistics for each sample separately (over all classes).
-              Each statistic is represented by a ``(N, )`` 1d tensor.
-
-            .. note:: What is considered a sample in the multi-dimensional multi-class case
-                depends on the value of ``mdmc_reduce``.
-
-        num_classes:
-            Number of classes. Necessary for (multi-dimensional) multi-class or multi-label data.
-        ignore_index:
-            Specify a class (label) to ignore. If given, this class index does not contribute
-            to the returned score, regardless of reduction method. If an index is ignored, and
-            ``reduce='macro'``, the class statistics for the ignored class will all be returned
-            as ``-1``.
-        mdmc_reduce:
-            Defines how the multi-dimensional multi-class inputs are handeled. Should be
-            one of the following:
-
-            - ``None`` [default]: Should be left unchanged if your data is not multi-dimensional
-              multi-class (see :ref:`pages/classification:input types` for the definition of input types).
-
-            - ``'samplewise'``: In this case, the statistics are computed separately for each
-              sample on the ``N`` axis, and then the outputs are concatenated together. In each
-              sample the extra axes ``...`` are flattened to become the sub-sample axis, and
-              statistics for each sample are computed by treating the sub-sample axis as the
-              ``N`` axis for that sample.
-
-            - ``'global'``: In this case the ``N`` and ``...`` dimensions of the inputs are
-              flattened into a new ``N_X`` sample axis, i.e. the inputs are treated as if they
-              were ``(N_X, C)``. From here on the ``reduce`` parameter applies as usual.
-
-        multiclass:
-            Used only in certain special cases, where you want to treat inputs as a different type
-            than what they appear to be. See the parameter's
-            :ref:`documentation section <pages/classification:using the multiclass parameter>`
-            for a more detailed explanation and examples.
-
-    Return:
-        The metric returns a tensor of shape ``(..., 5)``, where the last dimension corresponds
-        to ``[tp, fp, tn, fn, sup]`` (``sup`` stands for support and equals ``tp + fn``). The
-        shape depends on the ``reduce`` and ``mdmc_reduce`` (in case of multi-dimensional
-        multi-class data) parameters:
-
-        - If the data is not multi-dimensional multi-class, then
-
-          - If ``reduce='micro'``, the shape will be ``(5, )``
-          - If ``reduce='macro'``, the shape will be ``(C, 5)``, where ``C`` stands for the number of classes
-          - If ``reduce='samples'``, the shape will be ``(N, 5)``, where ``N`` stands for
-            the number of samples
-
-        - If the data is multi-dimensional multi-class and ``mdmc_reduce='global'``, then
-
-          - If ``reduce='micro'``, the shape will be ``(5, )``
-          - If ``reduce='macro'``, the shape will be ``(C, 5)``
-          - If ``reduce='samples'``, the shape will be ``(N*X, 5)``, where ``X`` stands for
-            the product of sizes of all "extra" dimensions of the data (i.e. all dimensions
-            except for ``C`` and ``N``)
-
-        - If the data is multi-dimensional multi-class and ``mdmc_reduce='samplewise'``, then
-
-          - If ``reduce='micro'``, the shape will be ``(N, 5)``
-          - If ``reduce='macro'``, the shape will be ``(N, C, 5)``
-          - If ``reduce='samples'``, the shape will be ``(N, X, 5)``
-
-    Raises:
-        ValueError:
-            If ``reduce`` is none of ``"micro"``, ``"macro"`` or ``"samples"``.
-        ValueError:
-            If ``mdmc_reduce`` is none of ``None``, ``"samplewise"``, ``"global"``.
-        ValueError:
-            If ``reduce`` is set to ``"macro"`` and ``num_classes`` is not provided.
-        ValueError:
-            If ``num_classes`` is set and ``ignore_index`` is not in the range ``[0, num_classes)``.
-        ValueError:
-            If ``ignore_index`` is used with ``binary data``.
-        ValueError:
-            If inputs are ``multi-dimensional multi-class`` and ``mdmc_reduce`` is not provided.
-
-    Example:
-        >>> from torchmetrics.functional import stat_scores
-        >>> preds  = torch.tensor([1, 0, 2, 1])
-        >>> target = torch.tensor([1, 1, 2, 0])
-        >>> stat_scores(preds, target, reduce='macro', num_classes=3)
-        tensor([[0, 1, 2, 1, 1],
-                [1, 1, 1, 1, 2],
-                [1, 0, 3, 0, 1]])
-        >>> stat_scores(preds, target, reduce='micro')
-        tensor([2, 2, 6, 2, 4])
+    Where :math:`y` is a tensor of target values, and :math:`\hat{y}` is a tensor of predictions.
 
+    This function is a simple wrapper to get the task specific versions of this metric, which is done by setting the
+    ``task`` argument to either ``'binary'``, ``'multiclass'`` or ``multilabel``. See the documentation of
+    :func:`binary_accuracy`, :func:`multiclass_accuracy` and :func:`multilabel_accuracy` for the specific details of
+    each argument influence and examples.
+
+    Legacy Example:
+        >>> from torch import tensor
+        >>> target = tensor([0, 1, 2, 3])
+        >>> preds = tensor([0, 2, 1, 3])
+        >>> accuracy(preds, target, task="multiclass", num_classes=4)
+        tensor(0.5000)
+
+        >>> target = tensor([0, 1, 2])
+        >>> preds = tensor([[0.1, 0.9, 0], [0.3, 0.1, 0.6], [0.2, 0.5, 0.3]])
+        >>> accuracy(preds, target, task="multiclass", num_classes=3, top_k=2)
+        tensor(0.6667)
     """
-    if reduce not in ["micro", "macro", "samples"]:
-        raise ValueError(f"The `reduce` {reduce} is not valid.")
+    task = ClassificationTask.from_str(task)
 
-    if mdmc_reduce not in [None, "samplewise", "global"]:
-        raise ValueError(f"The `mdmc_reduce` {mdmc_reduce} is not valid.")
-
-    if reduce == "macro" and (not num_classes or num_classes < 1):
-        raise ValueError("When you set `reduce` as 'macro', you have to provide the number of classes.")
-
-    if num_classes and ignore_index is not None and (not 0 <= ignore_index < num_classes or num_classes == 1):
-        raise ValueError(f"The `ignore_index` {ignore_index} is not valid for inputs with {num_classes} classes")
-
-    tp, fp, tn, fn = _stat_scores_update(
-        preds,
-        target,
-        reduce=reduce,
-        mdmc_reduce=mdmc_reduce,
-        top_k=top_k,
-        threshold=threshold,
-        num_classes=num_classes,
-        multiclass=multiclass,
-        ignore_index=ignore_index,
-    )
-    return _stat_scores_compute(tp, fp, tn, fn)
+    if task == ClassificationTask.BINARY:
+        return binary_accuracy(preds, target, threshold, multidim_average, ignore_index, validate_args)
+    if task == ClassificationTask.MULTICLASS:
+        if not isinstance(num_classes, int):
+            raise ValueError(
+                f"Optional arg `num_classes` must be type `int` when task is {task}. Got {type(num_classes)}"
+            )
+        if not isinstance(top_k, int):
+            raise ValueError(f"Optional arg `top_k` must be type `int` when task is {task}. Got {type(top_k)}")
+        return multiclass_accuracy(
+            preds, target, num_classes, average, top_k, multidim_average, ignore_index, validate_args
+        )
+    if task == ClassificationTask.MULTILABEL:
+        if not isinstance(num_labels, int):
+            raise ValueError(
+                f"Optional arg `num_labels` must be type `int` when task is {task}. Got {type(num_labels)}"
+            )
+        return multilabel_accuracy(
+            preds, target, num_labels, threshold, average, multidim_average, ignore_index, validate_args
+        )
+    raise ValueError(f"Not handled value: {task}")  # this is for compliant of mypy
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/image/__init__.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,23 +1,42 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from torchmetrics.functional.image.d_lambda import spectral_distortion_index  # noqa: F401
-from torchmetrics.functional.image.ergas import error_relative_global_dimensionless_synthesis  # noqa: F401
-from torchmetrics.functional.image.gradients import image_gradients  # noqa: F401
-from torchmetrics.functional.image.psnr import peak_signal_noise_ratio  # noqa: F401
-from torchmetrics.functional.image.sam import spectral_angle_mapper  # noqa: F401
-from torchmetrics.functional.image.ssim import (  # noqa: F401
+from torchmetrics.functional.image.d_lambda import spectral_distortion_index
+from torchmetrics.functional.image.ergas import error_relative_global_dimensionless_synthesis
+from torchmetrics.functional.image.gradients import image_gradients
+from torchmetrics.functional.image.psnr import peak_signal_noise_ratio
+from torchmetrics.functional.image.psnrb import peak_signal_noise_ratio_with_blocked_effect
+from torchmetrics.functional.image.rase import relative_average_spectral_error
+from torchmetrics.functional.image.rmse_sw import root_mean_squared_error_using_sliding_window
+from torchmetrics.functional.image.sam import spectral_angle_mapper
+from torchmetrics.functional.image.ssim import (
     multiscale_structural_similarity_index_measure,
     structural_similarity_index_measure,
 )
-from torchmetrics.functional.image.uqi import universal_image_quality_index  # noqa: F401
+from torchmetrics.functional.image.tv import total_variation
+from torchmetrics.functional.image.uqi import universal_image_quality_index
+
+__all__ = [
+    "spectral_distortion_index",
+    "error_relative_global_dimensionless_synthesis",
+    "image_gradients",
+    "peak_signal_noise_ratio",
+    "peak_signal_noise_ratio_with_blocked_effect",
+    "relative_average_spectral_error",
+    "root_mean_squared_error_using_sliding_window",
+    "spectral_angle_mapper",
+    "multiscale_structural_similarity_index_measure",
+    "structural_similarity_index_measure",
+    "total_variation",
+    "universal_image_quality_index",
+]
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/image/d_lambda.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/d_lambda.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -20,22 +20,20 @@
 
 from torchmetrics.functional.image.uqi import universal_image_quality_index
 from torchmetrics.utilities.checks import _check_same_shape
 from torchmetrics.utilities.distributed import reduce
 
 
 def _spectral_distortion_index_update(preds: Tensor, target: Tensor) -> Tuple[Tensor, Tensor]:
-    """Updates and returns variables required to compute Spectral Distortion Index. Checks for same shape and type
-    of the input tensors.
+    """Update and returns variables required to compute Spectral Distortion Index.
 
     Args:
         preds: Low resolution multispectral image
         target: High resolution fused image
     """
-
     if preds.dtype != target.dtype:
         raise TypeError(
             f"Expected `ms` and `fused` to have the same data type. Got ms: {preds.dtype} and fused: {target.dtype}."
         )
     _check_same_shape(preds, target)
     if len(preds.shape) != 4:
         raise ValueError(
@@ -46,15 +44,15 @@
 
 def _spectral_distortion_index_compute(
     preds: Tensor,
     target: Tensor,
     p: int = 1,
     reduction: Literal["elementwise_mean", "sum", "none"] = "elementwise_mean",
 ) -> Tensor:
-    """Computes Spectral Distortion Index (SpectralDistortionIndex_)
+    """Compute Spectral Distortion Index (SpectralDistortionIndex_).
 
     Args:
         preds: Low resolution multispectral image
         target: High resolution fused image
         p: a parameter to emphasize large spectral difference
         reduction: a method to reduce metric score over labels.
 
@@ -90,16 +88,17 @@
 
 def spectral_distortion_index(
     preds: Tensor,
     target: Tensor,
     p: int = 1,
     reduction: Literal["elementwise_mean", "sum", "none"] = "elementwise_mean",
 ) -> Tensor:
-    """Spectral Distortion Index (SpectralDistortionIndex_) also now as D_lambda is used to compare the spectral
-    distortion between two images.
+    """Calculate `Spectral Distortion Index`_ (SpectralDistortionIndex_) also known as D_lambda.
+
+    Metric is used to compare the spectral distortion between two images.
 
     Args:
         preds: Low resolution multispectral image
         target: High resolution fused image
         p: Large spectral differences
         reduction: a method to reduce metric score over labels.
 
@@ -115,15 +114,15 @@
             If ``preds`` and ``target`` don't have the same data type.
         ValueError:
             If ``preds`` and ``target`` don't have ``BxCxHxW shape``.
         ValueError:
             If ``p`` is not a positive integer.
 
     Example:
-        >>> from torchmetrics.functional import spectral_distortion_index
+        >>> from torchmetrics.functional.image import spectral_distortion_index
         >>> _ = torch.manual_seed(42)
         >>> preds = torch.rand([16, 3, 16, 16])
         >>> target = torch.rand([16, 3, 16, 16])
         >>> spectral_distortion_index(preds, target)
         tensor(0.0234)
     """
     if not isinstance(p, int) or p <= 0:
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/image/ergas.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/precision_recall_curve.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,126 +1,99 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Tuple, Union
+from typing import Optional, Tuple
 
 import torch
 from torch import Tensor
-from typing_extensions import Literal
+from torch.nn.functional import pad
 
-from torchmetrics.utilities.checks import _check_same_shape
-from torchmetrics.utilities.distributed import reduce
+from torchmetrics.utilities.checks import _check_retrieval_functional_inputs
+from torchmetrics.utilities.data import _cumsum
 
 
-def _ergas_update(preds: Tensor, target: Tensor) -> Tuple[Tensor, Tensor]:
-    """Updates and returns variables required to compute Erreur Relative Globale Adimensionnelle de Synthse.
-    Checks for same shape and type of the input tensors.
+def retrieval_precision_recall_curve(
+    preds: Tensor, target: Tensor, max_k: Optional[int] = None, adaptive_k: bool = False
+) -> Tuple[Tensor, Tensor, Tensor]:
+    """Compute precision-recall pairs for different k (from 1 to `max_k`).
 
-    Args:
-        preds: Predicted tensor
-        target: Ground truth tensor
-    """
+    In a ranked retrieval context, appropriate sets of retrieved documents are naturally given by
+    the top k retrieved documents.
 
-    if preds.dtype != target.dtype:
-        raise TypeError(
-            "Expected `preds` and `target` to have the same data type."
-            f" Got preds: {preds.dtype} and target: {target.dtype}."
-        )
-    _check_same_shape(preds, target)
-    if len(preds.shape) != 4:
-        raise ValueError(
-            "Expected `preds` and `target` to have BxCxHxW shape."
-            f" Got preds: {preds.shape} and target: {target.shape}."
-        )
-    return preds, target
-
-
-def _ergas_compute(
-    preds: Tensor,
-    target: Tensor,
-    ratio: Union[int, float] = 4,
-    reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
-) -> Tensor:
-    """Erreur Relative Globale Adimensionnelle de Synthse.
+    Recall is the fraction of relevant documents retrieved among all the relevant documents.
+    Precision is the fraction of relevant documents among all the retrieved documents.
 
-    Args:
-        preds: estimated image
-        target: ground truth image
-        ratio: ratio of high resolution to low resolution
-        reduction: a method to reduce metric score over labels.
-
-            - ``'elementwise_mean'``: takes the mean (default)
-            - ``'sum'``: takes the sum
-            - ``'none'`` or ``None``: no reduction will be applied
+    For each such set, precision and recall values can be plotted to give a recall-precision
+    curve.
 
-    Example:
-        >>> preds = torch.rand([16, 1, 16, 16], generator=torch.manual_seed(42))
-        >>> target = preds * 0.75
-        >>> preds, target = _ergas_update(preds, target)
-        >>> torch.round(_ergas_compute(preds, target))
-        tensor(154.)
-    """
-    b, c, h, w = preds.shape
-    preds = preds.reshape(b, c, h * w)
-    target = target.reshape(b, c, h * w)
-
-    diff = preds - target
-    sum_squared_error = torch.sum(diff * diff, dim=2)
-    rmse_per_band = torch.sqrt(sum_squared_error / (h * w))
-    mean_target = torch.mean(target, dim=2)
-
-    ergas_score = 100 * ratio * torch.sqrt(torch.sum((rmse_per_band / mean_target) ** 2, dim=1) / c)
-    return reduce(ergas_score, reduction)
-
-
-def error_relative_global_dimensionless_synthesis(
-    preds: Tensor,
-    target: Tensor,
-    ratio: Union[int, float] = 4,
-    reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
-) -> Tensor:
-    """Erreur Relative Globale Adimensionnelle de Synthse.
+    ``preds`` and ``target`` should be of the same shape and live on the same device. If no ``target`` is ``True``,
+    ``0`` is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
+    otherwise an error is raised.
 
     Args:
-        preds: estimated image
-        target: ground truth image
-        ratio: ratio of high resolution to low resolution
-        reduction: a method to reduce metric score over labels.
-
-            - ``'elementwise_mean'``: takes the mean (default)
-            - ``'sum'``: takes the sum
-            - ``'none'`` or ``None``: no reduction will be applied
-
-    Return:
-        Tensor with RelativeG score
+        preds: estimated probabilities of each document to be relevant.
+        target: ground truth about each document being relevant or not.
+        max_k: Calculate recall and precision for all possible top k from 1 to max_k
+               (default: `None`, which considers all possible top k)
+        adaptive_k: adjust `max_k` to `min(max_k, number of documents)` for each query
+
+    Returns:
+        Tensor with the precision values for each k (at ``top_k``) from 1 to `max_k`
+        Tensor with the recall values for each k (at ``top_k``) from 1 to `max_k`
+        Tensor with all possibles k
 
     Raises:
-        TypeError:
-            If ``preds`` and ``target`` don't have the same data type.
         ValueError:
-            If ``preds`` and ``target`` don't have ``BxCxHxW shape``.
+            If ``max_k`` is not `None` or an integer larger than 0.
+        ValueError:
+            If ``adaptive_k`` is not boolean.
 
     Example:
-        >>> from torchmetrics.functional import error_relative_global_dimensionless_synthesis
-        >>> preds = torch.rand([16, 1, 16, 16], generator=torch.manual_seed(42))
-        >>> target = preds * 0.75
-        >>> ergds = error_relative_global_dimensionless_synthesis(preds, target)
-        >>> torch.round(ergds)
-        tensor(154.)
-
-    References:
-        [1] Qian Du; Nicholas H. Younan; Roger King; Vijay P. Shah, "On the Performance Evaluation of
-        Pan-Sharpening Techniques" in IEEE Geoscience and Remote Sensing Letters, vol. 4, no. 4, pp. 518-522,
-        15 October 2007, doi: 10.1109/LGRS.2007.896328.
+        >>> from torch import tensor
+        >>> from  torchmetrics.functional import retrieval_precision_recall_curve
+        >>> preds = tensor([0.2, 0.3, 0.5])
+        >>> target = tensor([True, False, True])
+        >>> precisions, recalls, top_k = retrieval_precision_recall_curve(preds, target, max_k=2)
+        >>> precisions
+        tensor([1.0000, 0.5000])
+        >>> recalls
+        tensor([0.5000, 0.5000])
+        >>> top_k
+        tensor([1, 2])
     """
-    preds, target = _ergas_update(preds, target)
-    return _ergas_compute(preds, target, ratio, reduction)
+    preds, target = _check_retrieval_functional_inputs(preds, target)
+
+    if not isinstance(adaptive_k, bool):
+        raise ValueError("`adaptive_k` has to be a boolean")
+
+    if max_k is None:
+        max_k = preds.shape[-1]
+
+    if not (isinstance(max_k, int) and max_k > 0):
+        raise ValueError("`max_k` has to be a positive integer or None")
+
+    if adaptive_k and max_k > preds.shape[-1]:
+        topk = torch.arange(1, preds.shape[-1] + 1, device=preds.device)
+        topk = pad(topk, (0, max_k - preds.shape[-1]), "constant", float(preds.shape[-1]))
+    else:
+        topk = torch.arange(1, max_k + 1, device=preds.device)
+
+    if not target.sum():
+        return torch.zeros(max_k, device=preds.device), torch.zeros(max_k, device=preds.device), topk
+
+    relevant = target[preds.topk(min(max_k, preds.shape[-1]), dim=-1)[1]].float()
+    relevant = _cumsum(pad(relevant, (0, max(0, max_k - len(relevant))), "constant", 0.0), dim=0)
+
+    recall = relevant / target.sum()
+    precision = relevant / topk
+
+    return precision, recall, topk
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/image/gradients.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/gradients.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -14,25 +14,23 @@
 from typing import Tuple
 
 import torch
 from torch import Tensor
 
 
 def _image_gradients_validate(img: Tensor) -> None:
-    """Validates whether img is a 4D torch Tensor."""
-
+    """Validate whether img is a 4D torch Tensor."""
     if not isinstance(img, Tensor):
         raise TypeError(f"The `img` expects a value of <Tensor> type but got {type(img)}")
     if img.ndim != 4:
         raise RuntimeError(f"The `img` expects a 4D tensor but got {img.ndim}D tensor")
 
 
 def _compute_image_gradients(img: Tensor) -> Tuple[Tensor, Tensor]:
-    """Computes image gradients (dy/dx) for a given image."""
-
+    """Compute image gradients (dy/dx) for a given image."""
     batch_size, channels, height, width = img.shape
 
     dy = img[..., 1:, :] - img[..., :-1, :]
     dx = img[..., :, 1:] - img[..., :, :-1]
 
     shapey = [batch_size, channels, 1, width]
     dy = torch.cat([dy, torch.zeros(shapey, device=img.device, dtype=img.dtype)], dim=2)
@@ -42,30 +40,30 @@
     dx = torch.cat([dx, torch.zeros(shapex, device=img.device, dtype=img.dtype)], dim=3)
     dx = dx.view(img.shape)
 
     return dy, dx
 
 
 def image_gradients(img: Tensor) -> Tuple[Tensor, Tensor]:
-    """Computes `Gradient Computation of Image`_ of a given image using finite difference.
+    """Compute `Gradient Computation of Image`_ of a given image using finite difference.
 
     Args:
         img: An ``(N, C, H, W)`` input tensor where ``C`` is the number of image channels
 
     Return:
         Tuple of ``(dy, dx)`` with each gradient of shape ``[N, C, H, W]``
 
     Raises:
         TypeError:
-            If ``img`` is not of the type ``torch.Tensor``.
+            If ``img`` is not of the type :class:`~torch.Tensor`.
         RuntimeError:
             If ``img`` is not a 4D tensor.
 
     Example:
-        >>> from torchmetrics.functional import image_gradients
+        >>> from torchmetrics.functional.image import image_gradients
         >>> image = torch.arange(0, 1*1*5*5, dtype=torch.float32)
         >>> image = torch.reshape(image, (1, 1, 5, 5))
         >>> dy, dx = image_gradients(image)
         >>> dy[0, 0, :, :]
         tensor([[5., 5., 5., 5., 5.],
                 [5., 5., 5., 5., 5.],
                 [5., 5., 5., 5., 5.],
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/image/psnr.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/psnr.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -23,15 +23,15 @@
 def _psnr_compute(
     sum_squared_error: Tensor,
     n_obs: Tensor,
     data_range: Tensor,
     base: float = 10.0,
     reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
 ) -> Tensor:
-    """Computes peak signal-to-noise ratio.
+    """Compute peak signal-to-noise ratio.
 
     Args:
         sum_squared_error: Sum of square of errors over all observations
         n_obs: Number of predictions or observations
         data_range: the range of the data. If None, it is determined from the data (max - min).
            ``data_range`` must be given when ``dim`` is not None.
         base: a base of a logarithm to use
@@ -45,70 +45,67 @@
         >>> preds = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
         >>> target = torch.tensor([[3.0, 2.0], [1.0, 0.0]])
         >>> data_range = target.max() - target.min()
         >>> sum_squared_error, n_obs = _psnr_update(preds, target)
         >>> _psnr_compute(sum_squared_error, n_obs, data_range)
         tensor(2.5527)
     """
-
     psnr_base_e = 2 * torch.log(data_range) - torch.log(sum_squared_error / n_obs)
     psnr_vals = psnr_base_e * (10 / torch.log(tensor(base)))
     return reduce(psnr_vals, reduction=reduction)
 
 
 def _psnr_update(
     preds: Tensor,
     target: Tensor,
     dim: Optional[Union[int, Tuple[int, ...]]] = None,
 ) -> Tuple[Tensor, Tensor]:
-    """Updates and returns variables required to compute peak signal-to-noise ratio.
+    """Update and return variables required to compute peak signal-to-noise ratio.
 
     Args:
         preds: Predicted tensor
         target: Ground truth tensor
-        dim: Dimensions to reduce PSNR scores over provided as either an integer or a list of integers. Default is
-            None meaning scores will be reduced across all dimensions.
+        dim: Dimensions to reduce PSNR scores over provided as either an integer or a list of integers.
+            Default is None meaning scores will be reduced across all dimensions.
     """
-
     if dim is None:
         sum_squared_error = torch.sum(torch.pow(preds - target, 2))
         n_obs = tensor(target.numel(), device=target.device)
         return sum_squared_error, n_obs
 
     diff = preds - target
     sum_squared_error = torch.sum(diff * diff, dim=dim)
 
-    if isinstance(dim, int):
-        dim_list = [dim]
-    else:
-        dim_list = list(dim)
+    dim_list = [dim] if isinstance(dim, int) else list(dim)
     if not dim_list:
         n_obs = tensor(target.numel(), device=target.device)
     else:
         n_obs = tensor(target.size(), device=target.device)[dim_list].prod()
         n_obs = n_obs.expand_as(sum_squared_error)
 
     return sum_squared_error, n_obs
 
 
 def peak_signal_noise_ratio(
     preds: Tensor,
     target: Tensor,
-    data_range: Optional[float] = None,
+    data_range: Optional[Union[float, Tuple[float, float]]] = None,
     base: float = 10.0,
     reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
     dim: Optional[Union[int, Tuple[int, ...]]] = None,
 ) -> Tensor:
-    """Computes the peak signal-to-noise ratio.
+    """Compute the peak signal-to-noise ratio.
 
     Args:
         preds: estimated signal
         target: groun truth signal
-        data_range: the range of the data. If None, it is determined from the data (max - min).
-            ``data_range`` must be given when ``dim`` is not None.
+        data_range:
+            the range of the data. If None, it is determined from the data (max - min). If a tuple is provided then
+            the range is calculated as the difference and input is clamped between the values.
+            The ``data_range`` must be given when ``dim`` is not None.
         base: a base of a logarithm to use
         reduction: a method to reduce metric score over labels.
 
             - ``'elementwise_mean'``: takes the mean (default)
             - ``'sum'``: takes the sum
             - ``'none'`` or None``: no reduction will be applied
 
@@ -120,15 +117,15 @@
         Tensor with PSNR score
 
     Raises:
         ValueError:
             If ``dim`` is not ``None`` and ``data_range`` is not provided.
 
     Example:
-        >>> from torchmetrics.functional import peak_signal_noise_ratio
+        >>> from torchmetrics.functional.image import peak_signal_noise_ratio
         >>> pred = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
         >>> target = torch.tensor([[3.0, 2.0], [1.0, 0.0]])
         >>> peak_signal_noise_ratio(pred, target)
         tensor(2.5527)
 
     .. note::
         Half precision is only support on GPU for this metric
@@ -139,11 +136,16 @@
     if data_range is None:
         if dim is not None:
             # Maybe we could use `torch.amax(target, dim=dim) - torch.amin(target, dim=dim)` in PyTorch 1.7 to calculate
             # `data_range` in the future.
             raise ValueError("The `data_range` must be given when `dim` is not None.")
 
         data_range = target.max() - target.min()
+    elif isinstance(data_range, tuple):
+        preds = torch.clamp(preds, min=data_range[0], max=data_range[1])
+        target = torch.clamp(target, min=data_range[0], max=data_range[1])
+        data_range = tensor(data_range[1] - data_range[0])
     else:
         data_range = tensor(float(data_range))
+
     sum_squared_error, n_obs = _psnr_update(preds, target, dim=dim)
     return _psnr_compute(sum_squared_error, n_obs, data_range, base=base, reduction=reduction)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/image/sam.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/sam.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -18,22 +18,20 @@
 from typing_extensions import Literal
 
 from torchmetrics.utilities.checks import _check_same_shape
 from torchmetrics.utilities.distributed import reduce
 
 
 def _sam_update(preds: Tensor, target: Tensor) -> Tuple[Tensor, Tensor]:
-    """Updates and returns variables required to compute Spectral Angle Mapper. Checks for same shape and type of
-    the input tensors.
+    """Update and returns variables required to compute Spectral Angle Mapper.
 
     Args:
         preds: Predicted tensor
         target: Ground truth tensor
     """
-
     if preds.dtype != target.dtype:
         raise TypeError(
             "Expected `preds` and `target` to have the same data type."
             f" Got preds: {preds.dtype} and target: {target.dtype}."
         )
     _check_same_shape(preds, target)
     if len(preds.shape) != 4:
@@ -50,15 +48,15 @@
 
 
 def _sam_compute(
     preds: Tensor,
     target: Tensor,
     reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
 ) -> Tensor:
-    """Computes Spectral Angle Mapper.
+    """Compute Spectral Angle Mapper.
 
     Args:
         preds: estimated image
         target: ground truth image
         reduction: a method to reduce metric score over labels.
 
             - ``'elementwise_mean'``: takes the mean (default)
@@ -101,15 +99,15 @@
     Raises:
         TypeError:
             If ``preds`` and ``target`` don't have the same data type.
         ValueError:
             If ``preds`` and ``target`` don't have ``BxCxHxW shape``.
 
     Example:
-        >>> from torchmetrics.functional import spectral_angle_mapper
+        >>> from torchmetrics.functional.image import spectral_angle_mapper
         >>> preds = torch.rand([16, 3, 16, 16], generator=torch.manual_seed(42))
         >>> target = torch.rand([16, 3, 16, 16], generator=torch.manual_seed(123))
         >>> spectral_angle_mapper(preds, target)
         tensor(0.5943)
 
     References:
         [1] Roberta H. Yuhas, Alexander F. H. Goetz and Joe W. Boardman, "Discrimination among semi-arid
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/image/ssim.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/ssim.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,91 +11,72 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing import List, Optional, Sequence, Tuple, Union
 
 import torch
 from torch import Tensor
-from torch.nn import functional as F
+from torch.nn import functional as F  # noqa: N812
 from typing_extensions import Literal
 
 from torchmetrics.functional.image.helper import _gaussian_kernel_2d, _gaussian_kernel_3d, _reflection_pad_3d
 from torchmetrics.utilities.checks import _check_same_shape
 from torchmetrics.utilities.distributed import reduce
 
 
-def _ssim_update(preds: Tensor, target: Tensor) -> Tuple[Tensor, Tensor]:
-    """Updates and returns variables required to compute Structural Similarity Index Measure. Checks for same shape
-    and type of the input tensors.
+def _ssim_check_inputs(preds: Tensor, target: Tensor) -> Tuple[Tensor, Tensor]:
+    """Update and returns variables required to compute Structural Similarity Index Measure.
 
     Args:
         preds: Predicted tensor
         target: Ground truth tensor
     """
-
     if preds.dtype != target.dtype:
-        raise TypeError(
-            "Expected `preds` and `target` to have the same data type."
-            f" Got preds: {preds.dtype} and target: {target.dtype}."
-        )
+        target = target.to(preds.dtype)
     _check_same_shape(preds, target)
     if len(preds.shape) not in (4, 5):
         raise ValueError(
             "Expected `preds` and `target` to have BxCxHxW or BxCxDxHxW shape."
             f" Got preds: {preds.shape} and target: {target.shape}."
         )
     return preds, target
 
 
-def _ssim_compute(
+def _ssim_update(
     preds: Tensor,
     target: Tensor,
     gaussian_kernel: bool = True,
     sigma: Union[float, Sequence[float]] = 1.5,
     kernel_size: Union[int, Sequence[int]] = 11,
-    reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
-    data_range: Optional[float] = None,
+    data_range: Optional[Union[float, Tuple[float, float]]] = None,
     k1: float = 0.01,
     k2: float = 0.03,
     return_full_image: bool = False,
     return_contrast_sensitivity: bool = False,
 ) -> Union[Tensor, Tuple[Tensor, Tensor]]:
-    """Computes Structual Similarity Index Measure.
+    """Compute Structual Similarity Index Measure.
 
     Args:
         preds: estimated image
         target: ground truth image
         gaussian_kernel: If true (default), a gaussian kernel is used, if false a uniform kernel is used
         sigma: Standard deviation of the gaussian kernel, anisotropic kernels are possible.
             Ignored if a uniform kernel is used
         kernel_size: the size of the uniform kernel, anisotropic kernels are possible.
             Ignored if a Gaussian kernel is used
-        reduction: a method to reduce metric score over labels.
-
-            - ``'elementwise_mean'``: takes the mean
-            - ``'sum'``: takes the sum
-            - ``'none'`` or ``None``: no reduction will be applied
-
         data_range: Range of the image. If ``None``, it is determined from the image (max - min)
         k1: Parameter of SSIM.
         k2: Parameter of SSIM.
         return_full_image: If true, the full ``ssim`` image is returned as a second argument.
             Mutually exlusive with ``return_contrast_sensitivity``
         return_contrast_sensitivity: If true, the contrast term is returned as a second argument.
             The luminance term can be obtained with luminance=ssim/contrast
             Mutually exclusive with ``return_full_image``
-
-    Example:
-        >>> preds = torch.rand([16, 1, 16, 16])
-        >>> target = preds * 0.75
-        >>> preds, target = _ssim_update(preds, target)
-        >>> _ssim_compute(preds, target)
-        tensor(0.9219)
     """
-    is_3d = len(preds.shape) == 5
+    is_3d = preds.ndim == 5
 
     if not isinstance(kernel_size, Sequence):
         kernel_size = 3 * [kernel_size] if is_3d else 2 * [kernel_size]
     if not isinstance(sigma, Sequence):
         sigma = 3 * [sigma] if is_3d else 2 * [sigma]
 
     if len(kernel_size) != len(target.shape) - 2:
@@ -113,22 +94,29 @@
             f" which is: {len(target.shape)}"
         )
     if len(sigma) not in (2, 3):
         raise ValueError(
             f"Expected `kernel_size` dimension to be 2 or 3. `kernel_size` dimensionality: {len(kernel_size)}"
         )
 
+    if return_full_image and return_contrast_sensitivity:
+        raise ValueError("Arguments `return_full_image` and `return_contrast_sensitivity` are mutually exclusive.")
+
     if any(x % 2 == 0 or x <= 0 for x in kernel_size):
         raise ValueError(f"Expected `kernel_size` to have odd positive number. Got {kernel_size}.")
 
     if any(y <= 0 for y in sigma):
         raise ValueError(f"Expected `sigma` to have positive number. Got {sigma}.")
 
     if data_range is None:
         data_range = max(preds.max() - preds.min(), target.max() - target.min())
+    elif isinstance(data_range, tuple):
+        preds = torch.clamp(preds, min=data_range[0], max=data_range[1])
+        target = torch.clamp(target, min=data_range[0], max=data_range[1])
+        data_range = data_range[1] - data_range[0]
 
     c1 = pow(k1 * data_range, 2)
     c2 = pow(k2 * data_range, 2)
     device = preds.device
 
     channel = preds.size(1)
     dtype = preds.dtype
@@ -152,68 +140,86 @@
     if not gaussian_kernel:
         kernel = torch.ones((channel, 1, *kernel_size), dtype=dtype, device=device) / torch.prod(
             torch.tensor(kernel_size, dtype=dtype, device=device)
         )
 
     input_list = torch.cat((preds, target, preds * preds, target * target, preds * target))  # (5 * B, C, H, W)
 
-    if is_3d:
-        outputs = F.conv3d(input_list, kernel, groups=channel)
-    else:
-        outputs = F.conv2d(input_list, kernel, groups=channel)
+    outputs = F.conv3d(input_list, kernel, groups=channel) if is_3d else F.conv2d(input_list, kernel, groups=channel)
 
     output_list = outputs.split(preds.shape[0])
 
     mu_pred_sq = output_list[0].pow(2)
     mu_target_sq = output_list[1].pow(2)
     mu_pred_target = output_list[0] * output_list[1]
 
     sigma_pred_sq = output_list[2] - mu_pred_sq
     sigma_target_sq = output_list[3] - mu_target_sq
     sigma_pred_target = output_list[4] - mu_pred_target
 
-    upper = 2 * sigma_pred_target + c2
-    lower = sigma_pred_sq + sigma_target_sq + c2
+    upper = 2 * sigma_pred_target.to(dtype) + c2
+    lower = (sigma_pred_sq + sigma_target_sq).to(dtype) + c2
 
     ssim_idx_full_image = ((2 * mu_pred_target + c1) * upper) / ((mu_pred_sq + mu_target_sq + c1) * lower)
 
     if is_3d:
         ssim_idx = ssim_idx_full_image[..., pad_h:-pad_h, pad_w:-pad_w, pad_d:-pad_d]
     else:
         ssim_idx = ssim_idx_full_image[..., pad_h:-pad_h, pad_w:-pad_w]
 
     if return_contrast_sensitivity:
         contrast_sensitivity = upper / lower
-        contrast_sensitivity = contrast_sensitivity[..., pad_h:-pad_h, pad_w:-pad_w]
-        return reduce(ssim_idx.reshape(ssim_idx.shape[0], -1).mean(-1), reduction), reduce(
-            contrast_sensitivity.reshape(contrast_sensitivity.shape[0], -1).mean(-1), reduction
-        )
+        if is_3d:
+            contrast_sensitivity = contrast_sensitivity[..., pad_h:-pad_h, pad_w:-pad_w, pad_d:-pad_d]
+        else:
+            contrast_sensitivity = contrast_sensitivity[..., pad_h:-pad_h, pad_w:-pad_w]
+        return ssim_idx.reshape(ssim_idx.shape[0], -1).mean(-1), contrast_sensitivity.reshape(
+            contrast_sensitivity.shape[0], -1
+        ).mean(-1)
 
-    elif return_full_image:
-        return reduce(ssim_idx.reshape(ssim_idx.shape[0], -1).mean(-1), reduction), reduce(
-            ssim_idx_full_image, reduction
-        )
+    if return_full_image:
+        return ssim_idx.reshape(ssim_idx.shape[0], -1).mean(-1), ssim_idx_full_image
+
+    return ssim_idx.reshape(ssim_idx.shape[0], -1).mean(-1)
+
+
+def _ssim_compute(
+    similarities: Tensor,
+    reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
+) -> Tensor:
+    """Apply the specified reduction to pre-computed structural similarity.
+
+    Args:
+        similarities: per image similarities for a batch of images.
+        reduction: a method to reduce metric score over individual batch scores
 
-    return reduce(ssim_idx.reshape(ssim_idx.shape[0], -1).mean(-1), reduction)
+                - ``'elementwise_mean'``: takes the mean
+                - ``'sum'``: takes the sum
+                - ``'none'`` or ``None``: no reduction will be applied
+
+    Returns:
+        The reduced SSIM score
+    """
+    return reduce(similarities, reduction)
 
 
 def structural_similarity_index_measure(
     preds: Tensor,
     target: Tensor,
     gaussian_kernel: bool = True,
     sigma: Union[float, Sequence[float]] = 1.5,
     kernel_size: Union[int, Sequence[int]] = 11,
     reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
-    data_range: Optional[float] = None,
+    data_range: Optional[Union[float, Tuple[float, float]]] = None,
     k1: float = 0.01,
     k2: float = 0.03,
     return_full_image: bool = False,
     return_contrast_sensitivity: bool = False,
 ) -> Union[Tensor, Tuple[Tensor, Tensor]]:
-    """Computes Structual Similarity Index Measure.
+    """Compute Structual Similarity Index Measure.
 
     Args:
         preds: estimated image
         target: ground truth image
         gaussian_kernel: If true (default), a gaussian kernel is used, if false a uniform kernel is used
         sigma: Standard deviation of the gaussian kernel, anisotropic kernels are possible.
             Ignored if a uniform kernel is used
@@ -221,15 +227,17 @@
             Ignored if a Gaussian kernel is used
         reduction: a method to reduce metric score over labels.
 
             - ``'elementwise_mean'``: takes the mean
             - ``'sum'``: takes the sum
             - ``'none'`` or ``None``: no reduction will be applied
 
-        data_range: Range of the image. If ``None``, it is determined from the image (max - min)
+        data_range:
+            the range of the data. If None, it is determined from the data (max - min). If a tuple is provided then
+            the range is calculated as the difference and input is clamped between the values.
         k1: Parameter of SSIM.
         k2: Parameter of SSIM.
         return_full_image: If true, the full ``ssim`` image is returned as a second argument.
             Mutually exclusive with ``return_contrast_sensitivity``
         return_contrast_sensitivity: If true, the constant term is returned as a second argument.
             The luminance term can be obtained with luminance=ssim/contrast
             Mutually exclusive with ``return_full_image``
@@ -246,94 +254,98 @@
             If the length of ``kernel_size`` or ``sigma`` is not ``2``.
         ValueError:
             If one of the elements of ``kernel_size`` is not an ``odd positive number``.
         ValueError:
             If one of the elements of ``sigma`` is not a ``positive number``.
 
     Example:
-        >>> from torchmetrics.functional import structural_similarity_index_measure
-        >>> preds = torch.rand([16, 1, 16, 16])
+        >>> from torchmetrics.functional.image import structural_similarity_index_measure
+        >>> preds = torch.rand([3, 3, 256, 256])
         >>> target = preds * 0.75
         >>> structural_similarity_index_measure(preds, target)
         tensor(0.9219)
     """
-    preds, target = _ssim_update(preds, target)
-    return _ssim_compute(
+    preds, target = _ssim_check_inputs(preds, target)
+    similarity_pack = _ssim_update(
         preds,
         target,
         gaussian_kernel,
         sigma,
         kernel_size,
-        reduction,
         data_range,
         k1,
         k2,
         return_full_image,
         return_contrast_sensitivity,
     )
 
+    if isinstance(similarity_pack, tuple):
+        similarity, image = similarity_pack
+        return _ssim_compute(similarity, reduction), image
+
+    similarity = similarity_pack
+    return _ssim_compute(similarity, reduction)
+
 
 def _get_normalized_sim_and_cs(
     preds: Tensor,
     target: Tensor,
     gaussian_kernel: bool = True,
     sigma: Union[float, Sequence[float]] = 1.5,
     kernel_size: Union[int, Sequence[int]] = 11,
-    reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
-    data_range: Optional[float] = None,
+    data_range: Optional[Union[float, Tuple[float, float]]] = None,
     k1: float = 0.01,
     k2: float = 0.03,
     normalize: Optional[Literal["relu", "simple"]] = None,
 ) -> Tuple[Tensor, Tensor]:
-    sim, contrast_sensitivity = _ssim_compute(
+    sim, contrast_sensitivity = _ssim_update(
         preds,
         target,
         gaussian_kernel,
         sigma,
         kernel_size,
-        reduction,
         data_range,
         k1,
         k2,
         return_contrast_sensitivity=True,
     )
     if normalize == "relu":
         sim = torch.relu(sim)
         contrast_sensitivity = torch.relu(contrast_sensitivity)
     return sim, contrast_sensitivity
 
 
-def _multiscale_ssim_compute(
+def _multiscale_ssim_update(
     preds: Tensor,
     target: Tensor,
     gaussian_kernel: bool = True,
     sigma: Union[float, Sequence[float]] = 1.5,
     kernel_size: Union[int, Sequence[int]] = 11,
-    reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
-    data_range: Optional[float] = None,
+    data_range: Optional[Union[float, Tuple[float, float]]] = None,
     k1: float = 0.01,
     k2: float = 0.03,
     betas: Union[Tuple[float, float, float, float, float], Tuple[float, ...]] = (
         0.0448,
         0.2856,
         0.3001,
         0.2363,
         0.1333,
     ),
     normalize: Optional[Literal["relu", "simple"]] = None,
 ) -> Tensor:
-    """Computes Multi-Scale Structual Similarity Index Measure.
+    """Compute Multi-Scale Structual Similarity Index Measure.
 
     Adapted from: https://github.com/jorge-pessoa/pytorch-msssim/blob/master/pytorch_msssim/__init__.py.
 
     Args:
         preds: estimated image
         target: ground truth image
-        kernel_size: size of the gaussian kernel
+        gaussian_kernel: If true, a gaussian kernel is used, if false a uniform kernel is used
         sigma: Standard deviation of the gaussian kernel
+        kernel_size: size of the gaussian kernel
         reduction: a method to reduce metric score over labels.
 
             - ``'elementwise_mean'``: takes the mean
             - ``'sum'``: takes the sum
             - ``'none'`` or ``None``: no reduction will be applied
 
         data_range: Range of the image. If ``None``, it is determined from the image (max - min)
@@ -349,18 +361,17 @@
         ValueError:
             If the image height or width is smaller then ``2 ** len(betas)``.
         ValueError:
             If the image height is smaller than ``(kernel_size[0] - 1) * max(1, (len(betas) - 1)) ** 2``.
         ValueError:
             If the image width is smaller than ``(kernel_size[0] - 1) * max(1, (len(betas) - 1)) ** 2``.
     """
-    sim_list: List[Tensor] = []
-    cs_list: List[Tensor] = []
+    mcs_list: List[Tensor] = []
 
-    is_3d = len(preds.shape) == 5
+    is_3d = preds.ndim == 5
 
     if not isinstance(kernel_size, Sequence):
         kernel_size = 3 * [kernel_size] if is_3d else 2 * [kernel_size]
     if not isinstance(sigma, Sequence):
         sigma = 3 * [sigma] if is_3d else 2 * [sigma]
 
     if preds.size()[-1] < 2 ** len(betas) or preds.size()[-2] < 2 ** len(betas):
@@ -379,73 +390,91 @@
         raise ValueError(
             f"For a given number of `betas` parameters {len(betas)} and kernel size {kernel_size[1]},"
             f" the image width must be larger than {(kernel_size[1] - 1) * _betas_div}."
         )
 
     for _ in range(len(betas)):
         sim, contrast_sensitivity = _get_normalized_sim_and_cs(
-            preds, target, gaussian_kernel, sigma, kernel_size, reduction, data_range, k1, k2, normalize=normalize
+            preds, target, gaussian_kernel, sigma, kernel_size, data_range, k1, k2, normalize=normalize
         )
-        sim_list.append(sim)
-        cs_list.append(contrast_sensitivity)
+        mcs_list.append(contrast_sensitivity)
+
         if len(kernel_size) == 2:
             preds = F.avg_pool2d(preds, (2, 2))
             target = F.avg_pool2d(target, (2, 2))
         elif len(kernel_size) == 3:
             preds = F.avg_pool3d(preds, (2, 2, 2))
             target = F.avg_pool3d(target, (2, 2, 2))
         else:
             raise ValueError("length of kernel_size is neither 2 nor 3")
-    sim_stack = torch.stack(sim_list)
-    cs_stack = torch.stack(cs_list)
+
+    mcs_list[-1] = sim
+    mcs_stack = torch.stack(mcs_list)
 
     if normalize == "simple":
-        sim_stack = (sim_stack + 1) / 2
-        cs_stack = (cs_stack + 1) / 2
+        mcs_stack = (mcs_stack + 1) / 2
+
+    betas = torch.tensor(betas, device=mcs_stack.device).view(-1, 1)
+    mcs_weighted = mcs_stack**betas
+    return torch.prod(mcs_weighted, axis=0)
 
-    if reduction is None or reduction == "none":
-        betas = torch.tensor(betas).unsqueeze(1).repeat(1, sim_stack.shape[0])
-        sim_stack = sim_stack ** torch.tensor(betas, device=sim_stack.device)
-        cs_stack = cs_stack ** torch.tensor(betas, device=cs_stack.device)
-        cs_and_sim = torch.cat((cs_stack[:-1], sim_stack[-1:]), axis=0)
-        return torch.prod(cs_and_sim, axis=0)
-    else:
-        sim_stack = sim_stack ** torch.tensor(betas, device=sim_stack.device)
-        cs_stack = cs_stack ** torch.tensor(betas, device=cs_stack.device)
-        return torch.prod(cs_stack[:-1]) * sim_stack[-1]
+
+def _multiscale_ssim_compute(
+    mcs_per_image: Tensor,
+    reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
+) -> Tensor:
+    """Apply the specified reduction to pre-computed multi-scale structural similarity.
+
+    Args:
+        mcs_per_image: per image similarities for a batch of images.
+        reduction: a method to reduce metric score over individual batch scores
+
+                - ``'elementwise_mean'``: takes the mean
+                - ``'sum'``: takes the sum
+                - ``'none'`` or ``None``: no reduction will be applied
+
+    Returns:
+        The reduced multi-scale structural similarity
+    """
+    return reduce(mcs_per_image, reduction)
 
 
 def multiscale_structural_similarity_index_measure(
     preds: Tensor,
     target: Tensor,
     gaussian_kernel: bool = True,
     sigma: Union[float, Sequence[float]] = 1.5,
     kernel_size: Union[int, Sequence[int]] = 11,
     reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
-    data_range: Optional[float] = None,
+    data_range: Optional[Union[float, Tuple[float, float]]] = None,
     k1: float = 0.01,
     k2: float = 0.03,
     betas: Tuple[float, ...] = (0.0448, 0.2856, 0.3001, 0.2363, 0.1333),
-    normalize: Optional[Literal["relu", "simple"]] = None,
+    normalize: Optional[Literal["relu", "simple"]] = "relu",
 ) -> Tensor:
-    """Computes `MultiScaleSSIM`_, Multi-scale Structual Similarity Index Measure, which is a generalization of
-    Structual Similarity Index Measure by incorporating image details at different resolution scores.
+    """Compute `MultiScaleSSIM`_, Multi-scale Structual Similarity Index Measure.
+
+    This metric is a generalization of Structual Similarity Index Measure by incorporating image details at different
+    resolution scores.
 
     Args:
         preds: Predictions from model of shape ``[N, C, H, W]``
         target: Ground truth values of shape ``[N, C, H, W]``
-        kernel_size: size of the gaussian kernel
+        gaussian_kernel: If true, a gaussian kernel is used, if false a uniform kernel is used
         sigma: Standard deviation of the gaussian kernel
+        kernel_size: size of the gaussian kernel
         reduction: a method to reduce metric score over labels.
 
             - ``'elementwise_mean'``: takes the mean
             - ``'sum'``: takes the sum
             - ``'none'`` or ``None``: no reduction will be applied
 
-        data_range: Range of the image. If ``None``, it is determined from the image (max - min)
+        data_range:
+            the range of the data. If None, it is determined from the data (max - min). If a tuple is provided then
+            the range is calculated as the difference and input is clamped between the values.
         k1: Parameter of structural similarity index measure.
         k2: Parameter of structural similarity index measure.
         betas: Exponent parameters for individual similarities and contrastive sensitivies returned by different image
             resolutions.
         normalize: When MultiScaleSSIM loss is used for training, it is desirable to use normalizes to improve the
             training stability. This `normalize` argument is out of scope of the original implementation [1], and it is
             adapted from https://github.com/jorge-pessoa/pytorch-msssim instead.
@@ -462,28 +491,29 @@
             If the length of ``kernel_size`` or ``sigma`` is not ``2``.
         ValueError:
             If one of the elements of ``kernel_size`` is not an ``odd positive number``.
         ValueError:
             If one of the elements of ``sigma`` is not a ``positive number``.
 
     Example:
-        >>> from torchmetrics.functional import multiscale_structural_similarity_index_measure
-        >>> preds = torch.rand([1, 1, 256, 256], generator=torch.manual_seed(42))
+        >>> from torchmetrics.functional.image import multiscale_structural_similarity_index_measure
+        >>> preds = torch.rand([3, 3, 256, 256], generator=torch.manual_seed(42))
         >>> target = preds * 0.75
-        >>> multiscale_structural_similarity_index_measure(preds, target)
-        tensor(0.9558)
+        >>> multiscale_structural_similarity_index_measure(preds, target, data_range=1.0)
+        tensor(0.9627)
 
     References:
         [1] Multi-Scale Structural Similarity For Image Quality Assessment by Zhou Wang, Eero P. Simoncelli and Alan C.
         Bovik `MultiScaleSSIM`_
     """
     if not isinstance(betas, tuple):
         raise ValueError("Argument `betas` is expected to be of a type tuple.")
     if isinstance(betas, tuple) and not all(isinstance(beta, float) for beta in betas):
         raise ValueError("Argument `betas` is expected to be a tuple of floats.")
     if normalize and normalize not in ("relu", "simple"):
         raise ValueError("Argument `normalize` to be expected either `None` or one of 'relu' or 'simple'")
 
-    preds, target = _ssim_update(preds, target)
-    return _multiscale_ssim_compute(
-        preds, target, gaussian_kernel, sigma, kernel_size, reduction, data_range, k1, k2, betas, normalize
+    preds, target = _ssim_check_inputs(preds, target)
+    mcs_per_image = _multiscale_ssim_update(
+        preds, target, gaussian_kernel, sigma, kernel_size, data_range, k1, k2, betas, normalize
     )
+    return _multiscale_ssim_compute(mcs_per_image, reduction)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/image/uqi.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/image/uqi.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,31 +11,29 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing import Optional, Sequence, Tuple, Union
 
 import torch
 from torch import Tensor
-from torch.nn import functional as F
+from torch.nn import functional as F  # noqa: N812
 from typing_extensions import Literal
 
 from torchmetrics.functional.image.helper import _gaussian_kernel_2d
 from torchmetrics.utilities.checks import _check_same_shape
 from torchmetrics.utilities.distributed import reduce
 
 
 def _uqi_update(preds: Tensor, target: Tensor) -> Tuple[Tensor, Tensor]:
-    """Updates and returns variables required to compute Universal Image Quality Index. Checks for same shape and
-    type of the input tensors.
+    """Update and returns variables required to compute Universal Image Quality Index.
 
     Args:
         preds: Predicted tensor
         target: Ground truth tensor
     """
-
     if preds.dtype != target.dtype:
         raise TypeError(
             "Expected `preds` and `target` to have the same data type."
             f" Got preds: {preds.dtype} and target: {target.dtype}."
         )
     _check_same_shape(preds, target)
     if len(preds.shape) != 4:
@@ -49,17 +47,16 @@
 def _uqi_compute(
     preds: Tensor,
     target: Tensor,
     kernel_size: Sequence[int] = (11, 11),
     sigma: Sequence[float] = (1.5, 1.5),
     reduction: Optional[Literal["elementwise_mean", "sum", "none"]] = "elementwise_mean",
     data_range: Optional[float] = None,
-    return_contrast_sensitivity: bool = False,
 ) -> Union[Tensor, Tuple[Tensor, Tensor]]:
-    """Computes Universal Image Quality Index.
+    """Compute Universal Image Quality Index.
 
     Args:
         preds: estimated image
         target: ground truth image
         kernel_size: size of the gaussian kernel
         sigma: Standard deviation of the gaussian kernel
         reduction: a method to reduce metric score over labels.
@@ -158,15 +155,15 @@
             If the length of ``kernel_size`` or ``sigma`` is not ``2``.
         ValueError:
             If one of the elements of ``kernel_size`` is not an ``odd positive number``.
         ValueError:
             If one of the elements of ``sigma`` is not a ``positive number``.
 
     Example:
-        >>> from torchmetrics.functional import universal_image_quality_index
+        >>> from torchmetrics.functional.image import universal_image_quality_index
         >>> preds = torch.rand([16, 1, 16, 16])
         >>> target = preds * 0.75
         >>> universal_image_quality_index(preds, target)
         tensor(0.9216)
 
     References:
         [1] Zhou Wang and A. C. Bovik, "A universal image quality index," in IEEE Signal Processing Letters, vol. 9,
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/pairwise/__init__.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/pairwise/__init__.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,17 +1,26 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from torchmetrics.functional.pairwise.cosine import pairwise_cosine_similarity  # noqa: F401
-from torchmetrics.functional.pairwise.euclidean import pairwise_euclidean_distance  # noqa: F401
-from torchmetrics.functional.pairwise.linear import pairwise_linear_similarity  # noqa: F401
-from torchmetrics.functional.pairwise.manhattan import pairwise_manhattan_distance  # noqa: F401
+from torchmetrics.functional.pairwise.cosine import pairwise_cosine_similarity
+from torchmetrics.functional.pairwise.euclidean import pairwise_euclidean_distance
+from torchmetrics.functional.pairwise.linear import pairwise_linear_similarity
+from torchmetrics.functional.pairwise.manhattan import pairwise_manhattan_distance
+from torchmetrics.functional.pairwise.minkowski import pairwise_minkowski_distance
+
+__all__ = [
+    "pairwise_cosine_similarity",
+    "pairwise_euclidean_distance",
+    "pairwise_linear_similarity",
+    "pairwise_manhattan_distance",
+    "pairwise_minkowski_distance",
+]
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/pairwise/cosine.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/pairwise/linear.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,90 +1,82 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing import Optional
 
-import torch
 from torch import Tensor
 from typing_extensions import Literal
 
 from torchmetrics.functional.pairwise.helpers import _check_input, _reduce_distance_matrix
 from torchmetrics.utilities.compute import _safe_matmul
 
 
-def _pairwise_cosine_similarity_update(
+def _pairwise_linear_similarity_update(
     x: Tensor, y: Optional[Tensor] = None, zero_diagonal: Optional[bool] = None
 ) -> Tensor:
-    """Calculates the pairwise cosine similarity matrix.
+    """Calculate the pairwise linear similarity matrix.
 
     Args:
         x: tensor of shape ``[N,d]``
         y: tensor of shape ``[M,d]``
         zero_diagonal: determines if the diagonal of the distance matrix should be set to zero
     """
     x, y, zero_diagonal = _check_input(x, y, zero_diagonal)
 
-    norm = torch.norm(x, p=2, dim=1)
-    x /= norm.unsqueeze(1)
-    norm = torch.norm(y, p=2, dim=1)
-    y /= norm.unsqueeze(1)
-
     distance = _safe_matmul(x, y)
     if zero_diagonal:
         distance.fill_diagonal_(0)
     return distance
 
 
-def pairwise_cosine_similarity(
+def pairwise_linear_similarity(
     x: Tensor,
     y: Optional[Tensor] = None,
     reduction: Literal["mean", "sum", "none", None] = None,
     zero_diagonal: Optional[bool] = None,
 ) -> Tensor:
-    r"""Calculates pairwise cosine similarity:
+    r"""Calculate pairwise linear similarity.
 
     .. math::
-        s_{cos}(x,y) = \frac{<x,y>}{||x|| \cdot ||y||}
-                     = \frac{\sum_{d=1}^D x_d \cdot y_d }{\sqrt{\sum_{d=1}^D x_i^2} \cdot \sqrt{\sum_{d=1}^D y_i^2}}
+        s_{lin}(x,y) = <x,y> = \sum_{d=1}^D x_d \cdot y_d
 
-    If both :math:`x` and :math:`y` are passed in, the calculation will be performed pairwise
-    between the rows of :math:`x` and :math:`y`.
+    If both :math:`x` and :math:`y` are passed in, the calculation will be performed pairwise between
+    the rows of :math:`x` and :math:`y`.
     If only :math:`x` is passed in, the calculation will be performed between the rows of :math:`x`.
 
     Args:
         x: Tensor with shape ``[N, d]``
         y: Tensor with shape ``[M, d]``, optional
         reduction: reduction to apply along the last dimension. Choose between `'mean'`, `'sum'`
             (applied along column dimension) or  `'none'`, `None` for no reduction
-        zero_diagonal: if the diagonal of the distance matrix should be set to 0. If only :math:`x` is given
-            this defaults to ``True`` else if :math:`y` is also given it defaults to ``False``
+        zero_diagonal: if the diagonal of the distance matrix should be set to 0. If only `x` is given
+            this defaults to `True` else if `y` is also given it defaults to `False`
 
     Returns:
         A ``[N,N]`` matrix of distances if only ``x`` is given, else a ``[N,M]`` matrix
 
     Example:
         >>> import torch
-        >>> from torchmetrics.functional import pairwise_cosine_similarity
+        >>> from torchmetrics.functional.pairwise import pairwise_linear_similarity
         >>> x = torch.tensor([[2, 3], [3, 5], [5, 8]], dtype=torch.float32)
         >>> y = torch.tensor([[1, 0], [2, 1]], dtype=torch.float32)
-        >>> pairwise_cosine_similarity(x, y)
-        tensor([[0.5547, 0.8682],
-                [0.5145, 0.8437],
-                [0.5300, 0.8533]])
-        >>> pairwise_cosine_similarity(x)
-        tensor([[0.0000, 0.9989, 0.9996],
-                [0.9989, 0.0000, 0.9998],
-                [0.9996, 0.9998, 0.0000]])
-
+        >>> pairwise_linear_similarity(x, y)
+        tensor([[ 2.,  7.],
+                [ 3., 11.],
+                [ 5., 18.]])
+        >>> pairwise_linear_similarity(x)
+        tensor([[ 0., 21., 34.],
+                [21.,  0., 55.],
+                [34., 55.,  0.]])
     """
-    distance = _pairwise_cosine_similarity_update(x, y, zero_diagonal)
+    distance = _pairwise_linear_similarity_update(x, y, zero_diagonal)
     return _reduce_distance_matrix(distance, reduction)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/pairwise/euclidean.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/pairwise/euclidean.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,54 +1,59 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing import Optional
 
+import torch
 from torch import Tensor
 from typing_extensions import Literal
 
 from torchmetrics.functional.pairwise.helpers import _check_input, _reduce_distance_matrix
 
 
 def _pairwise_euclidean_distance_update(
     x: Tensor, y: Optional[Tensor] = None, zero_diagonal: Optional[bool] = None
 ) -> Tensor:
-    """Calculates the pairwise euclidean distance matrix.
+    """Calculate the pairwise euclidean distance matrix.
 
     Args:
         x: tensor of shape ``[N,d]``
         y: tensor of shape ``[M,d]``
         zero_diagonal: determines if the diagonal of the distance matrix should be set to zero
     """
     x, y, zero_diagonal = _check_input(x, y, zero_diagonal)
-    x_norm = x.norm(dim=1, keepdim=True)
-    y_norm = y.norm(dim=1).T
-    distance = x_norm * x_norm + y_norm * y_norm - 2 * x.mm(y.T)
+    # upcast to float64 to prevent precision issues
+    _orig_dtype = x.dtype
+    x = x.to(torch.float64)
+    y = y.to(torch.float64)
+    x_norm = (x * x).sum(dim=1, keepdim=True)
+    y_norm = (y * y).sum(dim=1)
+    distance = (x_norm + y_norm - 2 * x.mm(y.T)).to(_orig_dtype)
     if zero_diagonal:
         distance.fill_diagonal_(0)
     return distance.sqrt()
 
 
 def pairwise_euclidean_distance(
     x: Tensor,
     y: Optional[Tensor] = None,
     reduction: Literal["mean", "sum", "none", None] = None,
     zero_diagonal: Optional[bool] = None,
 ) -> Tensor:
-    r"""Calculates pairwise euclidean distances:
+    r"""Calculate pairwise euclidean distances.
 
     .. math::
         d_{euc}(x,y) = ||x - y||_2 = \sqrt{\sum_{d=1}^D (x_d - y_d)^2}
 
     If both :math:`x` and :math:`y` are passed in, the calculation will be performed pairwise between
     the rows of :math:`x` and :math:`y`.
     If only :math:`x` is passed in, the calculation will be performed between the rows of :math:`x`.
@@ -62,22 +67,21 @@
             this defaults to `True` else if `y` is also given it defaults to `False`
 
     Returns:
         A ``[N,N]`` matrix of distances if only ``x`` is given, else a ``[N,M]`` matrix
 
     Example:
         >>> import torch
-        >>> from torchmetrics.functional import pairwise_euclidean_distance
+        >>> from torchmetrics.functional.pairwise import pairwise_euclidean_distance
         >>> x = torch.tensor([[2, 3], [3, 5], [5, 8]], dtype=torch.float32)
         >>> y = torch.tensor([[1, 0], [2, 1]], dtype=torch.float32)
         >>> pairwise_euclidean_distance(x, y)
         tensor([[3.1623, 2.0000],
                 [5.3852, 4.1231],
                 [8.9443, 7.6158]])
         >>> pairwise_euclidean_distance(x)
         tensor([[0.0000, 2.2361, 5.8310],
                 [2.2361, 0.0000, 3.6056],
                 [5.8310, 3.6056, 0.0000]])
-
     """
     distance = _pairwise_euclidean_distance_update(x, y, zero_diagonal)
     return _reduce_distance_matrix(distance, reduction)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/pairwise/helpers.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/pairwise/helpers.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -15,16 +15,15 @@
 
 from torch import Tensor
 
 
 def _check_input(
     x: Tensor, y: Optional[Tensor] = None, zero_diagonal: Optional[bool] = None
 ) -> Tuple[Tensor, Tensor, bool]:
-    """Check that input has the right dimensionality and sets the ``zero_diagonal`` argument if user has not
-    provided import module.
+    """Check that input has the right dimensionality and sets the ``zero_diagonal`` argument if user has not set it.
 
     Args:
         x: tensor of shape ``[N,d]``
         y: if provided, a tensor of shape ``[M,d]``
         zero_diagonal: determines if the diagonal of the distance matrix should be set to zero
     """
     if x.ndim != 2:
@@ -40,15 +39,15 @@
     else:
         y = x.clone()
         zero_diagonal = True if zero_diagonal is None else zero_diagonal
     return x, y, zero_diagonal
 
 
 def _reduce_distance_matrix(distmat: Tensor, reduction: Optional[str] = None) -> Tensor:
-    """Final reduction of distance matrix.
+    """Reduction of distance matrix.
 
     Args:
         distmat: a ``[N,M]`` matrix
         reduction: string determining how to reduce along last dimension
     """
     if reduction == "mean":
         return distmat.mean(dim=-1)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/pairwise/linear.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/pairwise/manhattan.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -13,46 +13,44 @@
 # limitations under the License.
 from typing import Optional
 
 from torch import Tensor
 from typing_extensions import Literal
 
 from torchmetrics.functional.pairwise.helpers import _check_input, _reduce_distance_matrix
-from torchmetrics.utilities.compute import _safe_matmul
 
 
-def _pairwise_linear_similarity_update(
+def _pairwise_manhattan_distance_update(
     x: Tensor, y: Optional[Tensor] = None, zero_diagonal: Optional[bool] = None
 ) -> Tensor:
-    """Calculates the pairwise linear similarity matrix.
+    """Calculate the pairwise manhattan similarity matrix.
 
     Args:
         x: tensor of shape ``[N,d]``
-        y: tensor of shape ``[M,d]``
+        y: if provided, a tensor of shape ``[M,d]``
         zero_diagonal: determines if the diagonal of the distance matrix should be set to zero
     """
     x, y, zero_diagonal = _check_input(x, y, zero_diagonal)
 
-    distance = _safe_matmul(x, y)
+    distance = (x.unsqueeze(1) - y.unsqueeze(0).repeat(x.shape[0], 1, 1)).abs().sum(dim=-1)
     if zero_diagonal:
         distance.fill_diagonal_(0)
     return distance
 
 
-def pairwise_linear_similarity(
+def pairwise_manhattan_distance(
     x: Tensor,
     y: Optional[Tensor] = None,
     reduction: Literal["mean", "sum", "none", None] = None,
     zero_diagonal: Optional[bool] = None,
 ) -> Tensor:
-    r"""
-    Calculates pairwise linear similarity:
+    r"""Calculate pairwise manhattan distance.
 
     .. math::
-        s_{lin}(x,y) = <x,y> = \sum_{d=1}^D x_d \cdot y_d
+        d_{man}(x,y) = ||x-y||_1 = \sum_{d=1}^D |x_d - y_d|
 
     If both :math:`x` and :math:`y` are passed in, the calculation will be performed pairwise between
     the rows of :math:`x` and :math:`y`.
     If only :math:`x` is passed in, the calculation will be performed between the rows of :math:`x`.
 
     Args:
         x: Tensor with shape ``[N, d]``
@@ -63,22 +61,21 @@
             this defaults to `True` else if `y` is also given it defaults to `False`
 
     Returns:
         A ``[N,N]`` matrix of distances if only ``x`` is given, else a ``[N,M]`` matrix
 
     Example:
         >>> import torch
-        >>> from torchmetrics.functional import pairwise_linear_similarity
+        >>> from torchmetrics.functional.pairwise import pairwise_manhattan_distance
         >>> x = torch.tensor([[2, 3], [3, 5], [5, 8]], dtype=torch.float32)
         >>> y = torch.tensor([[1, 0], [2, 1]], dtype=torch.float32)
-        >>> pairwise_linear_similarity(x, y)
-        tensor([[ 2.,  7.],
-                [ 3., 11.],
-                [ 5., 18.]])
-        >>> pairwise_linear_similarity(x)
-        tensor([[ 0., 21., 34.],
-                [21.,  0., 55.],
-                [34., 55.,  0.]])
-
+        >>> pairwise_manhattan_distance(x, y)
+        tensor([[ 4.,  2.],
+                [ 7.,  5.],
+                [12., 10.]])
+        >>> pairwise_manhattan_distance(x)
+        tensor([[0., 3., 8.],
+                [3., 0., 5.],
+                [8., 5., 0.]])
     """
-    distance = _pairwise_linear_similarity_update(x, y, zero_diagonal)
+    distance = _pairwise_manhattan_distance_update(x, y, zero_diagonal)
     return _reduce_distance_matrix(distance, reduction)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/regression/cosine_similarity.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/cosine_similarity.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -19,61 +19,58 @@
 from torchmetrics.utilities.checks import _check_same_shape
 
 
 def _cosine_similarity_update(
     preds: Tensor,
     target: Tensor,
 ) -> Tuple[Tensor, Tensor]:
-    """Updates and returns variables required to compute Cosine Similarity. Checks for same shape of input tensors.
+    """Update and returns variables required to compute Cosine Similarity. Checks for same shape of input tensors.
 
     Args:
         preds: Predicted tensor
         target: Ground truth tensor
     """
-
     _check_same_shape(preds, target)
     preds = preds.float()
     target = target.float()
 
     return preds, target
 
 
 def _cosine_similarity_compute(preds: Tensor, target: Tensor, reduction: Optional[str] = "sum") -> Tensor:
-    """Computes Cosine Similarity.
+    """Compute Cosine Similarity.
 
     Args:
         preds: Predicted tensor
         target: Ground truth tensor
         reduction:
             The method of reducing along the batch dimension using sum, mean or taking the individual scores
 
     Example:
         >>> target = torch.tensor([[1, 2, 3, 4], [1, 2, 3, 4]])
         >>> preds = torch.tensor([[1, 2, 3, 4], [-1, -2, -3, -4]])
         >>> preds, target = _cosine_similarity_update(preds, target)
         >>> _cosine_similarity_compute(preds, target, 'none')
         tensor([ 1.0000, -1.0000])
     """
-
     dot_product = (preds * target).sum(dim=-1)
     preds_norm = preds.norm(dim=-1)
     target_norm = target.norm(dim=-1)
     similarity = dot_product / (preds_norm * target_norm)
     reduction_mapping = {
         "sum": torch.sum,
         "mean": torch.mean,
         "none": lambda x: x,
         None: lambda x: x,
     }
-    return reduction_mapping[reduction](similarity)
+    return reduction_mapping[reduction](similarity)  # type: ignore[operator]
 
 
 def cosine_similarity(preds: Tensor, target: Tensor, reduction: Optional[str] = "sum") -> Tensor:
-    r"""
-    Computes the `Cosine Similarity`_ between targets and predictions:
+    r"""Compute the `Cosine Similarity`_.
 
     .. math::
         cos_{sim}(x,y) = \frac{x \cdot y}{||x|| \cdot ||y||} =
         \frac{\sum_{i=1}^n x_i y_i}{\sqrt{\sum_{i=1}^n x_i^2}\sqrt{\sum_{i=1}^n y_i^2}}
 
     where :math:`y` is a tensor of target values, and :math:`x` is a tensor of predictions.
 
@@ -87,11 +84,10 @@
         >>> from torchmetrics.functional.regression import cosine_similarity
         >>> target = torch.tensor([[1, 2, 3, 4],
         ...                        [1, 2, 3, 4]])
         >>> preds = torch.tensor([[1, 2, 3, 4],
         ...                       [-1, -2, -3, -4]])
         >>> cosine_similarity(preds, target, 'none')
         tensor([ 1.0000, -1.0000])
-
     """
     preds, target = _cosine_similarity_update(preds, target)
     return _cosine_similarity_compute(preds, target, reduction)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/regression/explained_variance.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/explained_variance.py`

 * *Files 11% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,49 +11,50 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing import Sequence, Tuple, Union
 
 import torch
 from torch import Tensor
+from typing_extensions import Literal
 
 from torchmetrics.utilities.checks import _check_same_shape
 
+ALLOWED_MULTIOUTPUT = ("raw_values", "uniform_average", "variance_weighted")
+
 
 def _explained_variance_update(preds: Tensor, target: Tensor) -> Tuple[int, Tensor, Tensor, Tensor, Tensor]:
-    """Updates and returns variables required to compute Explained Variance. Checks for same shape of input
-    tensors.
+    """Update and returns variables required to compute Explained Variance. Checks for same shape of input tensors.
 
     Args:
         preds: Predicted tensor
         target: Ground truth tensor
     """
-
     _check_same_shape(preds, target)
 
     n_obs = preds.size(0)
     sum_error = torch.sum(target - preds, dim=0)
     diff = target - preds
     sum_squared_error = torch.sum(diff * diff, dim=0)
 
     sum_target = torch.sum(target, dim=0)
     sum_squared_target = torch.sum(target * target, dim=0)
 
     return n_obs, sum_error, sum_squared_error, sum_target, sum_squared_target
 
 
 def _explained_variance_compute(
-    n_obs: Tensor,
+    n_obs: Union[int, Tensor],
     sum_error: Tensor,
     sum_squared_error: Tensor,
     sum_target: Tensor,
     sum_squared_target: Tensor,
-    multioutput: str = "uniform_average",
+    multioutput: Literal["raw_values", "uniform_average", "variance_weighted"] = "uniform_average",
 ) -> Tensor:
-    """Computes Explained Variance.
+    """Compute Explained Variance.
 
     Args:
         n_obs: Number of predictions or observations
         sum_error: Sum of errors over all observations
         sum_squared_error: Sum of square of errors over all observations
         sum_target: Sum of target values
         sum_squared_target: Sum of squares of target values
@@ -67,15 +68,14 @@
     Example:
         >>> target = torch.tensor([[0.5, 1], [-1, 1], [7, -6]])
         >>> preds = torch.tensor([[0, 2], [-1, 2], [8, -5]])
         >>> n_obs, sum_error, ss_error, sum_target, ss_target = _explained_variance_update(preds, target)
         >>> _explained_variance_compute(n_obs, sum_error, ss_error, sum_target, ss_target, multioutput='raw_values')
         tensor([0.9677, 1.0000])
     """
-
     diff_avg = sum_error / n_obs
     numerator = sum_squared_error / n_obs - (diff_avg * diff_avg)
 
     target_avg = sum_target / n_obs
     denominator = sum_squared_target / n_obs - (target_avg * target_avg)
 
     # Take care of division by zero
@@ -88,48 +88,49 @@
 
     # Decide what to do in multioutput case
     # Todo: allow user to pass in tensor with weights
     if multioutput == "raw_values":
         return output_scores
     if multioutput == "uniform_average":
         return torch.mean(output_scores)
-    if multioutput == "variance_weighted":
-        denom_sum = torch.sum(denominator)
-        return torch.sum(denominator / denom_sum * output_scores)
+    denom_sum = torch.sum(denominator)
+    return torch.sum(denominator / denom_sum * output_scores)
 
 
 def explained_variance(
     preds: Tensor,
     target: Tensor,
-    multioutput: str = "uniform_average",
+    multioutput: Literal["raw_values", "uniform_average", "variance_weighted"] = "uniform_average",
 ) -> Union[Tensor, Sequence[Tensor]]:
-    """Computes explained variance.
+    """Compute explained variance.
 
     Args:
         preds: estimated labels
         target: ground truth labels
         multioutput: Defines aggregation in the case of multiple output scores. Can be one
             of the following strings):
 
             * ``'raw_values'`` returns full set of scores
             * ``'uniform_average'`` scores are uniformly averaged
             * ``'variance_weighted'`` scores are weighted by their individual variances
 
     Example:
-        >>> from torchmetrics.functional import explained_variance
+        >>> from torchmetrics.functional.regression import explained_variance
         >>> target = torch.tensor([3, -0.5, 2, 7])
         >>> preds = torch.tensor([2.5, 0.0, 2, 8])
         >>> explained_variance(preds, target)
         tensor(0.9572)
 
         >>> target = torch.tensor([[0.5, 1], [-1, 1], [7, -6]])
         >>> preds = torch.tensor([[0, 2], [-1, 2], [8, -5]])
         >>> explained_variance(preds, target, multioutput='raw_values')
         tensor([0.9677, 1.0000])
     """
+    if multioutput not in ALLOWED_MULTIOUTPUT:
+        raise ValueError(f"Invalid input to argument `multioutput`. Choose one of the following: {ALLOWED_MULTIOUTPUT}")
     n_obs, sum_error, sum_squared_error, sum_target, sum_squared_target = _explained_variance_update(preds, target)
     return _explained_variance_compute(
         n_obs,
         sum_error,
         sum_squared_error,
         sum_target,
         sum_squared_target,
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/regression/log_mse.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/log_mse.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,73 +1,71 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Tuple
+from typing import Tuple, Union
 
 import torch
 from torch import Tensor
 
 from torchmetrics.utilities.checks import _check_same_shape
 
 
 def _mean_squared_log_error_update(preds: Tensor, target: Tensor) -> Tuple[Tensor, int]:
-    """Returns variables required to compute Mean Squared Log Error. Checks for same shape of tensors.
+    """Return variables required to compute Mean Squared Log Error. Checks for same shape of tensors.
 
     Args:
         preds: Predicted tensor
         target: Ground truth tensor
     """
-
     _check_same_shape(preds, target)
     sum_squared_log_error = torch.sum(torch.pow(torch.log1p(preds) - torch.log1p(target), 2))
     n_obs = target.numel()
     return sum_squared_log_error, n_obs
 
 
-def _mean_squared_log_error_compute(sum_squared_log_error: Tensor, n_obs: int) -> Tensor:
-    """Computes Mean Squared Log Error.
+def _mean_squared_log_error_compute(sum_squared_log_error: Tensor, n_obs: Union[int, Tensor]) -> Tensor:
+    """Compute Mean Squared Log Error.
 
     Args:
         sum_squared_log_error:
             Sum of square of log errors over all observations ``(log error = log(target) - log(prediction))``
         n_obs: Number of predictions or observations
 
     Example:
         >>> preds = torch.tensor([0., 1, 2, 3])
         >>> target = torch.tensor([0., 1, 2, 2])
         >>> sum_squared_log_error, n_obs = _mean_squared_log_error_update(preds, target)
         >>> _mean_squared_log_error_compute(sum_squared_log_error, n_obs)
         tensor(0.0207)
     """
-
     return sum_squared_log_error / n_obs
 
 
 def mean_squared_log_error(preds: Tensor, target: Tensor) -> Tensor:
-    """Computes mean squared log error.
+    """Compute mean squared log error.
 
     Args:
         preds: estimated labels
         target: ground truth labels
 
     Return:
         Tensor with RMSLE
 
     Example:
-        >>> from torchmetrics.functional import mean_squared_log_error
+        >>> from torchmetrics.functional.regression import mean_squared_log_error
         >>> x = torch.tensor([0., 1, 2, 3])
         >>> y = torch.tensor([0., 1, 2, 2])
         >>> mean_squared_log_error(x, y)
         tensor(0.0207)
 
     .. note::
         Half precision is only support on GPU for this metric
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/regression/mae.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/mae.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,74 +1,74 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Tuple
+from typing import Tuple, Union
 
 import torch
 from torch import Tensor
 
 from torchmetrics.utilities.checks import _check_same_shape
 
 
 def _mean_absolute_error_update(preds: Tensor, target: Tensor) -> Tuple[Tensor, int]:
-    """Updates and returns variables required to compute Mean Absolute Error.
+    """Update and returns variables required to compute Mean Absolute Error.
 
-    Checks for same shape of input tensors.
+    Check for same shape of input tensors.
 
     Args:
         preds: Predicted tensor
         target: Ground truth tensor
     """
-
     _check_same_shape(preds, target)
+    preds = preds if preds.is_floating_point else preds.float()  # type: ignore[truthy-function] # todo
+    target = target if target.is_floating_point else target.float()  # type: ignore[truthy-function] # todo
     sum_abs_error = torch.sum(torch.abs(preds - target))
     n_obs = target.numel()
     return sum_abs_error, n_obs
 
 
-def _mean_absolute_error_compute(sum_abs_error: Tensor, n_obs: int) -> Tensor:
-    """Computes Mean Absolute Error.
+def _mean_absolute_error_compute(sum_abs_error: Tensor, n_obs: Union[int, Tensor]) -> Tensor:
+    """Compute Mean Absolute Error.
 
     Args:
         sum_abs_error: Sum of absolute value of errors over all observations
         n_obs: Number of predictions or observations
 
     Example:
         >>> preds = torch.tensor([0., 1, 2, 3])
         >>> target = torch.tensor([0., 1, 2, 2])
         >>> sum_abs_error, n_obs = _mean_absolute_error_update(preds, target)
         >>> _mean_absolute_error_compute(sum_abs_error, n_obs)
         tensor(0.2500)
     """
-
     return sum_abs_error / n_obs
 
 
 def mean_absolute_error(preds: Tensor, target: Tensor) -> Tensor:
-    """Computes mean absolute error.
+    """Compute mean absolute error.
 
     Args:
         preds: estimated labels
         target: ground truth labels
 
     Return:
         Tensor with MAE
 
     Example:
-        >>> from torchmetrics.functional import mean_absolute_error
+        >>> from torchmetrics.functional.regression import mean_absolute_error
         >>> x = torch.tensor([0., 1, 2, 3])
         >>> y = torch.tensor([0., 1, 2, 2])
         >>> mean_absolute_error(x, y)
         tensor(0.2500)
     """
     sum_abs_error, n_obs = _mean_absolute_error_update(preds, target)
     return _mean_absolute_error_compute(sum_abs_error, n_obs)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/regression/mape.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/mape.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,92 +1,88 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Tuple
+from typing import Tuple, Union
 
 import torch
 from torch import Tensor
 
 from torchmetrics.utilities.checks import _check_same_shape
 
 
 def _mean_absolute_percentage_error_update(
     preds: Tensor,
     target: Tensor,
     epsilon: float = 1.17e-06,
 ) -> Tuple[Tensor, int]:
-    """Updates and returns variables required to compute Mean Percentage Error.
+    """Update and returns variables required to compute Mean Percentage Error.
 
-    Checks for same shape of input tensors.
+    Check for same shape of input tensors.
 
     Args:
         preds: Predicted tensor
         target: Ground truth tensor
         epsilon: Specifies the lower bound for target values. Any target value below epsilon
             is set to epsilon (avoids ``ZeroDivisionError``).
     """
-
     _check_same_shape(preds, target)
 
     abs_diff = torch.abs(preds - target)
     abs_per_error = abs_diff / torch.clamp(torch.abs(target), min=epsilon)
 
     sum_abs_per_error = torch.sum(abs_per_error)
 
     num_obs = target.numel()
 
     return sum_abs_per_error, num_obs
 
 
-def _mean_absolute_percentage_error_compute(sum_abs_per_error: Tensor, num_obs: int) -> Tensor:
-    """Computes Mean Absolute Percentage Error.
+def _mean_absolute_percentage_error_compute(sum_abs_per_error: Tensor, num_obs: Union[int, Tensor]) -> Tensor:
+    """Compute Mean Absolute Percentage Error.
 
     Args:
         sum_abs_per_error: Sum of absolute value of percentage errors over all observations
             ``(percentage error = (target - prediction) / target)``
         num_obs: Number of predictions or observations
 
     Example:
         >>> target = torch.tensor([1, 10, 1e6])
         >>> preds = torch.tensor([0.9, 15, 1.2e6])
         >>> sum_abs_per_error, num_obs = _mean_absolute_percentage_error_update(preds, target)
         >>> _mean_absolute_percentage_error_compute(sum_abs_per_error, num_obs)
         tensor(0.2667)
     """
-
     return sum_abs_per_error / num_obs
 
 
 def mean_absolute_percentage_error(preds: Tensor, target: Tensor) -> Tensor:
-    """Computes mean absolute percentage error.
+    """Compute mean absolute percentage error.
 
     Args:
         preds: estimated labels
         target: ground truth labels
 
     Return:
         Tensor with MAPE
 
     Note:
         The epsilon value is taken from `scikit-learn's implementation of MAPE`_.
 
     Example:
-        >>> from torchmetrics.functional import mean_absolute_percentage_error
+        >>> from torchmetrics.functional.regression import mean_absolute_percentage_error
         >>> target = torch.tensor([1, 10, 1e6])
         >>> preds = torch.tensor([0.9, 15, 1.2e6])
         >>> mean_absolute_percentage_error(preds, target)
         tensor(0.2667)
     """
     sum_abs_per_error, num_obs = _mean_absolute_percentage_error_update(preds, target)
-    mean_ape = _mean_absolute_percentage_error_compute(sum_abs_per_error, num_obs)
-
-    return mean_ape
+    return _mean_absolute_percentage_error_compute(sum_abs_per_error, num_obs)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/regression/mse.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/mse.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,46 +1,46 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Tuple
+from typing import Tuple, Union
 
 import torch
 from torch import Tensor
 
 from torchmetrics.utilities.checks import _check_same_shape
 
 
 def _mean_squared_error_update(preds: Tensor, target: Tensor) -> Tuple[Tensor, int]:
-    """Updates and returns variables required to compute Mean Squared Error.
+    """Update and returns variables required to compute Mean Squared Error.
 
-    Checks for same shape of input tensors.
+    Check for same shape of input tensors.
 
     Args:
         preds: Predicted tensor
         target: Ground truth tensor
     """
     _check_same_shape(preds, target)
     diff = preds - target
     sum_squared_error = torch.sum(diff * diff)
     n_obs = target.numel()
     return sum_squared_error, n_obs
 
 
-def _mean_squared_error_compute(sum_squared_error: Tensor, n_obs: int, squared: bool = True) -> Tensor:
-    """Computes Mean Squared Error.
+def _mean_squared_error_compute(sum_squared_error: Tensor, n_obs: Union[int, Tensor], squared: bool = True) -> Tensor:
+    """Compute Mean Squared Error.
 
     Args:
         sum_squared_error: Sum of square of errors over all observations
         n_obs: Number of predictions or observations
         squared: Returns RMSE value if set to False.
 
     Example:
@@ -50,26 +50,26 @@
         >>> _mean_squared_error_compute(sum_squared_error, n_obs)
         tensor(0.2500)
     """
     return sum_squared_error / n_obs if squared else torch.sqrt(sum_squared_error / n_obs)
 
 
 def mean_squared_error(preds: Tensor, target: Tensor, squared: bool = True) -> Tensor:
-    """Computes mean squared error.
+    """Compute mean squared error.
 
     Args:
         preds: estimated labels
         target: ground truth labels
         squared: returns RMSE value if set to False
 
     Return:
         Tensor with MSE
 
     Example:
-        >>> from torchmetrics.functional import mean_squared_error
+        >>> from torchmetrics.functional.regression import mean_squared_error
         >>> x = torch.tensor([0., 1, 2, 3])
         >>> y = torch.tensor([0., 1, 2, 2])
         >>> mean_squared_error(x, y)
         tensor(0.2500)
     """
     sum_squared_error, n_obs = _mean_squared_error_update(preds, target)
     return _mean_squared_error_compute(sum_squared_error, n_obs, squared=squared)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/regression/pearson.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/pearson.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,66 +12,80 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing import Tuple
 
 import torch
 from torch import Tensor
 
+from torchmetrics.functional.regression.utils import _check_data_shape_to_num_outputs
 from torchmetrics.utilities.checks import _check_same_shape
 
 
 def _pearson_corrcoef_update(
     preds: Tensor,
     target: Tensor,
     mean_x: Tensor,
     mean_y: Tensor,
     var_x: Tensor,
     var_y: Tensor,
     corr_xy: Tensor,
     n_prior: Tensor,
+    num_outputs: int,
 ) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]:
-    """Updates and returns variables required to compute Pearson Correlation Coefficient.
+    """Update and returns variables required to compute Pearson Correlation Coefficient.
 
-    Checks for same shape of input tensors.
+    Check for same shape of input tensors.
 
     Args:
+        preds: estimated scores
+        target: ground truth scores
         mean_x: current mean estimate of x tensor
         mean_y: current mean estimate of y tensor
         var_x: current variance estimate of x tensor
         var_y: current variance estimate of y tensor
         corr_xy: current covariance estimate between x and y tensor
         n_prior: current number of observed observations
+        num_outputs: Number of outputs in multioutput setting
     """
     # Data checking
     _check_same_shape(preds, target)
-    preds = preds.squeeze()
-    target = target.squeeze()
-    if preds.ndim > 1 or target.ndim > 1:
-        raise ValueError("Expected both predictions and target to be 1 dimensional tensors.")
-
-    n_obs = preds.numel()
-    mx_new = (n_prior * mean_x + preds.mean() * n_obs) / (n_prior + n_obs)
-    my_new = (n_prior * mean_y + target.mean() * n_obs) / (n_prior + n_obs)
+    _check_data_shape_to_num_outputs(preds, target, num_outputs)
+    cond = n_prior.mean() > 0
+
+    n_obs = preds.shape[0]
+    if cond:
+        mx_new = (n_prior * mean_x + preds.sum(0)) / (n_prior + n_obs)
+        my_new = (n_prior * mean_y + target.sum(0)) / (n_prior + n_obs)
+    else:
+        mx_new = preds.mean(0)
+        my_new = target.mean(0)
+
     n_prior += n_obs
-    var_x += ((preds - mx_new) * (preds - mean_x)).sum()
-    var_y += ((target - my_new) * (target - mean_y)).sum()
-    corr_xy += ((preds - mx_new) * (target - mean_y)).sum()
+
+    if cond:
+        var_x += ((preds - mx_new) * (preds - mean_x)).sum(0)
+        var_y += ((target - my_new) * (target - mean_y)).sum(0)
+
+    else:
+        var_x += preds.var(0) * (n_obs - 1)
+        var_y += target.var(0) * (n_obs - 1)
+    corr_xy += ((preds - mx_new) * (target - mean_y)).sum(0)
     mean_x = mx_new
     mean_y = my_new
 
     return mean_x, mean_y, var_x, var_y, corr_xy, n_prior
 
 
 def _pearson_corrcoef_compute(
     var_x: Tensor,
     var_y: Tensor,
     corr_xy: Tensor,
     nb: Tensor,
 ) -> Tensor:
-    """Computes the final pearson correlation based on accumulated statistics.
+    """Compute the final pearson correlation based on accumulated statistics.
 
     Args:
         var_x: variance estimate of x tensor
         var_y: variance estimate of y tensor
         corr_xy: covariance estimate between x and y tensor
         nb: number of observations
     """
@@ -79,25 +93,35 @@
     var_y /= nb - 1
     corr_xy /= nb - 1
     corrcoef = (corr_xy / (var_x * var_y).sqrt()).squeeze()
     return torch.clamp(corrcoef, -1.0, 1.0)
 
 
 def pearson_corrcoef(preds: Tensor, target: Tensor) -> Tensor:
-    """Computes pearson correlation coefficient.
+    """Compute pearson correlation coefficient.
 
     Args:
         preds: estimated scores
         target: ground truth scores
 
-    Example:
-        >>> from torchmetrics.functional import pearson_corrcoef
+    Example (single output regression):
+        >>> from torchmetrics.functional.regression import pearson_corrcoef
         >>> target = torch.tensor([3, -0.5, 2, 7])
         >>> preds = torch.tensor([2.5, 0.0, 2, 8])
         >>> pearson_corrcoef(preds, target)
         tensor(0.9849)
+
+    Example (multi output regression):
+        >>> from torchmetrics.functional.regression import pearson_corrcoef
+        >>> target = torch.tensor([[3, -0.5], [2, 7]])
+        >>> preds = torch.tensor([[2.5, 0.0], [2, 8]])
+        >>> pearson_corrcoef(preds, target)
+        tensor([1., 1.])
     """
-    _temp = torch.zeros(1, dtype=preds.dtype, device=preds.device)
+    d = preds.shape[1] if preds.ndim == 2 else 1
+    _temp = torch.zeros(d, dtype=preds.dtype, device=preds.device)
     mean_x, mean_y, var_x = _temp.clone(), _temp.clone(), _temp.clone()
     var_y, corr_xy, nb = _temp.clone(), _temp.clone(), _temp.clone()
-    _, _, var_x, var_y, corr_xy, nb = _pearson_corrcoef_update(preds, target, mean_x, mean_y, var_x, var_y, corr_xy, nb)
+    _, _, var_x, var_y, corr_xy, nb = _pearson_corrcoef_update(
+        preds, target, mean_x, mean_y, var_x, var_y, corr_xy, nb, num_outputs=1 if preds.ndim == 1 else preds.shape[-1]
+    )
     return _pearson_corrcoef_compute(var_x, var_y, corr_xy, nb)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/regression/r2.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/r2.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,64 +1,62 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Tuple
+from typing import Tuple, Union
 
 import torch
 from torch import Tensor
 
 from torchmetrics.utilities import rank_zero_warn
 from torchmetrics.utilities.checks import _check_same_shape
 
 
-def _r2_score_update(preds: Tensor, target: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
-    """Updates and returns variables required to compute R2 score.
+def _r2_score_update(preds: Tensor, target: Tensor) -> Tuple[Tensor, Tensor, Tensor, int]:
+    """Update and returns variables required to compute R2 score.
 
-    Checks for same shape and 1D/2D input tensors.
+    Check for same shape and 1D/2D input tensors.
 
     Args:
         preds: Predicted tensor
         target: Ground truth tensor
     """
-
     _check_same_shape(preds, target)
     if preds.ndim > 2:
         raise ValueError(
             "Expected both prediction and target to be 1D or 2D tensors,"
             f" but received tensors with dimension {preds.shape}"
         )
 
     sum_obs = torch.sum(target, dim=0)
     sum_squared_obs = torch.sum(target * target, dim=0)
     residual = target - preds
     rss = torch.sum(residual * residual, dim=0)
     n_obs = target.size(0)
-
     return sum_squared_obs, sum_obs, rss, n_obs
 
 
 def _r2_score_compute(
     sum_squared_obs: Tensor,
     sum_obs: Tensor,
     rss: Tensor,
-    n_obs: Tensor,
+    n_obs: Union[int, Tensor],
     adjusted: int = 0,
     multioutput: str = "uniform_average",
 ) -> Tensor:
-    """Computes R2 score.
+    """Compute R2 score.
 
     Args:
         sum_squared_obs: Sum of square of all observations
         sum_obs: Sum of all observations
         rss: Residual sum of squares
         n_obs: Number of predictions or observations
         adjusted: number of independent regressors for calculating adjusted r2 score.
@@ -76,15 +74,23 @@
         tensor([0.9654, 0.9082])
     """
     if n_obs < 2:
         raise ValueError("Needs at least two samples to calculate r2 score.")
 
     mean_obs = sum_obs / n_obs
     tss = sum_squared_obs - sum_obs * mean_obs
-    raw_scores = 1 - (rss / tss)
+
+    # Account for near constant targets
+    cond_rss = ~torch.isclose(rss, torch.zeros_like(rss), atol=1e-4)
+    cond_tss = ~torch.isclose(tss, torch.zeros_like(tss), atol=1e-4)
+    cond = cond_rss & cond_tss
+
+    raw_scores = torch.ones_like(rss)
+    raw_scores[cond] = 1 - (rss[cond] / tss[cond])
+    raw_scores[cond_rss & ~cond_tss] = 0.0
 
     if multioutput == "raw_values":
         r2 = raw_scores
     elif multioutput == "uniform_average":
         r2 = torch.mean(raw_scores)
     elif multioutput == "variance_weighted":
         tss_sum = torch.sum(tss)
@@ -104,26 +110,25 @@
                 "More independent regressions than data points in"
                 " adjusted r2 score. Falls back to standard r2 score.",
                 UserWarning,
             )
         elif adjusted == n_obs - 1:
             rank_zero_warn("Division by zero in adjusted r2 score. Falls back to" " standard r2 score.", UserWarning)
         else:
-            r2 = 1 - (1 - r2) * (n_obs - 1) / (n_obs - adjusted - 1)
+            return 1 - (1 - r2) * (n_obs - 1) / (n_obs - adjusted - 1)
     return r2
 
 
 def r2_score(
     preds: Tensor,
     target: Tensor,
     adjusted: int = 0,
     multioutput: str = "uniform_average",
 ) -> Tensor:
-    r"""
-    Computes r2 score also known as `R2 Score_Coefficient Determination`_:
+    r"""Compute r2 score also known as `R2 Score_Coefficient Determination`_.
 
     .. math:: R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
 
     where :math:`SS_{res}=\sum_i (y_i - f(x_i))^2` is the sum of residual squares, and
     :math:`SS_{tot}=\sum_i (y_i - \bar{y})^2` is total sum of squares. Can also calculate
     adjusted r2 score given by
 
@@ -149,21 +154,20 @@
             If ``len(preds)`` is less than ``2`` since at least ``2`` sampels are needed to calculate r2 score.
         ValueError:
             If ``multioutput`` is not one of ``raw_values``, ``uniform_average`` or ``variance_weighted``.
         ValueError:
             If ``adjusted`` is not an ``integer`` greater than ``0``.
 
     Example:
-        >>> from torchmetrics.functional import r2_score
+        >>> from torchmetrics.functional.regression import r2_score
         >>> target = torch.tensor([3, -0.5, 2, 7])
         >>> preds = torch.tensor([2.5, 0.0, 2, 8])
         >>> r2_score(preds, target)
         tensor(0.9486)
 
         >>> target = torch.tensor([[0.5, 1], [-1, 1], [7, -6]])
         >>> preds = torch.tensor([[0, 2], [-1, 2], [8, -5]])
         >>> r2_score(preds, target, multioutput='raw_values')
         tensor([0.9654, 0.9082])
-
     """
     sum_squared_obs, sum_obs, rss, n_obs = _r2_score_update(preds, target)
     return _r2_score_compute(sum_squared_obs, sum_obs, rss, n_obs, adjusted, multioutput)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/regression/spearman.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/spearman.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,19 +12,20 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing import Tuple
 
 import torch
 from torch import Tensor
 
+from torchmetrics.functional.regression.utils import _check_data_shape_to_num_outputs
 from torchmetrics.utilities.checks import _check_same_shape
 
 
 def _find_repeats(data: Tensor) -> Tensor:
-    """find and return values which have repeats i.e. the same value are more than once in the tensor."""
+    """Find and return values which have repeats i.e. the same value are more than once in the tensor."""
     temp = data.detach().clone()
     temp = temp.sort()[0]
 
     change = torch.cat([torch.tensor([True], device=temp.device), temp[1:] != temp[:-1]])
     unique = temp[change]
     change_idx = torch.cat([torch.nonzero(change), torch.tensor([[temp.numel()]], device=temp.device)]).flatten()
     freq = change_idx[1:] - change_idx[:-1]
@@ -48,84 +49,89 @@
     repeats = _find_repeats(data)
     for r in repeats:
         condition = data == r
         rank[condition] = rank[condition].mean()
     return rank
 
 
-def _spearman_corrcoef_update(preds: Tensor, target: Tensor) -> Tuple[Tensor, Tensor]:
-    """Updates and returns variables required to compute Spearman Correlation Coefficient.
+def _spearman_corrcoef_update(preds: Tensor, target: Tensor, num_outputs: int) -> Tuple[Tensor, Tensor]:
+    """Update and returns variables required to compute Spearman Correlation Coefficient.
 
-    Checks for same shape and type of input tensors.
+    Check for same shape and type of input tensors.
 
     Args:
         preds: Predicted tensor
         target: Ground truth tensor
+        num_outputs: Number of outputs in multioutput setting
     """
-
-    if preds.dtype != target.dtype:
+    if not (preds.is_floating_point() and target.is_floating_point()):
         raise TypeError(
-            "Expected `preds` and `target` to have the same data type."
-            f" Got preds: {preds.dtype} and target: {target.dtype}."
+            "Expected `preds` and `target` both to be floating point tensors, but got {pred.dtype} and {target.dtype}"
         )
     _check_same_shape(preds, target)
-    preds = preds.squeeze()
-    target = target.squeeze()
-    if preds.ndim > 1 or target.ndim > 1:
-        raise ValueError("Expected both predictions and target to be 1 dimensional tensors.")
+    _check_data_shape_to_num_outputs(preds, target, num_outputs)
+
     return preds, target
 
 
 def _spearman_corrcoef_compute(preds: Tensor, target: Tensor, eps: float = 1e-6) -> Tensor:
-    """Computes Spearman Correlation Coefficient.
+    """Compute Spearman Correlation Coefficient.
 
     Args:
         preds: Predicted tensor
         target: Ground truth tensor
         eps: Avoids ``ZeroDivisionError``.
 
     Example:
         >>> target = torch.tensor([3, -0.5, 2, 7])
         >>> preds = torch.tensor([2.5, 0.0, 2, 8])
-        >>> preds, target = _spearman_corrcoef_update(preds, target)
+        >>> preds, target = _spearman_corrcoef_update(preds, target, num_outputs=1)
         >>> _spearman_corrcoef_compute(preds, target)
         tensor(1.0000)
     """
-
-    preds = _rank_data(preds)
-    target = _rank_data(target)
-
-    preds_diff = preds - preds.mean()
-    target_diff = target - target.mean()
-
-    cov = (preds_diff * target_diff).mean()
-    preds_std = torch.sqrt((preds_diff * preds_diff).mean())
-    target_std = torch.sqrt((target_diff * target_diff).mean())
+    if preds.ndim == 1:
+        preds = _rank_data(preds)
+        target = _rank_data(target)
+    else:
+        preds = torch.stack([_rank_data(p) for p in preds.T]).T
+        target = torch.stack([_rank_data(t) for t in target.T]).T
+
+    preds_diff = preds - preds.mean(0)
+    target_diff = target - target.mean(0)
+
+    cov = (preds_diff * target_diff).mean(0)
+    preds_std = torch.sqrt((preds_diff * preds_diff).mean(0))
+    target_std = torch.sqrt((target_diff * target_diff).mean(0))
 
     corrcoef = cov / (preds_std * target_std + eps)
     return torch.clamp(corrcoef, -1.0, 1.0)
 
 
 def spearman_corrcoef(preds: Tensor, target: Tensor) -> Tensor:
-    r"""
-     Computes `spearmans rank correlation coefficient`_:
+    r"""Compute `spearmans rank correlation coefficient`_.
 
     .. math:
         r_s = = \frac{cov(rg_x, rg_y)}{\sigma_{rg_x} * \sigma_{rg_y}}
 
     where :math:`rg_x` and :math:`rg_y` are the rank associated to the variables x and y. Spearmans correlations
     coefficient corresponds to the standard pearsons correlation coefficient calculated on the rank variables.
 
     Args:
         preds: estimated scores
         target: ground truth scores
 
-    Example:
-        >>> from torchmetrics.functional import spearman_corrcoef
+    Example (single output regression):
+        >>> from torchmetrics.functional.regression import spearman_corrcoef
         >>> target = torch.tensor([3, -0.5, 2, 7])
         >>> preds = torch.tensor([2.5, 0.0, 2, 8])
         >>> spearman_corrcoef(preds, target)
         tensor(1.0000)
 
+    Example (multi output regression):
+        >>> from torchmetrics.functional.regression import spearman_corrcoef
+        >>> target = torch.tensor([[3, -0.5], [2, 7]])
+        >>> preds = torch.tensor([[2.5, 0.0], [2, 8]])
+        >>> spearman_corrcoef(preds, target)
+        tensor([1.0000, 1.0000])
     """
-    preds, target = _spearman_corrcoef_update(preds, target)
+    preds, target = _spearman_corrcoef_update(preds, target, num_outputs=1 if preds.ndim == 1 else preds.shape[-1])
     return _spearman_corrcoef_compute(preds, target)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/regression/symmetric_mape.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/symmetric_mape.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,100 +1,94 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Tuple
+from typing import Tuple, Union
 
 import torch
 from torch import Tensor
 
 from torchmetrics.utilities.checks import _check_same_shape
 
 
 def _symmetric_mean_absolute_percentage_error_update(
     preds: Tensor,
     target: Tensor,
     epsilon: float = 1.17e-06,
 ) -> Tuple[Tensor, int]:
-    """Updates and returns variables required to compute Symmetric Mean Absolute Percentage Error.
+    """Update and returns variables required to compute Symmetric Mean Absolute Percentage Error.
 
-    Checks for same shape of input tensors.
+    Check for same shape of input tensors.
 
     Args:
         preds: Predicted tensor
         target: Ground truth tensor
         epsilon: Avoids ``ZeroDivisionError``.
     """
-
     _check_same_shape(preds, target)
 
     abs_diff = torch.abs(preds - target)
     abs_per_error = abs_diff / torch.clamp(torch.abs(target) + torch.abs(preds), min=epsilon)
 
     sum_abs_per_error = 2 * torch.sum(abs_per_error)
 
     num_obs = target.numel()
 
     return sum_abs_per_error, num_obs
 
 
-def _symmetric_mean_absolute_percentage_error_compute(sum_abs_per_error: Tensor, num_obs: int) -> Tensor:
-    """Computes Symmetric Mean Absolute Percentage Error.
+def _symmetric_mean_absolute_percentage_error_compute(sum_abs_per_error: Tensor, num_obs: Union[int, Tensor]) -> Tensor:
+    """Compute Symmetric Mean Absolute Percentage Error.
 
     Args:
         sum_abs_per_error: Sum of values of symmetric absolute percentage errors over all observations
             ``(symmetric absolute percentage error = 2 * |target - prediction| / (target + prediction))``
         num_obs: Number of predictions or observations
 
     Example:
         >>> target = torch.tensor([1, 10, 1e6])
         >>> preds = torch.tensor([0.9, 15, 1.2e6])
         >>> sum_abs_per_error, num_obs = _symmetric_mean_absolute_percentage_error_update(preds, target)
         >>> _symmetric_mean_absolute_percentage_error_compute(sum_abs_per_error, num_obs)
         tensor(0.2290)
     """
-
     return sum_abs_per_error / num_obs
 
 
 def symmetric_mean_absolute_percentage_error(preds: Tensor, target: Tensor) -> Tensor:
-    r"""
-    Computes symmetric mean absolute percentage error (SMAPE_):
+    r"""Compute symmetric mean absolute percentage error (SMAPE_).
 
     .. math:: \text{SMAPE} = \frac{2}{n}\sum_1^n\frac{|   y_i - \hat{y_i} |}{max(| y_i | + | \hat{y_i} |, \epsilon)}
 
     Where :math:`y` is a tensor of target values, and :math:`\hat{y}` is a tensor of predictions.
 
     Args:
         preds: estimated labels
         target: ground truth labels
 
     Return:
         Tensor with SMAPE.
 
     Example:
-        >>> from torchmetrics.functional import symmetric_mean_absolute_percentage_error
+        >>> from torchmetrics.functional.regression import symmetric_mean_absolute_percentage_error
         >>> target = torch.tensor([1, 10, 1e6])
         >>> preds = torch.tensor([0.9, 15, 1.2e6])
         >>> symmetric_mean_absolute_percentage_error(preds, target)
         tensor(0.2290)
-
     """
     sum_abs_per_error, num_obs = _symmetric_mean_absolute_percentage_error_update(
         preds,
         target,
     )
-    mean_ape = _symmetric_mean_absolute_percentage_error_compute(
+    return _symmetric_mean_absolute_percentage_error_compute(
         sum_abs_per_error,
         num_obs,
     )
-
-    return mean_ape
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/regression/tweedie_deviance.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/tweedie_deviance.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -17,17 +17,17 @@
 from torch import Tensor
 
 from torchmetrics.utilities.checks import _check_same_shape
 from torchmetrics.utilities.compute import _safe_xlogy
 
 
 def _tweedie_deviance_score_update(preds: Tensor, targets: Tensor, power: float = 0.0) -> Tuple[Tensor, Tensor]:
-    """Updates and returns variables required to compute Deviance Score for the given power.
+    """Update and returns variables required to compute Deviance Score for the given power.
 
-    Checks for same shape of input tensors.
+    Check for same shape of input tensors.
 
     Args:
         preds: Predicted tensor
         targets: Ground truth tensor
         power: see :func:`tweedie_deviance_score`
 
     Example:
@@ -80,61 +80,60 @@
     sum_deviance_score = torch.sum(deviance_score)
     num_observations = torch.tensor(torch.numel(deviance_score), device=preds.device)
 
     return sum_deviance_score, num_observations
 
 
 def _tweedie_deviance_score_compute(sum_deviance_score: Tensor, num_observations: Tensor) -> Tensor:
-    """Computes Deviance Score.
+    """Compute Deviance Score.
 
     Args:
         sum_deviance_score: Sum of deviance scores accumalated until now.
         num_observations: Number of observations encountered until now.
 
     Example:
         >>> targets = torch.tensor([1.0, 2.0, 3.0, 4.0])
         >>> preds = torch.tensor([4.0, 3.0, 2.0, 1.0])
         >>> sum_deviance_score, num_observations = _tweedie_deviance_score_update(preds, targets, power=2)
         >>> _tweedie_deviance_score_compute(sum_deviance_score, num_observations)
         tensor(1.2083)
     """
-
     return sum_deviance_score / num_observations
 
 
 def tweedie_deviance_score(preds: Tensor, targets: Tensor, power: float = 0.0) -> Tensor:
-    r"""Computes the `Tweedie Deviance Score`_ between targets and predictions:
+    r"""Compute the `Tweedie Deviance Score`_.
 
     .. math::
         deviance\_score(\hat{y},y) =
         \begin{cases}
-        (\hat{y} - y)^2, & \text{for }power=0\\
-        2 * (y * log(\frac{y}{\hat{y}}) + \hat{y} - y),  & \text{for }power=1\\
-        2 * (log(\frac{\hat{y}}{y}) + \frac{y}{\hat{y}} - 1),  & \text{for }power=2\\
-        2 * (\frac{(max(y,0))^{2}}{(1 - power)(2 - power)} - \frac{y(\hat{y})^{1 - power}}{1 - power} + \frac{(\hat{y})
-            ^{2 - power}}{2 - power}), & \text{otherwise}
+        (\hat{y} - y)^2, & \text{for }p=0\\
+        2 * (y * log(\frac{y}{\hat{y}}) + \hat{y} - y),  & \text{for }p=1\\
+        2 * (log(\frac{\hat{y}}{y}) + \frac{y}{\hat{y}} - 1),  & \text{for }p=2\\
+        2 * (\frac{(max(y,0))^{2 - p}}{(1 - p)(2 - p)} - \frac{y(\hat{y})^{1 - p}}{1 - p} + \frac{(
+            \hat{y})^{2 - p}}{2 - p}), & \text{otherwise}
         \end{cases}
 
-    where :math:`y` is a tensor of targets values, and :math:`\hat{y}` is a tensor of predictions.
+    where :math:`y` is a tensor of targets values, :math:`\hat{y}` is a tensor of predictions, and
+    :math:`p` is the `power`.
 
     Args:
         preds: Predicted tensor with shape ``(N,...)``
         targets: Ground truth tensor with shape ``(N,...)``
         power:
             - `power < 0` : Extreme stable distribution. (Requires: preds > 0.)
             - `power = 0` : Normal distribution. (Requires: targets and preds can be any real numbers.)
             - `power = 1` : Poisson distribution. (Requires: targets >= 0 and y_pred > 0.)
             - `1 < p < 2` : Compound Poisson distribution. (Requires: targets >= 0 and preds > 0.)
             - `power = 2` : Gamma distribution. (Requires: targets > 0 and preds > 0.)
             - `power = 3` : Inverse Gaussian distribution. (Requires: targets > 0 and preds > 0.)
             - `otherwise` : Positive stable distribution. (Requires: targets > 0 and preds > 0.)
 
     Example:
-        >>> from torchmetrics.functional import tweedie_deviance_score
+        >>> from torchmetrics.functional.regression import tweedie_deviance_score
         >>> targets = torch.tensor([1.0, 2.0, 3.0, 4.0])
         >>> preds = torch.tensor([4.0, 3.0, 2.0, 1.0])
         >>> tweedie_deviance_score(preds, targets, power=2)
         tensor(1.2083)
-
     """
     sum_deviance_score, num_observations = _tweedie_deviance_score_update(preds, targets, power=power)
     return _tweedie_deviance_score_compute(sum_deviance_score, num_observations)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/regression/wmape.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/regression/wmape.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -18,49 +18,48 @@
 
 from torchmetrics.utilities.checks import _check_same_shape
 
 
 def _weighted_mean_absolute_percentage_error_update(
     preds: Tensor,
     target: Tensor,
-) -> Tuple[Tensor, int]:
-    """Updates and returns variables required to compute Weighted Absolute Percentage Error.
+) -> Tuple[Tensor, Tensor]:
+    """Update and returns variables required to compute Weighted Absolute Percentage Error.
 
-    Checks for same shape of input tensors.
+    Check for same shape of input tensors.
 
     Args:
         preds: Predicted tensor
         target: Ground truth tensor
     """
-
     _check_same_shape(preds, target)
 
     sum_abs_error = (preds - target).abs().sum()
     sum_scale = target.abs().sum()
 
     return sum_abs_error, sum_scale
 
 
 def _weighted_mean_absolute_percentage_error_compute(
     sum_abs_error: Tensor,
     sum_scale: Tensor,
     epsilon: float = 1.17e-06,
 ) -> Tensor:
-    """Computes Weighted Absolute Percentage Error.
+    """Compute Weighted Absolute Percentage Error.
 
     Args:
         sum_abs_error: scalar with sum of absolute errors
         sum_scale: scalar with sum of target values
         epsilon: small float to prevent division by zero
     """
     return sum_abs_error / torch.clamp(sum_scale, min=epsilon)
 
 
 def weighted_mean_absolute_percentage_error(preds: Tensor, target: Tensor) -> Tensor:
-    r"""Computes weighted mean absolute percentage error (`WMAPE`_).
+    r"""Compute weighted mean absolute percentage error (`WMAPE`_).
 
     The output of WMAPE metric is a non-negative floating point, where the optimal value is 0. It is computes as:
 
     .. math::
         \text{WMAPE} = \frac{\sum_{t=1}^n | y_t - \hat{y}_t | }{\sum_{t=1}^n |y_t| }
 
     Where :math:`y` is a tensor of target values, and :math:`\hat{y}` is a tensor of predictions.
@@ -75,19 +74,10 @@
     Example:
         >>> import torch
         >>> _ = torch.manual_seed(42)
         >>> preds = torch.randn(20,)
         >>> target = torch.randn(20,)
         >>> weighted_mean_absolute_percentage_error(preds, target)
         tensor(1.3967)
-
     """
-    sum_abs_error, sum_scale = _weighted_mean_absolute_percentage_error_update(
-        preds,
-        target,
-    )
-    weighted_ape = _weighted_mean_absolute_percentage_error_compute(
-        sum_abs_error,
-        sum_scale,
-    )
-
-    return weighted_ape
+    sum_abs_error, sum_scale = _weighted_mean_absolute_percentage_error_update(preds, target)
+    return _weighted_mean_absolute_percentage_error_compute(sum_abs_error, sum_scale)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/retrieval/average_precision.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/hit_rate.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,49 +1,60 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+from typing import Optional
+
 import torch
-from torch import Tensor, tensor
+from torch import Tensor
 
 from torchmetrics.utilities.checks import _check_retrieval_functional_inputs
 
 
-def retrieval_average_precision(preds: Tensor, target: Tensor) -> Tensor:
-    """Computes average precision (for information retrieval), as explained in `IR Average precision`_.
+def retrieval_hit_rate(preds: Tensor, target: Tensor, top_k: Optional[int] = None) -> Tensor:
+    """Compute the hit rate for information retrieval.
+
+    The hit rate is 1.0 if there is at least one relevant document among all the top `k` retrieved documents.
 
     ``preds`` and ``target`` should be of the same shape and live on the same device. If no ``target`` is ``True``,
     ``0`` is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
-    otherwise an error is raised.
+    otherwise an error is raised. If you want to measure HitRate@K, ``top_k`` must be a positive integer.
 
     Args:
         preds: estimated probabilities of each document to be relevant.
         target: ground truth about each document being relevant or not.
+        top_k: consider only the top k elements (default: `None`, which considers them all)
 
-    Return:
-        a single-value tensor with the average precision (AP) of the predictions ``preds`` w.r.t. the labels ``target``.
+    Returns:
+        A single-value tensor with the hit rate (at ``top_k``) of the predictions ``preds`` w.r.t. the labels
+          ``target``.
+
+    Raises:
+        ValueError:
+            If ``top_k`` parameter is not `None` or an integer larger than 0
 
     Example:
-        >>> from torchmetrics.functional import retrieval_average_precision
+        >>> from torch import tensor
         >>> preds = tensor([0.2, 0.3, 0.5])
         >>> target = tensor([True, False, True])
-        >>> retrieval_average_precision(preds, target)
-        tensor(0.8333)
+        >>> retrieval_hit_rate(preds, target, top_k=2)
+        tensor(1.)
     """
     preds, target = _check_retrieval_functional_inputs(preds, target)
 
-    if not target.sum():
-        return tensor(0.0, device=preds.device)
+    if top_k is None:
+        top_k = preds.shape[-1]
+
+    if not (isinstance(top_k, int) and top_k > 0):
+        raise ValueError("`top_k` has to be a positive integer or None")
 
-    target = target[torch.argsort(preds, dim=-1, descending=True)]
-    positions = torch.arange(1, len(target) + 1, device=target.device, dtype=torch.float32)[target > 0]
-    res = torch.div((torch.arange(len(positions), device=positions.device, dtype=torch.float32) + 1), positions).mean()
-    return res
+    relevant = target[torch.argsort(preds, dim=-1, descending=True)][:top_k].sum()
+    return (relevant > 0).float()
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/retrieval/fall_out.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/precision.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,62 +1,67 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing import Optional
 
-import torch
 from torch import Tensor, tensor
 
 from torchmetrics.utilities.checks import _check_retrieval_functional_inputs
 
 
-def retrieval_fall_out(preds: Tensor, target: Tensor, k: Optional[int] = None) -> Tensor:
-    """Computes the Fall-out (for information retrieval), as explained in `IR Fall-out`_ Fall-out is the fraction
-    of non-relevant documents retrieved among all the non-relevant documents.
+def retrieval_precision(preds: Tensor, target: Tensor, top_k: Optional[int] = None, adaptive_k: bool = False) -> Tensor:
+    """Compute the precision metric for information retrieval.
+
+    Precision is the fraction of relevant documents among all the retrieved documents.
 
     ``preds`` and ``target`` should be of the same shape and live on the same device. If no ``target`` is ``True``,
     ``0`` is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
-    otherwise an error is raised. If you want to measure Fall-out@K, ``k`` must be a positive integer.
+    otherwise an error is raised. If you want to measure Precision@K, ``top_k`` must be a positive integer.
 
     Args:
         preds: estimated probabilities of each document to be relevant.
         target: ground truth about each document being relevant or not.
-        k: consider only the top k elements (default: ``None``, which considers them all)
+        top_k: consider only the top k elements (default: ``None``, which considers them all)
+        adaptive_k: adjust `k` to `min(k, number of documents)` for each query
 
     Returns:
-        a single-value tensor with the fall-out (at ``k``) of the predictions ``preds`` w.r.t. the labels ``target``.
+        A single-value tensor with the precision (at ``top_k``) of the predictions ``preds`` w.r.t. the labels
+          ``target``.
 
     Raises:
         ValueError:
-            If ``k`` parameter is not `None` or an integer larger than 0
+            If ``top_k`` is not `None` or an integer larger than 0.
+        ValueError:
+            If ``adaptive_k`` is not boolean.
 
     Example:
-        >>> from  torchmetrics.functional import retrieval_fall_out
         >>> preds = tensor([0.2, 0.3, 0.5])
         >>> target = tensor([True, False, True])
-        >>> retrieval_fall_out(preds, target, k=2)
-        tensor(1.)
+        >>> retrieval_precision(preds, target, top_k=2)
+        tensor(0.5000)
     """
     preds, target = _check_retrieval_functional_inputs(preds, target)
 
-    k = preds.shape[-1] if k is None else k
+    if not isinstance(adaptive_k, bool):
+        raise ValueError("`adaptive_k` has to be a boolean")
 
-    if not (isinstance(k, int) and k > 0):
-        raise ValueError("`k` has to be a positive integer or None")
+    if top_k is None or (adaptive_k and top_k > preds.shape[-1]):
+        top_k = preds.shape[-1]
 
-    target = 1 - target  # we want to compute the probability of getting a non-relevant doc among all non-relevant docs
+    if not (isinstance(top_k, int) and top_k > 0):
+        raise ValueError("`top_k` has to be a positive integer or None")
 
     if not target.sum():
         return tensor(0.0, device=preds.device)
 
-    relevant = target[torch.argsort(preds, dim=-1, descending=True)][:k].sum().float()
-    return relevant / target.sum()
+    relevant = target[preds.topk(min(top_k, preds.shape[-1]), dim=-1)[1]].sum().float()
+    return relevant / top_k
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/retrieval/hit_rate.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/recall.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -15,43 +15,48 @@
 
 import torch
 from torch import Tensor, tensor
 
 from torchmetrics.utilities.checks import _check_retrieval_functional_inputs
 
 
-def retrieval_hit_rate(preds: Tensor, target: Tensor, k: Optional[int] = None) -> Tensor:
-    """Computes the hit rate (for information retrieval). The hit rate is 1.0 if there is at least one relevant
-    document among all the top `k` retrieved documents.
+def retrieval_recall(preds: Tensor, target: Tensor, top_k: Optional[int] = None) -> Tensor:
+    """Compute the recall metric for information retrieval.
+
+    Recall is the fraction of relevant documents retrieved among all the relevant documents.
 
     ``preds`` and ``target`` should be of the same shape and live on the same device. If no ``target`` is ``True``,
     ``0`` is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
-    otherwise an error is raised. If you want to measure HitRate@K, ``k`` must be a positive integer.
+    otherwise an error is raised. If you want to measure Recall@K, ``top_k`` must be a positive integer.
 
     Args:
         preds: estimated probabilities of each document to be relevant.
         target: ground truth about each document being relevant or not.
-        k: consider only the top k elements (default: `None`, which considers them all)
+        top_k: consider only the top k elements (default: `None`, which considers them all)
 
     Returns:
-        a single-value tensor with the hit rate (at ``k``) of the predictions ``preds`` w.r.t. the labels ``target``.
+        A single-value tensor with the recall (at ``top_k``) of the predictions ``preds`` w.r.t. the labels ``target``.
 
     Raises:
         ValueError:
-            If ``k`` parameter is not `None` or an integer larger than 0
+            If ``top_k`` parameter is not `None` or an integer larger than 0
 
     Example:
+        >>> from  torchmetrics.functional import retrieval_recall
         >>> preds = tensor([0.2, 0.3, 0.5])
         >>> target = tensor([True, False, True])
-        >>> retrieval_hit_rate(preds, target, k=2)
-        tensor(1.)
+        >>> retrieval_recall(preds, target, top_k=2)
+        tensor(0.5000)
     """
     preds, target = _check_retrieval_functional_inputs(preds, target)
 
-    if k is None:
-        k = preds.shape[-1]
+    if top_k is None:
+        top_k = preds.shape[-1]
+
+    if not (isinstance(top_k, int) and top_k > 0):
+        raise ValueError("`top_k` has to be a positive integer or None")
 
-    if not (isinstance(k, int) and k > 0):
-        raise ValueError("`k` has to be a positive integer or None")
+    if not target.sum():
+        return tensor(0.0, device=preds.device)
 
-    relevant = target[torch.argsort(preds, dim=-1, descending=True)][:k].sum()
-    return (relevant > 0).float()
+    relevant = target[torch.argsort(preds, dim=-1, descending=True)][:top_k].sum().float()
+    return relevant / target.sum()
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/retrieval/ndcg.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/ndcg.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -16,54 +16,54 @@
 import torch
 from torch import Tensor
 
 from torchmetrics.utilities.checks import _check_retrieval_functional_inputs
 
 
 def _dcg(target: Tensor) -> Tensor:
-    """Computes Discounted Cumulative Gain for input tensor."""
+    """Compute Discounted Cumulative Gain for input tensor."""
     denom = torch.log2(torch.arange(target.shape[-1], device=target.device) + 2.0)
     return (target / denom).sum(dim=-1)
 
 
-def retrieval_normalized_dcg(preds: Tensor, target: Tensor, k: Optional[int] = None) -> Tensor:
-    """Computes `Normalized Discounted Cumulative Gain`_ (for information retrieval).
+def retrieval_normalized_dcg(preds: Tensor, target: Tensor, top_k: Optional[int] = None) -> Tensor:
+    """Compute `Normalized Discounted Cumulative Gain`_ (for information retrieval).
 
     ``preds`` and ``target`` should be of the same shape and live on the same device.
     ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
     otherwise an error is raised.
 
     Args:
         preds: estimated probabilities of each document to be relevant.
         target: ground truth about each document relevance.
-        k: consider only the top k elements (default: ``None``, which considers them all)
+        top_k: consider only the top k elements (default: ``None``, which considers them all)
 
     Return:
-        a single-value tensor with the nDCG of the predictions ``preds`` w.r.t. the labels ``target``.
+        A single-value tensor with the nDCG of the predictions ``preds`` w.r.t. the labels ``target``.
 
     Raises:
         ValueError:
-            If ``k`` parameter is not `None` or an integer larger than 0
+            If ``top_k`` parameter is not `None` or an integer larger than 0
 
     Example:
-        >>> from torchmetrics.functional import retrieval_normalized_dcg
+        >>> from torchmetrics.functional.retrieval import retrieval_normalized_dcg
         >>> preds = torch.tensor([.1, .2, .3, 4, 70])
         >>> target = torch.tensor([10, 0, 0, 1, 5])
         >>> retrieval_normalized_dcg(preds, target)
         tensor(0.6957)
     """
     preds, target = _check_retrieval_functional_inputs(preds, target, allow_non_binary_target=True)
 
-    k = preds.shape[-1] if k is None else k
+    top_k = preds.shape[-1] if top_k is None else top_k
 
-    if not (isinstance(k, int) and k > 0):
-        raise ValueError("`k` has to be a positive integer or None")
+    if not (isinstance(top_k, int) and top_k > 0):
+        raise ValueError("`top_k` has to be a positive integer or None")
 
-    sorted_target = target[torch.argsort(preds, dim=-1, descending=True)][:k]
-    ideal_target = torch.sort(target, descending=True)[0][:k]
+    sorted_target = target[torch.argsort(preds, dim=-1, descending=True)][:top_k]
+    ideal_target = torch.sort(target, descending=True)[0][:top_k]
 
     ideal_dcg = _dcg(ideal_target)
     target_dcg = _dcg(sorted_target)
 
     # filter undefined scores
     all_irrelevant = ideal_dcg == 0
     target_dcg[all_irrelevant] = 0
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/retrieval/precision.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/reciprocal_rank.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,65 +1,48 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Optional
-
+import torch
 from torch import Tensor, tensor
 
 from torchmetrics.utilities.checks import _check_retrieval_functional_inputs
 
 
-def retrieval_precision(preds: Tensor, target: Tensor, k: Optional[int] = None, adaptive_k: bool = False) -> Tensor:
-    """Computes the precision metric (for information retrieval). Precision is the fraction of relevant documents
-    among all the retrieved documents.
+def retrieval_reciprocal_rank(preds: Tensor, target: Tensor) -> Tensor:
+    """Compute reciprocal rank (for information retrieval). See `Mean Reciprocal Rank`_.
 
     ``preds`` and ``target`` should be of the same shape and live on the same device. If no ``target`` is ``True``,
-    ``0`` is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
-    otherwise an error is raised. If you want to measure Precision@K, ``k`` must be a positive integer.
+    0 is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
+    otherwise an error is raised.
 
     Args:
         preds: estimated probabilities of each document to be relevant.
         target: ground truth about each document being relevant or not.
-        k: consider only the top k elements (default: ``None``, which considers them all)
-        adaptive_k: adjust `k` to `min(k, number of documents)` for each query
-
-    Returns:
-        a single-value tensor with the precision (at ``k``) of the predictions ``preds`` w.r.t. the labels ``target``.
 
-    Raises:
-        ValueError:
-            If ``k`` is not `None` or an integer larger than 0.
-        ValueError:
-            If ``adaptive_k`` is not boolean.
+    Return:
+        a single-value tensor with the reciprocal rank (RR) of the predictions ``preds`` wrt the labels ``target``.
 
     Example:
-        >>> preds = tensor([0.2, 0.3, 0.5])
-        >>> target = tensor([True, False, True])
-        >>> retrieval_precision(preds, target, k=2)
+        >>> from torchmetrics.functional.retrieval import retrieval_reciprocal_rank
+        >>> preds = torch.tensor([0.2, 0.3, 0.5])
+        >>> target = torch.tensor([False, True, False])
+        >>> retrieval_reciprocal_rank(preds, target)
         tensor(0.5000)
     """
     preds, target = _check_retrieval_functional_inputs(preds, target)
 
-    if not isinstance(adaptive_k, bool):
-        raise ValueError("`adaptive_k` has to be a boolean")
-
-    if k is None or (adaptive_k and k > preds.shape[-1]):
-        k = preds.shape[-1]
-
-    if not (isinstance(k, int) and k > 0):
-        raise ValueError("`k` has to be a positive integer or None")
-
     if not target.sum():
         return tensor(0.0, device=preds.device)
 
-    relevant = target[preds.topk(min(k, preds.shape[-1]), dim=-1)[1]].sum().float()
-    return relevant / k
+    target = target[torch.argsort(preds, dim=-1, descending=True)]
+    position = torch.nonzero(target).view(-1)
+    return 1.0 / (position[0] + 1.0)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/retrieval/r_precision.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/retrieval/r_precision.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -14,28 +14,29 @@
 import torch
 from torch import Tensor, tensor
 
 from torchmetrics.utilities.checks import _check_retrieval_functional_inputs
 
 
 def retrieval_r_precision(preds: Tensor, target: Tensor) -> Tensor:
-    """Computes the r-precision metric (for information retrieval). R-Precision is the fraction of relevant
-    documents among all the top ``k`` retrieved documents where ``k`` is equal to the total number of relevant
-    documents.
+    """Compute the r-precision metric for information retrieval.
+
+    R-Precision is the fraction of relevant documents among all the top ``k`` retrieved documents where ``k`` is equal
+    to the total number of relevant documents.
 
     ``preds`` and ``target`` should be of the same shape and live on the same device. If no ``target`` is ``True``,
     ``0`` is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
-    otherwise an error is raised. If you want to measure Precision@K, ``k`` must be a positive integer.
+    otherwise an error is raised. If you want to measure Precision@K, ``top_k`` must be a positive integer.
 
     Args:
         preds: estimated probabilities of each document to be relevant.
         target: ground truth about each document being relevant or not.
 
     Returns:
-        a single-value tensor with the r-precision of the predictions ``preds`` w.r.t. the labels ``target``.
+        A single-value tensor with the r-precision of the predictions ``preds`` w.r.t. the labels ``target``.
 
     Example:
         >>> preds = tensor([0.2, 0.3, 0.5])
         >>> target = tensor([True, False, True])
         >>> retrieval_r_precision(preds, target)
         tensor(0.5000)
     """
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/retrieval/recall.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/detection/giou.py`

 * *Files 26% similar despite different names*

```diff
@@ -10,52 +10,73 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing import Optional
 
 import torch
-from torch import Tensor, tensor
 
-from torchmetrics.utilities.checks import _check_retrieval_functional_inputs
+from torchmetrics.utilities.imports import _TORCHVISION_AVAILABLE, _TORCHVISION_GREATER_EQUAL_0_8
 
+if _TORCHVISION_AVAILABLE and _TORCHVISION_GREATER_EQUAL_0_8:
+    from torchvision.ops import generalized_box_iou
+else:
+    generalized_box_iou = None
+    __doctest_skip__ = ["generalized_intersection_over_union"]
+
+__doctest_requires__ = {("generalized_intersection_over_union",): ["torchvision"]}
+
+
+def _giou_update(
+    preds: torch.Tensor, target: torch.Tensor, iou_threshold: Optional[float], replacement_val: float = 0
+) -> torch.Tensor:
+    iou = generalized_box_iou(preds, target)
+    if iou_threshold is not None:
+        iou[iou < iou_threshold] = replacement_val
+    return iou
+
+
+def _giou_compute(iou: torch.Tensor, labels_eq: bool = True) -> torch.Tensor:
+    if labels_eq:
+        return iou.diag().mean()
+    return iou.mean()
+
+
+def generalized_intersection_over_union(
+    preds: torch.Tensor,
+    target: torch.Tensor,
+    iou_threshold: Optional[float] = None,
+    replacement_val: float = 0,
+    aggregate: bool = True,
+) -> torch.Tensor:
+    r"""Compute `Generalized Intersection over Union <https://arxiv.org/abs/1902.09630>`_ between two sets of boxes.
 
-def retrieval_recall(preds: Tensor, target: Tensor, k: Optional[int] = None) -> Tensor:
-    """Computes the recall metric (for information retrieval). Recall is the fraction of relevant documents
-    retrieved among all the relevant documents.
-
-    ``preds`` and ``target`` should be of the same shape and live on the same device. If no ``target`` is ``True``,
-    ``0`` is returned. ``target`` must be either `bool` or `integers` and ``preds`` must be ``float``,
-    otherwise an error is raised. If you want to measure Recall@K, ``k`` must be a positive integer.
+    Both sets of boxes are expected to be in (x1, y1, x2, y2) format with 0 <= x1 < x2 and 0 <= y1 < y2.
 
     Args:
-        preds: estimated probabilities of each document to be relevant.
-        target: ground truth about each document being relevant or not.
-        k: consider only the top k elements (default: `None`, which considers them all)
-
-    Returns:
-        a single-value tensor with the recall (at ``k``) of the predictions ``preds`` w.r.t. the labels ``target``.
-
-    Raises:
-        ValueError:
-            If ``k`` parameter is not `None` or an integer larger than 0
+        preds:
+            The input tensor containing the predicted bounding boxes.
+        target:
+            The tensor containing the ground truth.
+        iou_threshold:
+            Optional IoU thresholds for evaluation. If set to `None` the threshold is ignored.
+        replacement_val:
+            Value to replace values under the threshold with.
+        aggregate:
+            Return the average value instead of the complete IoU matrix.
 
     Example:
-        >>> from  torchmetrics.functional import retrieval_recall
-        >>> preds = tensor([0.2, 0.3, 0.5])
-        >>> target = tensor([True, False, True])
-        >>> retrieval_recall(preds, target, k=2)
-        tensor(0.5000)
+        >>> import torch
+        >>> from torchmetrics.functional.detection import generalized_intersection_over_union
+        >>> preds = torch.Tensor([[100, 100, 200, 200]])
+        >>> target = torch.Tensor([[110, 110, 210, 210]])
+        >>> generalized_intersection_over_union(preds, target)
+        tensor(0.6641)
     """
-    preds, target = _check_retrieval_functional_inputs(preds, target)
-
-    if k is None:
-        k = preds.shape[-1]
-
-    if not (isinstance(k, int) and k > 0):
-        raise ValueError("`k` has to be a positive integer or None")
-
-    if not target.sum():
-        return tensor(0.0, device=preds.device)
-
-    relevant = target[torch.argsort(preds, dim=-1, descending=True)][:k].sum().float()
-    return relevant / target.sum()
+    if not _TORCHVISION_GREATER_EQUAL_0_8:
+        raise ModuleNotFoundError(
+            f"`{generalized_intersection_over_union.__name__}` requires that `torchvision` version 0.8.0 or newer"
+            " is installed."
+            " Please install with `pip install torchvision>=0.8` or `pip install torchmetrics[detection]`."
+        )
+    iou = _giou_update(preds, target, iou_threshold, replacement_val)
+    return _giou_compute(iou) if aggregate else iou
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/text/bert.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/bert.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,284 +1,90 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import csv
-import math
+import os
 import urllib
-from collections import Counter, defaultdict
-from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union
+from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union
 from warnings import warn
 
 import torch
 from torch import Tensor
 from torch.nn import Module
-from torch.utils.data import DataLoader, Dataset
-
-from torchmetrics.utilities.imports import _TQDM_AVAILABLE, _TRANSFORMERS_AUTO_AVAILABLE
-
-if _TRANSFORMERS_AUTO_AVAILABLE:
-    from transformers.models.auto import AutoModel, AutoTokenizer
-else:
-    __doctest_skip__ = ["bert_score"]
-
-if _TQDM_AVAILABLE:
-    import tqdm
+from torch.utils.data import DataLoader
 
+from torchmetrics.functional.text.helper_embedding_metric import (
+    TextDataset,
+    TokenizedDataset,
+    _check_shape_of_model_output,
+    _get_progress_bar,
+    _input_data_collator,
+    _output_data_collator,
+    _process_attention_mask_for_special_tokens,
+)
+from torchmetrics.utilities.checks import _SKIP_SLOW_DOCTEST, _try_proceed_with_timeout
+from torchmetrics.utilities.imports import _TQDM_AVAILABLE, _TRANSFORMERS_AVAILABLE
 
 # Default model recommended in the original implementation.
 _DEFAULT_MODEL = "roberta-large"
 
+if _TRANSFORMERS_AVAILABLE:
+    from transformers import AutoModel, AutoTokenizer
 
-def _preprocess_text(
-    text: List[str],
-    tokenizer: Any,
-    max_length: int = 512,
-    truncation: bool = True,
-    sort_according_length: bool = True,
-    own_tokenizer: bool = False,
-) -> Dict[str, Tensor]:
-    """Default text pre-processing function using `transformers` `AutoTokenizer` instance.
-
-    Args:
-        text: An iterable of sentences.
-        tokenizer: Either ``AutoTokenizer`` instance from ``transformers`` package, or a user's own tokenizer.
-        max_length: A maximum sequence length.
-        truncation:
-            An indication of whether tokenized sequences should be padded only to the length of the longest sequence.
-        sort_according_length:
-            An indication of whether tokenized sequences should be sorted from shortest to longest. This is appropriate
-            to do for leveraging dynamic padding during embedding calculation and thereby to hasten inference.
-        own_tokenizer: An indication of whether a non-default user's own tokenizer is used.
-
-    Return:
-        A dictionary of tokenized sentences including ``input_ids`` and ``attention_mask``.
-
-    Raises:
-        BaseException:
-            If a tokenization with a user's own tokenizer is not successful.
-    """
-    if not own_tokenizer:
-        tokenized_data = tokenizer(
-            text, padding="max_length", max_length=max_length, truncation=truncation, return_tensors="pt"
-        )
-    else:
-        try:
-            tokenized_data = tokenizer(text, max_length)
-        except BaseException as ex:
-            raise BaseException(f"Tokenization was not successful: {ex}")
-
-    input_ids, attention_mask = (
-        _sort_data_according_length(tokenized_data["input_ids"], tokenized_data["attention_mask"])
-        if sort_according_length
-        else (tokenized_data["input_ids"], tokenized_data["attention_mask"])
-    )
-    return {"input_ids": input_ids, "attention_mask": attention_mask}
-
-
-def _process_attention_mask_for_special_tokens(attention_mask: Tensor) -> Tensor:
-    """Process attention mask to be zero for special [CLS] and [SEP] tokens as they're not included in a
-    calculation for BERT score.
-
-    Args:
-        attention_mask: An attention mask to be returned, for example, by a ``transformers`` tokenizer.
+    def _download_model() -> None:
+        """Download intensive operations."""
+        AutoTokenizer.from_pretrained(_DEFAULT_MODEL)
+        AutoModel.from_pretrained(_DEFAULT_MODEL)
 
-    Return:
-        A processed attention mask.
-    """
-    # Make attention_mask zero for [CLS] token
-    attention_mask[:, 0] = 0
-    # Make attention_mask zero for [SEP] token
-    sep_token_position = (attention_mask - 0.1).cumsum(-1).argmax(-1)
-    attention_mask[torch.arange(attention_mask.size(0)).long(), sep_token_position] = 0
-    return attention_mask
-
-
-def _sort_data_according_length(input_ids: Tensor, attention_mask: Tensor) -> Tuple[Tensor, Tensor]:
-    """Sort tokenized sentence from the shortest to the longest one."""
-    sorted_indices = attention_mask.sum(1).argsort()
-    input_ids = input_ids[sorted_indices]
-    attention_mask = attention_mask[sorted_indices]
-    return input_ids, attention_mask
-
-
-def _input_data_collator(
-    batch: Dict[str, Tensor], device: Optional[Union[str, torch.device]] = None
-) -> Dict[str, Tensor]:
-    """Helper function that trims model inputs to the longest sequence within the batch and put the input on the
-    proper device."""
-    max_len = int(batch["attention_mask"].sum(1).max().item())
-    input_ids = batch["input_ids"][:, :max_len].to(device)
-    attention_mask = batch["attention_mask"][:, :max_len].to(device)
-    batch.update({"input_ids": input_ids, "attention_mask": attention_mask})
-    return batch
-
-
-def _output_data_collator(model_output: Tensor, attention_mask: Tensor, target_len: int) -> Tuple[Tensor, Tensor]:
-    """Helper function that pads the model output and attention mask to the target length."""
-    zeros_shape = list(model_output.shape)
-    zeros_shape[2] = target_len - zeros_shape[2]
-    model_output = torch.cat(
-        [model_output, torch.zeros(zeros_shape, dtype=model_output.dtype).to(model_output.device)], dim=2
-    )
-    zeros = torch.zeros(zeros_shape[0], zeros_shape[2], dtype=attention_mask.dtype).to(attention_mask.device)
-    attention_mask = torch.cat([attention_mask, zeros], dim=1)
-    return model_output, attention_mask
-
-
-class TextDataset(Dataset):
-    """PyTorch dataset class for storing tokenized sentences and other properties used for BERT score
-    calculation."""
-
-    def __init__(
-        self,
-        text: List[str],
-        tokenizer: Any,
-        max_length: int = 512,
-        preprocess_text_fn: Callable[[List[str], Any, int], Dict[str, Tensor]] = _preprocess_text,
-        idf: bool = False,
-        tokens_idf: Optional[Dict[int, float]] = None,
-    ) -> None:
-        """
-        Args:
-            text: An iterable of sentences.
-            tokenizer: ``AutoTokenizer`` instance from ``transformers`` package.
-            max_length: A maximum sequence length.
-            preprocess_text_fn: A function used for processing the input sentences.
-            idf: An indication of whether calculate token inverse document frequencies to weight the model embeddings.
-            tokens_idf: Inverse document frequencies (these should be calculated on reference sentences).
-        """
-        self.text = preprocess_text_fn(text, tokenizer, max_length)
-        self.max_length = self.text["input_ids"].shape[1]
-        self.num_sentences = len(text)
-        self.idf = idf
-        self.tokens_idf = {}
-        if idf:
-            self.tokens_idf = tokens_idf if tokens_idf is not None else self._get_tokens_idf()
-
-    def __getitem__(self, idx: int) -> Dict[str, Tensor]:
-        input_ids = self.text["input_ids"][idx, :]
-        attention_mask = self.text["attention_mask"][idx, :]
-        inputs_dict = {"input_ids": input_ids, "attention_mask": attention_mask}
-        if self.idf:
-            input_ids_idf = torch.tensor([self.tokens_idf[input_idx] for input_idx in input_ids.tolist()])
-            inputs_dict["input_ids_idf"] = input_ids_idf
-        return inputs_dict
-
-    def __len__(self) -> int:
-        return self.num_sentences
-
-    def _get_tokens_idf(self) -> Dict[int, float]:
-        """Calculate token inverse document frequencies.
-
-        Return:
-            A python dictionary containing inverse document frequencies for token ids.
-        """
-        token_counter: Counter = Counter()
-        for tokens in map(self._set_of_tokens, self.text["input_ids"]):
-            token_counter.update(tokens)
-
-        tokens_idf: Dict[int, float] = defaultdict(self._get_tokens_idf_default_value)
-        tokens_idf.update(
-            {idx: math.log((self.num_sentences + 1) / (occurrence + 1)) for idx, occurrence in token_counter.items()}
-        )
-        return tokens_idf
-
-    def _get_tokens_idf_default_value(self) -> float:
-        """Helper function that ensures ``defaultdict`` to be pickled."""
-        return math.log((self.num_sentences + 1) / 1)
-
-    @staticmethod
-    def _set_of_tokens(input_ids: Tensor) -> Set:
-        """Return set of tokens from the ``input_ids``."""
-        return set(input_ids.tolist())
-
-
-class TokenizedDataset(TextDataset):
-    """The child class of ``TextDataset`` class used with already tokenized data."""
-
-    def __init__(
-        self,
-        input_ids: Tensor,
-        attention_mask: Tensor,
-        idf: bool = False,
-        tokens_idf: Optional[Dict[int, float]] = None,
-    ) -> None:
-        """
-        Args:
-            input_ids: Input ids.
-            attention_mask:  Attention mask.
-            idf: An indication of whether calculate token inverse document frequencies to weight the model embeddings.
-            tokens_idf: Inverse document frequencies (these should be calculated on reference sentences).
-        """
-        self.text = dict(zip(["input_ids", "attention_mask"], _sort_data_according_length(input_ids, attention_mask)))
-        self.text = _input_data_collator(self.text)
-        self.num_sentences = len(self.text["input_ids"])
-        self.max_length = self.text["input_ids"].shape[1]
-        self.idf = idf
-        self.tokens_idf = {}
-        if idf:
-            self.tokens_idf = tokens_idf if tokens_idf is not None else self._get_tokens_idf()
-
-
-def _get_progress_bar(dataloader: DataLoader, verbose: bool = False) -> Union[DataLoader, "tqdm.auto.tqdm"]:
-    """Helper function returning either the dataloader itself when ``verbose = False``, or it wraps the dataloader with
-    ``tqdm.auto.tqdm``, when ``verbose = True`` to display a progress bar during the embeddings calculation."""
-    return tqdm.auto.tqdm(dataloader) if verbose else dataloader
-
-
-def _check_shape_of_model_output(output: Tensor, input_ids: Tensor) -> None:
-    """Check if the shape of the user's own model output."""
-    bs, seq_len = input_ids.shape[:2]
-    invalid_out_shape = len(output.shape) != 3 or output.shape[0] != bs or output.shape[1] != seq_len
-    if invalid_out_shape:
-        raise ValueError(
-            "The model output must be `torch.Tensor` of a shape `[batch_size, seq_len, model_dim]` "
-            f"i.e. [{bs}, {seq_len}. , `model_dim`], but got {output.shape}."
-        )
+    if _SKIP_SLOW_DOCTEST and not _try_proceed_with_timeout(_download_model):
+        __doctest_skip__ = ["bert_score"]
+else:
+    __doctest_skip__ = ["bert_score"]
 
 
 def _get_embeddings_and_idf_scale(
     dataloader: DataLoader,
     target_len: int,
     model: Module,
     device: Optional[Union[str, torch.device]] = None,
     num_layers: Optional[int] = None,
     all_layers: bool = False,
     idf: bool = False,
     verbose: bool = False,
-    user_forward_fn: Callable[[Module, Dict[str, Tensor]], Tensor] = None,
+    user_forward_fn: Optional[Callable[[Module, Dict[str, Tensor]], Tensor]] = None,
 ) -> Tuple[Tensor, Tensor]:
     """Calculate sentence embeddings and the inverse-document-frequency scaling factor.
+
     Args:
         dataloader: dataloader instance.
         target_len: A length of the longest sequence in the data. Used for padding the model output.
         model: BERT model.
         device: A device to be used for calculation.
         num_layers: The layer of representation to use.
         all_layers: An indication whether representation from all model layers should be used for BERTScore.
         idf: An Indication whether normalization using inverse document frequencies should be used.
         verbose: An indication of whether a progress bar to be displayed during the embeddings' calculation.
         user_forward_fn:
             A user's own forward function used in a combination with ``user_model``. This function must
             take ``user_model`` and a python dictionary of containing ``"input_ids"`` and ``"attention_mask"``
-            represented by ``torch.Tensor`` as an input and return the model's output represented by the single
-            ``torch.Tensor``.
+            represented by :class:`~torch.Tensor` as an input and return the model's output represented by the single
+            :class:`~torch.Tensor`.
 
     Return:
-        A tuple of ``torch.Tensor``s containing the model's embeddings and the normalized tokens IDF.
+        A tuple of :class:`~torch.Tensor`s containing the model's embeddings and the normalized tokens IDF.
         When ``idf = False``, tokens IDF is not calculated, and a matrix of mean weights is returned instead.
         For a single sentence, ``mean_weight = 1/seq_len``, where ``seq_len`` is a sum over the corresponding
         ``attention_mask``.
 
     Raises:
         ValueError:
             If ``all_layers = True`` and a model, which is not from the ``transformers`` package, is used.
@@ -322,21 +128,20 @@
     embeddings = torch.cat(embeddings_list)
     idf_scale = torch.cat(idf_scale_list)
 
     return embeddings, idf_scale
 
 
 def _get_scaled_precision_or_recall(cos_sim: Tensor, metric: str, idf_scale: Tensor) -> Tensor:
-    """Helper function that calculates precision or recall, transpose it and scale it with idf_scale factor."""
+    """Calculate precision or recall, transpose it and scale it with idf_scale factor."""
     dim = 3 if metric == "precision" else 2
     res = cos_sim.max(dim=dim).values
     res = torch.einsum("bls, bs -> bls", res, idf_scale).sum(-1)
     # We transpose the results and squeeze if possible to match the format of the original BERTScore implementation
-    res = res.transpose(0, 1).squeeze()
-    return res
+    return res.transpose(0, 1).squeeze()
 
 
 def _get_precision_recall_f1(
     preds_embeddings: Tensor, target_embeddings: Tensor, preds_idf_scale: Tensor, target_idf_scale: Tensor
 ) -> Tuple[Tensor, Tensor, Tensor]:
     """Calculate precision, recall and F1 score over candidate and reference sentences.
 
@@ -358,44 +163,41 @@
     f1_score = 2 * precision * recall / (precision + recall)
     f1_score = f1_score.masked_fill(torch.isnan(f1_score), 0.0)
 
     return precision, recall, f1_score
 
 
 def _get_hash(model_name_or_path: Optional[str] = None, num_layers: Optional[int] = None, idf: bool = False) -> str:
-    """Compute `BERT_score`_ (copied and adjusted)"""
-    msg = f"{model_name_or_path}_L{num_layers}{'_idf' if idf else '_no-idf'}"
-    return msg
+    """Compute `BERT_score`_ (copied and adjusted)."""
+    return f"{model_name_or_path}_L{num_layers}{'_idf' if idf else '_no-idf'}"
 
 
 def _read_csv_from_local_file(baseline_path: str) -> Tensor:
-    """Helper function which reads baseline the csv file from the local file.
+    """Read baseline from csv file from the local file.
 
     This method implemented to avoid `pandas` dependency.
     """
     with open(baseline_path) as fname:
         csv_file = csv.reader(fname)
         baseline_list = [[float(item) for item in row] for idx, row in enumerate(csv_file) if idx > 0]
-    baseline = torch.tensor(baseline_list)[:, 1:]
-    return baseline
+    return torch.tensor(baseline_list)[:, 1:]
 
 
 def _read_csv_from_url(baseline_url: str) -> Tensor:
-    """Helper function which reads the baseline csv file from URL.
+    """Read baseline from csv file from URL.
 
     This method is implemented to avoid `pandas` dependency.
     """
-    with urllib.request.urlopen(baseline_url) as http_request:  # type: ignore
+    with urllib.request.urlopen(baseline_url) as http_request:
         baseline_list = [
             [float(item) for item in row.strip().decode("utf-8").split(",")]
             for idx, row in enumerate(http_request)
             if idx > 0
         ]
-        baseline = torch.tensor(baseline_list)[:, 1:]
-    return baseline
+        return torch.tensor(baseline_list)[:, 1:]
 
 
 def _load_baseline(
     lang: str = "en",
     model_name_or_path: Optional[str] = None,
     baseline_path: Optional[str] = None,
     baseline_url: Optional[str] = None,
@@ -403,16 +205,16 @@
     """Load a CSV file with the baseline values used for rescaling."""
     if baseline_path:
         baseline: Optional[Tensor] = _read_csv_from_local_file(baseline_path)
     elif baseline_url:
         baseline = _read_csv_from_url(baseline_url)
     # Read default baseline from the original `bert-score` package https://github.com/Tiiiger/bert_score
     elif lang and model_name_or_path:
-        _URL_BASE = "https://raw.githubusercontent.com/Tiiiger/bert_score/master/bert_score/rescale_baseline"
-        baseline_url = f"{_URL_BASE}/{lang}/{model_name_or_path}.tsv"
+        url_base = "https://raw.githubusercontent.com/Tiiiger/bert_score/master/bert_score/rescale_baseline"
+        baseline_url = f"{url_base}/{lang}/{model_name_or_path}.tsv"
         baseline = _read_csv_from_url(baseline_url)
     else:
         baseline = None
         warn("Baseline was not successfully loaded. No baseline is going to be used.")
 
     return baseline
 
@@ -432,40 +234,40 @@
     baseline_scale = baseline.unsqueeze(1) if all_layers else baseline[num_layers]
     all_metrics = (all_metrics - baseline_scale) / (1 - baseline_scale)
 
     return all_metrics[..., 0], all_metrics[..., 1], all_metrics[..., 2]
 
 
 def bert_score(
-    preds: Union[List[str], Dict[str, Tensor]],
-    target: Union[List[str], Dict[str, Tensor]],
+    preds: Union[str, Sequence[str], Dict[str, Tensor]],
+    target: Union[str, Sequence[str], Dict[str, Tensor]],
     model_name_or_path: Optional[str] = None,
     num_layers: Optional[int] = None,
     all_layers: bool = False,
     model: Optional[Module] = None,
     user_tokenizer: Any = None,
-    user_forward_fn: Callable[[Module, Dict[str, Tensor]], Tensor] = None,
+    user_forward_fn: Optional[Callable[[Module, Dict[str, Tensor]], Tensor]] = None,
     verbose: bool = False,
     idf: bool = False,
     device: Optional[Union[str, torch.device]] = None,
     max_length: int = 512,
     batch_size: int = 64,
-    num_threads: int = 4,
+    num_threads: int = 0,
     return_hash: bool = False,
     lang: str = "en",
     rescale_with_baseline: bool = False,
     baseline_path: Optional[str] = None,
     baseline_url: Optional[str] = None,
-) -> Dict[str, Union[List[float], str]]:
-    """`Bert_score Evaluating Text Generation`_ leverages the pre-trained contextual embeddings from BERT and
-    matches words in candidate and reference sentences by cosine similarity.
-
-    It has been shown to correlate with human judgment on sentence-level and system-level evaluation.
-    Moreover, BERTScore computes precision, recall, and F1 measure, which can be useful for evaluating different
-    language generation tasks.
+) -> Dict[str, Union[Tensor, List[float], str]]:
+    """`Bert_score Evaluating Text Generation`_ for text similirity matching.
+
+    This metric leverages the pre-trained contextual embeddings from BERT and matches words in candidate and reference
+    sentences by cosine similarity. It has been shown to correlate with human judgment on sentence-level and
+    system-level evaluation. Moreover, BERTScore computes precision, recall, and F1 measure, which can be useful for
+    evaluating different language generation tasks.
 
     This implemenation follows the original implementation from `BERT_score`_.
 
     Args:
         preds: Either an iterable of predicted sentences or a ``Dict[input_ids, attention_mask]``.
         target: Either an iterable of target sentences or a  ``Dict[input_ids, attention_mask]``.
         model_name_or_path: A name or a model path used to load ``transformers`` pretrained model.
@@ -473,23 +275,23 @@
         all_layers:
             An indication of whether the representation from all model's layers should be used.
             If ``all_layers = True``, the argument ``num_layers`` is ignored.
         model: A user's own model.
         user_tokenizer:
             A user's own tokenizer used with the own model. This must be an instance with the ``__call__`` method.
             This method must take an iterable of sentences (``List[str]``) and must return a python dictionary
-            containing ``"input_ids"`` and ``"attention_mask"`` represented by ``torch.Tensor``.
-            It is up to the user's model of whether ``"input_ids"`` is a ``torch.Tensor`` of input ids or embedding
-            vectors. his tokenizer must prepend an equivalent of ``[CLS]`` token and append an equivalent of ``[SEP]``
-            token as `transformers` tokenizer does.
+            containing ``"input_ids"`` and ``"attention_mask"`` represented by :class:`~torch.Tensor`.
+            It is up to the user's model of whether ``"input_ids"`` is a :class:`~torch.Tensor` of input ids
+            or embedding vectors. his tokenizer must prepend an equivalent of ``[CLS]`` token and append an equivalent
+            of ``[SEP]`` token as `transformers` tokenizer does.
         user_forward_fn:
             A user's own forward function used in a combination with ``user_model``.
             This function must take ``user_model`` and a python dictionary of containing ``"input_ids"``
-            and ``"attention_mask"`` represented by ``torch.Tensor`` as an input and return the model's output
-            represented by the single ``torch.Tensor``.
+            and ``"attention_mask"`` represented by :class:`~torch.Tensor` as an input and return the model's output
+            represented by the single :class:`~torch.Tensor`.
         verbose: An indication of whether a progress bar to be displayed during the embeddings' calculation.
         idf: An indication of whether normalization using inverse document frequencies should be used.
         device: A device to be used for calculation.
         max_length: A maximum length of input sequences. Sequences longer than ``max_length`` are to be trimmed.
         batch_size: A batch size used for model processing.
         num_threads: A number of threads to use for a dataloader.
         return_hash: An indication of whether the correspodning ``hash_code`` should be returned.
@@ -515,33 +317,35 @@
             If ``transformers`` package is required and not installed.
         ValueError:
             If ``num_layer`` is larger than the number of the model layers.
         ValueError:
             If invalid input is provided.
 
     Example:
+        >>> from pprint import pprint
         >>> from torchmetrics.functional.text.bert import bert_score
         >>> preds = ["hello there", "general kenobi"]
         >>> target = ["hello there", "master kenobi"]
-        >>> score = bert_score(preds, target)
-        >>> from pprint import pprint
-        >>> rounded_score = {k: [round(v, 3) for v in vv] for k, vv in score.items()}
-        >>> pprint(rounded_score)
-        {'f1': [1.0, 0.996], 'precision': [1.0, 0.996], 'recall': [1.0, 0.996]}
+        >>> pprint(bert_score(preds, target))
+        {'f1': tensor([1.0000, 0.9961]), 'precision': tensor([1.0000, 0.9961]), 'recall': tensor([1.0000, 0.9961])}
     """
     if len(preds) != len(target):
         raise ValueError("Number of predicted and reference sententes must be the same!")
+    if not isinstance(preds, (str, list, dict)):  # dict for BERTScore class compute call
+        preds = list(preds)
+    if not isinstance(target, (str, list, dict)):  # dict for BERTScore class compute call
+        target = list(target)
 
     if verbose and (not _TQDM_AVAILABLE):
         raise ModuleNotFoundError(
             "An argument `verbose = True` requires `tqdm` package be installed. Install with `pip install tqdm`."
         )
 
     if model is None:
-        if not _TRANSFORMERS_AUTO_AVAILABLE:
+        if not _TRANSFORMERS_AVAILABLE:
             raise ModuleNotFoundError(
                 "`bert_score` metric with default models requires `transformers` package be installed."
                 " Either install with `pip install transformers>=4.0` or `pip install torchmetrics[text]`."
             )
         if model_name_or_path is None:
             warn(
                 "The argument `model_name_or_path` was not specified while it is required when default"
@@ -554,30 +358,30 @@
         tokenizer = user_tokenizer
     model.eval()
     model.to(device)
 
     try:
         if num_layers and num_layers > model.config.num_hidden_layers:  # type: ignore
             raise ValueError(
-                f"num_layers={num_layers} is forbidden for {model_name_or_path}. "  # type: ignore
-                f"Please use num_layers <= {model.config.num_hidden_layers}"  # type: ignore
+                f"num_layers={num_layers} is forbidden for {model_name_or_path}."  # type: ignore
+                f" Please use num_layers <= {model.config.num_hidden_layers}"
             )
     except AttributeError:
         warn("It was not possible to retrieve the parameter `num_layers` from the model specification.")
 
     _are_empty_lists = all(isinstance(text, list) and len(text) == 0 for text in (preds, target))
     _are_valid_lists = all(
         isinstance(text, list) and len(text) > 0 and isinstance(text[0], str) for text in (preds, target)
     )
     _are_valid_tensors = all(
         isinstance(text, dict) and isinstance(text["input_ids"], Tensor) for text in (preds, target)
     )
     if _are_empty_lists:
         warn("Predictions and references are empty.")
-        output_dict: Dict[str, Union[List[float], str]] = {
+        output_dict: Dict[str, Union[Tensor, List[float], str]] = {
             "precision": [0.0],
             "recall": [0.0],
             "f1": [0.0],
         }
         if return_hash:
             output_dict.update({"hash": _get_hash(model_name_or_path, num_layers, idf)})
         return output_dict
@@ -610,21 +414,30 @@
     preds_embeddings, preds_idf_scale = _get_embeddings_and_idf_scale(
         preds_loader, preds_dataset.max_length, model, device, num_layers, all_layers, idf, verbose, user_forward_fn
     )
 
     precision, recall, f1_score = _get_precision_recall_f1(
         preds_embeddings, target_embeddings, preds_idf_scale, target_idf_scale
     )
+    # Sort predictions
+    if len(precision.shape) == 1:  # i.e. when all_layers = False
+        precision = precision[preds_loader.dataset.sorting_indices]
+        recall = recall[preds_loader.dataset.sorting_indices]
+        f1_score = f1_score[preds_loader.dataset.sorting_indices]
+    elif len(precision.shape) == 2:  # i.e. when all_layers = True
+        precision = precision[:, preds_loader.dataset.sorting_indices]
+        recall = recall[:, preds_loader.dataset.sorting_indices]
+        f1_score = f1_score[:, preds_loader.dataset.sorting_indices]
 
     if baseline is not None:
         precision, recall, f1_score = _rescale_metrics_with_baseline(
             precision, recall, f1_score, baseline, num_layers, all_layers
         )
 
     output_dict = {
-        "precision": precision.tolist(),
-        "recall": recall.tolist(),
-        "f1": f1_score.tolist(),
+        "precision": precision,
+        "recall": recall,
+        "f1": f1_score,
     }
     if return_hash:
         output_dict.update({"hash": _get_hash(model_name_or_path, num_layers, idf)})
     return output_dict
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/text/bleu.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/bleu.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -20,24 +20,23 @@
 from typing import Callable, Optional, Sequence, Tuple, Union
 
 import torch
 from torch import Tensor, tensor
 
 
 def _count_ngram(ngram_input_list: Sequence[str], n_gram: int) -> Counter:
-    """Counting how many times each word appears in a given text with ngram.
+    """Count how many times each word appears in a given text with ngram.
 
     Args:
         ngram_input_list: A list of translated text or reference texts
         n_gram: gram value ranged 1 to 4
 
     Return:
         ngram_counter: a collections.Counter object of ngram
     """
-
     ngram_counter: Counter = Counter()
 
     for i in range(1, n_gram + 1):
         for j in range(len(ngram_input_list) - i + 1):
             ngram_key = tuple(ngram_input_list[j : (i + j)])
             ngram_counter[ngram_key] += 1
 
@@ -62,30 +61,31 @@
     numerator: Tensor,
     denominator: Tensor,
     preds_len: Tensor,
     target_len: Tensor,
     n_gram: int = 4,
     tokenizer: Callable[[str], Sequence[str]] = _tokenize_fn,
 ) -> Tuple[Tensor, Tensor]:
-    """Updates and returns variables required to compute the BLEU score.
+    """Update and returns variables required to compute the BLEU score.
 
     Args:
         preds: An iterable of machine translated corpus
         target: An iterable of iterables of reference corpus
         numerator: Numerator of precision score (true positives)
         denominator: Denominator of precision score (true positives + false positives)
         preds_len: count of words in a candidate prediction
+        target_len: count of words in a reference translation
         target: count of words in a reference translation
         n_gram: gram value ranged 1 to 4
         tokenizer: A function that turns sentence into list of words
     """
     target_: Sequence[Sequence[Sequence[str]]] = [[tokenizer(line) if line else [] for line in t] for t in target]
     preds_: Sequence[Sequence[str]] = [tokenizer(line) if line else [] for line in preds]
 
-    for (pred, targets) in zip(preds_, target_):
+    for pred, targets in zip(preds_, target_):
         preds_len += len(pred)
         target_len_list = [len(tgt) for tgt in targets]
         target_len_diff = [abs(len(pred) - x) for x in target_len_list]
         target_len += target_len_list[target_len_diff.index(min(target_len_diff))]
         preds_counter: Counter = _count_ngram(pred, n_gram)
         target_counter: Counter = Counter()
 
@@ -108,15 +108,15 @@
     target_len: Tensor,
     numerator: Tensor,
     denominator: Tensor,
     n_gram: int,
     weights: Sequence[float],
     smooth: bool,
 ) -> Tensor:
-    """Computes the BLEU score.
+    """Compute the BLEU score.
 
     Args:
         preds_len: count of words in a candidate translation
         target_len: count of words in a reference translation
         numerator: Numerator of precision score (true positives)
         denominator: Denominator of precision score (true positives + false positives)
         n_gram: gram value ranged 1 to 4
@@ -135,17 +135,15 @@
         precision_scores[0] = numerator[0] / denominator[0]
     else:
         precision_scores = numerator / denominator
 
     log_precision_scores = tensor(weights, device=device) * torch.log(precision_scores)
     geometric_mean = torch.exp(torch.sum(log_precision_scores))
     brevity_penalty = tensor(1.0, device=device) if preds_len > target_len else torch.exp(1 - (target_len / preds_len))
-    bleu = brevity_penalty * geometric_mean
-
-    return bleu
+    return brevity_penalty * geometric_mean
 
 
 def bleu_score(
     preds: Union[str, Sequence[str]],
     target: Sequence[Union[str, Sequence[str]]],
     n_gram: int = 4,
     smooth: bool = False,
@@ -166,15 +164,15 @@
         Tensor with BLEU Score
 
     Raises:
         ValueError: If ``preds`` and ``target`` corpus have different lengths.
         ValueError: If a length of a list of weights is not ``None`` and not equal to ``n_gram``.
 
     Example:
-        >>> from torchmetrics.functional import bleu_score
+        >>> from torchmetrics.functional.text import bleu_score
         >>> preds = ['the cat is on the mat']
         >>> target = [['there is a cat on the mat', 'a cat is on the mat']]
         >>> bleu_score(preds, target)
         tensor(0.7598)
 
     References:
         [1] BLEU: a Method for Automatic Evaluation of Machine Translation by Papineni,
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/text/cer.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/cer.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -58,16 +58,17 @@
     Returns:
         Character error rate score
     """
     return errors / total
 
 
 def char_error_rate(preds: Union[str, List[str]], target: Union[str, List[str]]) -> Tensor:
-    """character error rate is a common metric of the performance of an automatic speech recognition system. This
-    value indicates the percentage of characters that were incorrectly predicted. The lower the value, the better
+    """Compute Character Rrror Rate used for performance of an automatic speech recognition system.
+
+    This value indicates the percentage of characters that were incorrectly predicted. The lower the value, the better
     the performance of the ASR system with a CER of 0 being a perfect score.
 
     Args:
         preds: Transcription(s) to score as a string or list of strings
         target: Reference(s) for each speech input as a string or list of strings
 
     Returns:
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/text/chrf.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/chrf.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -46,16 +46,15 @@
 
 
 def _prepare_n_grams_dicts(
     n_char_order: int, n_word_order: int
 ) -> Tuple[
     Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor]
 ]:
-    """Prepare dictionaries dictionaries with default zero values for total reference, hypothesis and matching
-    character and word n-grams.
+    """Prepare dictionaries with default zero values for total ref, hypothesis and matching chraracter and word n-grams.
 
     Args:
         n_char_order: A character n-gram order.
         n_word_order: A word n-gram order.
 
     Return:
         Dictionaries with default zero values for total reference, hypothesis and matching character and word
@@ -90,17 +89,19 @@
     """
     if whitespace:
         return list(sentence)
     return list(sentence.strip().replace(" ", ""))
 
 
 def _separate_word_and_punctiation(word: str) -> List[str]:
-    """
-    Separates out punctuations from beginning and end of words for chrF. Adapted from https://github.com/m-popovic/chrF
-    and https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/chrf.py.
+    """Separates out punctuations from beginning and end of words for chrF.
+
+    Adapted from https://github.com/m-popovic/chrF and
+    https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/chrf.py.
+
     Args:
         word: An input word to be separated from a punctuation if present.
 
     Return:
         A list of a single word or a separated word and punctuation.
     """
     if len(word) == 1:
@@ -122,15 +123,16 @@
     Return:
         An aggregated list of separated words and punctuations.
     """
     return sum((_separate_word_and_punctiation(word) for word in sentence.strip().split()), [])
 
 
 def _ngram_counts(char_or_word_list: List[str], n_gram_order: int) -> Dict[int, Dict[Tuple[str, ...], Tensor]]:
-    """
+    """Calculate n-gram counts.
+
     Args:
         char_or_word_list: A list of characters of words
         n_gram_order: The largest number of n-gram.
 
     Return:
         A dictionary of dictionaries with a counts of given n-grams.
     """
@@ -145,15 +147,16 @@
     sentence: str, n_char_order: int, n_word_order: int, lowercase: bool, whitespace: bool
 ) -> Tuple[
     Dict[int, Dict[Tuple[str, ...], Tensor]],
     Dict[int, Dict[Tuple[str, ...], Tensor]],
     Dict[int, Tensor],
     Dict[int, Tensor],
 ]:
-    """
+    """Get n-grams and total n-grams.
+
     Args:
         sentence: An input sentence
         n_char_order: A character n-gram order.
         n_word_order: A word n-gram order.
         lowercase: An indication whether to enable case-insensitivity.
         whitespace: An indication whether to keep whitespaces during character n-gram extraction.
 
@@ -193,16 +196,16 @@
 def _get_ngram_matches(
     hyp_n_grams_counts: Dict[int, Dict[Tuple[str, ...], Tensor]],
     ref_n_grams_counts: Dict[int, Dict[Tuple[str, ...], Tensor]],
 ) -> Dict[int, Tensor]:
     """Get a number of n-gram matches between reference and hypothesis n-grams.
 
     Args:
-        hyp_n_grams_counts:
-        ref_n_grams_counts:
+        hyp_n_grams_counts: n-grams counts for hypothesis
+        ref_n_grams_counts: n-grams counts for reference
 
     Return:
         matching_n_grams
     """
     matching_n_grams: Dict[int, Tensor] = defaultdict(lambda: tensor(0.0))
     for n in hyp_n_grams_counts:
         matching_n_grams[n] = tensor(
@@ -278,40 +281,39 @@
         }
 
         return f_score
 
     char_n_gram_f_score = _get_n_gram_fscore(matching_char_n_grams, ref_char_n_grams, hyp_char_n_grams, beta)
     word_n_gram_f_score = _get_n_gram_fscore(matching_word_n_grams, ref_word_n_grams, hyp_word_n_grams, beta)
 
-    f_score = (sum(char_n_gram_f_score.values()) + sum(word_n_gram_f_score.values())) / tensor(n_order)  # type: ignore
-    return f_score
+    return (sum(char_n_gram_f_score.values()) + sum(word_n_gram_f_score.values())) / tensor(n_order)
 
 
 def _calculate_sentence_level_chrf_score(
     targets: List[str],
     pred_char_n_grams_counts: Dict[int, Dict[Tuple[str, ...], Tensor]],
     pred_word_n_grams_counts: Dict[int, Dict[Tuple[str, ...], Tensor]],
-    preds_char_n_grams: Dict[int, Tensor],
-    preds_word_n_grams: Dict[int, Tensor],
+    pred_char_n_grams: Dict[int, Tensor],
+    pred_word_n_grams: Dict[int, Tensor],
     n_char_order: int,
     n_word_order: int,
     n_order: float,
     beta: float,
     lowercase: bool,
     whitespace: bool,
 ) -> Tuple[Tensor, Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor]]:
     """Calculate the best sentence-level chrF/chrF++ score.
 
     For a given pre-processed hypothesis, all references are evaluated and score and statistics
     for the best matching reference is returned.
 
     Args:
         targets: An iterable of references.
-        preds_char_n_grams_counts: A dictionary of dictionaries with hypothesis character n-grams.
-        preds_word_n_grams_counts: A dictionary of dictionaries with hypothesis word n-grams.
+        pred_char_n_grams_counts: A dictionary of dictionaries with hypothesis character n-grams.
+        pred_word_n_grams_counts: A dictionary of dictionaries with hypothesis word n-grams.
         pred_char_n_grams: A total number of hypothesis character n-grams.
         pred_word_n_grams: A total number of hypothesis word n-grams.
         n_char_order: A character n-gram order.
         n_word_order: A word n-gram order.
         n_order: A sum of character and word n-gram order.
         beta: A parameter determining an importance of recall w.r.t. precision. If `beta=1`, their importance is equal.
         lowercase: An indication whether to enable case-insensitivity.
@@ -324,15 +326,14 @@
         matching_char_n_grams:
             A total number of matching character n-grams between the best matching reference and hypothesis.
         matching_word_n_grams:
             A total number of matching word n-grams between the best matching reference and hypothesis.
         target_char_n_grams: A total number of reference character n-grams.
         target_word_n_grams: A total number of reference word n-grams.
     """
-
     best_f_score = tensor(0.0)
     best_matching_char_n_grams: Dict[int, Tensor] = defaultdict(lambda: tensor(0.0))
     best_matching_word_n_grams: Dict[int, Tensor] = defaultdict(lambda: tensor(0.0))
     best_target_char_n_grams: Dict[int, Tensor] = defaultdict(lambda: tensor(0.0))
     best_target_word_n_grams: Dict[int, Tensor] = defaultdict(lambda: tensor(0.0))
 
     for target in targets:
@@ -344,16 +345,16 @@
         ) = _get_n_grams_counts_and_total_ngrams(target, n_char_order, n_word_order, lowercase, whitespace)
         matching_char_n_grams = _get_ngram_matches(target_char_n_grams_counts, pred_char_n_grams_counts)
         matching_word_n_grams = _get_ngram_matches(target_word_n_grams_counts, pred_word_n_grams_counts)
 
         f_score = _calculate_fscore(
             matching_char_n_grams,
             matching_word_n_grams,
-            preds_char_n_grams,
-            preds_word_n_grams,
+            pred_char_n_grams,
+            pred_word_n_grams,
             target_char_n_grams,
             target_word_n_grams,
             n_order,
             beta,
         )
 
         if f_score > best_f_score:
@@ -393,15 +394,16 @@
     Dict[int, Tensor],
     Dict[int, Tensor],
     Dict[int, Tensor],
     Dict[int, Tensor],
     Dict[int, Tensor],
     Optional[List[Tensor]],
 ]:
-    """
+    """Update function for chrf score.
+
     Args:
         preds: An iterable of hypothesis corpus.
         target: An iterable of iterables of reference corpus.
         total_preds_char_n_grams: A dictionary containing a total number of hypothesis character n-grams.
         total_preds_word_n_grams: A dictionary containing a total number of hypothesis word n-grams.
         total_target_char_n_grams: A dictionary containing a total number of reference character n-grams.
         total_target_word_n_grams: A dictionary containing a total number of reference word n-grams.
@@ -428,15 +430,15 @@
 
     Raises:
         ValueError:
             If length of ``preds`` and ``target`` differs.
     """
     target_corpus, preds = _validate_inputs(target, preds)
 
-    for (pred, targets) in zip(preds, target_corpus):
+    for pred, targets in zip(preds, target_corpus):
         (
             pred_char_n_grams_counts,
             pred_word_n_grams_counts,
             pred_char_n_grams,
             pred_word_n_grams,
         ) = _get_n_grams_counts_and_total_ngrams(pred, n_char_order, n_word_order, lowercase, whitespace)
         total_preds_char_n_grams = _sum_over_dicts(total_preds_char_n_grams, pred_char_n_grams)
@@ -503,40 +505,40 @@
         n_order: A sum of character and word n-gram order.
         beta:
             A parameter determining an importance of recall w.r.t. precision. If `beta=1`, their importance is equal.
 
     Return:
         A corpus-level chrF/chrF++ score.
     """
-    chrf_f_score = _calculate_fscore(
+    return _calculate_fscore(
         total_matching_char_n_grams,
         total_matching_word_n_grams,
         total_preds_char_n_grams,
         total_preds_word_n_grams,
         total_target_char_n_grams,
         total_target_word_n_grams,
         n_order,
         beta,
     )
-    return chrf_f_score
 
 
 def chrf_score(
     preds: Union[str, Sequence[str]],
     target: Sequence[Union[str, Sequence[str]]],
     n_char_order: int = 6,
     n_word_order: int = 2,
     beta: float = 2.0,
     lowercase: bool = False,
     whitespace: bool = False,
     return_sentence_level_score: bool = False,
 ) -> Union[Tensor, Tuple[Tensor, Tensor]]:
-    """Calculate `chrF score`_  of machine translated text with one or more references. This implementation
-    supports both chrF score computation introduced in [1] and chrF++ score introduced in `chrF++ score`_. This
-    implementation follows the implmenetaions from https://github.com/m-popovic/chrF and
+    """Calculate `chrF score`_  of machine translated text with one or more references.
+
+    This implementation supports both chrF score computation introduced in [1] and chrF++ score introduced in
+    `chrF++ score`_. This implementation follows the implmenetaions from https://github.com/m-popovic/chrF and
     https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/chrf.py.
 
     Args:
         preds: An iterable of hypothesis corpus.
         target: An iterable of iterables of reference corpus.
         n_char_order:
             A character n-gram order. If `n_char_order=6`, the metrics refers to the official chrF/chrF++.
@@ -558,15 +560,15 @@
             If ``n_char_order`` is not an integer greater than or equal to 1.
         ValueError:
             If ``n_word_order`` is not an integer greater than or equal to 0.
         ValueError:
             If ``beta`` is smaller than 0.
 
     Example:
-        >>> from torchmetrics.functional import chrf_score
+        >>> from torchmetrics.functional.text import chrf_score
         >>> preds = ['the cat is on the mat']
         >>> target = [['there is a cat on the mat', 'a cat is on the mat']]
         >>> chrf_score(preds, target)
         tensor(0.8640)
 
     References:
         [1] chrF: character n-gram F-score for automatic MT evaluation by Maja Popovi `chrF score`_
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/text/eed.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/eed.py`

 * *Files 1% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -94,16 +94,17 @@
 from torch import Tensor, stack, tensor
 from typing_extensions import Literal
 
 from torchmetrics.functional.text.helper import _validate_inputs
 
 
 def _distance_between_words(preds_word: str, target_word: str) -> int:
-    """Distance measure used for substitutions/identity operation. Code adapted from
-    https://github.com/rwth-i6/ExtendedEditDistance/blob/master/EED.py.
+    """Distance measure used for substitutions/identity operation.
+
+    Code adapted from https://github.com/rwth-i6/ExtendedEditDistance/blob/master/EED.py.
 
     Args:
         preds_word: hypothesis word string
         target_word: reference word string
 
     Return:
         0 for match, 1 for no match
@@ -115,15 +116,15 @@
     hyp: str,
     ref: str,
     alpha: float = 2.0,
     rho: float = 0.3,
     deletion: float = 0.2,
     insertion: float = 1.0,
 ) -> float:
-    """Computes extended edit distance score for two lists of strings: hyp and ref.
+    """Compute extended edit distance score for two lists of strings: hyp and ref.
 
     Code adapted from: https://github.com/rwth-i6/ExtendedEditDistance/blob/master/EED.py.
 
     Args:
         hyp: A hypothesis string
         ref: A reference string
         alpha: optimal jump penalty, penalty for jumps between characters
@@ -140,15 +141,14 @@
     row = [1.0] * (len(hyp) + 1)
 
     row[0] = 0.0  # CDER initialisation 0,0 = 0.0, rest 1.0
     next_row = [inf] * (len(hyp) + 1)
 
     for w in range(1, len(ref) + 1):
         for i in range(0, len(hyp) + 1):
-
             if i > 0:
                 next_row[i] = min(
                     next_row[i - 1] + deletion,
                     row[i - 1] + _distance_between_words(hyp[i - 1], ref[w - 1]),
                     row[i] + insertion,
                 )
             else:
@@ -167,15 +167,17 @@
 
     coverage = rho * sum(x if x >= 0 else 1 for x in number_of_visits)
 
     return min(1, (row[-1] + coverage) / (float(len(ref)) + coverage))
 
 
 def _preprocess_en(sentence: str) -> str:
-    """Copied from https://github.com/rwth-i6/ExtendedEditDistance/blob/master/util.py.
+    """Preprocess english sentences.
+
+    Copied from https://github.com/rwth-i6/ExtendedEditDistance/blob/master/util.py.
 
     Raises:
         ValueError: If input sentence is not of a type `str`.
     """
     if not isinstance(sentence, str):
         raise ValueError(f"Only strings allowed during preprocessing step, found {type(sentence)} instead")
 
@@ -211,42 +213,42 @@
     # add space to beginning and end of string
     sentence = " " + sentence + " "
 
     return sentence
 
 
 def _preprocess_ja(sentence: str) -> str:
-    """Copied from https://github.com/rwth-i6/ExtendedEditDistance/blob/master/util.py.
+    """Preprocess japanese sentences.
+
+    Copy from https://github.com/rwth-i6/ExtendedEditDistance/blob/master/util.py.
 
     Raises:
         ValueError: If input sentence is not of a type `str`.
     """
     if not isinstance(sentence, str):
         raise ValueError(f"Only strings allowed during preprocessing step, found {type(sentence)} instead")
 
     sentence = sentence.rstrip()  # trailing space, tab, newline
     # characters which look identical actually are identical
-    sentence = unicodedata.normalize("NFKC", sentence)
-    return sentence
+    return unicodedata.normalize("NFKC", sentence)
 
 
 def _eed_compute(sentence_level_scores: List[Tensor]) -> Tensor:
-    """Final step in extended edit distance.
+    """Reduction for extended edit distance.
 
     Args:
         sentence_level_scores: list of sentence-level scores as floats
 
     Return:
         average of scores as a tensor
     """
     if len(sentence_level_scores) == 0:
         return tensor(0.0)
 
-    average = sum(sentence_level_scores) / tensor(len(sentence_level_scores))
-    return average
+    return sum(sentence_level_scores) / tensor(len(sentence_level_scores))
 
 
 def _preprocess_sentences(
     preds: Union[str, Sequence[str]],
     target: Sequence[Union[str, Sequence[str]]],
     language: Union[Literal["en"], Literal["ja"]],
 ) -> Tuple[Union[str, Sequence[str]], Sequence[Union[str, Sequence[str]]]]:
@@ -262,15 +264,15 @@
 
     Raises:
         ValueError: If a different language than ``'en'`` or ``'ja'`` is used
         ValueError: If length of target not equal to length of preds
         ValueError: If objects in reference and hypothesis corpus are not strings
     """
     # sanity checks
-    target, preds = _validate_inputs(hypothesis_corpus=preds, reference_corpus=target)
+    target, preds = _validate_inputs(hypothesis_corpus=preds, ref_corpus=target)
 
     # preprocess string
     if language == "en":
         preprocess_function = _preprocess_en
     elif language == "ja":
         preprocess_function = _preprocess_ja
     else:
@@ -360,16 +362,17 @@
     language: Literal["en", "ja"] = "en",
     return_sentence_level_score: bool = False,
     alpha: float = 2.0,
     rho: float = 0.3,
     deletion: float = 0.2,
     insertion: float = 1.0,
 ) -> Union[Tensor, Tuple[Tensor, Tensor]]:
-    """Computes extended edit distance score (`ExtendedEditDistance`_) [1] for strings or list of strings. The
-    metric utilises the Levenshtein distance and extends it by adding a jump operation.
+    """Compute extended edit distance score (`ExtendedEditDistance`_) [1] for strings or list of strings.
+
+    The metric utilises the Levenshtein distance and extends it by adding a jump operation.
 
     Args:
         preds: An iterable of hypothesis corpus.
         target: An iterable of iterables of reference corpus.
         language: Language used in sentences. Only supports English (en) and Japanese (ja) for now. Defaults to en
         return_sentence_level_score: An indication of whether sentence-level EED score is to be returned.
         alpha: optimal jump penalty, penalty for jumps between characters
@@ -377,15 +380,15 @@
         deletion: penalty for deletion of character
         insertion: penalty for insertion or substitution of character
 
     Return:
         Extended edit distance score as a tensor
 
     Example:
-        >>> from torchmetrics.functional import extended_edit_distance
+        >>> from torchmetrics.functional.text import extended_edit_distance
         >>> preds = ["this is the prediction", "here is an other sample"]
         >>> target = ["this is the reference", "here is another one"]
         >>> extended_edit_distance(preds=preds, target=target)
         tensor(0.3078)
 
     References:
         [1] P. Stanchev, W. Wang, and H. Ney, EED: Extended Edit Distance Measure for Machine Translation,
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/text/helper.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/helper.py`

 * *Files 23% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -37,41 +37,40 @@
 
 # Sacrebleu-inspired limits
 _MAX_CACHE_SIZE = 10000
 _INT_INFINITY = int(1e16)
 
 
 @unique
-class _EDIT_OPERATIONS(str, Enum):
+class _EditOperations(str, Enum):
     """Enumerations for the Levenhstein edit operations."""
 
     OP_INSERT = "insert"
     OP_DELETE = "delete"
     OP_SUBSTITUTE = "substitute"
     OP_NOTHING = "nothing"
     OP_UNDEFINED = "undefined"
 
 
-class _EDIT_OPERATIONS_COST(IntEnum):
+class _EditOperationsCost(IntEnum):
     """Enumerations for the Levenhstein edit operation costs."""
 
     OP_INSERT = 1
     OP_DELETE = 1
     OP_SUBSTITUTE = 1
     OP_NOTHING = 0
     OP_UNDEFINED = _INT_INFINITY
 
 
 class _LevenshteinEditDistance:
-    """A convenience class for calculating the Levenshtein edit distance, which also caches some intermediate
-    values to hasten the calculation.
+    """A convenience class for calculating the Levenshtein edit distance.
 
-    The implementation follows the implemenation from
-    https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/lib_ter.py, where the most of this implementation
-    is adapted and copied from.
+    Class will cache some intermediate values to hasten the calculation. The implementation follows the implemenation
+    from https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/lib_ter.py, where the most of this
+    implementation is adapted and copied from.
     """
 
     def __init__(self, reference_tokens: List[str]) -> None:
         """Initialize _LevenshteinEditDistance object.
 
         Args:
             reference_tokens:
@@ -79,25 +78,23 @@
         """
         self.reference_tokens = reference_tokens
         self.reference_len = len(reference_tokens)
 
         self.cache: Dict[str, Tuple[int, str]] = {}
         self.cache_size = 0
 
-    def __call__(self, prediction_tokens: List[str]) -> Tuple[int, Tuple[_EDIT_OPERATIONS, ...]]:
-        """Calculate edit distance between self._words_ref and the hypothesis. Uses cache to skip some of the
-        computation.
+    def __call__(self, prediction_tokens: List[str]) -> Tuple[int, Tuple[_EditOperations, ...]]:
+        """Calculate edit distance between self._words_ref and the hypothesis. Uses cache to skip some computations.
 
         Args:
             prediction_tokens: A tokenized predicted sentence.
 
         Return:
             A tuple of a calculated edit distance and a trace of executed operations.
         """
-
         # Use cached edit distance for already computed words
         start_position, cached_edit_distance = self._find_cache(prediction_tokens)
         # Calculate the rest of the edit distance matrix
         edit_distance_int, edit_distance, trace = self._levenshtein_edit_distance(
             prediction_tokens, start_position, cached_edit_distance
         )
         # Update our cache with the newly calculated rows
@@ -105,80 +102,80 @@
 
         return edit_distance_int, trace
 
     def _levenshtein_edit_distance(
         self,
         prediction_tokens: List[str],
         prediction_start: int,
-        cache: List[List[Tuple[int, _EDIT_OPERATIONS]]],
-    ) -> Tuple[int, List[List[Tuple[int, _EDIT_OPERATIONS]]], Tuple[_EDIT_OPERATIONS, ...]]:
-        """A dynamic programming algorithm to compute the Levenhstein edit distance.
+        cache: List[List[Tuple[int, _EditOperations]]],
+    ) -> Tuple[int, List[List[Tuple[int, _EditOperations]]], Tuple[_EditOperations, ...]]:
+        """Dynamic programming algorithm to compute the Levenhstein edit distance.
 
         Args:
             prediction_tokens: A tokenized predicted sentence.
             prediction_start: An index where a predicted sentence to be considered from.
             cache: A cached Levenshtein edit distance.
 
         Returns:
             Edit distance between the predicted sentence and the reference sentence
         """
         prediction_len = len(prediction_tokens)
 
-        empty_rows: List[List[Tuple[int, _EDIT_OPERATIONS]]] = [
+        empty_rows: List[List[Tuple[int, _EditOperations]]] = [
             list(self._get_empty_row(self.reference_len)) for _ in range(prediction_len - prediction_start)
         ]
-        edit_distance: List[List[Tuple[int, _EDIT_OPERATIONS]]] = cache + empty_rows
+        edit_distance: List[List[Tuple[int, _EditOperations]]] = cache + empty_rows
         length_ratio = self.reference_len / prediction_len if prediction_tokens else 1.0
 
         # Ensure to not end up with zero overlaip with previous role
-        beam_width = math.ceil(length_ratio / 2 + _BEAM_WIDTH) if _BEAM_WIDTH < length_ratio / 2 else _BEAM_WIDTH
+        beam_width = math.ceil(length_ratio / 2 + _BEAM_WIDTH) if length_ratio / 2 > _BEAM_WIDTH else _BEAM_WIDTH
 
         # Calculate the Levenshtein distance
         for i in range(prediction_start + 1, prediction_len + 1):
             pseudo_diag = math.floor(i * length_ratio)
             min_j = max(0, pseudo_diag - beam_width)
             max_j = (
                 self.reference_len + 1 if i == prediction_len else min(self.reference_len + 1, pseudo_diag + beam_width)
             )
 
             for j in range(min_j, max_j):
                 if j == 0:
                     edit_distance[i][j] = (
-                        edit_distance[i - 1][j][0] + _EDIT_OPERATIONS_COST.OP_DELETE,
-                        _EDIT_OPERATIONS.OP_DELETE,
+                        edit_distance[i - 1][j][0] + _EditOperationsCost.OP_DELETE,
+                        _EditOperations.OP_DELETE,
                     )
                 else:
                     if prediction_tokens[i - 1] == self.reference_tokens[j - 1]:
-                        cost_substitute = _EDIT_OPERATIONS_COST.OP_NOTHING
-                        operation_substitute = _EDIT_OPERATIONS.OP_NOTHING
+                        cost_substitute = _EditOperationsCost.OP_NOTHING
+                        operation_substitute = _EditOperations.OP_NOTHING
                     else:
-                        cost_substitute = _EDIT_OPERATIONS_COST.OP_SUBSTITUTE
-                        operation_substitute = _EDIT_OPERATIONS.OP_SUBSTITUTE
+                        cost_substitute = _EditOperationsCost.OP_SUBSTITUTE
+                        operation_substitute = _EditOperations.OP_SUBSTITUTE
 
                     # Tercom prefers no-op/sub, then insertion, then deletion. But since we flip the trace and compute
                     # the alignment from the inverse, we need to swap order of insertion and  deletion in the
                     # preference.
                     # Copied from: https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/ter.py.
                     operations = (
                         (edit_distance[i - 1][j - 1][0] + cost_substitute, operation_substitute),
-                        (edit_distance[i - 1][j][0] + _EDIT_OPERATIONS_COST.OP_DELETE, _EDIT_OPERATIONS.OP_DELETE),
-                        (edit_distance[i][j - 1][0] + _EDIT_OPERATIONS_COST.OP_INSERT, _EDIT_OPERATIONS.OP_INSERT),
+                        (edit_distance[i - 1][j][0] + _EditOperationsCost.OP_DELETE, _EditOperations.OP_DELETE),
+                        (edit_distance[i][j - 1][0] + _EditOperationsCost.OP_INSERT, _EditOperations.OP_INSERT),
                     )
 
                     for operation_cost, operation_name in operations:
                         if edit_distance[i][j][0] > operation_cost:
                             edit_distance[i][j] = operation_cost, operation_name
 
         trace = self._get_trace(prediction_len, edit_distance)
 
         return edit_distance[-1][-1][0], edit_distance[len(cache) :], trace
 
     def _get_trace(
-        self, prediction_len: int, edit_distance: List[List[Tuple[int, _EDIT_OPERATIONS]]]
-    ) -> Tuple[_EDIT_OPERATIONS, ...]:
+        self, prediction_len: int, edit_distance: List[List[Tuple[int, _EditOperations]]]
+    ) -> Tuple[_EditOperations, ...]:
         """Get a trace of executed operations from the edit distance matrix.
 
         Args:
             prediction_len: A length of a tokenized predicted sentence.
             edit_distance:
                 A matrix of the Levenshtedin edit distance. The element part of the matrix is a tuple of an edit
                 operation cost and an edit operation itself.
@@ -186,37 +183,38 @@
         Return:
             A trace of executed operations returned as a tuple of `_EDIT_OPERATIONS` enumerates.
 
         Raises:
             ValueError:
                 If an unknown operation has been applied.
         """
-        trace: Tuple[_EDIT_OPERATIONS, ...] = ()
+        trace: Tuple[_EditOperations, ...] = ()
         i = prediction_len
         j = self.reference_len
 
         while i > 0 or j > 0:
             operation = edit_distance[i][j][1]
             trace = (operation,) + trace
-            if operation in (_EDIT_OPERATIONS.OP_SUBSTITUTE, _EDIT_OPERATIONS.OP_NOTHING):
+            if operation in (_EditOperations.OP_SUBSTITUTE, _EditOperations.OP_NOTHING):
                 i -= 1
                 j -= 1
-            elif operation == _EDIT_OPERATIONS.OP_INSERT:
+            elif operation == _EditOperations.OP_INSERT:
                 j -= 1
-            elif operation == _EDIT_OPERATIONS.OP_DELETE:
+            elif operation == _EditOperations.OP_DELETE:
                 i -= 1
             else:
                 raise ValueError(f"Unknown operation {operation!r}")
 
         return trace
 
-    def _add_cache(self, prediction_tokens: List[str], edit_distance: List[List[Tuple[int, _EDIT_OPERATIONS]]]) -> None:
-        """Add newly computed rows to cache. Since edit distance is only calculated on the hypothesis suffix that
-        was not in cache, the number of rows in `edit_distance` matrx may be shorter than hypothesis length. In
-        that case we skip over these initial words.
+    def _add_cache(self, prediction_tokens: List[str], edit_distance: List[List[Tuple[int, _EditOperations]]]) -> None:
+        """Add newly computed rows to cache.
+
+        Since edit distance is only calculated on the hypothesis suffix that was not in cache, the number of rows in
+        `edit_distance` matrx may be shorter than hypothesis length. In that case we skip over these initial words.
 
         Args:
             prediction_tokens: A tokenized predicted sentence.
             edit_distance:
                 A matrix of the Levenshtedin edit distance. The element part of the matrix is a tuple of an edit
                 operation cost and an edit operation itself.
         """
@@ -236,15 +234,15 @@
         for word, row in zip(prediction_tokens[skip_num:], edit_distance):
             if word not in node:
                 node[word] = ({}, tuple(row))  # type: ignore
                 self.cache_size += 1
             value = node[word]
             node = value[0]  # type: ignore
 
-    def _find_cache(self, prediction_tokens: List[str]) -> Tuple[int, List[List[Tuple[int, _EDIT_OPERATIONS]]]]:
+    def _find_cache(self, prediction_tokens: List[str]) -> Tuple[int, List[List[Tuple[int, _EditOperations]]]]:
         """Find the already calculated rows of the Levenshtein edit distance matric.
 
         Args:
             prediction_tokens: A tokenized predicted sentence.
 
         Return:
             A tuple of a start hypothesis position and `edit_distance` matrix.
@@ -252,90 +250,83 @@
             prediction_start: An index where a predicted sentence to be considered from.
             edit_distance:
                 A matrix of the cached Levenshtedin edit distance. The element part of the matrix is a tuple of an edit
                 operation cost and an edit operation itself.
         """
         node = self.cache
         start_position = 0
-        edit_distance: List[List[Tuple[int, _EDIT_OPERATIONS]]] = [self._get_initial_row(self.reference_len)]
+        edit_distance: List[List[Tuple[int, _EditOperations]]] = [self._get_initial_row(self.reference_len)]
         for word in prediction_tokens:
             if word in node:
                 start_position += 1
                 node, row = node[word]  # type: ignore
                 edit_distance.append(row)  # type: ignore
             else:
                 break
 
         return start_position, edit_distance
 
     @staticmethod
-    def _get_empty_row(length: int) -> List[Tuple[int, _EDIT_OPERATIONS]]:
+    def _get_empty_row(length: int) -> List[Tuple[int, _EditOperations]]:
         """Precomputed empty matrix row for Levenhstein edit distance.
 
         Args:
             length: A length of a tokenized sentence.
 
         Return:
             A list of tuples containing infinite edit operation costs and yet undefined edit operations.
         """
-        empty_row = [(int(_EDIT_OPERATIONS_COST.OP_UNDEFINED), _EDIT_OPERATIONS.OP_UNDEFINED)] * (length + 1)
-        return empty_row
+        return [(int(_EditOperationsCost.OP_UNDEFINED), _EditOperations.OP_UNDEFINED)] * (length + 1)
 
     @staticmethod
-    def _get_initial_row(length: int) -> List[Tuple[int, _EDIT_OPERATIONS]]:
-        """First row corresponds to insertion operations of the reference, so we do 1 edit operation per reference
-        word.
+    def _get_initial_row(length: int) -> List[Tuple[int, _EditOperations]]:
+        """First row corresponds to insertion operations of the reference, so we do 1 edit operation per reference word.
 
         Args:
             length: A length of a tokenized sentence.
 
         Return:
             A list of tuples containing edit operation costs of insert and insert edit operations.
         """
-        initial_row = [(i * _EDIT_OPERATIONS_COST.OP_INSERT, _EDIT_OPERATIONS.OP_INSERT) for i in range(length + 1)]
-        return initial_row
+        return [(i * _EditOperationsCost.OP_INSERT, _EditOperations.OP_INSERT) for i in range(length + 1)]
 
 
 def _validate_inputs(
-    reference_corpus: Union[Sequence[str], Sequence[Sequence[str]]],
+    ref_corpus: Union[Sequence[str], Sequence[Sequence[str]]],
     hypothesis_corpus: Union[str, Sequence[str]],
 ) -> Tuple[Sequence[Sequence[str]], Sequence[str]]:
-    """Check and update (if needed) the format of reference and hypothesis corpora for various text evaluation
-    metrics.
+    """Check and update (if needed) the format of reference and hypothesis corpora for various text evaluation metrics.
 
     Args:
-        reference_corpus: An iterable of iterables of reference corpus.
+        ref_corpus: An iterable of iterables of reference corpus.
         hypothesis_corpus: An iterable of hypothesis corpus.
 
     Return:
-        reference_corpus: An iterable of iterables of reference corpus.
+        ref_corpus: An iterable of iterables of reference corpus.
         hypothesis_corpus: An iterable of hypothesis corpus.
 
     Raises:
         ValueError:
-            If length of `reference_corpus` and `hypothesis_corpus` differs.
+            If length of `ref_corpus` and `hypothesis_corpus` differs.
     """
     if isinstance(hypothesis_corpus, str):
         hypothesis_corpus = [hypothesis_corpus]
 
     # Ensure reference corpus is properly of a type Sequence[Sequence[str]]
-    if all(isinstance(ref, str) for ref in reference_corpus):
-        if len(hypothesis_corpus) == 1:
-            reference_corpus = [reference_corpus]  # type: ignore
-        else:
-            reference_corpus = [[ref] for ref in reference_corpus]  # type: ignore
+    if all(isinstance(ref, str) for ref in ref_corpus):
+        ref_corpus = [ref_corpus] if len(hypothesis_corpus) == 1 else [[ref] for ref in ref_corpus]  # type: ignore
 
-    if hypothesis_corpus and all(ref for ref in reference_corpus) and len(reference_corpus) != len(hypothesis_corpus):
-        raise ValueError(f"Corpus has different size {len(reference_corpus)} != {len(hypothesis_corpus)}")
+    if hypothesis_corpus and all(ref for ref in ref_corpus) and len(ref_corpus) != len(hypothesis_corpus):
+        raise ValueError(f"Corpus has different size {len(ref_corpus)} != {len(hypothesis_corpus)}")
 
-    return reference_corpus, hypothesis_corpus
+    return ref_corpus, hypothesis_corpus
 
 
 def _edit_distance(prediction_tokens: List[str], reference_tokens: List[str]) -> int:
-    """Standard dynamic programming algorithm to compute the edit distance.
+    """Dynamic programming algorithm to compute the edit distance.
 
     Args:
         prediction_tokens: A tokenized predicted sentence
         reference_tokens: A tokenized reference sentence
     Returns:
         Edit distance between the predicted sentence and the reference sentence
     """
@@ -349,42 +340,42 @@
             if prediction_tokens[i - 1] == reference_tokens[j - 1]:
                 dp[i][j] = dp[i - 1][j - 1]
             else:
                 dp[i][j] = min(dp[i - 1][j], dp[i][j - 1], dp[i - 1][j - 1]) + 1
     return dp[-1][-1]
 
 
-def _flip_trace(trace: Tuple[_EDIT_OPERATIONS, ...]) -> Tuple[_EDIT_OPERATIONS, ...]:
-    """Flip the trace of edit operations. Instead of rewriting a->b, get a recipe for rewriting b->a. Simply flips
-    insertions and deletions.
+def _flip_trace(trace: Tuple[_EditOperations, ...]) -> Tuple[_EditOperations, ...]:
+    """Flip the trace of edit operations.
+
+    Instead of rewriting a->b, get a recipe for rewriting b->a. Simply flips insertions and deletions.
 
     Args:
         trace: A tuple of edit operations.
 
     Return:
         inverted_trace:
             A tuple of inverted edit operations.
     """
-    _flip_operations: Dict[_EDIT_OPERATIONS, _EDIT_OPERATIONS] = {
-        _EDIT_OPERATIONS.OP_INSERT: _EDIT_OPERATIONS.OP_DELETE,
-        _EDIT_OPERATIONS.OP_DELETE: _EDIT_OPERATIONS.OP_INSERT,
+    _flip_operations: Dict[_EditOperations, _EditOperations] = {
+        _EditOperations.OP_INSERT: _EditOperations.OP_DELETE,
+        _EditOperations.OP_DELETE: _EditOperations.OP_INSERT,
     }
 
     def _replace_operation_or_retain(
-        operation: _EDIT_OPERATIONS, _flip_operations: Dict[_EDIT_OPERATIONS, _EDIT_OPERATIONS]
-    ) -> _EDIT_OPERATIONS:
+        operation: _EditOperations, _flip_operations: Dict[_EditOperations, _EditOperations]
+    ) -> _EditOperations:
         if operation in _flip_operations:
             return _flip_operations.get(operation)  # type: ignore
         return operation
 
-    inverted_trace = tuple(_replace_operation_or_retain(operation, _flip_operations) for operation in trace)
-    return inverted_trace
+    return tuple(_replace_operation_or_retain(operation, _flip_operations) for operation in trace)
 
 
-def _trace_to_alignment(trace: Tuple[_EDIT_OPERATIONS, ...]) -> Tuple[Dict[int, int], List[int], List[int]]:
+def _trace_to_alignment(trace: Tuple[_EditOperations, ...]) -> Tuple[Dict[int, int], List[int], List[int]]:
     """Transform trace of edit operations into an alignment of the sequences.
 
     Args:
         trace: A trace of edit operations as a tuple of `_EDIT_OPERATIONS` enumerates.
 
     Return:
         alignments: A dictionary mapping aligned positions between a reference and a hypothesis.
@@ -398,30 +389,30 @@
     reference_position = hypothesis_position = -1
     reference_errors: List[int] = []
     hypothesis_errors: List[int] = []
     alignments: Dict[int, int] = {}
 
     # we are rewriting a into b
     for operation in trace:
-        if operation == _EDIT_OPERATIONS.OP_NOTHING:
+        if operation == _EditOperations.OP_NOTHING:
             hypothesis_position += 1
             reference_position += 1
             alignments[reference_position] = hypothesis_position
             reference_errors.append(0)
             hypothesis_errors.append(0)
-        elif operation == _EDIT_OPERATIONS.OP_SUBSTITUTE:
+        elif operation == _EditOperations.OP_SUBSTITUTE:
             hypothesis_position += 1
             reference_position += 1
             alignments[reference_position] = hypothesis_position
             reference_errors.append(1)
             hypothesis_errors.append(1)
-        elif operation == _EDIT_OPERATIONS.OP_INSERT:
+        elif operation == _EditOperations.OP_INSERT:
             hypothesis_position += 1
             hypothesis_errors.append(1)
-        elif operation == _EDIT_OPERATIONS.OP_DELETE:
+        elif operation == _EditOperations.OP_DELETE:
             reference_position += 1
             alignments[reference_position] = hypothesis_position
             reference_errors.append(1)
         else:
             raise ValueError(f"Unknown operation {operation!r}.")
 
     return alignments, reference_errors, hypothesis_errors
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/text/mer.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/mer.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -58,21 +58,19 @@
 
     Returns:
         Match error rate score
     """
     return errors / total
 
 
-def match_error_rate(
-    preds: Union[str, List[str]],
-    target: Union[str, List[str]],
-) -> Tensor:
-    """Match error rate is a metric of the performance of an automatic speech recognition system. This value
-    indicates the percentage of words that were incorrectly predicted and inserted. The lower the value, the better
-    the performance of the ASR system with a MatchErrorRate of 0 being a perfect score.
+def match_error_rate(preds: Union[str, List[str]], target: Union[str, List[str]]) -> Tensor:
+    """Match error rate is a metric of the performance of an automatic speech recognition system.
+
+    This value indicates the percentage of words that were incorrectly predicted and inserted. The lower the value, the
+    better the performance of the ASR system with a MatchErrorRate of 0 being a perfect score.
 
     Args:
         preds: Transcription(s) to score as a string or list of strings
         target: Reference(s) for each speech input as a string or list of strings
 
     Returns:
         Match error rate score
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/text/rouge.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/rouge.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,23 +1,25 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import re
+import urllib.request
 from collections import Counter
 from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union
+from urllib.request import HTTPError
 
 import torch
 from torch import Tensor, tensor
 from typing_extensions import Literal
 
 from torchmetrics.utilities.imports import _NLTK_AVAILABLE
 
@@ -35,52 +37,74 @@
     "rouge9": 9,
     "rougeL": "L",
     "rougeLsum": "Lsum",
 }
 ALLOWED_ACCUMULATE_VALUES = ("avg", "best")
 
 
+def _ensure_nltk_punkt_is_downloaded() -> None:
+    """Check whether `nltk` `punkt` is downloaded.
+
+    If not, try to download if a machine is connected to the internet.
+    """
+    import nltk
+
+    try:
+        nltk.data.find("tokenizers/punkt.zip")
+    except LookupError:
+        try:
+            nltk.download("punkt", quiet=True, force=False, halt_on_error=False, raise_on_error=True)
+        except ValueError:
+            raise OSError(
+                "`nltk` resource `punkt` is not available on a disk and cannot be downloaded as a machine is not "
+                "connected to the internet."
+            )
+
+
 def _split_sentence(x: str) -> Sequence[str]:
-    """The sentence is split to get rougeLsum scores matching published rougeL scores for BART and PEGASUS."""
+    """Split sentence to get rougeLsum scores matching published rougeL scores for BART and PEGASUS."""
     if not _NLTK_AVAILABLE:
         raise ModuleNotFoundError("ROUGE-Lsum calculation requires that `nltk` is installed. Use `pip install nltk`.")
     import nltk
 
-    nltk.download("punkt", quiet=True, force=False)
+    _ensure_nltk_punkt_is_downloaded()
 
     re.sub("<n>", "", x)  # remove pegasus newline char
     return nltk.sent_tokenize(x)
 
 
 def _compute_metrics(hits_or_lcs: int, pred_len: int, target_len: int) -> Dict[str, Tensor]:
-    """This computes precision, recall and F1 score based on hits/lcs, and the length of lists of tokenizer
+    """Compute overall metrics.
+
+    This function computes precision, recall and F1 score based on hits/lcs, the length of lists of tokenizer
     predicted and target sentences.
 
     Args:
         hits_or_lcs: A number of matches or a length of the longest common subsequence.
         pred_len: A length of a tokenized predicted sentence.
         target_len: A length of a tokenized target sentence.
     """
     precision = hits_or_lcs / pred_len
     recall = hits_or_lcs / target_len
     if precision == recall == 0.0:
-        return dict(precision=tensor(0.0), recall=tensor(0.0), fmeasure=tensor(0.0))
+        return {"precision": tensor(0.0), "recall": tensor(0.0), "fmeasure": tensor(0.0)}
 
     fmeasure = 2 * precision * recall / (precision + recall)
-    return dict(precision=tensor(precision), recall=tensor(recall), fmeasure=tensor(fmeasure))
+    return {"precision": tensor(precision), "recall": tensor(recall), "fmeasure": tensor(fmeasure)}
 
 
 def _lcs(
     pred_tokens: Sequence[str], target_tokens: Sequence[str], return_full_table: bool = False
 ) -> Union[int, Sequence[Sequence[int]]]:
-    """Common DP algorithm to compute the length of the longest common subsequence.
+    """DP algorithm to compute the length of the longest common subsequence.
 
     Args:
         pred_tokens: A tokenized predicted sentence.
         target_tokens: A tokenized target sentence.
+        return_full_table: If the full table of logest common subsequence should be returned or just the largest
     """
     lcs = [[0] * (len(pred_tokens) + 1) for _ in range(len(target_tokens) + 1)]
     for i in range(1, len(target_tokens) + 1):
         for j in range(1, len(pred_tokens) + 1):
             if target_tokens[i - 1] == pred_tokens[j - 1]:
                 lcs[i][j] = lcs[i - 1][j - 1] + 1
             else:
@@ -112,77 +136,71 @@
             i -= 1
         else:
             j -= 1
     return backtracked_lcs
 
 
 def _union_lcs(pred_tokens_list: Sequence[Sequence[str]], target_tokens: Sequence[str]) -> Sequence[str]:
-    """Find union LCS between a target sentence and iterable of predicted tokens.
+    r"""Find union LCS between a target sentence and iterable of predicted tokens.
 
     Args:
-        pred_tokens_list: A tokenized predicted sentence split by '\n'.
-        target_tokens: A tokenized single part of target sentence split by '\n'.
-
-    Return:
+        pred_tokens_list: A tokenized predicted sentence split by ``'\n'``.
+        target_tokens: A tokenized single part of target sentence split by ``'\n'``.
     """
 
     def lcs_ind(pred_tokens: Sequence[str], target_tokens: Sequence[str]) -> Sequence[int]:
-        """Returns one of the longest of longest common subsequence via backtracked lcs table."""
+        """Return one of the longest of longest common subsequence via backtracked lcs table."""
         lcs_table: Sequence[Sequence[int]] = _lcs(pred_tokens, target_tokens, return_full_table=True)  # type: ignore
-        backtracked_lcs_table = _backtracked_lcs(lcs_table, pred_tokens, target_tokens)
-        return backtracked_lcs_table
+        return _backtracked_lcs(lcs_table, pred_tokens, target_tokens)
 
     def find_union(lcs_tables: Sequence[Sequence[int]]) -> Sequence[int]:
         """Find union LCS given a list of LCS."""
-        return sorted(list(set().union(*lcs_tables)))  # type: ignore
+        return sorted(set().union(*lcs_tables))
 
     lcs_tables = [lcs_ind(pred_tokens, target_tokens) for pred_tokens in pred_tokens_list]
-    union_lcs = [target_tokens[i] for i in find_union(lcs_tables)]
-    return union_lcs
+    return [target_tokens[i] for i in find_union(lcs_tables)]
 
 
 def _normalize_and_tokenize_text(
     text: str,
     stemmer: Optional[Any] = None,
-    normalizer: Callable[[str], str] = None,
-    tokenizer: Callable[[str], Sequence[str]] = None,
+    normalizer: Optional[Callable[[str], str]] = None,
+    tokenizer: Optional[Callable[[str], Sequence[str]]] = None,
 ) -> Sequence[str]:
-    """Rouge score should be calculated only over lowercased words and digits. Optionally, Porter stemmer can be
-    used to strip word suffixes to improve matching. The text normalization follows the implemantion from `Rouge
-    score_Text Normalizition`_
+    """Rouge score should be calculated only over lowercased words and digits.
+
+    Optionally, Porter stemmer can be used to strip word suffixes to improve matching. The text normalization follows
+    the implemantion from `Rouge score_Text Normalizition`_.
 
     Args:
         text: An input sentence.
         stemmer: Porter stemmer instance to strip word suffixes to improve matching.
         normalizer: A user's own normalizer function.
             If this is ``None``, replacing any non-alpha-numeric characters with spaces is default.
             This function must take a ``str`` and return a ``str``.
         tokenizer:
             A user's own tokenizer function. If this is ``None``, splitting by spaces is default
             This function must take a ``str`` and return ``Sequence[str]``
     """
-
     # If normalizer is none, replace any non-alpha-numeric characters with spaces.
     text = normalizer(text) if callable(normalizer) else re.sub(r"[^a-z0-9]+", " ", text.lower())
 
     # If tokenizer is none, spliting by spaces
     tokens = tokenizer(text) if callable(tokenizer) else re.split(r"\s+", text)
 
     if stemmer:
         # Only stem words more than 3 characters long.
         tokens = [stemmer.stem(x) if len(x) > 3 else x for x in tokens]
 
     # One final check to drop any empty or invalid tokens.
-    tokens = [x for x in tokens if (isinstance(x, str) and len(x) > 0)]
-
-    return tokens
+    return [x for x in tokens if (isinstance(x, str) and len(x) > 0)]
 
 
 def _rouge_n_score(pred: Sequence[str], target: Sequence[str], n_gram: int) -> Dict[str, Tensor]:
-    """This computes precision, recall and F1 score for the Rouge-N metric.
+    """Compute precision, recall and F1 score for the Rouge-N metric.
 
     Args:
         pred: A predicted sentence.
         target: A target sentence.
         n_gram: N-gram overlap.
     """
 
@@ -191,52 +209,54 @@
         for ngram in (tuple(tokens[i : i + n]) for i in range(len(tokens) - n + 1)):
             ngrams[ngram] += 1
         return ngrams
 
     pred_ngrams, target_ngrams = _create_ngrams(pred, n_gram), _create_ngrams(target, n_gram)
     pred_len, target_len = sum(pred_ngrams.values()), sum(target_ngrams.values())
     if 0 in (pred_len, target_len):
-        return dict(precision=tensor(0.0), recall=tensor(0.0), fmeasure=tensor(0.0))
+        return {"precision": tensor(0.0), "recall": tensor(0.0), "fmeasure": tensor(0.0)}
 
     # It is sufficient to take a set(pred_tokenized) for hits count as we consider intersenction of pred & target
     hits = sum(min(pred_ngrams[w], target_ngrams[w]) for w in set(pred_ngrams))
     return _compute_metrics(hits, max(pred_len, 1), max(target_len, 1))
 
 
 def _rouge_l_score(pred: Sequence[str], target: Sequence[str]) -> Dict[str, Tensor]:
-    """This computes precision, recall and F1 score for the Rouge-L metric.
+    """Compute precision, recall and F1 score for the Rouge-L metric.
 
     Args:
         pred: A predicted sentence.
         target: A target sentence.
     """
     pred_len, target_len = len(pred), len(target)
     if 0 in (pred_len, target_len):
-        return dict(precision=tensor(0.0), recall=tensor(0.0), fmeasure=tensor(0.0))
+        return {"precision": tensor(0.0), "recall": tensor(0.0), "fmeasure": tensor(0.0)}
 
     lcs: int = _lcs(pred, target)  # type: ignore
     return _compute_metrics(lcs, pred_len, target_len)
 
 
 def _rouge_lsum_score(pred: Sequence[Sequence[str]], target: Sequence[Sequence[str]]) -> Dict[str, Tensor]:
-    """This computes precision, recall and F1 score for the Rouge-LSum metric. More information can be found in Section
-    3.2 of the referenced paper [1]. This implementation follow the official implementation from:
-    https://github.com/google-research/google-research/blob/master/rouge/rouge_scorer.py
+    r"""Compute precision, recall and F1 score for the Rouge-LSum metric.
+
+    More information can be found in Section 3.2 of the referenced paper [1]. This implementation follow the official
+    implementation from:
+    https://github.com/google-research/google-research/blob/master/rouge/rouge_scorer.py.
 
     Args:
         pred: An iterable of predicted sentence split by '\n'.
         target: An iterable target sentence split by '\n'.
 
-    References
+    References:
         [1] ROUGE: A Package for Automatic Evaluation of Summaries by Chin-Yew Lin. https://aclanthology.org/W04-1013/
     """
     pred_len = sum(map(len, pred))
     target_len = sum(map(len, target))
     if 0 in (pred_len, target_len):
-        return dict(precision=tensor(0.0), recall=tensor(0.0), fmeasure=tensor(0.0))
+        return {"precision": tensor(0.0), "recall": tensor(0.0), "fmeasure": tensor(0.0)}
 
     # Get token counts
     def _get_token_counts(sentences: Sequence[Sequence[str]]) -> Counter:
         ngrams: Counter = Counter()
         for sentence in sentences:
             ngrams.update(sentence)
         return ngrams
@@ -259,16 +279,16 @@
 
 def _rouge_score_update(
     preds: Sequence[str],
     target: Sequence[Sequence[str]],
     rouge_keys_values: List[Union[int, str]],
     accumulate: str,
     stemmer: Optional[Any] = None,
-    normalizer: Callable[[str], str] = None,
-    tokenizer: Callable[[str], Sequence[str]] = None,
+    normalizer: Optional[Callable[[str], str]] = None,
+    tokenizer: Optional[Callable[[str], Sequence[str]]] = None,
 ) -> Dict[Union[int, str], List[Dict[str, Tensor]]]:
     """Update the rouge score with the current set of predicted and target sentences.
 
     Args:
         preds: An iterable of predicted sentences.
         target: An iterable of iterable of target sentences.
         rouge_keys_values: List of N-grams/'L'/'Lsum' arguments.
@@ -311,18 +331,19 @@
     results: Dict[Union[int, str], List[Dict[str, Tensor]]] = {rouge_key: [] for rouge_key in rouge_keys_values}
 
     for pred_raw, target_raw in zip(preds, target):
         result_inner: Dict[Union[int, str], Dict[str, Tensor]] = {rouge_key: {} for rouge_key in rouge_keys_values}
         result_avg: Dict[Union[int, str], List[Dict[str, Tensor]]] = {rouge_key: [] for rouge_key in rouge_keys_values}
         list_results = []
         pred = _normalize_and_tokenize_text(pred_raw, stemmer, normalizer, tokenizer)
-        pred_lsum = [
-            _normalize_and_tokenize_text(pred_sentence, stemmer, normalizer, tokenizer)
-            for pred_sentence in _split_sentence(pred_raw)
-        ]
+        if "Lsum" in rouge_keys_values:
+            pred_lsum = [
+                _normalize_and_tokenize_text(pred_sentence, stemmer, normalizer, tokenizer)
+                for pred_sentence in _split_sentence(pred_raw)
+            ]
 
         for target_raw_inner in target_raw:
             tgt = _normalize_and_tokenize_text(target_raw_inner, stemmer, normalizer, tokenizer)
 
             if "Lsum" in rouge_keys_values:
                 target_lsum = [
                     _normalize_and_tokenize_text(tgt_sentence, stemmer, normalizer, tokenizer)
@@ -388,17 +409,17 @@
 
 
 def rouge_score(
     preds: Union[str, Sequence[str]],
     target: Union[str, Sequence[str], Sequence[Sequence[str]]],
     accumulate: Literal["avg", "best"] = "best",
     use_stemmer: bool = False,
-    normalizer: Callable[[str], str] = None,
-    tokenizer: Callable[[str], Sequence[str]] = None,
-    rouge_keys: Union[str, Tuple[str, ...]] = ("rouge1", "rouge2", "rougeL", "rougeLsum"),  # type: ignore
+    normalizer: Optional[Callable[[str], str]] = None,
+    tokenizer: Optional[Callable[[str], Sequence[str]]] = None,
+    rouge_keys: Union[str, Tuple[str, ...]] = ("rouge1", "rouge2", "rougeL", "rougeLsum"),
 ) -> Dict[str, Tensor]:
     """Calculate `Calculate Rouge Score`_ , used for automatic summarization.
 
     Args:
         preds: An iterable of predicted sentences or a single predicted sentence.
         target:
             An iterable of iterables of target sentences or an iterable of target sentences or a single target sentence.
@@ -445,15 +466,14 @@
             If the python package ``nltk`` is not installed.
         ValueError:
             If any of the ``rouge_keys`` does not belong to the allowed set of keys.
 
     References:
         [1] ROUGE: A Package for Automatic Evaluation of Summaries by Chin-Yew Lin. https://aclanthology.org/W04-1013/
     """
-
     if use_stemmer:
         if not _NLTK_AVAILABLE:
             raise ModuleNotFoundError("Stemmer requires that `nltk` is installed. Use `pip install nltk`.")
         import nltk
 
     stemmer = nltk.stem.porter.PorterStemmer() if use_stemmer else None
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/text/sacre_bleu.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/sacre_bleu.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -131,40 +131,38 @@
     ) -> Sequence[str]:
         tokenize_fn = getattr(cls, cls._TOKENIZE_FN[tokenize])
         tokenized_line = tokenize_fn(line)
         return cls._lower(tokenized_line, lowercase).split()
 
     @classmethod
     def _tokenize_regex(cls, line: str) -> str:
-        """Common post-processing tokenizer for `13a` and `zh` tokenizers.
+        """Post-processing tokenizer for `13a` and `zh` tokenizers.
 
         Args:
             line: a segment to tokenize
 
         Return:
             the tokenized line
         """
-        for (_re, repl) in cls._REGEX:
+        for _re, repl in cls._REGEX:
             line = _re.sub(repl, line)
         # no leading or trailing spaces, single space within words
         return " ".join(line.split())
 
     @staticmethod
     def _is_chinese_char(uchar: str) -> bool:
-        """
+        """Check if character is chinese.
+
         Args:
-            uchar: input char in unicode
+            uchar: input char in unicode.
 
         Return:
             whether the input char is a Chinese character.
         """
-        for start, end in _UCODE_RANGES:
-            if start <= uchar <= end:
-                return True
-        return False
+        return any(start <= uchar <= end for start, end in _UCODE_RANGES)
 
     @classmethod
     def _tokenize_base(cls, line: str) -> str:
         """Tokenizes an input line with the tokenizer.
 
         Args:
             line: a segment to tokenize
@@ -172,16 +170,15 @@
         Return:
             the tokenized line
         """
         return line
 
     @classmethod
     def _tokenize_13a(cls, line: str) -> str:
-        """Tokenizes an input line using a relatively minimal tokenization that is however equivalent to
-        mteval-v13a, used by WMT.
+        """Tokenizes an line using a relatively minimal tokenization that is equivalent to mteval-v13a, used by WMT.
 
         Args:
             line: input sentence
 
         Return:
             tokenized sentence
         """
@@ -196,26 +193,26 @@
             line = line.replace("&lt;", "<")
             line = line.replace("&gt;", ">")
 
         return cls._tokenize_regex(line)
 
     @classmethod
     def _tokenize_zh(cls, line: str) -> str:
-        """The tokenization of Chinese text in this script contains two
-        steps: separate each Chinese characters (by utf-8 encoding); tokenize
-        the Chinese part (following the `13a` i.e. mteval tokenizer).
-        Author: Shujian Huang huangsj@nju.edu.cn
+        """Tokenization of Chinese text.
+
+        This is done in two steps: separate each Chinese characters (by utf-8 encoding) and afterwards tokenize the
+        Chinese part (following the `13a` i.e. mteval tokenizer).
+        Author: Shujian Huang huangsj@nju.edu.cn.
 
         Args:
             line: input sentence
 
         Return:
             tokenized sentence
         """
-
         line = line.strip()
         line_in_chars = ""
 
         for char in line:
             if cls._is_chinese_char(char):
                 line_in_chars += " "
                 line_in_chars += char
@@ -223,15 +220,15 @@
             else:
                 line_in_chars += char
 
         return cls._tokenize_regex(line_in_chars)
 
     @classmethod
     def _tokenize_international(cls, line: str) -> str:
-        """Tokenizes a string following the official BLEU implementation.
+        r"""Tokenizes a string following the official BLEU implementation.
 
         See github.com/moses-smt/mosesdecoder/blob/master/scripts/generic/mteval-v14.pl#L954-L983
 
         In our case, the input string is expected to be just one line.
         We just tokenize on punctuation and symbols,
         except when a punctuation is preceded and followed by a digit
         (e.g. a comma/dot as a thousand/decimal separator).
@@ -248,15 +245,15 @@
 
         Args:
             line: the input string to tokenize.
 
         Return:
             The tokenized string.
         """
-        for (_re, repl) in cls._INT_REGEX:
+        for _re, repl in cls._INT_REGEX:
             line = _re.sub(repl, line)
 
         return " ".join(line.split())
 
     @classmethod
     def _tokenize_char(cls, line: str) -> str:
         """Tokenizes all the characters in the input line.
@@ -281,16 +278,17 @@
     target: Sequence[Sequence[str]],
     n_gram: int = 4,
     smooth: bool = False,
     tokenize: Literal["none", "13a", "zh", "intl", "char"] = "13a",
     lowercase: bool = False,
     weights: Optional[Sequence[float]] = None,
 ) -> Tensor:
-    """Calculate `BLEU score`_ [1] of machine translated text with one or more references. This implementation
-    follows the behaviour of SacreBLEU [2] implementation from https://github.com/mjpost/sacrebleu.
+    """Calculate `BLEU score`_ [1] of machine translated text with one or more references.
+
+    This implementation follows the behaviour of SacreBLEU [2] implementation from https://github.com/mjpost/sacrebleu.
 
     Args:
         preds: An iterable of machine translated corpus
         target: An iterable of iterables of reference corpus
         n_gram: Gram value ranged from 1 to 4
         smooth: Whether to apply smoothing  see [2]
         tokenize: Tokenization technique to be used.
@@ -304,30 +302,29 @@
         Tensor with BLEU Score
 
     Raises:
         ValueError: If ``preds`` and ``target`` corpus have different lengths.
         ValueError: If a length of a list of weights is not ``None`` and not equal to ``n_gram``.
 
     Example:
-        >>> from torchmetrics.functional import sacre_bleu_score
+        >>> from torchmetrics.functional.text import sacre_bleu_score
         >>> preds = ['the cat is on the mat']
         >>> target = [['there is a cat on the mat', 'a cat is on the mat']]
         >>> sacre_bleu_score(preds, target)
         tensor(0.7598)
 
     References:
         [1] BLEU: a Method for Automatic Evaluation of Machine Translation by Papineni,
         Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu `BLEU`_
 
         [2] A Call for Clarity in Reporting BLEU Scores by Matt Post.
 
         [3] Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence
         and Skip-Bigram Statistics by Chin-Yew Lin and Franz Josef Och `Machine Translation Evolution`_
     """
-
     if tokenize not in AVAILABLE_TOKENIZERS:
         raise ValueError(f"Argument `tokenize` expected to be one of {AVAILABLE_TOKENIZERS} but got {tokenize}.")
 
     if tokenize not in _SacreBLEUTokenizer._TOKENIZE_FN.keys():
         raise ValueError(
             f"Unsupported tokenizer selected. Please, choose one of {list(_SacreBLEUTokenizer._TOKENIZE_FN.keys())}"
         )
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/text/squad.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/squad.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -71,72 +71,68 @@
     if len(target_tokens) == 0 or len(predicted_tokens) == 0:
         # If either is no-answer, then F1 is 1 if they agree, 0 otherwise
         return tensor(int(target_tokens == predicted_tokens))
     if num_same == 0:
         return tensor(0.0)
     precision = 1.0 * num_same / tensor(len(predicted_tokens))
     recall = 1.0 * num_same / tensor(len(target_tokens))
-    f1 = (2 * precision * recall) / (precision + recall)
-    return f1
+    return (2 * precision * recall) / (precision + recall)
 
 
 def _compute_exact_match_score(prediction: str, ground_truth: str) -> Tensor:
     """Compute Exact Match for two sentences."""
     return tensor(int(_normalize_text(prediction) == _normalize_text(ground_truth)))
 
 
 def _metric_max_over_ground_truths(
     metric_fn: Callable[[str, str], Tensor], prediction: str, ground_truths: List[str]
 ) -> Tensor:
     """Calculate maximum score for a predicted answer with all reference answers."""
-    return max(metric_fn(prediction, truth) for truth in ground_truths)
+    return max(metric_fn(prediction, truth) for truth in ground_truths)  # type: ignore[type-var]
 
 
 def _squad_input_check(
     preds: PREDS_TYPE, targets: TARGETS_TYPE
 ) -> Tuple[Dict[str, str], List[Dict[str, List[Dict[str, List[Dict[str, Any]]]]]]]:
     """Check for types and convert the input to necessary format to compute the input."""
-
     if isinstance(preds, Dict):
         preds = [preds]
 
     if isinstance(targets, Dict):
         targets = [targets]
 
     for pred in preds:
-        keys = pred.keys()
-        if "prediction_text" not in keys or "id" not in keys:
+        pred_keys = pred.keys()
+        if "prediction_text" not in pred_keys or "id" not in pred_keys:
             raise KeyError(
                 "Expected keys in a single prediction are 'prediction_text' and 'id'."
                 "Please make sure that 'prediction_text' maps to the answer string and 'id' maps to the key string."
             )
 
     for target in targets:
-        keys = target.keys()
-        if "answers" not in keys or "id" not in keys:
+        target_keys = target.keys()
+        if "answers" not in target_keys or "id" not in target_keys:
             raise KeyError(
                 "Expected keys in a single target are 'answers' and 'id'."
                 "Please make sure that 'answers' maps to a `SQuAD` format dictionary and 'id' maps to the key string.\n"
                 "SQuAD Format: "
                 f"{SQuAD_FORMAT}"
             )
 
-        answers: Dict[str, Union[List[str], List[int]]] = target["answers"]  # type: ignore
+        answers: Dict[str, Union[List[str], List[int]]] = target["answers"]  # type: ignore[assignment]
         if "text" not in answers.keys():
             raise KeyError(
                 "Expected keys in a 'answers' are 'text'."
                 "Please make sure that 'answer' maps to a `SQuAD` format dictionary.\n"
                 "SQuAD Format: "
                 f"{SQuAD_FORMAT}"
             )
 
     preds_dict = {prediction["id"]: prediction["prediction_text"] for prediction in preds}
-    _fn_answer = lambda tgt: dict(
-        answers=[dict(text=txt) for txt in tgt["answers"]["text"]], id=tgt["id"]  # type: ignore
-    )
+    _fn_answer = lambda tgt: {"answers": [{"text": txt} for txt in tgt["answers"]["text"]], "id": tgt["id"]}
     targets_dict = [{"paragraphs": [{"qas": [_fn_answer(target) for target in targets]}]}]
     return preds_dict, targets_dict
 
 
 def _squad_update(
     preds: Dict[str, str],
     target: List[Dict[str, List[Dict[str, List[Dict[str, Any]]]]]],
@@ -171,16 +167,16 @@
     for article in target:
         for paragraph in article["paragraphs"]:
             for qa in paragraph["qas"]:
                 total += 1
                 if qa["id"] not in preds:
                     rank_zero_warn(f"Unanswered question {qa['id']} will receive score 0.")
                     continue
-                ground_truths = list(map(lambda x: x["text"], qa["answers"]))  # type: ignore
-                pred = preds[qa["id"]]  # type: ignore
+                ground_truths = [x["text"] for x in qa["answers"]]
+                pred = preds[qa["id"]]
                 exact_match += _metric_max_over_ground_truths(_compute_exact_match_score, pred, ground_truths)
                 f1 += _metric_max_over_ground_truths(_compute_f1_score, pred, ground_truths)
 
     return f1, exact_match, total
 
 
 def _squad_compute(f1: Tensor, exact_match: Tensor, total: Tensor) -> Dict[str, Tensor]:
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/text/ter.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/ter.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -167,28 +167,26 @@
         # 30A030FF Katakana
         # 31F031FF Katakana Phonetic Extensions
         sentence = re.sub(r"(^|^[\u3040-\u309f])([\u3040-\u309f]+)(?=$|^[\u3040-\u309f])", r"\1 \2 ", sentence)
         sentence = re.sub(r"(^|^[\u30a0-\u30ff])([\u30a0-\u30ff]+)(?=$|^[\u30a0-\u30ff])", r"\1 \2 ", sentence)
         sentence = re.sub(r"(^|^[\u31f0-\u31ff])([\u31f0-\u31ff]+)(?=$|^[\u31f0-\u31ff])", r"\1 \2 ", sentence)
 
         sentence = re.sub(cls._ASIAN_PUNCTUATION, r" \1 ", sentence)
-        sentence = re.sub(cls._FULL_WIDTH_PUNCTUATION, r" \1 ", sentence)
-        return sentence
+        return re.sub(cls._FULL_WIDTH_PUNCTUATION, r" \1 ", sentence)
 
     @staticmethod
     def _remove_punct(sentence: str) -> str:
         """Remove punctuation from an input sentence string."""
         return re.sub(r"[\.,\?:;!\"\(\)]", "", sentence)
 
     @classmethod
     def _remove_asian_punct(cls, sentence: str) -> str:
         """Remove asian punctuation from an input sentence string."""
         sentence = re.sub(cls._ASIAN_PUNCTUATION, r"", sentence)
-        sentence = re.sub(cls._FULL_WIDTH_PUNCTUATION, r"", sentence)
-        return sentence
+        return re.sub(cls._FULL_WIDTH_PUNCTUATION, r"", sentence)
 
 
 def _preprocess_sentence(sentence: str, tokenizer: _TercomTokenizer) -> str:
     """Given a sentence, apply tokenization.
 
     Args:
         sentence: The input sentence string.
@@ -242,16 +240,15 @@
     alignments: Dict[int, int],
     pred_errors: List[int],
     target_errors: List[int],
     pred_start: int,
     target_start: int,
     length: int,
 ) -> bool:
-    """A helper function which returns ``True`` if any of corner cases has been met. Otherwise, ``False`` is
-    returned.
+    """Return ``True`` if any of corner cases has been met. Otherwise, ``False`` is returned.
 
     Args:
         alignments: A dictionary mapping aligned positions between a reference and a hypothesis.
         pred_errors: A list of error positions in a hypothesis.
         target_errors: A list of error positions in a reference.
         pred_start: A hypothesis start index.
         target_start: A reference start index.
@@ -310,20 +307,21 @@
 
 def _shift_words(
     pred_words: List[str],
     target_words: List[str],
     cached_edit_distance: _LevenshteinEditDistance,
     checked_candidates: int,
 ) -> Tuple[int, List[str], int]:
-    """Attempt to shift words to match a hypothesis with a reference. It returns the lowest number of required
-    edits between a hypothesis and a provided reference, a list of shifted words and number of checked candidates.
+    """Attempt to shift words to match a hypothesis with a reference.
 
-    Note that the filtering of possible shifts and shift selection are heavily based on somewhat arbitrary heuristics.
-    The code here follows as closely as possible the logic in Tercom, not always justifying the particular design
-    choices. (The paragraph copied from https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/lib_ter.py)
+    It returns the lowest number of required edits between a hypothesis and a provided reference, a list of shifted
+    words and number of checked candidates. Note that the filtering of possible shifts and shift selection are heavily
+    based on somewhat arbitrary heuristics. The code here follows as closely as possible the logic in Tercom, not
+    always justifying the particular design choices.
+    The paragraph copied from https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/lib_ter.py.
 
     Args:
         pred_words: A list of tokenized hypothesis sentence.
         target_words: A list of lists of tokenized reference sentences.
         cached_edit_distance: A pre-computed edit distance between a hypothesis and a reference.
         checked_candidates: A number of checked hypothesis candidates to match a provided reference.
 
@@ -454,20 +452,18 @@
         num_edits: A number of required edits to match hypothesis and reference sentences.
         tgt_length: An average length of reference sentences.
 
     Return:
         A corpus-level TER score or 1 if reference_length == 0.
     """
     if tgt_length > 0 and num_edits > 0:
-        score = num_edits / tgt_length
-    elif tgt_length == 0 and num_edits > 0:
-        score = tensor(1.0)
-    else:
-        score = tensor(0.0)
-    return score
+        return num_edits / tgt_length
+    if tgt_length == 0 and num_edits > 0:
+        return tensor(1.0)
+    return tensor(0.0)
 
 
 def _ter_update(
     preds: Union[str, Sequence[str]],
     target: Sequence[Union[str, Sequence[str]]],
     tokenizer: _TercomTokenizer,
     total_num_edits: Tensor,
@@ -475,17 +471,18 @@
     sentence_ter: Optional[List[Tensor]] = None,
 ) -> Tuple[Tensor, Tensor, Optional[List[Tensor]]]:
     """Update TER statistics.
 
     Args:
         preds: An iterable of hypothesis corpus.
         target: An iterable of iterables of reference corpus.
-        tokenizer:
+        tokenizer: An instance of ``_TercomTokenizer`` handling a sentence tokenization.
         total_num_edits: A total number of required edits to match hypothesis and reference sentences.
         total_tgt_length: A total average length of reference sentences.
+        sentence_ter: A list of sentence-level TER values
 
     Return:
         total_num_edits:
             A total number of required edits to match hypothesis and reference sentences.
         total_tgt_length:
             A total average length of reference sentences.
         sentence_ter:
@@ -493,27 +490,28 @@
 
     Raises:
         ValueError:
             If length of ``preds`` and ``target`` differs.
     """
     target, preds = _validate_inputs(target, preds)
 
-    for (pred, tgt) in zip(preds, target):
+    for pred, tgt in zip(preds, target):
         tgt_words_: List[List[str]] = [_preprocess_sentence(_tgt, tokenizer).split() for _tgt in tgt]
         pred_words_: List[str] = _preprocess_sentence(pred, tokenizer).split()
         num_edits, tgt_length = _compute_sentence_statistics(pred_words_, tgt_words_)
         total_num_edits += num_edits
         total_tgt_length += tgt_length
         if sentence_ter is not None:
             sentence_ter.append(_compute_ter_score_from_statistics(num_edits, tgt_length).unsqueeze(0))
     return total_num_edits, total_tgt_length, sentence_ter
 
 
 def _ter_compute(total_num_edits: Tensor, total_tgt_length: Tensor) -> Tensor:
     """Compute TER based on pre-computed a total number of edits and a total average reference length.
+
     Args:
         total_num_edits: A total number of required edits to match hypothesis and reference sentences.
         total_tgt_length: A total average length of reference sentences.
 
     Return:
         A corpus-level TER score.
     """
@@ -525,16 +523,17 @@
     target: Sequence[Union[str, Sequence[str]]],
     normalize: bool = False,
     no_punctuation: bool = False,
     lowercase: bool = True,
     asian_support: bool = False,
     return_sentence_level_score: bool = False,
 ) -> Union[Tensor, Tuple[Tensor, List[Tensor]]]:
-    """Calculate Translation edit rate (`TER`_)  of machine translated text with one or more references. This
-    implementation follows the implmenetaions from
+    """Calculate Translation edit rate (`TER`_)  of machine translated text with one or more references.
+
+    This implementation follows the implmenetaions from
     https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/ter.py. The `sacrebleu` implmenetation is a
     near-exact reimplementation of the Tercom algorithm, produces identical results on all "sane" outputs.
 
     Args:
         preds: An iterable of hypothesis corpus.
         target: An iterable of iterables of reference corpus.
         normalize: An indication whether a general tokenization to be applied.
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/text/wer.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/wil.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -10,74 +10,82 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 from typing import List, Tuple, Union
 
-import torch
 from torch import Tensor, tensor
 
 from torchmetrics.functional.text.helper import _edit_distance
 
 
-def _wer_update(
+def _wil_update(
     preds: Union[str, List[str]],
     target: Union[str, List[str]],
-) -> Tuple[Tensor, Tensor]:
-    """Update the wer score with the current set of references and predictions.
+) -> Tuple[Tensor, Tensor, Tensor]:
+    """Update the wil score with the current set of references and predictions.
 
     Args:
         preds: Transcription(s) to score as a string or list of strings
         target: Reference(s) for each speech input as a string or list of strings
 
     Returns:
         Number of edit operations to get from the reference to the prediction, summed over all samples
         Number of words overall references
+        Number of words overall predictions
     """
     if isinstance(preds, str):
         preds = [preds]
     if isinstance(target, str):
         target = [target]
-    errors = tensor(0, dtype=torch.float)
-    total = tensor(0, dtype=torch.float)
+    total = tensor(0.0)
+    errors = tensor(0.0)
+    target_total = tensor(0.0)
+    preds_total = tensor(0.0)
     for pred, tgt in zip(preds, target):
         pred_tokens = pred.split()
-        tgt_tokens = tgt.split()
-        errors += _edit_distance(pred_tokens, tgt_tokens)
-        total += len(tgt_tokens)
-    return errors, total
+        target_tokens = tgt.split()
+        errors += _edit_distance(pred_tokens, target_tokens)
+        target_total += len(target_tokens)
+        preds_total += len(pred_tokens)
+        total += max(len(target_tokens), len(pred_tokens))
 
+    return errors - total, target_total, preds_total
 
-def _wer_compute(errors: Tensor, total: Tensor) -> Tensor:
-    """Compute the word error rate.
+
+def _wil_compute(errors: Tensor, target_total: Tensor, preds_total: Tensor) -> Tensor:
+    """Compute the Word Information Lost.
 
     Args:
         errors: Number of edit operations to get from the reference to the prediction, summed over all samples
-        total: Number of words overall references
+        target_total: Number of words overall references
+        preds_total: Number of words overall prediction
 
     Returns:
-        Word error rate score
+        Word Information Lost score
     """
-    return errors / total
+    return 1 - ((errors / target_total) * (errors / preds_total))
+
 
+def word_information_lost(preds: Union[str, List[str]], target: Union[str, List[str]]) -> Tensor:
+    """Word Information Lost rate is a metric of the performance of an automatic speech recognition system.
 
-def word_error_rate(preds: Union[str, List[str]], target: Union[str, List[str]]) -> Tensor:
-    """Word error rate (WordErrorRate_) is a common metric of the performance of an automatic speech recognition
-    system. This value indicates the percentage of words that were incorrectly predicted. The lower the value, the
-    better the performance of the ASR system with a WER of 0 being a perfect score.
+    This value indicates the percentage of characters that were incorrectly predicted. The lower the value, the better
+    the performance of the ASR system with a Word Information Lost rate of 0 being a perfect score.
 
     Args:
         preds: Transcription(s) to score as a string or list of strings
         target: Reference(s) for each speech input as a string or list of strings
 
     Returns:
-        Word error rate score
+        Word Information Lost rate
 
     Examples:
+        >>> from torchmetrics.functional.text import word_information_lost
         >>> preds = ["this is the prediction", "there is an other sample"]
         >>> target = ["this is the reference", "there is another one"]
-        >>> word_error_rate(preds=preds, target=target)
-        tensor(0.5000)
+        >>> word_information_lost(preds, target)
+        tensor(0.6528)
     """
-    errors, total = _wer_update(preds, target)
-    return _wer_compute(errors, total)
+    errors, target_total, preds_total = _wil_update(preds, target)
+    return _wil_compute(errors, target_total, preds_total)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/functional/text/wil.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/text/wip.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,42 +1,41 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
 from typing import List, Tuple, Union
 
 from torch import Tensor, tensor
 
 from torchmetrics.functional.text.helper import _edit_distance
 
 
-def _wil_update(
+def _wip_update(
     preds: Union[str, List[str]],
     target: Union[str, List[str]],
 ) -> Tuple[Tensor, Tensor, Tensor]:
-    """Update the wil score with the current set of references and predictions.
+    """Update the wip score with the current set of references and predictions.
 
     Args:
         preds: Transcription(s) to score as a string or list of strings
         target: Reference(s) for each speech input as a string or list of strings
 
     Returns:
         Number of edit operations to get from the reference to the prediction, summed over all samples
         Number of words overall references
-        Number of words overall predictions
+        Number of words overall prediction
     """
     if isinstance(preds, str):
         preds = [preds]
     if isinstance(target, str):
         target = [target]
     total = tensor(0.0)
     errors = tensor(0.0)
@@ -49,45 +48,43 @@
         target_total += len(target_tokens)
         preds_total += len(pred_tokens)
         total += max(len(target_tokens), len(pred_tokens))
 
     return errors - total, target_total, preds_total
 
 
-def _wil_compute(errors: Tensor, target_total: Tensor, preds_total: Tensor) -> Tensor:
-    """Compute the Word Information Lost.
+def _wip_compute(errors: Tensor, target_total: Tensor, preds_total: Tensor) -> Tensor:
+    """Compute the Word Information Perserved.
 
     Args:
         errors: Number of edit operations to get from the reference to the prediction, summed over all samples
         target_total: Number of words overall references
         preds_total: Number of words overall prediction
 
     Returns:
-        Word Information Lost score
+        Word Information Perserved score
     """
-    return 1 - ((errors / target_total) * (errors / preds_total))
+    return (errors / target_total) * (errors / preds_total)
 
 
-def word_information_lost(
-    preds: Union[str, List[str]],
-    target: Union[str, List[str]],
-) -> Tensor:
-    """Word Information Lost rate is a metric of the performance of an automatic speech recognition system. This
-    value indicates the percentage of characters that were incorrectly predicted. The lower the value, the better
-    the performance of the ASR system with a Word Information Lost rate of 0 being a perfect score.
+def word_information_preserved(preds: Union[str, List[str]], target: Union[str, List[str]]) -> Tensor:
+    """Word Information Preserved rate is a metric of the performance of an automatic speech recognition system.
+
+    This value indicates the percentage of characters that were incorrectly predicted. The lower the value, the
+    better the performance of the ASR system with a Word Information preserved rate of 0 being a perfect score.
 
     Args:
         preds: Transcription(s) to score as a string or list of strings
         target: Reference(s) for each speech input as a string or list of strings
 
     Returns:
-        Word Information Lost rate
+        Word Information preserved rate
 
     Examples:
-        >>> from torchmetrics.functional import word_information_lost
+        >>> from torchmetrics.functional.text import word_information_preserved
         >>> preds = ["this is the prediction", "there is an other sample"]
         >>> target = ["this is the reference", "there is another one"]
-        >>> word_information_lost(preds, target)
-        tensor(0.6528)
+        >>> word_information_preserved(preds, target)
+        tensor(0.3472)
     """
-    errors, target_total, preds_total = _wil_update(preds, target)
-    return _wil_compute(errors, target_total, preds_total)
+    errors, reference_total, prediction_total = _wip_update(preds, target)
+    return _wip_compute(errors, reference_total, prediction_total)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/image/__init__.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/text/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,31 +1,48 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from torchmetrics.image.d_lambda import SpectralDistortionIndex  # noqa: F401
-from torchmetrics.image.ergas import ErrorRelativeGlobalDimensionlessSynthesis  # noqa: F401
-from torchmetrics.image.psnr import PeakSignalNoiseRatio  # noqa: F401
-from torchmetrics.image.sam import SpectralAngleMapper  # noqa: F401
-from torchmetrics.image.ssim import (  # noqa: F401
-    MultiScaleStructuralSimilarityIndexMeasure,
-    StructuralSimilarityIndexMeasure,
-)
-from torchmetrics.image.uqi import UniversalImageQualityIndex  # noqa: F401
-from torchmetrics.utilities.imports import _LPIPS_AVAILABLE, _TORCH_FIDELITY_AVAILABLE
+from torchmetrics.text.bleu import BLEUScore
+from torchmetrics.text.cer import CharErrorRate
+from torchmetrics.text.chrf import CHRFScore
+from torchmetrics.text.eed import ExtendedEditDistance
+from torchmetrics.text.mer import MatchErrorRate
+from torchmetrics.text.perplexity import Perplexity
+from torchmetrics.text.rouge import ROUGEScore
+from torchmetrics.text.sacre_bleu import SacreBLEUScore
+from torchmetrics.text.squad import SQuAD
+from torchmetrics.text.ter import TranslationEditRate
+from torchmetrics.text.wer import WordErrorRate
+from torchmetrics.text.wil import WordInfoLost
+from torchmetrics.text.wip import WordInfoPreserved
+from torchmetrics.utilities.imports import _TRANSFORMERS_AVAILABLE
 
-if _TORCH_FIDELITY_AVAILABLE:
-    from torchmetrics.image.fid import FrechetInceptionDistance  # noqa: F401
-    from torchmetrics.image.inception import InceptionScore  # noqa: F401
-    from torchmetrics.image.kid import KernelInceptionDistance  # noqa: F401
+if _TRANSFORMERS_AVAILABLE:
+    from torchmetrics.text.bert import BERTScore  # noqa: F401
+    from torchmetrics.text.infolm import InfoLM  # noqa: F401
 
-if _LPIPS_AVAILABLE:
-    from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity  # noqa: F401
+
+__all__ = [
+    "BLEUScore",
+    "CharErrorRate",
+    "CHRFScore",
+    "ExtendedEditDistance",
+    "MatchErrorRate",
+    "Perplexity",
+    "ROUGEScore",
+    "SacreBLEUScore",
+    "SQuAD",
+    "TranslationEditRate",
+    "WordErrorRate",
+    "WordInfoLost",
+    "WordInfoPreserved",
+]
```

### Comparing `torchmetrics-0.9.3/torchmetrics/image/fid.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/image/inception.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,289 +1,213 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, List, Optional, Union
+from typing import Any, List, Optional, Sequence, Tuple, Union
 
-import numpy as np
 import torch
 from torch import Tensor
-from torch.autograd import Function
 from torch.nn import Module
 
+from torchmetrics.image.fid import NoTrainInceptionV3
 from torchmetrics.metric import Metric
-from torchmetrics.utilities import rank_zero_info, rank_zero_warn
+from torchmetrics.utilities import rank_zero_warn
 from torchmetrics.utilities.data import dim_zero_cat
-from torchmetrics.utilities.imports import _SCIPY_AVAILABLE, _TORCH_FIDELITY_AVAILABLE
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE, _TORCH_FIDELITY_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
-if _TORCH_FIDELITY_AVAILABLE:
-    from torch_fidelity.feature_extractor_inceptionv3 import FeatureExtractorInceptionV3
-else:
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["InceptionScore.plot"]
 
-    class FeatureExtractorInceptionV3(Module):  # type: ignore
-        pass
 
-    __doctest_skip__ = ["FrechetInceptionDistance", "FID"]
+__doctest_requires__ = {("InceptionScore", "InceptionScore.plot"): ["torch_fidelity"]}
 
 
-if _SCIPY_AVAILABLE:
-    import scipy
-
-
-class NoTrainInceptionV3(FeatureExtractorInceptionV3):
-    def __init__(
-        self,
-        name: str,
-        features_list: List[str],
-        feature_extractor_weights_path: Optional[str] = None,
-    ) -> None:
-        super().__init__(name, features_list, feature_extractor_weights_path)
-        # put into evaluation mode
-        self.eval()
-
-    def train(self, mode: bool) -> "NoTrainInceptionV3":
-        """the inception network should not be able to be switched away from evaluation mode."""
-        return super().train(False)
-
-    def forward(self, x: Tensor) -> Tensor:
-        out = super().forward(x)
-        return out[0].reshape(x.shape[0], -1)
-
-
-class MatrixSquareRoot(Function):
-    """Square root of a positive definite matrix.
-
-    All credit to `Square Root of a Positive Definite Matrix`_
-    """
-
-    @staticmethod
-    def forward(ctx: Any, input_data: Tensor) -> Tensor:
-        # TODO: update whenever pytorch gets an matrix square root function
-        # Issue: https://github.com/pytorch/pytorch/issues/9983
-        m = input_data.detach().cpu().numpy().astype(np.float_)
-        scipy_res, _ = scipy.linalg.sqrtm(m, disp=False)
-        sqrtm = torch.from_numpy(scipy_res.real).to(input_data)
-        ctx.save_for_backward(sqrtm)
-        return sqrtm
-
-    @staticmethod
-    def backward(ctx: Any, grad_output: Tensor) -> Tensor:
-        grad_input = None
-        if ctx.needs_input_grad[0]:
-            (sqrtm,) = ctx.saved_tensors
-            sqrtm = sqrtm.data.cpu().numpy().astype(np.float_)
-            gm = grad_output.data.cpu().numpy().astype(np.float_)
-
-            # Given a positive semi-definite matrix X,
-            # since X = X^{1/2}X^{1/2}, we can compute the gradient of the
-            # matrix square root dX^{1/2} by solving the Sylvester equation:
-            # dX = (d(X^{1/2})X^{1/2} + X^{1/2}(dX^{1/2}).
-            grad_sqrtm = scipy.linalg.solve_sylvester(sqrtm, sqrtm, gm)
-
-            grad_input = torch.from_numpy(grad_sqrtm).to(grad_output)
-        return grad_input
-
-
-sqrtm = MatrixSquareRoot.apply
-
-
-def _compute_fid(mu1: Tensor, sigma1: Tensor, mu2: Tensor, sigma2: Tensor, eps: float = 1e-6) -> Tensor:
-    r"""
-    Adjusted version of `Fid Score`_
-
-    The Frechet Inception Distance between two multivariate Gaussians X_x ~ N(mu_1, sigm_1)
-    and X_y ~ N(mu_2, sigm_2) is d^2 = ||mu_1 - mu_2||^2 + Tr(sigm_1 + sigm_2 - 2*sqrt(sigm_1*sigm_2)).
-
-    Args:
-        mu1: mean of activations calculated on predicted (x) samples
-        sigma1: covariance matrix over activations calculated on predicted (x) samples
-        mu2: mean of activations calculated on target (y) samples
-        sigma2: covariance matrix over activations calculated on target (y) samples
-        eps: offset constant - used if sigma_1 @ sigma_2 matrix is singular
-
-    Returns:
-        Scalar value of the distance between sets.
-    """
-    diff = mu1 - mu2
-
-    covmean = sqrtm(sigma1.mm(sigma2))
-    # Product might be almost singular
-    if not torch.isfinite(covmean).all():
-        rank_zero_info(f"FID calculation produces singular product; adding {eps} to diagonal of covariance estimates")
-        offset = torch.eye(sigma1.size(0), device=mu1.device, dtype=mu1.dtype) * eps
-        covmean = sqrtm((sigma1 + offset).mm(sigma2 + offset))
-
-    tr_covmean = torch.trace(covmean)
-    return diff.dot(diff) + torch.trace(sigma1) + torch.trace(sigma2) - 2 * tr_covmean
-
-
-class FrechetInceptionDistance(Metric):
-    r"""
-    Calculates Frchet inception distance (FID_) which is used to access the quality of generated images. Given by
+class InceptionScore(Metric):
+    r"""Calculate the Inception Score (IS) which is used to access how realistic generated images are.
 
     .. math::
-        FID = |\mu - \mu_w| + tr(\Sigma + \Sigma_w - 2(\Sigma \Sigma_w)^{\frac{1}{2}})
-
-    where :math:`\mathcal{N}(\mu, \Sigma)` is the multivariate normal distribution estimated from Inception v3 [1]
-    features calculated on real life images and :math:`\mathcal{N}(\mu_w, \Sigma_w)` is the multivariate normal
-    distribution estimated from Inception v3 features calculated on generated (fake) images. The metric was
-    originally proposed in [1].
-
-    Using the default feature extraction (Inception v3 using the original weights from [2]), the input is
-    expected to be mini-batches of 3-channel RGB images of shape (``3 x H x W``) with dtype uint8. All images
-    will be resized to 299 x 299 which is the size of the original training data. The boolian flag ``real``
-    determines if the images should update the statistics of the real distribution or the fake distribution.
+        IS = exp(\mathbb{E}_x KL(p(y | x ) || p(y)))
 
-    .. note:: using this metrics requires you to have ``scipy`` install. Either install as ``pip install
-        torchmetrics[image]`` or ``pip install scipy``
+    where :math:`KL(p(y | x) || p(y))` is the KL divergence between the conditional distribution :math:`p(y|x)`
+    and the margianl distribution :math:`p(y)`. Both the conditional and marginal distribution is calculated
+    from features extracted from the images. The score is calculated on random splits of the images such that
+    both a mean and standard deviation of the score are returned. The metric was originally proposed in
+    `inception ref1`_.
+
+    Using the default feature extraction (Inception v3 using the original weights from `inception ref2`_), the input
+    is expected to be mini-batches of 3-channel RGB images of shape ``(3 x H x W)``. If argument ``normalize``
+    is ``True`` images are expected to be dtype ``float`` and have values in the ``[0, 1]`` range, else if
+    ``normalize`` is set to ``False`` images are expected to have dtype uint8 and take values in the ``[0, 255]``
+    range. All images will be resized to 299 x 299 which is the size of the original training data.
 
     .. note:: using this metric with the default feature extractor requires that ``torch-fidelity``
         is installed. Either install as ``pip install torchmetrics[image]`` or
         ``pip install torch-fidelity``
 
+    As input to ``forward`` and ``update`` the metric accepts the following input
+
+    - ``imgs`` (:class:`~torch.Tensor`): tensor with images feed to the feature extractor
+
+    As output of `forward` and `compute` the metric returns the following output
+
+    - ``fid`` (:class:`~torch.Tensor`): float scalar tensor with mean FID value over samples
+
     Args:
         feature:
-            Either an integer or ``nn.Module``:
+            Either an str, integer or ``nn.Module``:
 
-            - an integer will indicate the inceptionv3 feature layer to choose. Can be one of the following:
-              64, 192, 768, 2048
+            - an str or integer will indicate the inceptionv3 feature layer to choose. Can be one of the following:
+              'logits_unbiased', 64, 192, 768, 2048
             - an ``nn.Module`` for using a custom feature extractor. Expects that its forward method returns
-              an ``[N,d]`` matrix where ``N`` is the batch size and ``d`` is the feature size.
+              an ``(N,d)`` matrix where ``N`` is the batch size and ``d`` is the feature size.
 
-        reset_real_features: Whether to also reset the real features. Since in many cases the real dataset does not
-            change, the features can cached them to avoid recomputing them which is costly. Set this to ``False`` if
-            your dataset does not change.
+        splits: integer determining how many splits the inception score calculation should be split among
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
-    References:
-        [1] Rethinking the Inception Architecture for Computer Vision
-        Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, Zbigniew Wojna
-        https://arxiv.org/abs/1512.00567
-
-        [2] GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,
-        Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter
-        https://arxiv.org/abs/1706.08500
-
     Raises:
         ValueError:
-            If ``feature`` is set to an ``int`` (default settings) and ``torch-fidelity`` is not installed
+            If ``feature`` is set to an ``str`` or ``int`` and ``torch-fidelity`` is not installed
         ValueError:
-            If ``feature`` is set to an ``int`` not in [64, 192, 768, 2048]
+            If ``feature`` is set to an ``str`` or ``int`` and not one of ``('logits_unbiased', 64, 192, 768, 2048)``
         TypeError:
             If ``feature`` is not an ``str``, ``int`` or ``torch.nn.Module``
-        ValueError:
-            If ``reset_real_features`` is not an ``bool``
 
     Example:
         >>> import torch
         >>> _ = torch.manual_seed(123)
-        >>> from torchmetrics.image.fid import FrechetInceptionDistance
-        >>> fid = FrechetInceptionDistance(feature=64)
-        >>> # generate two slightly overlapping image intensity distributions
-        >>> imgs_dist1 = torch.randint(0, 200, (100, 3, 299, 299), dtype=torch.uint8)
-        >>> imgs_dist2 = torch.randint(100, 255, (100, 3, 299, 299), dtype=torch.uint8)
-        >>> fid.update(imgs_dist1, real=True)
-        >>> fid.update(imgs_dist2, real=False)
-        >>> fid.compute()
-        tensor(12.7202)
-
+        >>> from torchmetrics.image.inception import InceptionScore
+        >>> inception = InceptionScore()
+        >>> # generate some images
+        >>> imgs = torch.randint(0, 255, (100, 3, 299, 299), dtype=torch.uint8)
+        >>> inception.update(imgs)
+        >>> inception.compute()
+        (tensor(1.0544), tensor(0.0117))
     """
-
-    higher_is_better: bool = False
     is_differentiable: bool = False
+    higher_is_better: bool = True
     full_state_update: bool = False
+    plot_lower_bound: float = 0.0
 
-    real_features: List[Tensor]
-    fake_features: List[Tensor]
+    features: List
 
     def __init__(
         self,
-        feature: Union[int, Module] = 2048,
-        reset_real_features: bool = True,
+        feature: Union[str, int, Module] = "logits_unbiased",
+        splits: int = 10,
+        normalize: bool = False,
         **kwargs: Any,
     ) -> None:
         super().__init__(**kwargs)
 
         rank_zero_warn(
-            "Metric `FrechetInceptionDistance` will save all extracted features in buffer."
+            "Metric `InceptionScore` will save all extracted features in buffer."
             " For large datasets this may lead to large memory footprint.",
             UserWarning,
         )
 
-        if isinstance(feature, int):
+        if isinstance(feature, (str, int)):
             if not _TORCH_FIDELITY_AVAILABLE:
                 raise ModuleNotFoundError(
-                    "FrechetInceptionDistance metric requires that `Torch-fidelity` is installed."
+                    "InceptionScore metric requires that `Torch-fidelity` is installed."
                     " Either install as `pip install torchmetrics[image]` or `pip install torch-fidelity`."
                 )
-            valid_int_input = [64, 192, 768, 2048]
+            valid_int_input = ("logits_unbiased", 64, 192, 768, 2048)
             if feature not in valid_int_input:
                 raise ValueError(
-                    f"Integer input to argument `feature` must be one of {valid_int_input}, but got {feature}."
+                    f"Integer input to argument `feature` must be one of {valid_int_input}," f" but got {feature}."
                 )
 
             self.inception = NoTrainInceptionV3(name="inception-v3-compat", features_list=[str(feature)])
         elif isinstance(feature, Module):
             self.inception = feature
         else:
             raise TypeError("Got unknown input to argument `feature`")
 
-        if not isinstance(reset_real_features, bool):
-            raise ValueError("Argument `reset_real_features` expected to be a bool")
-        self.reset_real_features = reset_real_features
-
-        self.add_state("real_features", [], dist_reduce_fx=None)
-        self.add_state("fake_features", [], dist_reduce_fx=None)
+        if not isinstance(normalize, bool):
+            raise ValueError("Argument `normalize` expected to be a bool")
+        self.normalize = normalize
+
+        self.splits = splits
+        self.add_state("features", [], dist_reduce_fx=None)
+
+    def update(self, imgs: Tensor) -> None:
+        """Update the state with extracted features."""
+        imgs = (imgs * 255).byte() if self.normalize else imgs
+        features = self.inception(imgs)
+        self.features.append(features)
 
-    def update(self, imgs: Tensor, real: bool) -> None:  # type: ignore
-        """Update the state with extracted features.
+    def compute(self) -> Tuple[Tensor, Tensor]:
+        """Compute metric."""
+        features = dim_zero_cat(self.features)
+        # random permute the features
+        idx = torch.randperm(features.shape[0])
+        features = features[idx]
+
+        # calculate probs and logits
+        prob = features.softmax(dim=1)
+        log_prob = features.log_softmax(dim=1)
+
+        # split into groups
+        prob = prob.chunk(self.splits, dim=0)
+        log_prob = log_prob.chunk(self.splits, dim=0)
+
+        # calculate score per split
+        mean_prob = [p.mean(dim=0, keepdim=True) for p in prob]
+        kl_ = [p * (log_p - m_p.log()) for p, log_p, m_p in zip(prob, log_prob, mean_prob)]
+        kl_ = [k.sum(dim=1).mean().exp() for k in kl_]
+        kl = torch.stack(kl_)
+
+        # return mean and std
+        return kl.mean(), kl.std()
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
 
         Args:
-            imgs: tensor with images feed to the feature extractor
-            real: bool indicating if ``imgs`` belong to the real or the fake distribution
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> import torch
+            >>> from torchmetrics.image.inception import InceptionScore
+            >>> metric = InceptionScore()
+            >>> metric.update(torch.randint(0, 255, (50, 3, 299, 299), dtype=torch.uint8))
+            >>> fig_, ax_ = metric.plot()  # the returned plot only shows the mean value by default
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> import torch
+            >>> from torchmetrics.image.inception import InceptionScore
+            >>> metric = InceptionScore()
+            >>> values = [ ]
+            >>> for _ in range(3):
+            ...     # we index by 0 such that only the mean value is plotted
+            ...     values.append(metric(torch.randint(0, 255, (50, 3, 299, 299), dtype=torch.uint8))[0])
+            >>> fig_, ax_ = metric.plot(values)
         """
-        features = self.inception(imgs)
-
-        if real:
-            self.real_features.append(features)
-        else:
-            self.fake_features.append(features)
-
-    def compute(self) -> Tensor:
-        """Calculate FID score based on accumulated extracted features from the two distributions."""
-        real_features = dim_zero_cat(self.real_features)
-        fake_features = dim_zero_cat(self.fake_features)
-        # computation is extremely sensitive so it needs to happen in double precision
-        orig_dtype = real_features.dtype
-        real_features = real_features.double()
-        fake_features = fake_features.double()
-
-        # calculate mean and covariance
-        n = real_features.shape[0]
-        m = fake_features.shape[0]
-        mean1 = real_features.mean(dim=0)
-        mean2 = fake_features.mean(dim=0)
-        diff1 = real_features - mean1
-        diff2 = fake_features - mean2
-        cov1 = 1.0 / (n - 1) * diff1.t().mm(diff1)
-        cov2 = 1.0 / (m - 1) * diff2.t().mm(diff2)
-
-        # compute fid
-        return _compute_fid(mean1, cov1, mean2, cov2).to(orig_dtype)
-
-    def reset(self) -> None:
-        if not self.reset_real_features:
-            # remove temporarily to avoid resetting
-            value = self._defaults.pop("real_features")
-            super().reset()
-            self._defaults["real_features"] = value
-        else:
-            super().reset()
+        val = val or self.compute()[0]  # by default we select the mean to plot
+        return self._plot(val, ax)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `torchmetrics-0.9.3/torchmetrics/image/inception.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/audio/pesq.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,162 +1,167 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, List, Tuple, Union
+from typing import Any, Optional, Sequence, Union
 
-import torch
-from torch import Tensor
-from torch.nn import Module
+from torch import Tensor, tensor
 
-from torchmetrics.image.fid import NoTrainInceptionV3
+from torchmetrics.functional.audio.pesq import perceptual_evaluation_speech_quality
 from torchmetrics.metric import Metric
-from torchmetrics.utilities import rank_zero_warn
-from torchmetrics.utilities.data import dim_zero_cat
-from torchmetrics.utilities.imports import _TORCH_FIDELITY_AVAILABLE
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE, _PESQ_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
-__doctest_requires__ = {("InceptionScore", "IS"): ["torch_fidelity"]}
+__doctest_requires__ = {"PerceptualEvaluationSpeechQuality": ["pesq"]}
 
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["PerceptualEvaluationSpeechQuality.plot"]
 
-class InceptionScore(Metric):
-    r"""
-    Calculates the Inception Score (IS) which is used to access how realistic generated images are.
-    It is defined as
 
-    .. math::
-        IS = exp(\mathbb{E}_x KL(p(y | x ) || p(y)))
+class PerceptualEvaluationSpeechQuality(Metric):
+    """Calculate `Perceptual Evaluation of Speech Quality`_ (PESQ).
 
-    where :math:`KL(p(y | x) || p(y))` is the KL divergence between the conditional distribution :math:`p(y|x)`
-    and the margianl distribution :math:`p(y)`. Both the conditional and marginal distribution is calculated
-    from features extracted from the images. The score is calculated on random splits of the images such that
-    both a mean and standard deviation of the score are returned. The metric was originally proposed in [1].
+    It's a recognized industry standard for audio quality that takes into considerations characteristics such as:
+    audio sharpness, call volume, background noise, clipping, audio interference ect. PESQ returns a score between
+    -0.5 and 4.5 with the higher scores indicating a better quality.
 
-    Using the default feature extraction (Inception v3 using the original weights from [2]), the input is
-    expected to be mini-batches of 3-channel RGB images of shape (3 x H x W) with dtype uint8. All images
-    will be resized to 299 x 299 which is the size of the original training data.
+    This metric is a wrapper for the `pesq package`_. Note that input will be moved to ``cpu`` to perform the metric
+    calculation.
 
-    .. note:: using this metric with the default feature extractor requires that ``torch-fidelity``
-        is installed. Either install as ``pip install torchmetrics[image]`` or
-        ``pip install torch-fidelity``
+    As input to ``forward`` and ``update`` the metric accepts the following input
 
-    Args:
-        feature:
-            Either an str, integer or ``nn.Module``:
+    - ``preds`` (:class:`~torch.Tensor`): float tensor with shape ``(...,time)``
+    - ``target`` (:class:`~torch.Tensor`): float tensor with shape ``(...,time)``
 
-            - an str or integer will indicate the inceptionv3 feature layer to choose. Can be one of the following:
-              'logits_unbiased', 64, 192, 768, 2048
-            - an ``nn.Module`` for using a custom feature extractor. Expects that its forward method returns
-              an ``[N,d]`` matrix where ``N`` is the batch size and ``d`` is the feature size.
+    As output of `forward` and `compute` the metric returns the following output
 
-        splits: integer determining how many splits the inception score calculation should be split among
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+    - ``pesq`` (:class:`~torch.Tensor`): float tensor with shape ``(...,)`` of PESQ value per sample
 
-    References:
-        [1] Improved Techniques for Training GANs
-        Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen
-        https://arxiv.org/abs/1606.03498
-
-        [2] GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,
-        Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter
-        https://arxiv.org/abs/1706.08500
+    .. note:: using this metrics requires you to have ``pesq`` install. Either install as ``pip install
+        torchmetrics[audio]`` or ``pip install pesq``. ``pesq`` will compile with your currently
+        installed version of numpy, meaning that if you upgrade numpy at some point in the future you will
+        most likely have to reinstall ``pesq``.
+
+    Args:
+        fs: sampling frequency, should be 16000 or 8000 (Hz)
+        mode: ``'wb'`` (wide-band) or ``'nb'`` (narrow-band)
+        keep_same_device: whether to move the pesq value to the device of preds
+        n_processes: integer specifiying the number of processes to run in parallel for the metric calculation.
+            Only applies to batches of data and if ``multiprocessing`` package is installed.
+        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
     Raises:
+        ModuleNotFoundError:
+            If ``pesq`` package is not installed
         ValueError:
-            If ``feature`` is set to an ``str`` or ``int`` and ``torch-fidelity`` is not installed
+            If ``fs`` is not either  ``8000`` or ``16000``
         ValueError:
-            If ``feature`` is set to an ``str`` or ``int`` and not one of ``['logits_unbiased', 64, 192, 768, 2048]``
-        TypeError:
-            If ``feature`` is not an ``str``, ``int`` or ``torch.nn.Module``
+            If ``mode`` is not either ``"wb"`` or ``"nb"``
 
     Example:
         >>> import torch
-        >>> _ = torch.manual_seed(123)
-        >>> from torchmetrics.image.inception import InceptionScore
-        >>> inception = InceptionScore()
-        >>> # generate some images
-        >>> imgs = torch.randint(0, 255, (100, 3, 299, 299), dtype=torch.uint8)
-        >>> inception.update(imgs)
-        >>> inception.compute()
-        (tensor(1.0544), tensor(0.0117))
-
+        >>> from torchmetrics.audio import PerceptualEvaluationSpeechQuality
+        >>> g = torch.manual_seed(1)
+        >>> preds = torch.randn(8000)
+        >>> target = torch.randn(8000)
+        >>> nb_pesq = PerceptualEvaluationSpeechQuality(8000, 'nb')
+        >>> nb_pesq(preds, target)
+        tensor(2.2076)
+        >>> wb_pesq = PerceptualEvaluationSpeechQuality(16000, 'wb')
+        >>> wb_pesq(preds, target)
+        tensor(1.7359)
     """
+
+    sum_pesq: Tensor
+    total: Tensor
+    full_state_update: bool = False
     is_differentiable: bool = False
     higher_is_better: bool = True
-    full_state_update: bool = False
-
-    features: List
+    plot_lower_bound: float = 1.0
+    plot_upper_bound: float = 4.5
 
     def __init__(
         self,
-        feature: Union[str, int, Module] = "logits_unbiased",
-        splits: int = 10,
+        fs: int,
+        mode: str,
+        n_processes: int = 1,
         **kwargs: Any,
     ) -> None:
         super().__init__(**kwargs)
+        if not _PESQ_AVAILABLE:
+            raise ModuleNotFoundError(
+                "PerceptualEvaluationSpeechQuality metric requires that `pesq` is installed."
+                " Either install as `pip install torchmetrics[audio]` or `pip install pesq`."
+            )
+        if fs not in (8000, 16000):
+            raise ValueError(f"Expected argument `fs` to either be 8000 or 16000 but got {fs}")
+        self.fs = fs
+        if mode not in ("wb", "nb"):
+            raise ValueError(f"Expected argument `mode` to either be 'wb' or 'nb' but got {mode}")
+        self.mode = mode
+        if not isinstance(n_processes, int) and n_processes <= 0:
+            raise ValueError(f"Expected argument `n_processes` to be an int larger than 0 but got {n_processes}")
+        self.n_processes = n_processes
+
+        self.add_state("sum_pesq", default=tensor(0.0), dist_reduce_fx="sum")
+        self.add_state("total", default=tensor(0), dist_reduce_fx="sum")
+
+    def update(self, preds: Tensor, target: Tensor) -> None:
+        """Update state with predictions and targets."""
+        pesq_batch = perceptual_evaluation_speech_quality(
+            preds, target, self.fs, self.mode, False, self.n_processes
+        ).to(self.sum_pesq.device)
+
+        self.sum_pesq += pesq_batch.sum()
+        self.total += pesq_batch.numel()
+
+    def compute(self) -> Tensor:
+        """Compute metric."""
+        return self.sum_pesq / self.total
 
-        rank_zero_warn(
-            "Metric `InceptionScore` will save all extracted features in buffer."
-            " For large datasets this may lead to large memory footprint.",
-            UserWarning,
-        )
-
-        if isinstance(feature, (str, int)):
-            if not _TORCH_FIDELITY_AVAILABLE:
-                raise ModuleNotFoundError(
-                    "InceptionScore metric requires that `Torch-fidelity` is installed."
-                    " Either install as `pip install torchmetrics[image]` or `pip install torch-fidelity`."
-                )
-            valid_int_input = ("logits_unbiased", 64, 192, 768, 2048)
-            if feature not in valid_int_input:
-                raise ValueError(
-                    f"Integer input to argument `feature` must be one of {valid_int_input}," f" but got {feature}."
-                )
-
-            self.inception = NoTrainInceptionV3(name="inception-v3-compat", features_list=[str(feature)])
-        elif isinstance(feature, Module):
-            self.inception = feature
-        else:
-            raise TypeError("Got unknown input to argument `feature`")
-
-        self.splits = splits
-        self.add_state("features", [], dist_reduce_fx=None)
-
-    def update(self, imgs: Tensor) -> None:  # type: ignore
-        """Update the state with extracted features.
+    def plot(self, val: Union[Tensor, Sequence[Tensor], None] = None, ax: Optional[_AX_TYPE] = None) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
 
         Args:
-            imgs: tensor with images feed to the feature extractor
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> import torch
+            >>> from torchmetrics.audio import PerceptualEvaluationSpeechQuality
+            >>> metric = PerceptualEvaluationSpeechQuality(8000, 'nb')
+            >>> metric.update(torch.rand(8000), torch.rand(8000))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> import torch
+            >>> from torchmetrics.audio import PerceptualEvaluationSpeechQuality
+            >>> metric = PerceptualEvaluationSpeechQuality(8000, 'nb')
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(torch.rand(8000), torch.rand(8000)))
+            >>> fig_, ax_ = metric.plot(values)
         """
-        features = self.inception(imgs)
-        self.features.append(features)
-
-    def compute(self) -> Tuple[Tensor, Tensor]:
-        features = dim_zero_cat(self.features)
-        # random permute the features
-        idx = torch.randperm(features.shape[0])
-        features = features[idx]
-
-        # calculate probs and logits
-        prob = features.softmax(dim=1)
-        log_prob = features.log_softmax(dim=1)
-
-        # split into groups
-        prob = prob.chunk(self.splits, dim=0)
-        log_prob = log_prob.chunk(self.splits, dim=0)
-
-        # calculate score per split
-        mean_prob = [p.mean(dim=0, keepdim=True) for p in prob]
-        kl_ = [p * (log_p - m_p.log()) for p, log_p, m_p in zip(prob, log_prob, mean_prob)]
-        kl_ = [k.sum(dim=1).mean().exp() for k in kl_]
-        kl = torch.stack(kl_)
-
-        # return mean and std
-        return kl.mean(), kl.std()
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/image/kid.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/image/kid.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,37 +1,41 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, List, Optional, Tuple, Union
+from typing import Any, List, Optional, Sequence, Tuple, Union
 
 import torch
 from torch import Tensor
 from torch.nn import Module
 
 from torchmetrics.image.fid import NoTrainInceptionV3
 from torchmetrics.metric import Metric
 from torchmetrics.utilities import rank_zero_warn
 from torchmetrics.utilities.data import dim_zero_cat
-from torchmetrics.utilities.imports import _TORCH_FIDELITY_AVAILABLE
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE, _TORCH_FIDELITY_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
-__doctest_requires__ = {("KernelInceptionDistance", "KID"): ["torch_fidelity"]}
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["KernelInceptionDistance.plot"]
+
+__doctest_requires__ = {("KernelInceptionDistance", "KernelInceptionDistance.plot"): ["torch_fidelity"]}
 
 
 def maximum_mean_discrepancy(k_xx: Tensor, k_xy: Tensor, k_yy: Tensor) -> Tensor:
-    """Adapted from `KID Score`_"""
+    """Adapted from `KID Score`_."""
     m = k_xx.shape[0]
 
     diag_x = torch.diag(k_xx)
     diag_y = torch.diag(k_yy)
 
     kt_xx_sums = k_xx.sum(dim=-1) - diag_x
     kt_yy_sums = k_yy.sum(dim=-1) - diag_y
@@ -43,88 +47,91 @@
 
     value = (kt_xx_sum + kt_yy_sum) / (m * (m - 1))
     value -= 2 * k_xy_sum / (m**2)
     return value
 
 
 def poly_kernel(f1: Tensor, f2: Tensor, degree: int = 3, gamma: Optional[float] = None, coef: float = 1.0) -> Tensor:
-    """Adapted from `KID Score`_"""
+    """Adapted from `KID Score`_."""
     if gamma is None:
         gamma = 1.0 / f1.shape[1]
-    kernel = (f1 @ f2.T * gamma + coef) ** degree
-    return kernel
+    return (f1 @ f2.T * gamma + coef) ** degree
 
 
 def poly_mmd(
     f_real: Tensor, f_fake: Tensor, degree: int = 3, gamma: Optional[float] = None, coef: float = 1.0
 ) -> Tensor:
-    """Adapted from `KID Score`_"""
+    """Adapted from `KID Score`_."""
     k_11 = poly_kernel(f_real, f_real, degree, gamma, coef)
     k_22 = poly_kernel(f_fake, f_fake, degree, gamma, coef)
     k_12 = poly_kernel(f_real, f_fake, degree, gamma, coef)
     return maximum_mean_discrepancy(k_11, k_12, k_22)
 
 
 class KernelInceptionDistance(Metric):
-    r"""
-    Calculates Kernel Inception Distance (KID) which is used to access the quality of generated images. Given by
+    r"""Calculate Kernel Inception Distance (KID) which is used to access the quality of generated images.
 
     .. math::
         KID = MMD(f_{real}, f_{fake})^2
 
     where :math:`MMD` is the maximum mean discrepancy and :math:`I_{real}, I_{fake}` are extracted features
-    from real and fake images, see [1] for more details. In particular, calculating the MMD requires the
+    from real and fake images, see `kid ref1`_ for more details. In particular, calculating the MMD requires the
     evaluation of a polynomial kernel function :math:`k`
 
     .. math::
         k(x,y) = (\gamma * x^T y + coef)^{degree}
 
     which controls the distance between two features. In practise the MMD is calculated over a number of
     subsets to be able to both get the mean and standard deviation of KID.
 
-    Using the default feature extraction (Inception v3 using the original weights from [2]), the input is
-    expected to be mini-batches of 3-channel RGB images of shape (3 x H x W) with dtype uint8. All images
-    will be resized to 299 x 299 which is the size of the original training data.
+    Using the default feature extraction (Inception v3 using the original weights from `kid ref2`_), the input is
+    expected to be mini-batches of 3-channel RGB images of shape ``(3 x H x W)``. If argument ``normalize``
+    is ``True`` images are expected to be dtype ``float`` and have values in the ``[0, 1]`` range, else if
+    ``normalize`` is set to ``False`` images are expected to have dtype ``uint8`` and take values in the ``[0, 255]``
+    range. All images will be resized to 299 x 299 which is the size of the original training data. The boolian
+    flag ``real`` determines if the images should update the statistics of the real distribution or the
+    fake distribution.
 
     .. note:: using this metric with the default feature extractor requires that ``torch-fidelity``
         is installed. Either install as ``pip install torchmetrics[image]`` or
         ``pip install torch-fidelity``
 
+    As input to ``forward`` and ``update`` the metric accepts the following input
+
+    - ``imgs`` (:class:`~torch.Tensor`): tensor with images feed to the feature extractor of shape ``(N,C,H,W)``
+    - ``real`` (`bool`): bool indicating if ``imgs`` belong to the real or the fake distribution
+
+    As output of `forward` and `compute` the metric returns the following output
+
+    - ``kid_mean`` (:class:`~torch.Tensor`): float scalar tensor with mean value over subsets
+    - ``kid_std`` (:class:`~torch.Tensor`): float scalar tensor with mean value over subsets
+
     Args:
         feature: Either an str, integer or ``nn.Module``:
 
             - an str or integer will indicate the inceptionv3 feature layer to choose. Can be one of the following:
               'logits_unbiased', 64, 192, 768, 2048
             - an ``nn.Module`` for using a custom feature extractor. Expects that its forward method returns
-              an ``[N,d]`` matrix where ``N`` is the batch size and ``d`` is the feature size.
+              an ``(N,d)`` matrix where ``N`` is the batch size and ``d`` is the feature size.
 
         subsets: Number of subsets to calculate the mean and standard deviation scores over
         subset_size: Number of randomly picked samples in each subset
         degree: Degree of the polynomial kernel function
         gamma: Scale-length of polynomial kernel. If set to ``None`` will be automatically set to the feature size
         coef: Bias term in the polynomial kernel.
         reset_real_features: Whether to also reset the real features. Since in many cases the real dataset does not
             change, the features can cached them to avoid recomputing them which is costly. Set this to ``False`` if
             your dataset does not change.
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
-    References:
-        [1] Demystifying MMD GANs
-        Mikoaj Bikowski, Danica J. Sutherland, Michael Arbel, Arthur Gretton
-        https://arxiv.org/abs/1801.01401
-
-        [2] GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium,
-        Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter
-        https://arxiv.org/abs/1706.08500
-
     Raises:
         ValueError:
             If ``feature`` is set to an ``int`` (default settings) and ``torch-fidelity`` is not installed
         ValueError:
-            If ``feature`` is set to an ``int`` not in ``[64, 192, 768, 2048]``
+            If ``feature`` is set to an ``int`` not in ``(64, 192, 768, 2048)``
         ValueError:
             If ``subsets`` is not an integer larger than 0
         ValueError:
             If ``subset_size`` is not an integer larger than 0
         ValueError:
             If ``degree`` is not an integer larger than 0
         ValueError:
@@ -143,32 +150,34 @@
         >>> imgs_dist1 = torch.randint(0, 200, (100, 3, 299, 299), dtype=torch.uint8)
         >>> imgs_dist2 = torch.randint(100, 255, (100, 3, 299, 299), dtype=torch.uint8)
         >>> kid.update(imgs_dist1, real=True)
         >>> kid.update(imgs_dist2, real=False)
         >>> kid_mean, kid_std = kid.compute()
         >>> print((kid_mean, kid_std))
         (tensor(0.0337), tensor(0.0023))
-
     """
     higher_is_better: bool = False
     is_differentiable: bool = False
     full_state_update: bool = False
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
 
     real_features: List[Tensor]
     fake_features: List[Tensor]
 
     def __init__(
         self,
         feature: Union[str, int, Module] = 2048,
         subsets: int = 100,
         subset_size: int = 1000,
         degree: int = 3,
-        gamma: Optional[float] = None,  # type: ignore
+        gamma: Optional[float] = None,
         coef: float = 1.0,
         reset_real_features: bool = True,
+        normalize: bool = False,
         **kwargs: Any,
     ) -> None:
         super().__init__(**kwargs)
 
         rank_zero_warn(
             "Metric `Kernel Inception Distance` will save all extracted features in buffer."
             " For large datasets this may lead to large memory footprint.",
@@ -213,35 +222,34 @@
             raise ValueError("Argument `coef` expected to be float larger than 0")
         self.coef = coef
 
         if not isinstance(reset_real_features, bool):
             raise ValueError("Arugment `reset_real_features` expected to be a bool")
         self.reset_real_features = reset_real_features
 
+        if not isinstance(normalize, bool):
+            raise ValueError("Argument `normalize` expected to be a bool")
+        self.normalize = normalize
+
         # states for extracted features
         self.add_state("real_features", [], dist_reduce_fx=None)
         self.add_state("fake_features", [], dist_reduce_fx=None)
 
-    def update(self, imgs: Tensor, real: bool) -> None:  # type: ignore
-        """Update the state with extracted features.
-
-        Args:
-            imgs: tensor with images feed to the feature extractor
-            real: bool indicating if ``imgs`` belong to the real or the fake distribution
-        """
+    def update(self, imgs: Tensor, real: bool) -> None:
+        """Update the state with extracted features."""
+        imgs = (imgs * 255).byte() if self.normalize else imgs
         features = self.inception(imgs)
 
         if real:
             self.real_features.append(features)
         else:
             self.fake_features.append(features)
 
     def compute(self) -> Tuple[Tensor, Tensor]:
-        """Calculate KID score based on accumulated extracted features from the two distributions. Returns a tuple
-        of mean and standard deviation of KID scores calculated on subsets of extracted features.
+        """Calculate KID score based on accumulated extracted features from the two distributions.
 
         Implementation inspired by `Fid Score`_
         """
         real_features = dim_zero_cat(self.real_features)
         fake_features = dim_zero_cat(self.fake_features)
 
         n_samples_real = real_features.shape[0]
@@ -260,14 +268,65 @@
 
             o = poly_mmd(f_real, f_fake, self.degree, self.gamma, self.coef)
             kid_scores_.append(o)
         kid_scores = torch.stack(kid_scores_)
         return kid_scores.mean(), kid_scores.std(unbiased=False)
 
     def reset(self) -> None:
+        """Reset metric states."""
         if not self.reset_real_features:
             # remove temporarily to avoid resetting
             value = self._defaults.pop("real_features")
             super().reset()
             self._defaults["real_features"] = value
         else:
             super().reset()
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> import torch
+            >>> from torchmetrics.image.kid import KernelInceptionDistance
+            >>> imgs_dist1 = torch.randint(0, 200, (30, 3, 299, 299), dtype=torch.uint8)
+            >>> imgs_dist2 = torch.randint(100, 255, (30, 3, 299, 299), dtype=torch.uint8)
+            >>> metric = KernelInceptionDistance(subsets=3, subset_size=20)
+            >>> metric.update(imgs_dist1, real=True)
+            >>> metric.update(imgs_dist2, real=False)
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> import torch
+            >>> from torchmetrics.image.kid import KernelInceptionDistance
+            >>> imgs_dist1 = lambda: torch.randint(0, 200, (30, 3, 299, 299), dtype=torch.uint8)
+            >>> imgs_dist2 = lambda: torch.randint(100, 255, (30, 3, 299, 299), dtype=torch.uint8)
+            >>> metric = KernelInceptionDistance(subsets=3, subset_size=20)
+            >>> values = [ ]
+            >>> for _ in range(3):
+            ...     metric.update(imgs_dist1(), real=True)
+            ...     metric.update(imgs_dist2(), real=False)
+            ...     values.append(metric.compute()[0])
+            ...     metric.reset()
+            >>> fig_, ax_ = metric.plot(values)
+        """
+        val = val or self.compute()[0]  # by default we select the mean to plot
+        return self._plot(val, ax)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `torchmetrics-0.9.3/torchmetrics/image/lpip.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/text/ter.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,145 +1,158 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, List
+
+from typing import Any, List, Optional, Sequence, Tuple, Union
 
 import torch
-from torch import Tensor
-from torch.nn import Module
-from typing_extensions import Literal
+from torch import Tensor, tensor
 
+from torchmetrics.functional.text.ter import _ter_compute, _ter_update, _TercomTokenizer
 from torchmetrics.metric import Metric
-from torchmetrics.utilities.imports import _LPIPS_AVAILABLE
-
-if _LPIPS_AVAILABLE:
-    from lpips import LPIPS as _LPIPS
-else:
-
-    class _LPIPS(Module):  # type: ignore
-        pass
-
-    __doctest_skip__ = ["LearnedPerceptualImagePatchSimilarity", "LPIPS"]
-
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
-class NoTrainLpips(_LPIPS):
-    def train(self, mode: bool) -> "NoTrainLpips":
-        """the network should not be able to be switched away from evaluation mode."""
-        return super().train(False)
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["TranslationEditRate.plot"]
 
 
-def _valid_img(img: Tensor) -> bool:
-    """check that input is a valid image to the network."""
-    return img.ndim == 4 and img.shape[1] == 3 and img.min() >= -1.0 and img.max() <= 1.0
+class TranslationEditRate(Metric):
+    """Calculate Translation edit rate (`TER`_)  of machine translated text with one or more references.
 
+    This implementation follows the one from `SacreBleu_ter`_, which is a
+    near-exact reimplementation of the Tercom algorithm, produces identical results on all "sane" outputs.
 
-class LearnedPerceptualImagePatchSimilarity(Metric):
-    """The Learned Perceptual Image Patch Similarity (`LPIPS_`) is used to judge the perceptual similarity between
-    two images. LPIPS essentially computes the similarity between the activations of two image patches for some
-    pre-defined network. This measure has been shown to match human perseption well. A low LPIPS score means that
-    image patches are perceptual similar.
+    As input to ``forward`` and ``update`` the metric accepts the following input:
 
-    Both input image patches are expected to have shape `[N, 3, H, W]` and be normalized to the [-1,1]
-    range. The minimum size of `H, W` depends on the chosen backbone (see `net_type` arg).
+    - ``preds`` (:class:`~Sequence`): An iterable of hypothesis corpus
+    - ``target`` (:class:`~Sequence`): An iterable of iterables of reference corpus
 
-    .. note:: using this metrics requires you to have ``lpips`` package installed. Either install
-        as ``pip install torchmetrics[image]`` or ``pip install lpips``
+    As output of ``forward`` and ``compute`` the metric returns the following output:
 
-    .. note:: this metric is not scriptable when using ``torch<1.8``. Please update your pytorch installation
-        if this is a issue.
+    - ``ter`` (:class:`~torch.Tensor`): if ``return_sentence_level_score=True`` return a corpus-level translation
+      edit rate with a list of sentence-level translation_edit_rate, else return a corpus-level translation edit rate
 
     Args:
-        net_type: str indicating backbone network type to use. Choose between `'alex'`, `'vgg'` or `'squeeze'`
-        reduction: str indicating how to reduce over the batch dimension. Choose between `'sum'` or `'mean'`.
+        normalize: An indication whether a general tokenization to be applied.
+        no_punctuation: An indication whteher a punctuation to be removed from the sentences.
+        lowercase: An indication whether to enable case-insesitivity.
+        asian_support: An indication whether asian characters to be processed.
+        return_sentence_level_score: An indication whether a sentence-level TER to be returned.
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
-    Raises:
-        ModuleNotFoundError:
-            If ``lpips`` package is not installed
-        ValueError:
-            If ``net_type`` is not one of ``"vgg"``, ``"alex"`` or ``"squeeze"``
-        ValueError:
-            If ``reduction`` is not one of ``"mean"`` or ``"sum"``
-
     Example:
-        >>> import torch
-        >>> _ = torch.manual_seed(123)
-        >>> from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity
-        >>> lpips = LearnedPerceptualImagePatchSimilarity(net_type='vgg')
-        >>> img1 = torch.rand(10, 3, 100, 100)
-        >>> img2 = torch.rand(10, 3, 100, 100)
-        >>> lpips(img1, img2)
-        tensor(0.3566, grad_fn=<SqueezeBackward0>)
+        >>> from torchmetrics.text import TranslationEditRate
+        >>> preds = ['the cat is on the mat']
+        >>> target = [['there is a cat on the mat', 'a cat is on the mat']]
+        >>> ter = TranslationEditRate()
+        >>> ter(preds, target)
+        tensor(0.1538)
     """
 
-    is_differentiable: bool = True
+    is_differentiable: bool = False
     higher_is_better: bool = False
     full_state_update: bool = False
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
 
-    real_features: List[Tensor]
-    fake_features: List[Tensor]
-
-    # due to the use of named tuple in the backbone the net variable cannot be scripted
-    __jit_ignored_attributes__ = ["net"]
+    total_num_edits: Tensor
+    total_tgt_len: Tensor
+    sentence_ter: Optional[List[Tensor]] = None
 
     def __init__(
         self,
-        net_type: str = "alex",
-        reduction: Literal["sum", "mean"] = "mean",
+        normalize: bool = False,
+        no_punctuation: bool = False,
+        lowercase: bool = True,
+        asian_support: bool = False,
+        return_sentence_level_score: bool = False,
         **kwargs: Any,
     ) -> None:
         super().__init__(**kwargs)
-
-        if not _LPIPS_AVAILABLE:
-            raise ModuleNotFoundError(
-                "LPIPS metric requires that lpips is installed."
-                " Either install as `pip install torchmetrics[image]` or `pip install lpips`."
-            )
-
-        valid_net_type = ("vgg", "alex", "squeeze")
-        if net_type not in valid_net_type:
-            raise ValueError(f"Argument `net_type` must be one of {valid_net_type}, but got {net_type}.")
-        self.net = NoTrainLpips(net=net_type, verbose=False)
-
-        valid_reduction = ("mean", "sum")
-        if reduction not in valid_reduction:
-            raise ValueError(f"Argument `reduction` must be one of {valid_reduction}, but got {reduction}")
-        self.reduction = reduction
-
-        self.add_state("sum_scores", torch.tensor(0.0), dist_reduce_fx="sum")
-        self.add_state("total", torch.tensor(0.0), dist_reduce_fx="sum")
-
-    def update(self, img1: Tensor, img2: Tensor) -> None:  # type: ignore
-        """Update internal states with lpips score.
+        if not isinstance(normalize, bool):
+            raise ValueError(f"Expected argument `normalize` to be of type boolean but got {normalize}.")
+        if not isinstance(no_punctuation, bool):
+            raise ValueError(f"Expected argument `no_punctuation` to be of type boolean but got {no_punctuation}.")
+        if not isinstance(lowercase, bool):
+            raise ValueError(f"Expected argument `lowercase` to be of type boolean but got {lowercase}.")
+        if not isinstance(asian_support, bool):
+            raise ValueError(f"Expected argument `asian_support` to be of type boolean but got {asian_support}.")
+
+        self.tokenizer = _TercomTokenizer(normalize, no_punctuation, lowercase, asian_support)
+        self.return_sentence_level_score = return_sentence_level_score
+
+        self.add_state("total_num_edits", tensor(0.0), dist_reduce_fx="sum")
+        self.add_state("total_tgt_len", tensor(0.0), dist_reduce_fx="sum")
+        if self.return_sentence_level_score:
+            self.add_state("sentence_ter", [], dist_reduce_fx="cat")
+
+    def update(self, preds: Union[str, Sequence[str]], target: Sequence[Union[str, Sequence[str]]]) -> None:
+        """Update state with predictions and targets."""
+        self.total_num_edits, self.total_tgt_len, self.sentence_ter = _ter_update(
+            preds,
+            target,
+            self.tokenizer,
+            self.total_num_edits,
+            self.total_tgt_len,
+            self.sentence_ter,
+        )
+
+    def compute(self) -> Union[Tensor, Tuple[Tensor, Tensor]]:
+        """Calculate the translate error rate (TER)."""
+        ter = _ter_compute(self.total_num_edits, self.total_tgt_len)
+        if self.sentence_ter is not None:
+            return ter, torch.cat(self.sentence_ter)
+        return ter
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
 
         Args:
-            img1: tensor with images of shape ``[N, 3, H, W]``
-            img2: tensor with images of shape ``[N, 3, H, W]``
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> from torchmetrics.text import TranslationEditRate
+            >>> metric = TranslationEditRate()
+            >>> preds = ['the cat is on the mat']
+            >>> target = [['there is a cat on the mat', 'a cat is on the mat']]
+            >>> metric.update(preds, target)
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> from torchmetrics.text import TranslationEditRate
+            >>> metric = TranslationEditRate()
+            >>> preds = ['the cat is on the mat']
+            >>> target = [['there is a cat on the mat', 'a cat is on the mat']]
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(preds, target))
+            >>> fig_, ax_ = metric.plot(values)
         """
-        if not (_valid_img(img1) and _valid_img(img2)):
-            raise ValueError(
-                "Expected both input arguments to be normalized tensors (all values in range [-1,1])"
-                f" and to have shape [N, 3, H, W] but `img1` have shape {img1.shape} with values in"
-                f" range {[img1.min(), img1.max()]} and `img2` have shape {img2.shape} with value"
-                f" in range {[img2.min(), img2.max()]}"
-            )
-
-        loss = self.net(img1, img2).squeeze()
-        self.sum_scores += loss.sum()
-        self.total += img1.shape[0]
-
-    def compute(self) -> Tensor:
-        """Compute final perceptual similarity metric."""
-        if self.reduction == "mean":
-            return self.sum_scores / self.total
-        if self.reduction == "sum":
-            return self.sum_scores
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/image/psnr.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/image/psnrb.py`

 * *Files 22% similar despite different names*

```diff
@@ -13,128 +13,127 @@
 # limitations under the License.
 from typing import Any, Optional, Sequence, Tuple, Union
 
 import torch
 from torch import Tensor, tensor
 from typing_extensions import Literal
 
-from torchmetrics.functional.image.psnr import _psnr_compute, _psnr_update
+from torchmetrics.functional.image.psnrb import _psnrb_compute, _psnrb_update
 from torchmetrics.metric import Metric
-from torchmetrics.utilities import rank_zero_warn
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["PeakSignalNoiseRatioWithBlockedEffect.plot"]
 
-class PeakSignalNoiseRatio(Metric):
-    r"""
-    Computes `Computes Peak Signal-to-Noise Ratio`_ (PSNR):
 
-    .. math:: \text{PSNR}(I, J) = 10 * \log_{10} \left(\frac{\max(I)^2}{\text{MSE}(I, J)}\right)
+class PeakSignalNoiseRatioWithBlockedEffect(Metric):
+    r"""Computes `Peak Signal to Noise Ratio With Blocked Effect`_ (PSNRB).
 
-    Where :math:`\text{MSE}` denotes the `mean-squared-error`_ function.
+    .. math::
+        \text{PSNRB}(I, J) = 10 * \log_{10} \left(\frac{\max(I)^2}{\text{MSE}(I, J)-\text{B}(I, J)}\right)
 
-    Args:
-        data_range:
-            the range of the data. If None, it is determined from the data (max - min).
-            The ``data_range`` must be given when ``dim`` is not None.
-        base: a base of a logarithm to use.
-        reduction: a method to reduce metric score over labels.
-
-            - ``'elementwise_mean'``: takes the mean (default)
-            - ``'sum'``: takes the sum
-            - ``'none'`` or ``None``: no reduction will be applied
-
-        dim:
-            Dimensions to reduce PSNR scores over, provided as either an integer or a list of integers. Default is
-            None meaning scores will be reduced across all dimensions and all batches.
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+    Where :math:`\text{MSE}` denotes the `mean-squared-error`_ function. This metric is a modified version of PSNR that
+    better supports evaluation of images with blocked artifacts, that oftens occur in compressed images.
 
-    Raises:
-        ValueError:
-            If ``dim`` is not ``None`` and ``data_range`` is not given.
+    .. note::
+        Metric only supports grayscale images. If you have RGB images, please convert them to grayscale first.
 
-    Example:
-        >>> from torchmetrics import PeakSignalNoiseRatio
-        >>> psnr = PeakSignalNoiseRatio()
-        >>> preds = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
-        >>> target = torch.tensor([[3.0, 2.0], [1.0, 0.0]])
-        >>> psnr(preds, target)
-        tensor(2.5527)
+    As input to ``forward`` and ``update`` the metric accepts the following input
 
-    .. note::
-        Half precision is only support on GPU for this metric
+    - ``preds`` (:class:`~torch.Tensor`): Predictions from model of shape ``(N,1,H,W)``
+    - ``target`` (:class:`~torch.Tensor`): Ground truth values of shape ``(N,1,H,W)``
 
+    As output of `forward` and `compute` the metric returns the following output
+
+    - ``psnrb`` (:class:`~torch.Tensor`): float scalar tensor with aggregated PSNRB value
+
+    Args:
+        block_size: integer indication the block size
+        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+
+    Example:
+        >>> import torch
+        >>> from torchmetrics.image import PeakSignalNoiseRatioWithBlockedEffect
+        >>> metric = PeakSignalNoiseRatioWithBlockedEffect()
+        >>> _ = torch.manual_seed(42)
+        >>> preds = torch.rand(2, 1, 10, 10)
+        >>> target = torch.rand(2, 1, 10, 10)
+        >>> metric(preds, target)
+        tensor(7.2893)
     """
     is_differentiable: bool = True
     higher_is_better: bool = True
     full_state_update: bool = False
 
-    min_target: Tensor
-    max_target: Tensor
+    sum_squared_error: Tensor
+    total: Tensor
+    bef: Tensor
+    data_range: Tensor
 
     def __init__(
         self,
-        data_range: Optional[float] = None,
-        base: float = 10.0,
-        reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
-        dim: Optional[Union[int, Tuple[int, ...]]] = None,
+        block_size: int = 8,
         **kwargs: Any,
     ) -> None:
         super().__init__(**kwargs)
+        if not isinstance(block_size, int) and block_size < 1:
+            raise ValueError("Argument ``block_size`` should be a positive integer")
+        self.block_size = block_size
+
+        self.add_state("sum_squared_error", default=tensor(0.0), dist_reduce_fx="sum")
+        self.add_state("total", default=tensor(0), dist_reduce_fx="sum")
+        self.add_state("bef", default=tensor(0.0), dist_reduce_fx="sum")
+        self.add_state("data_range", default=tensor(0), dist_reduce_fx="max")
+
+    def update(self, preds: Tensor, target: Tensor) -> None:
+        """Update state with predictions and targets."""
+        sum_squared_error, bef, n_obs = _psnrb_update(preds, target, block_size=self.block_size)
+        self.sum_squared_error += sum_squared_error
+        self.bef += bef
+        self.total += n_obs
+        self.data_range = torch.maximum(self.data_range, torch.max(target) - torch.min(target))
 
-        if dim is None and reduction != "elementwise_mean":
-            rank_zero_warn(f"The `reduction={reduction}` will not have any effect when `dim` is None.")
-
-        if dim is None:
-            self.add_state("sum_squared_error", default=tensor(0.0), dist_reduce_fx="sum")
-            self.add_state("total", default=tensor(0), dist_reduce_fx="sum")
-        else:
-            self.add_state("sum_squared_error", default=[], dist_reduce_fx="cat")
-            self.add_state("total", default=[], dist_reduce_fx="cat")
-
-        if data_range is None:
-            if dim is not None:
-                # Maybe we could use `torch.amax(target, dim=dim) - torch.amin(target, dim=dim)` in PyTorch 1.7 to
-                # calculate `data_range` in the future.
-                raise ValueError("The `data_range` must be given when `dim` is not None.")
-
-            self.data_range = None
-            self.add_state("min_target", default=tensor(0.0), dist_reduce_fx=torch.min)
-            self.add_state("max_target", default=tensor(0.0), dist_reduce_fx=torch.max)
-        else:
-            self.add_state("data_range", default=tensor(float(data_range)), dist_reduce_fx="mean")
-        self.base = base
-        self.reduction = reduction
-        self.dim = tuple(dim) if isinstance(dim, Sequence) else dim
+    def compute(self) -> Tensor:
+        """Compute peak signal-to-noise ratio over state."""
+        return _psnrb_compute(self.sum_squared_error, self.bef, self.total, self.data_range)
 
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
 
         Args:
-            preds: Predictions from model
-            target: Ground truth values
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> import torch
+            >>> from torchmetrics.image import PeakSignalNoiseRatioWithBlockedEffect
+            >>> metric = PeakSignalNoiseRatioWithBlockedEffect()
+            >>> metric.update(torch.rand(2, 1, 10, 10), torch.rand(2, 1, 10, 10))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> import torch
+            >>> from torchmetrics.image import PeakSignalNoiseRatioWithBlockedEffect
+            >>> metric = PeakSignalNoiseRatioWithBlockedEffect()
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(torch.rand(2, 1, 10, 10), torch.rand(2, 1, 10, 10)))
+            >>> fig_, ax_ = metric.plot(values)
         """
-        sum_squared_error, n_obs = _psnr_update(preds, target, dim=self.dim)
-        if self.dim is None:
-            if self.data_range is None:
-                # keep track of min and max target values
-                self.min_target = min(target.min(), self.min_target)
-                self.max_target = max(target.max(), self.max_target)
-
-            self.sum_squared_error += sum_squared_error
-            self.total += n_obs
-        else:
-            self.sum_squared_error.append(sum_squared_error)
-            self.total.append(n_obs)
-
-    def compute(self) -> Tensor:
-        """Compute peak signal-to-noise ratio over state."""
-        if self.data_range is not None:
-            data_range = self.data_range
-        else:
-            data_range = self.max_target - self.min_target
-
-        if self.dim is None:
-            sum_squared_error = self.sum_squared_error
-            total = self.total
-        else:
-            sum_squared_error = torch.cat([values.flatten() for values in self.sum_squared_error])
-            total = torch.cat([values.flatten() for values in self.total])
-        return _psnr_compute(sum_squared_error, total, data_range, base=self.base, reduction=self.reduction)
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/image/ssim.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/image/psnr.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,268 +1,198 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, List, Optional, Sequence, Tuple, Union
+from functools import partial
+from typing import Any, Optional, Sequence, Tuple, Union
 
-from torch import Tensor
+import torch
+from torch import Tensor, tensor
 from typing_extensions import Literal
 
-from torchmetrics.functional.image.ssim import _multiscale_ssim_compute, _ssim_compute, _ssim_update
+from torchmetrics.functional.image.psnr import _psnr_compute, _psnr_update
 from torchmetrics.metric import Metric
 from torchmetrics.utilities import rank_zero_warn
-from torchmetrics.utilities.data import dim_zero_cat
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["PeakSignalNoiseRatio.plot"]
 
-class StructuralSimilarityIndexMeasure(Metric):
-    """Computes Structual Similarity Index Measure (SSIM_).
 
-    Args:
-        preds: estimated image
-        target: ground truth image
-        gaussian_kernel: If ``True`` (default), a gaussian kernel is used, if ``False`` a uniform kernel is used
-        sigma: Standard deviation of the gaussian kernel, anisotropic kernels are possible.
-            Ignored if a uniform kernel is used
-        kernel_size: the size of the uniform kernel, anisotropic kernels are possible.
-            Ignored if a Gaussian kernel is used
-        reduction: a method to reduce metric score over labels.
+class PeakSignalNoiseRatio(Metric):
+    r"""`Compute Peak Signal-to-Noise Ratio`_ (PSNR).
 
-            - ``'elementwise_mean'``: takes the mean
-            - ``'sum'``: takes the sum
-            - ``'none'`` or ``None``: no reduction will be applied
+    .. math:: \text{PSNR}(I, J) = 10 * \log_{10} \left(\frac{\max(I)^2}{\text{MSE}(I, J)}\right)
 
-        data_range: Range of the image. If ``None``, it is determined from the image (max - min)
-        k1: Parameter of SSIM.
-        k2: Parameter of SSIM.
-        return_full_image: If true, the full ``ssim`` image is returned as a second argument.
-            Mutually exclusive with ``return_contrast_sensitivity``
-        return_contrast_sensitivity: If true, the constant term is returned as a second argument.
-            The luminance term can be obtained with luminance=ssim/contrast
-            Mutually exclusive with ``return_full_image``
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+    Where :math:`\text{MSE}` denotes the `mean-squared-error`_ function.
 
-    Return:
-        Tensor with SSIM score
+    As input to ``forward`` and ``update`` the metric accepts the following input
 
-    Example:
-        >>> from torchmetrics import StructuralSimilarityIndexMeasure
-        >>> import torch
-        >>> preds = torch.rand([16, 1, 16, 16])
-        >>> target = preds * 0.75
-        >>> ssim = StructuralSimilarityIndexMeasure()
-        >>> ssim(preds, target)
-        tensor(0.9219)
-    """
+    - ``preds`` (:class:`~torch.Tensor`): Predictions from model of shape ``(N,C,H,W)``
+    - ``target`` (:class:`~torch.Tensor`): Ground truth values of shape ``(N,C,H,W)``
 
-    higher_is_better: bool = True
-    is_differentiable: bool = True
-    full_state_update: bool = False
-
-    preds: List[Tensor]
-    target: List[Tensor]
-
-    def __init__(
-        self,
-        gaussian_kernel: bool = True,
-        sigma: Union[float, Sequence[float]] = 1.5,
-        kernel_size: Union[int, Sequence[int]] = 11,
-        reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
-        data_range: Optional[float] = None,
-        k1: float = 0.01,
-        k2: float = 0.03,
-        return_full_image: bool = False,
-        return_contrast_sensitivity: bool = False,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__(**kwargs)
-        rank_zero_warn(
-            "Metric `SSIM` will save all targets and"
-            " predictions in buffer. For large datasets this may lead"
-            " to large memory footprint."
-        )
-
-        self.add_state("preds", default=[], dist_reduce_fx="cat")
-        self.add_state("target", default=[], dist_reduce_fx="cat")
-        self.gaussian_kernel = gaussian_kernel
-        self.sigma = sigma
-        self.kernel_size = kernel_size
-        self.reduction = reduction
-        self.data_range = data_range
-        self.k1 = k1
-        self.k2 = k2
-        self.return_full_image = return_full_image
-        self.return_contrast_sensitivity = return_contrast_sensitivity
+    As output of `forward` and `compute` the metric returns the following output
 
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
-
-        Args:
-            preds: Predictions from model
-            target: Ground truth values
-        """
-        preds, target = _ssim_update(preds, target)
-        self.preds.append(preds)
-        self.target.append(target)
-
-    def compute(self) -> Tensor:
-        """Computes explained variance over state."""
-        preds = dim_zero_cat(self.preds)
-        target = dim_zero_cat(self.target)
-        return _ssim_compute(
-            preds,
-            target,
-            self.gaussian_kernel,
-            self.sigma,
-            self.kernel_size,
-            self.reduction,
-            self.data_range,
-            self.k1,
-            self.k2,
-            self.return_full_image,
-            self.return_contrast_sensitivity,
-        )
-
-
-class MultiScaleStructuralSimilarityIndexMeasure(Metric):
-    """Computes `MultiScaleSSIM`_, Multi-scale Structural Similarity Index Measure, which is a generalization of
-    Structural Similarity Index Measure by incorporating image details at different resolution scores.
+    - ``psnr`` (:class:`~torch.Tensor`): if ``reduction!='none'`` returns float scalar tensor with average PSNR value
+      over sample else returns tensor of shape ``(N,)`` with PSNR values per sample
 
     Args:
-        gaussian_kernel: If ``True`` (default), a gaussian kernel is used, if false a uniform kernel is used
-        kernel_size: size of the gaussian kernel
-        sigma: Standard deviation of the gaussian kernel
+        data_range:
+            the range of the data. If None, it is determined from the data (max - min). If a tuple is provided then
+            the range is calculated as the difference and input is clamped between the values.
+            The ``data_range`` must be given when ``dim`` is not None.
+        base: a base of a logarithm to use.
         reduction: a method to reduce metric score over labels.
 
-            - ``'elementwise_mean'``: takes the mean
+            - ``'elementwise_mean'``: takes the mean (default)
             - ``'sum'``: takes the sum
             - ``'none'`` or ``None``: no reduction will be applied
 
-        data_range: Range of the image. If ``None``, it is determined from the image (max - min)
-        k1: Parameter of structural similarity index measure.
-        k2: Parameter of structural similarity index measure.
-        betas: Exponent parameters for individual similarities and contrastive sensitivies returned by different image
-            resolutions.
-        normalize: When MultiScaleStructuralSimilarityIndexMeasure loss is used for training, it is desirable to use
-            normalizes to improve the training stability. This `normalize` argument is out of scope of the original
-            implementation [1], and it is adapted from https://github.com/jorge-pessoa/pytorch-msssim instead.
+        dim:
+            Dimensions to reduce PSNR scores over, provided as either an integer or a list of integers. Default is
+            None meaning scores will be reduced across all dimensions and all batches.
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
-    Return:
-        Tensor with Multi-Scale SSIM score
-
     Raises:
         ValueError:
-            If ``kernel_size`` is not an int or a Sequence of ints with size 2 or 3.
-        ValueError:
-            If ``betas`` is not a tuple of floats with lengt 2.
-        ValueError:
-            If ``normalize`` is neither `None`, `ReLU` nor `simple`.
+            If ``dim`` is not ``None`` and ``data_range`` is not given.
 
     Example:
-        >>> from torchmetrics import MultiScaleStructuralSimilarityIndexMeasure
-        >>> import torch
-        >>> preds = torch.rand([1, 1, 256, 256], generator=torch.manual_seed(42))
-        >>> target = preds * 0.75
-        >>> ms_ssim = MultiScaleStructuralSimilarityIndexMeasure()
-        >>> ms_ssim(preds, target)
-        tensor(0.9558)
-
-    References:
-        [1] Multi-Scale Structural Similarity For Image Quality Assessment by Zhou Wang, Eero P. Simoncelli and Alan C.
-        Bovik `MultiScaleSSIM`_
+        >>> from torchmetrics.image import PeakSignalNoiseRatio
+        >>> psnr = PeakSignalNoiseRatio()
+        >>> preds = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
+        >>> target = torch.tensor([[3.0, 2.0], [1.0, 0.0]])
+        >>> psnr(preds, target)
+        tensor(2.5527)
     """
-
-    higher_is_better: bool = True
     is_differentiable: bool = True
+    higher_is_better: bool = True
     full_state_update: bool = False
+    plot_lower_bound: float = 0.0
 
-    preds: List[Tensor]
-    target: List[Tensor]
+    min_target: Tensor
+    max_target: Tensor
 
     def __init__(
         self,
-        gaussian_kernel: bool = True,
-        kernel_size: Union[int, Sequence[int]] = 11,
-        sigma: Union[float, Sequence[float]] = 1.5,
+        data_range: Optional[Union[float, Tuple[float, float]]] = None,
+        base: float = 10.0,
         reduction: Literal["elementwise_mean", "sum", "none", None] = "elementwise_mean",
-        data_range: Optional[float] = None,
-        k1: float = 0.01,
-        k2: float = 0.03,
-        betas: Tuple[float, ...] = (0.0448, 0.2856, 0.3001, 0.2363, 0.1333),
-        normalize: Literal["relu", "simple", None] = None,
+        dim: Optional[Union[int, Tuple[int, ...]]] = None,
         **kwargs: Any,
     ) -> None:
         super().__init__(**kwargs)
-        rank_zero_warn(
-            "Metric `MS_SSIM` will save all targets and"
-            " predictions in buffer. For large datasets this may lead"
-            " to large memory footprint."
-        )
-
-        self.add_state("preds", default=[], dist_reduce_fx="cat")
-        self.add_state("target", default=[], dist_reduce_fx="cat")
-
-        if not (isinstance(kernel_size, (Sequence, int))):
-            raise ValueError(
-                f"Argument `kernel_size` expected to be an sequence or an int, or a single int. Got {kernel_size}"
-            )
-        if isinstance(kernel_size, Sequence) and (
-            len(kernel_size) not in (2, 3) or not all(isinstance(ks, int) for ks in kernel_size)
-        ):
-            raise ValueError(
-                "Argument `kernel_size` expected to be an sequence of size 2 or 3 where each element is an int, "
-                f"or a single int. Got {kernel_size}"
-            )
-
-        self.gaussian_kernel = gaussian_kernel
-        self.sigma = sigma
-        self.kernel_size = kernel_size
+
+        if dim is None and reduction != "elementwise_mean":
+            rank_zero_warn(f"The `reduction={reduction}` will not have any effect when `dim` is None.")
+
+        if dim is None:
+            self.add_state("sum_squared_error", default=tensor(0.0), dist_reduce_fx="sum")
+            self.add_state("total", default=tensor(0), dist_reduce_fx="sum")
+        else:
+            self.add_state("sum_squared_error", default=[], dist_reduce_fx="cat")
+            self.add_state("total", default=[], dist_reduce_fx="cat")
+
+        self.clamping_fn = None
+        if data_range is None:
+            if dim is not None:
+                # Maybe we could use `torch.amax(target, dim=dim) - torch.amin(target, dim=dim)` in PyTorch 1.7 to
+                # calculate `data_range` in the future.
+                raise ValueError("The `data_range` must be given when `dim` is not None.")
+
+            self.data_range = None
+            self.add_state("min_target", default=tensor(0.0), dist_reduce_fx=torch.min)
+            self.add_state("max_target", default=tensor(0.0), dist_reduce_fx=torch.max)
+        elif isinstance(data_range, tuple):
+            self.add_state("data_range", default=tensor(data_range[1] - data_range[0]), dist_reduce_fx="mean")
+            self.clamping_fn = partial(torch.clamp, min=data_range[0], max=data_range[1])
+        else:
+            self.add_state("data_range", default=tensor(float(data_range)), dist_reduce_fx="mean")
+        self.base = base
         self.reduction = reduction
-        self.data_range = data_range
-        self.k1 = k1
-        self.k2 = k2
-        if not isinstance(betas, tuple):
-            raise ValueError("Argument `betas` is expected to be of a type tuple.")
-        if isinstance(betas, tuple) and not all(isinstance(beta, float) for beta in betas):
-            raise ValueError("Argument `betas` is expected to be a tuple of floats.")
-        self.betas = betas
-        if normalize and normalize not in ("relu", "simple"):
-            raise ValueError("Argument `normalize` to be expected either `None` or one of 'relu' or 'simple'")
-        self.normalize = normalize
+        self.dim = tuple(dim) if isinstance(dim, Sequence) else dim
 
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
+    def update(self, preds: Tensor, target: Tensor) -> None:
+        """Update state with predictions and targets."""
+        if self.clamping_fn is not None:
+            preds = self.clamping_fn(preds)
+            target = self.clamping_fn(target)
+
+        sum_squared_error, n_obs = _psnr_update(preds, target, dim=self.dim)
+        if self.dim is None:
+            if self.data_range is None:
+                # keep track of min and max target values
+                self.min_target = min(target.min(), self.min_target)
+                self.max_target = max(target.max(), self.max_target)
+
+            self.sum_squared_error += sum_squared_error
+            self.total += n_obs
+        else:
+            self.sum_squared_error.append(sum_squared_error)
+            self.total.append(n_obs)
+
+    def compute(self) -> Tensor:
+        """Compute peak signal-to-noise ratio over state."""
+        data_range = self.data_range if self.data_range is not None else self.max_target - self.min_target
+
+        if self.dim is None:
+            sum_squared_error = self.sum_squared_error
+            total = self.total
+        else:
+            sum_squared_error = torch.cat([values.flatten() for values in self.sum_squared_error])
+            total = torch.cat([values.flatten() for values in self.total])
+        return _psnr_compute(sum_squared_error, total, data_range, base=self.base, reduction=self.reduction)
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
 
         Args:
-            preds: Predictions from model of shape ``[N, C, H, W]``
-            target: Ground truth values of shape ``[N, C, H, W]``
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> import torch
+            >>> from torchmetrics.image import PeakSignalNoiseRatio
+            >>> metric = PeakSignalNoiseRatio()
+            >>> preds = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
+            >>> target = torch.tensor([[3.0, 2.0], [1.0, 0.0]])
+            >>> metric.update(preds, target)
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> import torch
+            >>> from torchmetrics.image import PeakSignalNoiseRatio
+            >>> metric = PeakSignalNoiseRatio()
+            >>> preds = torch.tensor([[0.0, 1.0], [2.0, 3.0]])
+            >>> target = torch.tensor([[3.0, 2.0], [1.0, 0.0]])
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(preds, target))
+            >>> fig_, ax_ = metric.plot(values)
         """
-        preds, target = _ssim_update(preds, target)
-        self.preds.append(preds)
-        self.target.append(target)
-
-    def compute(self) -> Tensor:
-        """Computes explained variance over state."""
-        preds = dim_zero_cat(self.preds)
-        target = dim_zero_cat(self.target)
-        return _multiscale_ssim_compute(
-            preds,
-            target,
-            self.gaussian_kernel,
-            self.sigma,
-            self.kernel_size,
-            self.reduction,
-            self.data_range,
-            self.k1,
-            self.k2,
-            self.betas,
-            self.normalize,
-        )
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/metric.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/metric.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,52 +12,54 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 import functools
 import inspect
 from abc import ABC, abstractmethod
 from contextlib import contextmanager
 from copy import deepcopy
-from typing import Any, Callable, Dict, Generator, List, Optional, Sequence, Union
+from typing import Any, Callable, Dict, Generator, List, Optional, Sequence, Tuple, Union
 
 import torch
 from torch import Tensor
 from torch.nn import Module
 
-from torchmetrics.utilities import apply_to_collection, rank_zero_warn
-from torchmetrics.utilities.checks import is_overridden
 from torchmetrics.utilities.data import (
     _flatten,
     _squeeze_if_scalar,
+    apply_to_collection,
     dim_zero_cat,
     dim_zero_max,
     dim_zero_mean,
     dim_zero_min,
     dim_zero_sum,
 )
 from torchmetrics.utilities.distributed import gather_all_tensors
 from torchmetrics.utilities.exceptions import TorchMetricsUserError
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE, plot_single_or_multi_val
+from torchmetrics.utilities.prints import rank_zero_warn
 
 
 def jit_distributed_available() -> bool:
+    """Determine if distributed mode is initialized."""
     return torch.distributed.is_available() and torch.distributed.is_initialized()
 
 
 class Metric(Module, ABC):
     """Base class for all metrics present in the Metrics API.
 
     Implements ``add_state()``, ``forward()``, ``reset()`` and a few other things to
     handle distributed synchronization and per-step metric computation.
 
     Override ``update()`` and ``compute()`` functions to implement your own metric. Use
     ``add_state()`` to register metric state variables which keep track of state on each
     call of ``update()`` and are synchronized across processes when ``compute()`` is called.
 
     Note:
-        Metric state variables can either be ``torch.Tensors`` or an empty list which can we used
-        to store `torch.Tensors``.
+        Metric state variables can either be :class:`~torch.Tensor` or an empty list which can we used
+        to store :class:`~torch.Tensor`.
 
     Note:
         Different metrics only override ``update()`` and not ``forward()``. A call to ``update()``
         is valid, but it won't return the metric value at the current step. A call to ``forward()``
         automatically calls ``update()`` and also returns the metric value at the current step.
 
     Args:
@@ -65,23 +67,35 @@
 
             - compute_on_cpu: If metric state should be stored on CPU during computations. Only works
                 for list states.
             - dist_sync_on_step: If metric state should synchronize on ``forward()``. Default is ``False``
             - process_group: The process group on which the synchronization is called. Default is the world.
             - dist_sync_fn: function that performs the allgather option on the metric state. Default is an
                 custom implementation that calls ``torch.distributed.all_gather`` internally.
+            - distributed_available_fn: function that checks if the distributed backend is available.
+                Defaults to a check of ``torch.distributed.is_available()`` and ``torch.distributed.is_initialized()``.
             - sync_on_compute: If metric state should synchronize when ``compute`` is called. Default is ``True``-
     """
 
     __jit_ignored_attributes__ = ["device"]
-    __jit_unused_properties__ = ["is_differentiable"]
+    __jit_unused_properties__ = [
+        "is_differentiable",
+        "higher_is_better",
+        "plot_lower_bound",
+        "plot_upper_bound",
+        "plot_legend_name",
+    ]
     is_differentiable: Optional[bool] = None
     higher_is_better: Optional[bool] = None
     full_state_update: Optional[bool] = None
 
+    plot_lower_bound: Optional[float] = None
+    plot_upper_bound: Optional[float] = None
+    plot_legend_name: Optional[str] = None
+
     def __init__(
         self,
         **kwargs: Any,
     ) -> None:
         super().__init__()
 
         # see (https://github.com/pytorch/pytorch/blob/3e6bb5233f9ca2c5aa55d9cda22a7ee85439aa6e/
@@ -106,89 +120,93 @@
 
         self.dist_sync_fn = kwargs.pop("dist_sync_fn", None)
         if self.dist_sync_fn is not None and not callable(self.dist_sync_fn):
             raise ValueError(
                 f"Expected keyword argument `dist_sync_fn` to be an callable function but got {self.dist_sync_fn}"
             )
 
+        self.distributed_available_fn = kwargs.pop("distributed_available_fn", None) or jit_distributed_available
+
         self.sync_on_compute = kwargs.pop("sync_on_compute", True)
         if not isinstance(self.sync_on_compute, bool):
             raise ValueError(
                 f"Expected keyword argument `sync_on_compute` to be a `bool` but got {self.sync_on_compute}"
             )
 
+        if kwargs:
+            kwargs_ = [f"`{a}`" for a in sorted(kwargs)]
+            raise ValueError(f"Unexpected keyword arguments: {', '.join(kwargs_)}")
+
         # initialize
         self._update_signature = inspect.signature(self.update)
-        self.update: Callable = self._wrap_update(self.update)  # type: ignore
-        self.compute: Callable = self._wrap_compute(self.compute)  # type: ignore
+        self.update: Callable = self._wrap_update(self.update)  # type: ignore[method-assign]
+        self.compute: Callable = self._wrap_compute(self.compute)  # type: ignore[method-assign]
         self._computed = None
         self._forward_cache = None
         self._update_count = 0
         self._to_sync = self.sync_on_compute
         self._should_unsync = True
         self._enable_grad = False
+        self._dtype_convert = False
 
         # initialize state
         self._defaults: Dict[str, Union[List, Tensor]] = {}
         self._persistent: Dict[str, bool] = {}
         self._reductions: Dict[str, Union[str, Callable[..., Any], None]] = {}
 
         # state management
         self._is_synced = False
         self._cache: Optional[Dict[str, Union[List[Tensor], Tensor]]] = None
 
-        if self.full_state_update is None and not is_overridden("forward", self, Metric):
-            rank_zero_warn(
-                f"""Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has
-                not been set for this class ({self.__class__.__name__}). The property determines if `update` by
-                default needs access to the full metric state. If this is not the case, significant speedups can be
-                achieved and we recommend setting this to `False`.
-                We provide an checking function
-                `from torchmetrics.utilities import check_forward_full_state_property`
-                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,
-                default for now) or if `full_state_update=False` can be used safely.
-                """,
-                UserWarning,
-            )
-
     @property
     def _update_called(self) -> bool:
-        # Needed for lightning integration
+        # TODO: this is needed for internal lightning, remove after v0.12 and update on lightning side
+        return self._update_count > 0
+
+    @property
+    def update_called(self) -> bool:
+        """Returns `True` if `update` or `forward` has been called initialization or last `reset`."""
         return self._update_count > 0
 
+    @property
+    def update_count(self) -> int:
+        """Get the number of times `update` and/or `forward` has been called since initialization or last `reset`."""
+        return self._update_count
+
     def add_state(
         self,
         name: str,
         default: Union[list, Tensor],
         dist_reduce_fx: Optional[Union[str, Callable]] = None,
         persistent: bool = False,
     ) -> None:
-        """Adds metric state variable. Only used by subclasses.
+        """Add metric state variable. Only used by subclasses.
 
         Args:
             name: The name of the state variable. The variable will then be accessible at ``self.name``.
-            default: Default value of the state; can either be a ``torch.Tensor`` or an empty list. The state will be
-                reset to this value when ``self.reset()`` is called.
+            default: Default value of the state; can either be a :class:`~torch.Tensor` or an empty list.
+                The state will be reset to this value when ``self.reset()`` is called.
             dist_reduce_fx (Optional): Function to reduce state across multiple processes in distributed mode.
                 If value is ``"sum"``, ``"mean"``, ``"cat"``, ``"min"`` or ``"max"`` we will use ``torch.sum``,
                 ``torch.mean``, ``torch.cat``, ``torch.min`` and ``torch.max``` respectively, each with argument
                 ``dim=0``. Note that the ``"cat"`` reduction only makes sense if the state is a list, and not
                 a tensor. The user can also pass a custom function in this parameter.
             persistent (Optional): whether the state will be saved as part of the modules ``state_dict``.
                 Default is ``False``.
 
         Note:
             Setting ``dist_reduce_fx`` to None will return the metric state synchronized across different processes.
             However, there won't be any reduction function applied to the synchronized metric state.
 
             The metric states would be synced as follows
 
-            - If the metric state is ``torch.Tensor``, the synced value will be a stacked ``torch.Tensor`` across
-              the process dimension if the metric state was a ``torch.Tensor``. The original ``torch.Tensor`` metric
-              state retains dimension and hence the synchronized output will be of shape ``(num_process, ...)``.
+            - If the metric state is :class:`~torch.Tensor`, the synced value will be a stacked :class:`~torch.Tensor`
+              across the process dimension if the metric state was a :class:`~torch.Tensor`. The original
+              :class:`~torch.Tensor` metric state retains dimension and hence the synchronized output will be of shape
+              ``(num_process, ...)``.
 
             - If the metric state is a ``list``, the synced value will be a ``list`` containing the
               combined elements from all processes.
 
         Note:
             When passing a custom function to ``dist_reduce_fx``, expect the synchronized metric state to follow
             the format discussed in the above note.
@@ -209,32 +227,32 @@
         elif dist_reduce_fx == "max":
             dist_reduce_fx = dim_zero_max
         elif dist_reduce_fx == "min":
             dist_reduce_fx = dim_zero_min
         elif dist_reduce_fx == "cat":
             dist_reduce_fx = dim_zero_cat
         elif dist_reduce_fx is not None and not callable(dist_reduce_fx):
-            raise ValueError("`dist_reduce_fx` must be callable or one of ['mean', 'sum', 'cat', None]")
+            raise ValueError("`dist_reduce_fx` must be callable or one of ['mean', 'sum', 'cat', 'min', 'max', None]")
 
         if isinstance(default, Tensor):
             default = default.contiguous()
 
         setattr(self, name, default)
 
         self._defaults[name] = deepcopy(default)
         self._persistent[name] = persistent
         self._reductions[name] = dist_reduce_fx
 
     @torch.jit.unused
     def forward(self, *args: Any, **kwargs: Any) -> Any:
-        """``forward`` serves the dual purpose of both computing the metric on the current batch of inputs but also
-        add the batch statistics to the overall accumululating metric state.
+        """Aggregate and evaluate batch input directly.
 
-        Input arguments are the exact same as corresponding ``update`` method. The returned output is the exact same as
-        the output of ``compute``.
+        Serves the dual purpose of both computing the metric on the current batch of inputs but also add the batch
+        statistics to the overall accumululating metric state. Input arguments are the exact same as corresponding
+        ``update`` method. The returned output is the exact same as the output of ``compute``.
         """
         # check if states are already synced
         if self._is_synced:
             raise TorchMetricsUserError(
                 "The Metric shouldn't be synced when performing ``forward``. "
                 "HINT: Did you forget to call ``unsync`` ?."
             )
@@ -243,24 +261,25 @@
             self._forward_cache = self._forward_full_state_update(*args, **kwargs)
         else:
             self._forward_cache = self._forward_reduce_state_update(*args, **kwargs)
 
         return self._forward_cache
 
     def _forward_full_state_update(self, *args: Any, **kwargs: Any) -> Any:
-        """forward computation using two calls to `update` to calculate the metric value on the current batch and
-        accumulate global state.
+        """Forward computation using two calls to `update`.
 
         Doing this secures that metrics that need access to the full metric state during `update` works as expected.
+        This is the most safe method to use for any metric but also the slower version of the two forward
+        implementations.
         """
         # global accumulation
         self.update(*args, **kwargs)
         _update_count = self._update_count
 
-        self._to_sync = self.dist_sync_on_step  # type: ignore
+        self._to_sync = self.dist_sync_on_step
         # skip restore cache operation from compute as cache is stored below.
         self._should_unsync = False
         # skip computing on cpu for the batch
         _temp_compute_on_cpu = self.compute_on_cpu
         self.compute_on_cpu = False
 
         # save context before switch
@@ -280,25 +299,27 @@
         # restore context
         self._is_synced = False
         self._should_unsync = True
         self._to_sync = self.sync_on_compute
         self._computed = None
         self._enable_grad = False
         self.compute_on_cpu = _temp_compute_on_cpu
+        if self.compute_on_cpu:
+            self._move_list_states_to_cpu()
 
         return batch_val
 
     def _forward_reduce_state_update(self, *args: Any, **kwargs: Any) -> Any:
-        """forward computation using single call to `update` to calculate the metric value on the current batch and
-        accumulate global state.
+        """Forward computation using single call to `update`.
 
-        This can be done when the global metric state is a sinple reduction of batch states.
+        This can be done when the global metric state is a sinple reduction of batch states. This can be unsafe for
+        certain metric cases but is also the fastest way to both accumulate globally and compute locally.
         """
         # store global state and reset to default
-        global_state = {attr: getattr(self, attr) for attr in self._defaults.keys()}
+        global_state = {attr: getattr(self, attr) for attr in self._defaults}
         _update_count = self._update_count
         self.reset()
 
         # local syncronization settings
         self._to_sync = self.dist_sync_on_step
         self._should_unsync = False
         _temp_compute_on_cpu = self.compute_on_cpu
@@ -317,44 +338,47 @@
         # restore context
         self._is_synced = False
         self._should_unsync = True
         self._to_sync = self.sync_on_compute
         self._computed = None
         self._enable_grad = False
         self.compute_on_cpu = _temp_compute_on_cpu
+        if self.compute_on_cpu:
+            self._move_list_states_to_cpu()
 
         return batch_val
 
     def _reduce_states(self, incoming_state: Dict[str, Any]) -> None:
-        """Adds an incoming metric state to the current state of the metric.
+        """Add an incoming metric state to the current state of the metric.
 
         Args:
             incoming_state: a dict containing a metric state similar metric itself
         """
-        for attr in self._defaults.keys():
+        for attr in self._defaults:
             local_state = getattr(self, attr)
             global_state = incoming_state[attr]
             reduce_fn = self._reductions[attr]
             if reduce_fn == dim_zero_sum:
                 reduced = global_state + local_state
             elif reduce_fn == dim_zero_mean:
-                reduced = ((self._update_count - 1) * global_state + local_state) / self._update_count
+                reduced = ((self._update_count - 1) * global_state + local_state).float() / self._update_count
             elif reduce_fn == dim_zero_max:
                 reduced = torch.max(global_state, local_state)
             elif reduce_fn == dim_zero_min:
                 reduced = torch.min(global_state, local_state)
             elif reduce_fn == dim_zero_cat:
                 reduced = global_state + local_state
             elif reduce_fn is None and isinstance(global_state, Tensor):
                 reduced = torch.stack([global_state, local_state])
             elif reduce_fn is None and isinstance(global_state, list):
                 reduced = _flatten([global_state, local_state])
+            elif reduce_fn and callable(reduce_fn):
+                reduced = reduce_fn(torch.stack([global_state, local_state]))
             else:
-                reduced = reduce_fn(torch.stack([global_state, local_state]))  # type: ignore
-
+                raise TypeError(f"Unsupported reduce_fn: {reduce_fn}")
             setattr(self, attr, reduced)
 
     def _sync_dist(self, dist_sync_fn: Callable = gather_all_tensors, process_group: Optional[Any] = None) -> None:
         input_dict = {attr: getattr(self, attr) for attr in self._reductions}
 
         for attr, reduction_fn in self._reductions.items():
             # pre-concatenate metric states that are lists to reduce number of all_gather operations
@@ -367,14 +391,18 @@
             dist_sync_fn,
             group=process_group or self.process_group,
         )
 
         for attr, reduction_fn in self._reductions.items():
             # pre-processing ops (stack or flatten for inputs)
 
+            if isinstance(output_dict[attr], list) and len(output_dict[attr]) == 0:
+                setattr(self, attr, [])
+                continue
+
             if isinstance(output_dict[attr][0], Tensor):
                 output_dict[attr] = torch.stack(output_dict[attr])
             elif isinstance(output_dict[attr][0], list):
                 output_dict[attr] = _flatten(output_dict[attr])
 
             if not (callable(reduction_fn) or reduction_fn is None):
                 raise TypeError("reduction_fn must be callable or None")
@@ -388,41 +416,40 @@
             self._update_count += 1
             with torch.set_grad_enabled(self._enable_grad):
                 try:
                     update(*args, **kwargs)
                 except RuntimeError as err:
                     if "Expected all tensors to be on" in str(err):
                         raise RuntimeError(
-                            "Encountered different devices in metric calculation"
-                            " (see stacktrace for details)."
-                            "This could be due to the metric class not being on the same device as input."
-                            f"Instead of `metric={self.__class__.__name__}(...)` try to do"
+                            "Encountered different devices in metric calculation (see stacktrace for details)."
+                            " This could be due to the metric class not being on the same device as input."
+                            f" Instead of `metric={self.__class__.__name__}(...)` try to do"
                             f" `metric={self.__class__.__name__}(...).to(device)` where"
                             " device corresponds to the device of the input."
                         ) from err
                     raise err
 
             if self.compute_on_cpu:
                 self._move_list_states_to_cpu()
 
         return wrapped_func
 
     def _move_list_states_to_cpu(self) -> None:
         """Move list states to cpu to save GPU memory."""
-        for key in self._defaults.keys():
+        for key in self._defaults:
             current_val = getattr(self, key)
             if isinstance(current_val, Sequence):
                 setattr(self, key, [cur_v.to("cpu") for cur_v in current_val])
 
     def sync(
         self,
         dist_sync_fn: Optional[Callable] = None,
         process_group: Optional[Any] = None,
         should_sync: bool = True,
-        distributed_available: Optional[Callable] = jit_distributed_available,
+        distributed_available: Optional[Callable] = None,
     ) -> None:
         """Sync function for manually controlling when metrics states should be synced across processes.
 
         Args:
             dist_sync_fn: Function to be used to perform states synchronization
             process_group:
                 Specify the process group on which synchronization is called.
@@ -430,14 +457,17 @@
             should_sync: Whether to apply to state synchronization. This will have an impact
                 only when running in a distributed setting.
             distributed_available: Function to determine if we are running inside a distributed setting
         """
         if self._is_synced and should_sync:
             raise TorchMetricsUserError("The Metric has already been synced.")
 
+        if distributed_available is None and self.distributed_available_fn is not None:
+            distributed_available = self.distributed_available_fn
+
         is_distributed = distributed_available() if callable(distributed_available) else None
 
         if not should_sync or not is_distributed:
             return
 
         if dist_sync_fn is None:
             dist_sync_fn = gather_all_tensors
@@ -446,16 +476,15 @@
         self._cache = {attr: getattr(self, attr) for attr in self._defaults}
 
         # sync
         self._sync_dist(dist_sync_fn, process_group=process_group)
         self._is_synced = True
 
     def unsync(self, should_unsync: bool = True) -> None:
-        """Unsync function for manually controlling when metrics states should be reverted back to their local
-        states.
+        """Unsync function for manually controlling when metrics states should be reverted back to their local states.
 
         Args:
             should_unsync: Whether to perform unsync
         """
         if not should_unsync:
             return
 
@@ -474,18 +503,20 @@
     @contextmanager
     def sync_context(
         self,
         dist_sync_fn: Optional[Callable] = None,
         process_group: Optional[Any] = None,
         should_sync: bool = True,
         should_unsync: bool = True,
-        distributed_available: Optional[Callable] = jit_distributed_available,
+        distributed_available: Optional[Callable] = None,
     ) -> Generator:
-        """Context manager to synchronize the states between processes when running in a distributed setting and
-        restore the local cache states after yielding.
+        """Context manager to synchronize states.
+
+        This context manager is used in distributed setting and makes sure that the local cache states are restored
+        after yielding the syncronized state.
 
         Args:
             dist_sync_fn: Function to be used to perform states synchronization
             process_group:
                 Specify the process group on which synchronization is called.
                 default: `None` (which selects the entire world)
             should_sync: Whether to apply to state synchronization. This will have an impact
@@ -520,15 +551,15 @@
             if self._computed is not None:
                 return self._computed
 
             # compute relies on the sync context manager to gather the states across processes and apply reduction
             # if synchronization happened, the current rank accumulated states will be restored to keep
             # accumulation going if ``should_unsync=True``,
             with self.sync_context(
-                dist_sync_fn=self.dist_sync_fn,  # type: ignore
+                dist_sync_fn=self.dist_sync_fn,
                 should_sync=self._to_sync,
                 should_unsync=self._should_unsync,
             ):
                 value = compute(*args, **kwargs)
                 self._computed = _squeeze_if_scalar(value)
 
             return self._computed
@@ -537,19 +568,56 @@
 
     @abstractmethod
     def update(self, *_: Any, **__: Any) -> None:
         """Override this method to update the state variables of your metric class."""
 
     @abstractmethod
     def compute(self) -> Any:
-        """Override this method to compute the final metric value from state variables synchronized across the
-        distributed backend."""
+        """Override this method to compute the final metric value.
+
+        This method will automatically synchronize state variables when running in distributed backend.
+        """
+
+    def plot(self, *_: Any, **__: Any) -> Any:
+        """Override this method plot the metric value."""
+        raise NotImplementedError
+
+    def _plot(
+        self,
+        val: Optional[Union[Tensor, Sequence[Tensor], Dict[str, Tensor], Sequence[Dict[str, Tensor]]]] = None,
+        ax: Optional[_AX_TYPE] = None,
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+        """
+        val = val if val is not None else self.compute()
+        fig, ax = plot_single_or_multi_val(
+            val,
+            ax=ax,
+            higher_is_better=self.higher_is_better,
+            name=self.__class__.__name__,
+            lower_bound=self.plot_lower_bound,
+            upper_bound=self.plot_upper_bound,
+            legend_name=self.plot_legend_name,
+        )
+        return fig, ax
 
     def reset(self) -> None:
-        """This method automatically resets the metric state variables to their default value."""
+        """Reset metric state variables to their default value."""
         self._update_count = 0
         self._forward_cache = None
         self._computed = None
 
         for attr, default in self._defaults.items():
             current_val = getattr(self, attr)
             if isinstance(default, Tensor):
@@ -562,73 +630,95 @@
         self._is_synced = False
 
     def clone(self) -> "Metric":
         """Make a copy of the metric."""
         return deepcopy(self)
 
     def __getstate__(self) -> Dict[str, Any]:
+        """Get the current state, including all metric states, for the metric. Used for loading and saving a metric."""
         # ignore update and compute functions for pickling
         return {k: v for k, v in self.__dict__.items() if k not in ["update", "compute", "_update_signature"]}
 
     def __setstate__(self, state: Dict[str, Any]) -> None:
+        """Set the state of the metric, based on a input state. Used for loading and saving a metric."""
         # manually restore update and compute functions for pickling
         self.__dict__.update(state)
         self._update_signature = inspect.signature(self.update)
-        self.update: Callable = self._wrap_update(self.update)  # type: ignore
-        self.compute: Callable = self._wrap_compute(self.compute)  # type: ignore
+        self.update: Callable = self._wrap_update(self.update)  # type: ignore[method-assign]
+        self.compute: Callable = self._wrap_compute(self.compute)  # type: ignore[method-assign]
 
     def __setattr__(self, name: str, value: Any) -> None:
-        if name in ("higher_is_better", "is_differentiable", "full_state_update"):
+        """Overwrite default method to prevent specific attributes from being set by user."""
+        if name in (
+            "higher_is_better",
+            "is_differentiable",
+            "full_state_update",
+            "plot_lower_bound",
+            "plot_upper_bound",
+            "plot_legend_name",
+        ):
             raise RuntimeError(f"Can't change const `{name}`.")
         super().__setattr__(name, value)
 
     @property
     def device(self) -> "torch.device":
         """Return the device of the metric."""
         return self._device
 
     def type(self, dst_type: Union[str, torch.dtype]) -> "Metric":
-        """Method override default and prevent dtype casting.
+        """Override default and prevent dtype casting.
 
         Please use `metric.set_dtype(dtype)` instead.
         """
         return self
 
     def float(self) -> "Metric":
-        """Method override default and prevent dtype casting.
+        """Override default and prevent dtype casting.
 
         Please use `metric.set_dtype(dtype)` instead.
         """
         return self
 
     def double(self) -> "Metric":
-        """Method override default and prevent dtype casting.
+        """Override default and prevent dtype casting.
 
         Please use `metric.set_dtype(dtype)` instead.
         """
         return self
 
     def half(self) -> "Metric":
-        """Method override default and prevent dtype casting.
+        """Override default and prevent dtype casting.
 
         Please use `metric.set_dtype(dtype)` instead.
         """
         return self
 
     def set_dtype(self, dst_type: Union[str, torch.dtype]) -> "Metric":
-        """Special version of `type` for transferring all metric states to specific dtype
+        """Transfer all metric state to specific dtype. Special version of standard `type` method.
+
         Arguments:
-            dst_type (type or string): the desired type
+            dst_type (type or string): the desired type.
         """
-        return super().type(dst_type)
+        self._dtype_convert = True
+        out = super().type(dst_type)
+        out._dtype_convert = False
+        return out
 
     def _apply(self, fn: Callable) -> Module:
-        """Overwrite _apply function such that we can also move metric states to the correct device when `.to`,
-        `.cuda`, etc methods are called."""
+        """Overwrite _apply function such that we can also move metric states to the correct device.
+
+        This method is called by the base ``nn.Module`` class whenever `.to`, `.cuda`, `.float`, `.half` etc. methods
+        are called. Dtype conversion is garded and will only happen through the special `set_dtype` method.
+        """
         this = super()._apply(fn)
+        fs = str(fn)
+        cond = any(f in fs for f in ["Module.type", "Module.half", "Module.float", "Module.double", "Module.bfloat16"])
+        if not self._dtype_convert and cond:
+            return this
+
         # Also apply fn to metric states and defaults
         for key, value in this._defaults.items():
             if isinstance(value, Tensor):
                 this._defaults[key] = fn(value)
             elif isinstance(value, Sequence):
                 this._defaults[key] = [fn(v) for v in value]
 
@@ -651,81 +741,95 @@
             this._computed = apply_to_collection(this._computed, Tensor, fn)
         if this._forward_cache is not None:
             this._forward_cache = apply_to_collection(this._forward_cache, Tensor, fn)
 
         return this
 
     def persistent(self, mode: bool = False) -> None:
-        """Method for post-init to change if metric states should be saved to its state_dict."""
+        """Change post-init if metric states should be saved to its state_dict."""
         for key in self._persistent:
             self._persistent[key] = mode
 
-    def state_dict(
+    def state_dict(  # type: ignore[override]  # todo
         self,
-        destination: Dict[str, Any] = None,
+        destination: Optional[Dict[str, Any]] = None,
         prefix: str = "",
         keep_vars: bool = False,
-    ) -> Optional[Dict[str, Any]]:
-        destination = super().state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)
+    ) -> Dict[str, Any]:
+        """Get the current state of metric as an dictionary.
+
+        Args:
+            destination: Optional dictionary, that if provided, the state of module will be updated into the dict and
+                the same object is returned. Otherwise, an ``OrderedDict`` will be created and returned.
+            prefix: optional string, a prefix added to parameter and buffer names to compose the keys in state_dict.
+            keep_vars: by default the :class:`~torch.Tensor`s returned in the state dict are detached from autograd.
+                If set to ``True``, detaching will not be performed.
+        """
+        destination: Dict[str, Union[torch.Tensor, List, Any]] = super().state_dict(
+            destination=destination, prefix=prefix, keep_vars=keep_vars  # type: ignore[arg-type]
+        )
         # Register metric states to be part of the state_dict
         for key in self._defaults:
             if not self._persistent[key]:
                 continue
             current_val = getattr(self, key)
             if not keep_vars:
                 if isinstance(current_val, Tensor):
                     current_val = current_val.detach()
                 elif isinstance(current_val, list):
                     current_val = [cur_v.detach() if isinstance(cur_v, Tensor) else cur_v for cur_v in current_val]
-            destination[prefix + key] = deepcopy(current_val)  # type: ignore
+            destination[prefix + key] = deepcopy(current_val)
         return destination
 
     def _load_from_state_dict(
         self,
         state_dict: dict,
         prefix: str,
         local_metadata: dict,
         strict: bool,
         missing_keys: List[str],
         unexpected_keys: List[str],
         error_msgs: List[str],
     ) -> None:
-        """Loads metric states from state_dict."""
-
+        """Load metric states from state_dict."""
         for key in self._defaults:
             name = prefix + key
             if name in state_dict:
                 setattr(self, key, state_dict.pop(name))
         super()._load_from_state_dict(
             state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs
         )
 
     def _filter_kwargs(self, **kwargs: Any) -> Dict[str, Any]:
-        """filter kwargs such that they match the update signature of the metric."""
-
+        """Filter kwargs such that they match the update signature of the metric."""
         # filter all parameters based on update signature except those of
         # type VAR_POSITIONAL (*args) and VAR_KEYWORD (**kwargs)
         _params = (inspect.Parameter.VAR_POSITIONAL, inspect.Parameter.VAR_KEYWORD)
         _sign_params = self._update_signature.parameters
         filtered_kwargs = {
-            k: v for k, v in kwargs.items() if (k in _sign_params.keys() and _sign_params[k].kind not in _params)
+            k: v for k, v in kwargs.items() if (k in _sign_params and _sign_params[k].kind not in _params)
         }
 
         exists_var_keyword = any(v.kind == inspect.Parameter.VAR_KEYWORD for v in _sign_params.values())
         # if no kwargs filtered, return all kwargs as default
         if not filtered_kwargs and not exists_var_keyword:
             # no kwargs in update signature -> don't return any kwargs
-            filtered_kwargs = {}
-        elif exists_var_keyword:
+            return {}
+        if exists_var_keyword:
             # kwargs found in update signature -> return all kwargs to be sure to not omit any.
             # filtering logic is likely implemented within the update call.
-            filtered_kwargs = kwargs
+            return kwargs
         return filtered_kwargs
 
     def __hash__(self) -> int:
+        """Return an unique hash of the metric.
+
+        The hash depends on both the class itself but also the current metric state, which therefore enforces that two
+        instances of the same metrics never have the same hash even if they have been updated on the same data.
+        """
         # we need to add the id here, since PyTorch requires a module hash to be unique.
         # Internally, PyTorch nn.Module relies on that for children discovery
         # (see https://github.com/pytorch/pytorch/blob/v1.9.0/torch/nn/modules/module.py#L1544)
         # For metrics that include tensors it is not a problem,
         # since their hash is unique based on the memory location but we cannot rely on that for every metric.
         hash_vals = [self.__class__.__name__, id(self)]
 
@@ -736,188 +840,229 @@
             if hasattr(val, "__iter__") and not isinstance(val, Tensor):
                 hash_vals.extend(val)
             else:
                 hash_vals.append(val)
 
         return hash(tuple(hash_vals))
 
-    def __add__(self, other: "Metric") -> "Metric":
+    def __add__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the addition operator."""
         return CompositionalMetric(torch.add, self, other)
 
-    def __and__(self, other: "Metric") -> "Metric":
+    def __and__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the logical and operator."""
         return CompositionalMetric(torch.bitwise_and, self, other)
 
-    # Fixme: this shall return bool instead of Metric
-    def __eq__(self, other: "Metric") -> "Metric":  # type: ignore
+    def __eq__(self, other: "Metric") -> "CompositionalMetric":  # type: ignore[override]
+        """Construct compositional metric using the equal operator."""
         return CompositionalMetric(torch.eq, self, other)
 
-    def __floordiv__(self, other: "Metric") -> "Metric":
+    def __floordiv__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the floor division operator."""
         return CompositionalMetric(torch.floor_divide, self, other)
 
-    def __ge__(self, other: "Metric") -> "Metric":
+    def __ge__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the greater than or equal operator."""
         return CompositionalMetric(torch.ge, self, other)
 
-    def __gt__(self, other: "Metric") -> "Metric":
+    def __gt__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the greater than operator."""
         return CompositionalMetric(torch.gt, self, other)
 
-    def __le__(self, other: "Metric") -> "Metric":
+    def __le__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the less than or equal operator."""
         return CompositionalMetric(torch.le, self, other)
 
-    def __lt__(self, other: "Metric") -> "Metric":
+    def __lt__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the less than operator."""
         return CompositionalMetric(torch.lt, self, other)
 
-    def __matmul__(self, other: "Metric") -> "Metric":
+    def __matmul__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the matrix multiplication operator."""
         return CompositionalMetric(torch.matmul, self, other)
 
-    def __mod__(self, other: "Metric") -> "Metric":
+    def __mod__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the remainder operator."""
         return CompositionalMetric(torch.fmod, self, other)
 
-    def __mul__(self, other: "Metric") -> "Metric":
+    def __mul__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the multiplication operator."""
         return CompositionalMetric(torch.mul, self, other)
 
     # Fixme: this shall return bool instead of Metric
-    def __ne__(self, other: "Metric") -> "Metric":  # type: ignore
+    def __ne__(self, other: "Metric") -> "CompositionalMetric":  # type: ignore[override]
+        """Construct compositional metric using the not equal operator."""
         return CompositionalMetric(torch.ne, self, other)
 
-    def __or__(self, other: "Metric") -> "Metric":
+    def __or__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the logical or operator."""
         return CompositionalMetric(torch.bitwise_or, self, other)
 
-    def __pow__(self, other: "Metric") -> "Metric":
+    def __pow__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the exponential/power operator."""
         return CompositionalMetric(torch.pow, self, other)
 
-    def __radd__(self, other: "Metric") -> "Metric":
+    def __radd__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the addition operator."""
         return CompositionalMetric(torch.add, other, self)
 
-    def __rand__(self, other: "Metric") -> "Metric":
+    def __rand__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the logical and operator."""
         # swap them since bitwise_and only supports that way and it's commutative
         return CompositionalMetric(torch.bitwise_and, self, other)
 
-    def __rfloordiv__(self, other: "Metric") -> "Metric":
+    def __rfloordiv__(self, other: "CompositionalMetric") -> "Metric":
+        """Construct compositional metric using the floor division operator."""
         return CompositionalMetric(torch.floor_divide, other, self)
 
-    def __rmatmul__(self, other: "Metric") -> "Metric":
+    def __rmatmul__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the matrix multiplication operator."""
         return CompositionalMetric(torch.matmul, other, self)
 
-    def __rmod__(self, other: "Metric") -> "Metric":
+    def __rmod__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the remainder operator."""
         return CompositionalMetric(torch.fmod, other, self)
 
-    def __rmul__(self, other: "Metric") -> "Metric":
+    def __rmul__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the multiplication operator."""
         return CompositionalMetric(torch.mul, other, self)
 
-    def __ror__(self, other: "Metric") -> "Metric":
+    def __ror__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the logical or operator."""
         return CompositionalMetric(torch.bitwise_or, other, self)
 
-    def __rpow__(self, other: "Metric") -> "Metric":
+    def __rpow__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the exponential/power operator."""
         return CompositionalMetric(torch.pow, other, self)
 
-    def __rsub__(self, other: "Metric") -> "Metric":
+    def __rsub__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the subtraction operator."""
         return CompositionalMetric(torch.sub, other, self)
 
-    def __rtruediv__(self, other: "Metric") -> "Metric":
+    def __rtruediv__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the true divide operator."""
         return CompositionalMetric(torch.true_divide, other, self)
 
-    def __rxor__(self, other: "Metric") -> "Metric":
+    def __rxor__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the logical xor operator."""
         return CompositionalMetric(torch.bitwise_xor, other, self)
 
-    def __sub__(self, other: "Metric") -> "Metric":
+    def __sub__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the subtraction operator."""
         return CompositionalMetric(torch.sub, self, other)
 
-    def __truediv__(self, other: "Metric") -> "Metric":
+    def __truediv__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the true divide operator."""
         return CompositionalMetric(torch.true_divide, self, other)
 
-    def __xor__(self, other: "Metric") -> "Metric":
+    def __xor__(self, other: "Metric") -> "CompositionalMetric":
+        """Construct compositional metric using the logical xor operator."""
         return CompositionalMetric(torch.bitwise_xor, self, other)
 
-    def __abs__(self) -> "Metric":
+    def __abs__(self) -> "CompositionalMetric":
+        """Construct compositional metric using the absolute operator."""
         return CompositionalMetric(torch.abs, self, None)
 
-    def __inv__(self) -> "Metric":
+    def __inv__(self) -> "CompositionalMetric":
+        """Construct compositional metric using the not operator."""
         return CompositionalMetric(torch.bitwise_not, self, None)
 
-    def __invert__(self) -> "Metric":
+    def __invert__(self) -> "CompositionalMetric":
+        """Construct compositional metric using the not operator."""
         return self.__inv__()
 
-    def __neg__(self) -> "Metric":
+    def __neg__(self) -> "CompositionalMetric":
+        """Construct compositional metric using absolute negative operator."""
         return CompositionalMetric(_neg, self, None)
 
-    def __pos__(self) -> "Metric":
+    def __pos__(self) -> "CompositionalMetric":
+        """Construct compositional metric using absolute operator."""
         return CompositionalMetric(torch.abs, self, None)
 
-    def __getitem__(self, idx: int) -> "Metric":
+    def __getitem__(self, idx: int) -> "CompositionalMetric":
+        """Construct compositional metric using the get item operator."""
         return CompositionalMetric(lambda x: x[idx], self, None)
 
+    def __getnewargs__(self) -> Tuple:
+        """Needed method for construction of new metrics __new__ method."""
+        return tuple(
+            Metric.__str__(self),
+        )
+
+    __iter__ = None
+
 
 def _neg(x: Tensor) -> Tensor:
     return -torch.abs(x)
 
 
 class CompositionalMetric(Metric):
     """Composition of two metrics with a specific operator which will be executed upon metrics compute."""
 
     def __init__(
         self,
         operator: Callable,
         metric_a: Union[Metric, int, float, Tensor],
         metric_b: Union[Metric, int, float, Tensor, None],
     ) -> None:
-        """
+        """Class for creating compositions of metrics.
+
+        This metric class is the output of adding, multiplying etc. any other metric. The metric re-implements the
+        standard ``update``, ``forward``, ``reset`` and ``compute`` methods to redirect the arguments to the metrics
+        that formed this composition.
+
         Args:
-            operator: the operator taking in one (if metric_b is None)
-                or two arguments. Will be applied to outputs of metric_a.compute()
-                and (optionally if metric_b is not None) metric_b.compute()
-            metric_a: first metric whose compute() result is the first argument of operator
+            operator:
+                The operator taking in one (if metric_b is None) or two arguments. Will be applied to outputs of
+                metric_a.compute() and (optionally if metric_b is not None) metric_b.compute()
+            metric_a:
+                First metric whose compute() result is the first argument of operator
             metric_b: second metric whose compute() result is the second argument of operator.
-                For operators taking in only one input, this should be None
+                For operators taking in only one input, this should be None.
         """
         super().__init__()
 
         self.op = operator
 
         if isinstance(metric_a, Tensor):
-            self.register_buffer("metric_a", metric_a)
+            self.register_buffer("metric_a", metric_a, persistent=False)
         else:
             self.metric_a = metric_a
 
         if isinstance(metric_b, Tensor):
-            self.register_buffer("metric_b", metric_b)
+            self.register_buffer("metric_b", metric_b, persistent=False)
         else:
             self.metric_b = metric_b
 
     def _sync_dist(self, dist_sync_fn: Optional[Callable] = None, process_group: Optional[Any] = None) -> None:
-        # No syncing required here. syncing will be done in metric_a and metric_b
+        """No syncing required here. syncing will be done in metric_a and metric_b."""
         pass
 
     def update(self, *args: Any, **kwargs: Any) -> None:
+        """Redirect the call to the input which the conposition was formed from."""
         if isinstance(self.metric_a, Metric):
             self.metric_a.update(*args, **self.metric_a._filter_kwargs(**kwargs))
 
         if isinstance(self.metric_b, Metric):
             self.metric_b.update(*args, **self.metric_b._filter_kwargs(**kwargs))
 
     def compute(self) -> Any:
-
+        """Redirect the call to the input which the conposition was formed from."""
         # also some parsing for kwargs?
-        if isinstance(self.metric_a, Metric):
-            val_a = self.metric_a.compute()
-        else:
-            val_a = self.metric_a
-
-        if isinstance(self.metric_b, Metric):
-            val_b = self.metric_b.compute()
-        else:
-            val_b = self.metric_b
+        val_a = self.metric_a.compute() if isinstance(self.metric_a, Metric) else self.metric_a
+        val_b = self.metric_b.compute() if isinstance(self.metric_b, Metric) else self.metric_b
 
         if val_b is None:
             return self.op(val_a)
 
         return self.op(val_a, val_b)
 
     @torch.jit.unused
     def forward(self, *args: Any, **kwargs: Any) -> Any:
-
+        """Calculate metric on current batch and accumulate to global state."""
         val_a = (
             self.metric_a(*args, **self.metric_a._filter_kwargs(**kwargs))
             if isinstance(self.metric_a, Metric)
             else self.metric_a
         )
         val_b = (
             self.metric_b(*args, **self.metric_b._filter_kwargs(**kwargs))
@@ -935,27 +1080,34 @@
             # Unary op
             return self.op(val_a)
 
         # Binary op
         return self.op(val_a, val_b)
 
     def reset(self) -> None:
+        """Redirect the call to the input which the conposition was formed from."""
         if isinstance(self.metric_a, Metric):
             self.metric_a.reset()
 
         if isinstance(self.metric_b, Metric):
             self.metric_b.reset()
 
     def persistent(self, mode: bool = False) -> None:
+        """Change if metric state is persistent (save as part of state_dict) or not.
+
+        Args:
+            mode: bool indicating if all states should be persistent or not
+
+        """
         if isinstance(self.metric_a, Metric):
             self.metric_a.persistent(mode=mode)
         if isinstance(self.metric_b, Metric):
             self.metric_b.persistent(mode=mode)
 
     def __repr__(self) -> str:
+        """Return a representation of the compositional metric, including the two inputs it was formed from."""
         _op_metrics = f"(\n  {self.op.__name__}(\n    {repr(self.metric_a)},\n    {repr(self.metric_b)}\n  )\n)"
-        repr_str = self.__class__.__name__ + _op_metrics
-
-        return repr_str
+        return self.__class__.__name__ + _op_metrics
 
     def _wrap_compute(self, compute: Callable) -> Callable:
+        """No wrapping nessesary for compositional metrics."""
         return compute
```

### Comparing `torchmetrics-0.9.3/torchmetrics/regression/__init__.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/regression/__init__.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,25 +1,50 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from torchmetrics.regression.cosine_similarity import CosineSimilarity  # noqa: F401
-from torchmetrics.regression.explained_variance import ExplainedVariance  # noqa: F401
-from torchmetrics.regression.log_mse import MeanSquaredLogError  # noqa: F401
-from torchmetrics.regression.mae import MeanAbsoluteError  # noqa: F401
-from torchmetrics.regression.mape import MeanAbsolutePercentageError  # noqa: F401
-from torchmetrics.regression.mse import MeanSquaredError  # noqa: F401
-from torchmetrics.regression.pearson import PearsonCorrCoef  # noqa: F401
-from torchmetrics.regression.r2 import R2Score  # noqa: F401
-from torchmetrics.regression.spearman import SpearmanCorrCoef  # noqa: F401
-from torchmetrics.regression.symmetric_mape import SymmetricMeanAbsolutePercentageError  # noqa: F401
-from torchmetrics.regression.tweedie_deviance import TweedieDevianceScore  # noqa: F401
-from torchmetrics.regression.wmape import WeightedMeanAbsolutePercentageError  # noqa: F401
+from torchmetrics.regression.concordance import ConcordanceCorrCoef
+from torchmetrics.regression.cosine_similarity import CosineSimilarity
+from torchmetrics.regression.explained_variance import ExplainedVariance
+from torchmetrics.regression.kendall import KendallRankCorrCoef
+from torchmetrics.regression.kl_divergence import KLDivergence
+from torchmetrics.regression.log_cosh import LogCoshError
+from torchmetrics.regression.log_mse import MeanSquaredLogError
+from torchmetrics.regression.mae import MeanAbsoluteError
+from torchmetrics.regression.mape import MeanAbsolutePercentageError
+from torchmetrics.regression.minkowski import MinkowskiDistance
+from torchmetrics.regression.mse import MeanSquaredError
+from torchmetrics.regression.pearson import PearsonCorrCoef
+from torchmetrics.regression.r2 import R2Score
+from torchmetrics.regression.spearman import SpearmanCorrCoef
+from torchmetrics.regression.symmetric_mape import SymmetricMeanAbsolutePercentageError
+from torchmetrics.regression.tweedie_deviance import TweedieDevianceScore
+from torchmetrics.regression.wmape import WeightedMeanAbsolutePercentageError
+
+__all__ = [
+    "ConcordanceCorrCoef",
+    "CosineSimilarity",
+    "ExplainedVariance",
+    "KendallRankCorrCoef",
+    "KLDivergence",
+    "LogCoshError",
+    "MeanSquaredLogError",
+    "MeanAbsoluteError",
+    "MeanAbsolutePercentageError",
+    "MinkowskiDistance",
+    "MeanSquaredError",
+    "PearsonCorrCoef",
+    "R2Score",
+    "SpearmanCorrCoef",
+    "SymmetricMeanAbsolutePercentageError",
+    "TweedieDevianceScore",
+    "WeightedMeanAbsolutePercentageError",
+]
```

### Comparing `torchmetrics-0.9.3/torchmetrics/regression/explained_variance.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/image/rmse_sw.py`

 * *Files 26% similar despite different names*

```diff
@@ -7,115 +7,129 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, Sequence, Union
+
+from typing import Any, Dict, Optional, Sequence, Tuple, Union
 
 import torch
-from torch import Tensor, tensor
+from torch import Tensor
 
-from torchmetrics.functional.regression.explained_variance import (
-    _explained_variance_compute,
-    _explained_variance_update,
-)
+from torchmetrics.functional.image.rmse_sw import _rmse_sw_compute, _rmse_sw_update
 from torchmetrics.metric import Metric
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["RootMeanSquaredErrorUsingSlidingWindow.plot"]
 
-class ExplainedVariance(Metric):
-    r"""Computes `explained variance`_:
 
-    .. math:: \text{ExplainedVariance} = 1 - \frac{\text{Var}(y - \hat{y})}{\text{Var}(y)}
+class RootMeanSquaredErrorUsingSlidingWindow(Metric):
+    """Computes Root Mean Squared Error (RMSE) using sliding window.
 
-    Where :math:`y` is a tensor of target values, and :math:`\hat{y}` is a tensor of predictions.
+    As input to ``forward`` and ``update`` the metric accepts the following input
 
-    Forward accepts
+    - ``preds`` (:class:`~torch.Tensor`): Predictions from model of shape ``(N,C,H,W)``
+    - ``target`` (:class:`~torch.Tensor`): Ground truth values of shape ``(N,C,H,W)``
 
-    - ``preds`` (float tensor): ``(N,)`` or ``(N, ...)`` (multioutput)
-    - ``target`` (long tensor): ``(N,)`` or ``(N, ...)`` (multioutput)
+    As output of `forward` and `compute` the metric returns the following output
 
-    In the case of multioutput, as default the variances will be uniformly averaged over the additional dimensions.
-    Please see argument ``multioutput`` for changing this behavior.
+    - ``rmse_sw`` (:class:`~torch.Tensor`): returns float scalar tensor with average RMSE-SW value over sample
 
     Args:
-        multioutput:
-            Defines aggregation in the case of multiple output scores. Can be one
-            of the following strings (default is ``'uniform_average'``.):
-
-            * ``'raw_values'`` returns full set of scores
-            * ``'uniform_average'`` scores are uniformly averaged
-            * ``'variance_weighted'`` scores are weighted by their individual variances
-
+        window_size: Sliding window used for rmse calculation
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
-    Raises:
-        ValueError:
-            If ``multioutput`` is not one of ``"raw_values"``, ``"uniform_average"`` or ``"variance_weighted"``.
-
     Example:
-        >>> from torchmetrics import ExplainedVariance
-        >>> target = torch.tensor([3, -0.5, 2, 7])
-        >>> preds = torch.tensor([2.5, 0.0, 2, 8])
-        >>> explained_variance = ExplainedVariance()
-        >>> explained_variance(preds, target)
-        tensor(0.9572)
-
-        >>> target = torch.tensor([[0.5, 1], [-1, 1], [7, -6]])
-        >>> preds = torch.tensor([[0, 2], [-1, 2], [8, -5]])
-        >>> explained_variance = ExplainedVariance(multioutput='raw_values')
-        >>> explained_variance(preds, target)
-        tensor([0.9677, 1.0000])
+        >>> from torchmetrics.image import RootMeanSquaredErrorUsingSlidingWindow
+        >>> g = torch.manual_seed(22)
+        >>> preds = torch.rand(4, 3, 16, 16)
+        >>> target = torch.rand(4, 3, 16, 16)
+        >>> rmse_sw = RootMeanSquaredErrorUsingSlidingWindow()
+        >>> rmse_sw(preds, target)
+        tensor(0.3999)
 
+    Raises:
+        ValueError: If ``window_size`` is not a positive integer.
     """
+
+    higher_is_better: bool = False
     is_differentiable: bool = True
-    higher_is_better: bool = True
     full_state_update: bool = False
-    n_obs: Tensor
-    sum_error: Tensor
-    sum_squared_error: Tensor
-    sum_target: Tensor
-    sum_squared_target: Tensor
+    plot_lower_bound: float = 0.0
+
+    rmse_val_sum: Tensor
+    rmse_map: Optional[Tensor] = None
+    total_images: Tensor
 
     def __init__(
         self,
-        multioutput: str = "uniform_average",
-        **kwargs: Any,
+        window_size: int = 8,
+        **kwargs: Dict[str, Any],
     ) -> None:
         super().__init__(**kwargs)
-        allowed_multioutput = ("raw_values", "uniform_average", "variance_weighted")
-        if multioutput not in allowed_multioutput:
-            raise ValueError(
-                f"Invalid input to argument `multioutput`. Choose one of the following: {allowed_multioutput}"
-            )
-        self.multioutput: str = multioutput
-        self.add_state("sum_error", default=tensor(0.0), dist_reduce_fx="sum")
-        self.add_state("sum_squared_error", default=tensor(0.0), dist_reduce_fx="sum")
-        self.add_state("sum_target", default=tensor(0.0), dist_reduce_fx="sum")
-        self.add_state("sum_squared_target", default=tensor(0.0), dist_reduce_fx="sum")
-        self.add_state("n_obs", default=tensor(0.0), dist_reduce_fx="sum")
+        if not isinstance(window_size, int) or isinstance(window_size, int) and window_size < 1:
+            raise ValueError("Argument `window_size` is expected to be a positive integer.")
+        self.window_size = window_size
+
+        self.add_state("rmse_val_sum", default=torch.tensor(0.0), dist_reduce_fx="sum")
+        self.add_state("total_images", default=torch.tensor(0.0), dist_reduce_fx="sum")
+
+    def update(self, preds: Tensor, target: Tensor) -> None:
+        """Update state with predictions and targets."""
+        if self.rmse_map is None:
+            _img_shape = target.shape[1:]  # channels, width, height
+            self.rmse_map = torch.zeros(_img_shape, dtype=target.dtype, device=target.device)
 
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
+        self.rmse_val_sum, self.rmse_map, self.total_images = _rmse_sw_update(
+            preds, target, self.window_size, self.rmse_val_sum, self.rmse_map, self.total_images
+        )
+
+    def compute(self) -> Optional[Tensor]:
+        """Compute Root Mean Squared Error (using sliding window) and potentially return RMSE map."""
+        assert self.rmse_map is not None  # noqa: S101  # needed for mypy
+        rmse, _ = _rmse_sw_compute(self.rmse_val_sum, self.rmse_map, self.total_images)
+        return rmse
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
 
         Args:
-            preds: Predictions from model
-            target: Ground truth values
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> import torch
+            >>> from torchmetrics.image import RootMeanSquaredErrorUsingSlidingWindow
+            >>> metric = RootMeanSquaredErrorUsingSlidingWindow()
+            >>> metric.update(torch.rand(4, 3, 16, 16), torch.rand(4, 3, 16, 16))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> import torch
+            >>> from torchmetrics.image import RootMeanSquaredErrorUsingSlidingWindow
+            >>> metric = RootMeanSquaredErrorUsingSlidingWindow()
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(torch.rand(4, 3, 16, 16), torch.rand(4, 3, 16, 16)))
+            >>> fig_, ax_ = metric.plot(values)
         """
-        n_obs, sum_error, sum_squared_error, sum_target, sum_squared_target = _explained_variance_update(preds, target)
-        self.n_obs = self.n_obs + n_obs
-        self.sum_error = self.sum_error + sum_error
-        self.sum_squared_error = self.sum_squared_error + sum_squared_error
-        self.sum_target = self.sum_target + sum_target
-        self.sum_squared_target = self.sum_squared_target + sum_squared_target
-
-    def compute(self) -> Union[Tensor, Sequence[Tensor]]:
-        """Computes explained variance over state."""
-        return _explained_variance_compute(
-            self.n_obs,
-            self.sum_error,
-            self.sum_squared_error,
-            self.sum_target,
-            self.sum_squared_target,
-            self.multioutput,
-        )
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/regression/r2.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/regression/r2.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,46 +1,60 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any
+from typing import Any, Optional, Sequence, Union
 
 import torch
 from torch import Tensor, tensor
 
 from torchmetrics.functional.regression.r2 import _r2_score_compute, _r2_score_update
 from torchmetrics.metric import Metric
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
+
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["R2Score.plot"]
 
 
 class R2Score(Metric):
-    r"""Computes r2 score also known as `R2 Score_Coefficient Determination`_:
+    r"""Compute r2 score also known as `R2 Score_Coefficient Determination`_.
 
     .. math:: R^2 = 1 - \frac{SS_{res}}{SS_{tot}}
 
     where :math:`SS_{res}=\sum_i (y_i - f(x_i))^2` is the sum of residual squares, and
     :math:`SS_{tot}=\sum_i (y_i - \bar{y})^2` is total sum of squares. Can also calculate
     adjusted r2 score given by
 
     .. math:: R^2_{adj} = 1 - \frac{(1-R^2)(n-1)}{n-k-1}
 
     where the parameter :math:`k` (the number of independent regressors) should be provided as the `adjusted` argument.
+    The score is only proper defined when :math:`SS_{tot}\neq 0`, which can happen for near constant targets. In this
+    case a score of 0 is returned. By definition the score is bounded between 0 and 1, where 1 corresponds to the
+    predictions exactly matching the targets.
+
+    As input to ``forward`` and ``update`` the metric accepts the following input:
+
+    - ``preds`` (:class:`~torch.Tensor`): Predictions from model in float tensor with shape ``(N,)``
+      or ``(N, M)`` (multioutput)
+    - ``target`` (:class:`~torch.Tensor`): Ground truth values in float tensor with shape ``(N,)``
+      or ``(N, M)`` (multioutput)
 
-    Forward accepts
+    As output of ``forward`` and ``compute`` the metric returns the following output:
 
-    - ``preds`` (float tensor): ``(N,)`` or ``(N, M)`` (multioutput)
-    - ``target`` (float tensor): ``(N,)`` or ``(N, M)`` (multioutput)
+    - ``r2score`` (:class:`~torch.Tensor`): A tensor with the r2 score(s)
 
     In the case of multioutput, as default the variances will be uniformly averaged over the additional dimensions.
     Please see argument ``multioutput`` for changing this behavior.
 
     Args:
         num_outputs: Number of outputs in multioutput setting
         adjusted: number of independent regressors for calculating adjusted r2 score.
@@ -54,31 +68,33 @@
     Raises:
         ValueError:
             If ``adjusted`` parameter is not an integer larger or equal to 0.
         ValueError:
             If ``multioutput`` is not one of ``"raw_values"``, ``"uniform_average"`` or ``"variance_weighted"``.
 
     Example:
-        >>> from torchmetrics import R2Score
+        >>> from torchmetrics.regression import R2Score
         >>> target = torch.tensor([3, -0.5, 2, 7])
         >>> preds = torch.tensor([2.5, 0.0, 2, 8])
         >>> r2score = R2Score()
         >>> r2score(preds, target)
         tensor(0.9486)
 
         >>> target = torch.tensor([[0.5, 1], [-1, 1], [7, -6]])
         >>> preds = torch.tensor([[0, 2], [-1, 2], [8, -5]])
         >>> r2score = R2Score(num_outputs=2, multioutput='raw_values')
         >>> r2score(preds, target)
         tensor([0.9654, 0.9082])
-
     """
     is_differentiable: bool = True
     higher_is_better: bool = True
     full_state_update: bool = False
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
+
     sum_squared_error: Tensor
     sum_error: Tensor
     residual: Tensor
     total: Tensor
 
     def __init__(
         self,
@@ -103,26 +119,62 @@
         self.multioutput = multioutput
 
         self.add_state("sum_squared_error", default=torch.zeros(self.num_outputs), dist_reduce_fx="sum")
         self.add_state("sum_error", default=torch.zeros(self.num_outputs), dist_reduce_fx="sum")
         self.add_state("residual", default=torch.zeros(self.num_outputs), dist_reduce_fx="sum")
         self.add_state("total", default=tensor(0), dist_reduce_fx="sum")
 
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
-
-        Args:
-            preds: Predictions from model
-            target: Ground truth values
-        """
+    def update(self, preds: Tensor, target: Tensor) -> None:
+        """Update state with predictions and targets."""
         sum_squared_error, sum_error, residual, total = _r2_score_update(preds, target)
 
         self.sum_squared_error += sum_squared_error
         self.sum_error += sum_error
         self.residual += residual
         self.total += total
 
     def compute(self) -> Tensor:
-        """Computes r2 score over the metric states."""
+        """Compute r2 score over the metric states."""
         return _r2_score_compute(
             self.sum_squared_error, self.sum_error, self.residual, self.total, self.adjusted, self.multioutput
         )
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import randn
+            >>> # Example plotting a single value
+            >>> from torchmetrics.regression import R2Score
+            >>> metric = R2Score()
+            >>> metric.update(randn(10,), randn(10,))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import randn
+            >>> # Example plotting multiple values
+            >>> from torchmetrics.regression import R2Score
+            >>> metric = R2Score()
+            >>> values = []
+            >>> for _ in range(10):
+            ...     values.append(metric(randn(10,), randn(10,)))
+            >>> fig, ax = metric.plot(values)
+        """
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/regression/wmape.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/detection/diou.py`

 * *Files 26% similar despite different names*

```diff
@@ -7,68 +7,76 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any
+from typing import Optional
 
 import torch
-from torch import Tensor
 
-from torchmetrics.functional.regression.wmape import (
-    _weighted_mean_absolute_percentage_error_compute,
-    _weighted_mean_absolute_percentage_error_update,
-)
-from torchmetrics.metric import Metric
+from torchmetrics.utilities.imports import _TORCHVISION_AVAILABLE, _TORCHVISION_GREATER_EQUAL_0_13
 
+if _TORCHVISION_AVAILABLE and _TORCHVISION_GREATER_EQUAL_0_13:
+    from torchvision.ops import distance_box_iou
+else:
+    distance_box_iou = None
+    __doctest_skip__ = ["distance_intersection_over_union"]
+
+__doctest_requires__ = {("distance_intersection_over_union",): ["torchvision"]}
+
+
+def _diou_update(
+    preds: torch.Tensor, target: torch.Tensor, iou_threshold: Optional[float], replacement_val: float = 0
+) -> torch.Tensor:
+    iou = distance_box_iou(preds, target)
+    if iou_threshold is not None:
+        iou[iou < iou_threshold] = replacement_val
+    return iou
+
+
+def _diou_compute(iou: torch.Tensor, labels_eq: bool = True) -> torch.Tensor:
+    if labels_eq:
+        return iou.diag().mean()
+    return iou.mean()
+
+
+def distance_intersection_over_union(
+    preds: torch.Tensor,
+    target: torch.Tensor,
+    iou_threshold: Optional[float] = None,
+    replacement_val: float = 0,
+    aggregate: bool = True,
+) -> torch.Tensor:
+    r"""Compute `Distance Intersection over Union <https://arxiv.org/abs/1911.08287v1>`_ between two sets of boxes.
 
-class WeightedMeanAbsolutePercentageError(Metric):
-    r"""
-    Computes weighted mean absolute percentage error (`WMAPE`_). The output of WMAPE metric
-    is a non-negative floating point, where the optimal value is 0. It is computes as:
-
-    .. math::
-        \text{WMAPE} = \frac{\sum_{t=1}^n | y_t - \hat{y}_t | }{\sum_{t=1}^n |y_t| }
-
-    Where :math:`y` is a tensor of target values, and :math:`\hat{y}` is a tensor of predictions.
+    Both sets of boxes are expected to be in (x1, y1, x2, y2) format with 0 <= x1 < x2 and 0 <= y1 < y2.
 
     Args:
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+        preds:
+            The input tensor containing the predicted bounding boxes.
+        target:
+            The tensor containing the ground truth.
+        iou_threshold:
+            Optional IoU thresholds for evaluation. If set to `None` the threshold is ignored.
+        replacement_val:
+            Value to replace values under the threshold with.
+        aggregate:
+            Return the average value instead of the complete IoU matrix.
 
     Example:
         >>> import torch
-        >>> _ = torch.manual_seed(42)
-        >>> preds = torch.randn(20,)
-        >>> target = torch.randn(20,)
-        >>> metric = WeightedMeanAbsolutePercentageError()
-        >>> metric(preds, target)
-        tensor(1.3967)
-
+        >>> from torchmetrics.functional.detection import distance_intersection_over_union
+        >>> preds = torch.Tensor([[100, 100, 200, 200]])
+        >>> target = torch.Tensor([[110, 110, 210, 210]])
+        >>> distance_intersection_over_union(preds, target)
+        tensor(0.6724)
     """
-    is_differentiable: bool = True
-    higher_is_better: bool = False
-    full_state_update: bool = False
-    sum_abs_error: Tensor
-    sum_scale: Tensor
-
-    def __init__(self, **kwargs: Any) -> None:
-        super().__init__(**kwargs)
-        self.add_state("sum_abs_error", default=torch.tensor(0.0), dist_reduce_fx="sum")
-        self.add_state("sum_scale", default=torch.tensor(0.0), dist_reduce_fx="sum")
-
-    def update(self, preds: Tensor, target: Tensor) -> None:  # type: ignore
-        """Update state with predictions and targets.
-
-        Args:
-            preds: Predictions from model
-            target: Ground truth values
-        """
-        sum_abs_error, sum_scale = _weighted_mean_absolute_percentage_error_update(preds, target)
-
-        self.sum_abs_error += sum_abs_error
-        self.sum_scale += sum_scale
-
-    def compute(self) -> Tensor:
-        """Computes weighted mean absolute percentage error over state."""
-        return _weighted_mean_absolute_percentage_error_compute(self.sum_abs_error, self.sum_scale)
+    if not _TORCHVISION_GREATER_EQUAL_0_13:
+        raise ModuleNotFoundError(
+            f"`{distance_intersection_over_union.__name__}` requires that `torchvision` version 0.13.0 or newer"
+            " is installed."
+            " Please install with `pip install torchvision>=0.13` or `pip install torchmetrics[detection]`."
+        )
+    iou = _diou_update(preds, target, iou_threshold, replacement_val)
+    return _diou_compute(iou) if aggregate else iou
```

### Comparing `torchmetrics-0.9.3/torchmetrics/retrieval/base.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/retrieval/base.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -15,35 +15,38 @@
 from typing import Any, List, Optional
 
 import torch
 from torch import Tensor, tensor
 
 from torchmetrics import Metric
 from torchmetrics.utilities.checks import _check_retrieval_inputs
-from torchmetrics.utilities.data import get_group_indexes
-
-#: get_group_indexes is used to group predictions belonging to the same document
+from torchmetrics.utilities.data import _flexible_bincount, dim_zero_cat
 
 
 class RetrievalMetric(Metric, ABC):
     """Works with binary target data. Accepts float predictions from a model output.
 
-    Forward accepts
+    As input to ``forward`` and ``update`` the metric accepts the following input:
 
-    - ``preds`` (float tensor): ``(N, ...)``
-    - ``target`` (long or bool tensor): ``(N, ...)``
-    - ``indexes`` (long tensor): ``(N, ...)``
+    - ``preds`` (:class:`~torch.Tensor`): A float tensor of shape ``(N, ...)``
+    - ``target`` (:class:`~torch.Tensor`): A long or bool tensor of shape ``(N, ...)``
+    - ``indexes`` (:class:`~torch.Tensor`): A long tensor of shape ``(N, ...)`` which indicate to which query a
+      prediction belongs
 
-    ``indexes``, ``preds`` and ``target`` must have the same dimension and will be flatten
+    .. note:: ``indexes``, ``preds`` and ``target`` must have the same dimension and will be flatten
     to single dimension once provided.
 
-    ``indexes`` indicate to which query a prediction belongs.
-    Predictions will be first grouped by indexes. Then the
-    real metric, defined by overriding the `_metric` method,
-    will be computed as the mean of the scores over each query.
+    .. note::
+        Predictions will be first grouped by ``indexes`` and then the real metric, defined by overriding
+        the `_metric` method, will be computed as the mean of the scores over each query.
+
+    As output to ``forward`` and ``compute`` the metric returns the following output:
+
+    - ``metric`` (:class:`~torch.Tensor`): A tensor as computed by ``_metric`` if the number of positive targets is
+      at least 1, otherwise behave as specified by ``self.empty_target_action``.
 
     Args:
         empty_target_action:
             Specify what to do with queries that do not have at least a positive
             or negative (depend on metric) target. Choose from:
 
             - ``'neg'``: those queries count as ``0.0`` (default)
@@ -90,15 +93,15 @@
 
         self.ignore_index = ignore_index
 
         self.add_state("indexes", default=[], dist_reduce_fx=None)
         self.add_state("preds", default=[], dist_reduce_fx=None)
         self.add_state("target", default=[], dist_reduce_fx=None)
 
-    def update(self, preds: Tensor, target: Tensor, indexes: Tensor) -> None:  # type: ignore
+    def update(self, preds: Tensor, target: Tensor, indexes: Tensor) -> None:
         """Check shape, check and convert dtypes, flatten and add to accumulators."""
         if indexes is None:
             raise ValueError("Argument `indexes` cannot be None")
 
         indexes, preds, target = _check_retrieval_inputs(
             indexes, preds, target, allow_non_binary_target=self.allow_non_binary_target, ignore_index=self.ignore_index
         )
@@ -110,25 +113,28 @@
     def compute(self) -> Tensor:
         """First concat state ``indexes``, ``preds`` and ``target`` since they were stored as lists.
 
         After that, compute list of groups that will help in keeping together predictions about the same query. Finally,
         for each group compute the ``_metric`` if the number of positive targets is at least 1, otherwise behave as
         specified by ``self.empty_target_action``.
         """
-        indexes = torch.cat(self.indexes, dim=0)
-        preds = torch.cat(self.preds, dim=0)
-        target = torch.cat(self.target, dim=0)
+        indexes = dim_zero_cat(self.indexes)
+        preds = dim_zero_cat(self.preds)
+        target = dim_zero_cat(self.target)
+
+        indexes, indices = torch.sort(indexes)
+        preds = preds[indices]
+        target = target[indices]
 
-        res = []
-        groups = get_group_indexes(indexes)
-
-        for group in groups:
-            mini_preds = preds[group]
-            mini_target = target[group]
+        split_sizes = _flexible_bincount(indexes).detach().cpu().tolist()
 
+        res = []
+        for mini_preds, mini_target in zip(
+            torch.split(preds, split_sizes, dim=0), torch.split(target, split_sizes, dim=0)
+        ):
             if not mini_target.sum():
                 if self.empty_target_action == "error":
                     raise ValueError("`compute` method was provided with a query with no positive target.")
                 if self.empty_target_action == "pos":
                     res.append(tensor(1.0))
                 elif self.empty_target_action == "neg":
                     res.append(tensor(0.0))
```

### Comparing `torchmetrics-0.9.3/torchmetrics/retrieval/fall_out.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/retrieval/reciprocal_rank.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,125 +1,125 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, Optional
+from typing import Optional, Sequence, Union
 
-import torch
-from torch import Tensor, tensor
+from torch import Tensor
 
-from torchmetrics.functional.retrieval.fall_out import retrieval_fall_out
+from torchmetrics.functional.retrieval.reciprocal_rank import retrieval_reciprocal_rank
 from torchmetrics.retrieval.base import RetrievalMetric
-from torchmetrics.utilities.data import get_group_indexes
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["RetrievalMRR.plot"]
 
-class RetrievalFallOut(RetrievalMetric):
-    """Computes `Fall-out`_.
+
+class RetrievalMRR(RetrievalMetric):
+    """Compute `Mean Reciprocal Rank`_.
 
     Works with binary target data. Accepts float predictions from a model output.
 
-    Forward accepts:
+    As input to ``forward`` and ``update`` the metric accepts the following input:
+
+    - ``preds`` (:class:`~torch.Tensor`): A float tensor of shape ``(N, ...)``
+    - ``target`` (:class:`~torch.Tensor`): A long or bool tensor of shape ``(N, ...)``
+    - ``indexes`` (:class:`~torch.Tensor`): A long tensor of shape ``(N, ...)`` which indicate to which query a
+      prediction belongs
+
+    As output to ``forward`` and ``compute`` the metric returns the following output:
 
-    - ``preds`` (float tensor): ``(N, ...)``
-    - ``target`` (long or bool tensor): ``(N, ...)``
-    - ``indexes`` (long tensor): ``(N, ...)``
-
-    ``indexes``, ``preds`` and ``target`` must have the same dimension.
-    ``indexes`` indicate to which query a prediction belongs.
-    Predictions will be first grouped by ``indexes`` and then `Fall-out` will be computed as the mean
-    of the `Fall-out` over each query.
+    - ``mrr`` (:class:`~torch.Tensor`): A single-value tensor with the reciprocal rank (RR) of the predictions
+      ``preds`` w.r.t. the labels ``target``
+
+    All ``indexes``, ``preds`` and ``target`` must have the same dimension and will be flatten at the beginning,
+    so that for example, a tensor of shape ``(N, M)`` is treated as ``(N * M, )``. Predictions will be first grouped by
+    ``indexes`` and then will be computed as the mean of the metric over each query.
 
     Args:
         empty_target_action:
-            Specify what to do with queries that do not have at least a negative ``target``. Choose from:
+            Specify what to do with queries that do not have at least a positive ``target``. Choose from:
 
             - ``'neg'``: those queries count as ``0.0`` (default)
             - ``'pos'``: those queries count as ``1.0``
             - ``'skip'``: skip those queries; if all queries are skipped, ``0.0`` is returned
             - ``'error'``: raise a ``ValueError``
 
-        ignore_index:
-            Ignore predictions where the target is equal to this number.
-        k: consider only the top k elements for each query (default: `None`, which considers them all)
+        ignore_index: Ignore predictions where the target is equal to this number.
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
     Raises:
         ValueError:
             If ``empty_target_action`` is not one of ``error``, ``skip``, ``neg`` or ``pos``.
         ValueError:
             If ``ignore_index`` is not `None` or an integer.
-        ValueError:
-            If ``k`` parameter is not `None` or an integer larger than 0.
 
     Example:
-        >>> from torchmetrics import RetrievalFallOut
+        >>> from torch import tensor
+        >>> from torchmetrics.retrieval import RetrievalMRR
         >>> indexes = tensor([0, 0, 0, 1, 1, 1, 1])
         >>> preds = tensor([0.2, 0.3, 0.5, 0.1, 0.3, 0.5, 0.2])
         >>> target = tensor([False, False, True, False, True, False, True])
-        >>> fo = RetrievalFallOut(k=2)
-        >>> fo(preds, target, indexes=indexes)
-        tensor(0.5000)
+        >>> mrr = RetrievalMRR()
+        >>> mrr(preds, target, indexes=indexes)
+        tensor(0.7500)
     """
 
     is_differentiable: bool = False
-    higher_is_better: bool = False
+    higher_is_better: bool = True
     full_state_update: bool = False
-
-    def __init__(
-        self,
-        empty_target_action: str = "pos",
-        ignore_index: Optional[int] = None,
-        k: Optional[int] = None,
-        **kwargs: Any,
-    ) -> None:
-        super().__init__(
-            empty_target_action=empty_target_action,
-            ignore_index=ignore_index,
-            **kwargs,
-        )
-
-        if (k is not None) and not (isinstance(k, int) and k > 0):
-            raise ValueError("`k` has to be a positive integer or None")
-        self.k = k
-
-    def compute(self) -> Tensor:
-        """First concat state ``indexes``, ``preds`` and ``target`` since they were stored as lists.
-
-        After that, compute list of groups that will help in keeping together predictions about the same query. Finally,
-        for each group compute the `_metric` if the number of negative targets is at least 1, otherwise behave as
-        specified by `self.empty_target_action`.
-        """
-        indexes = torch.cat(self.indexes, dim=0)
-        preds = torch.cat(self.preds, dim=0)
-        target = torch.cat(self.target, dim=0)
-
-        res = []
-        groups = get_group_indexes(indexes)
-
-        for group in groups:
-            mini_preds = preds[group]
-            mini_target = target[group]
-
-            if not (1 - mini_target).sum():
-                if self.empty_target_action == "error":
-                    raise ValueError("`compute` method was provided with a query with no negative target.")
-                if self.empty_target_action == "pos":
-                    res.append(tensor(1.0))
-                elif self.empty_target_action == "neg":
-                    res.append(tensor(0.0))
-            else:
-                # ensure list containt only float tensors
-                res.append(self._metric(mini_preds, mini_target))
-
-        return torch.stack([x.to(preds) for x in res]).mean() if res else tensor(0.0).to(preds)
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
 
     def _metric(self, preds: Tensor, target: Tensor) -> Tensor:
-        return retrieval_fall_out(preds, target, k=self.k)
+        return retrieval_reciprocal_rank(preds, target)
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> import torch
+            >>> from torchmetrics.retrieval import RetrievalMRR
+            >>> # Example plotting a single value
+            >>> metric = RetrievalMRR()
+            >>> metric.update(torch.rand(10,), torch.randint(2, (10,)), indexes=torch.randint(2,(10,)))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> import torch
+            >>> from torchmetrics.retrieval import RetrievalMRR
+            >>> # Example plotting multiple values
+            >>> metric = RetrievalMRR()
+            >>> values = []
+            >>> for _ in range(10):
+            ...     values.append(metric(torch.rand(10,), torch.randint(2, (10,)), indexes=torch.randint(2,(10,))))
+            >>> fig, ax = metric.plot(values)
+        """
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/text/bert.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/text/infolm.py`

 * *Files 25% similar despite different names*

```diff
@@ -1,231 +1,242 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, Callable, Dict, List, Optional, Union
-from warnings import warn
+import os
+from typing import Any, Dict, List, Optional, Sequence, Tuple, Union
 
 import torch
 from torch import Tensor
-from torch.nn import Module
 
-from torchmetrics.functional.text.bert import _preprocess_text, bert_score
+from torchmetrics.functional.text.helper_embedding_metric import _load_tokenizer_and_model
+from torchmetrics.functional.text.infolm import (
+    _ALLOWED_INFORMATION_MEASURE_LITERAL,
+    _get_dataloader,
+    _get_special_tokens_map,
+    _infolm_compute,
+    _infolm_update,
+    _InformationMeasure,
+)
 from torchmetrics.metric import Metric
-from torchmetrics.utilities.imports import _TRANSFORMERS_AUTO_AVAILABLE
+from torchmetrics.utilities.data import dim_zero_cat
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE, _TRANSFORMERS_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
-if _TRANSFORMERS_AUTO_AVAILABLE:
-    from transformers.models.auto import AutoTokenizer
-else:
-    __doctest_skip__ = ["BERTScore"]
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["InfoLM.plot"]
 
+if not _TRANSFORMERS_AVAILABLE:
+    __doctest_skip__ = ["InfoLM", "InfoLM.plot"]
 
-# Default model recommended in the original implementation.
-_DEFAULT_MODEL = "roberta-large"
 
+class InfoLM(Metric):
+    """Calculate `InfoLM`_.
 
-def _get_input_dict(input_ids: List[Tensor], attention_mask: List[Tensor]) -> Dict[str, Tensor]:
-    """Create an input dictionary of ``input_ids`` and ``attention_mask`` for BERTScore calculation."""
-    output_dict = {"input_ids": torch.cat(input_ids), "attention_mask": torch.cat(attention_mask)}
-    return output_dict
+    InfoLM measures a distance/divergence between predicted and reference sentence discrete distribution using one of
+    the following information measures:
 
+        - `KL divergence`_
+        - `alpha divergence`_
+        - `beta divergence`_
+        - `AB divergence`_
+        - `Rnyi divergence`_
+        - L1 distance
+        - L2 distance
+        - L-infinity distance
+        - `Fisher-Rao distance`_
 
-class BERTScore(Metric):
-    """`Bert_score Evaluating Text Generation`_ leverages the pre-trained contextual embeddings from BERT and
-    matches words in candidate and reference sentences by cosine similarity. It has been shown to correlate with
-    human judgment on sentence-level and system-level evaluation. Moreover, BERTScore computes precision, recall,
-    and F1 measure, which can be useful for evaluating different language generation tasks.
+    `InfoLM`_ is a family of untrained embedding-based metrics which addresses some famous flaws of standard
+    string-based metrics thanks to the usage of pre-trained masked language models. This family of metrics is mainly
+    designed for summarization and data-to-text tasks.
 
-    This implemenation follows the original implementation from `BERT_score`_.
+    The implementation of this metric is fully based HuggingFace ``transformers``' package.
 
-    Args:
-        preds: An iterable of predicted sentences.
-        target: An iterable of target sentences.
-        model_type: A name or a model path used to load `transformers` pretrained model.
-        num_layers: A layer of representation to use.
-        all_layers:
-            An indication of whether the representation from all model's layers should be used.
-            If `all_layers = True`, the argument `num_layers` is ignored.
-        model:  A user's own model. Must be of `torch.nn.Module` instance.
-        user_tokenizer:
-            A user's own tokenizer used with the own model. This must be an instance with the `__call__` method.
-            This method must take an iterable of sentences (`List[str]`) and must return a python dictionary
-            containing `"input_ids"` and `"attention_mask"` represented by `torch.Tensor`. It is up to the user's model
-            of whether `"input_ids"` is a `torch.Tensor` of input ids or embedding vectors.
-            This tokenizer must prepend an equivalent of `[CLS]` token and append an equivalent of `[SEP]` token
-            as `transformers` tokenizer does.
-        user_forward_fn:
-            A user's own forward function used in a combination with `user_model`. This function must take `user_model`
-            and a python dictionary of containing `"input_ids"` and `"attention_mask"` represented by `torch.Tensor`
-            as an input and return the model's output represented by the single `torch.Tensor`.
-        verbose: An indication of whether a progress bar to be displayed during the embeddings' calculation.
-        idf: An indication whether normalization using inverse document frequencies should be used.
-        device: A device to be used for calculation.
-        max_length: A maximum length of input sequences. Sequences longer than `max_length` are to be trimmed.
-        batch_size: A batch size used for model processing.
-        num_threads: A number of threads to use for a dataloader.
-        return_hash: An indication of whether the correspodning `hash_code` should be returned.
-        lang: A language of input sentences.
-        rescale_with_baseline:
-            An indication of whether bertscore should be rescaled with a pre-computed baseline.
-            When a pretrained model from `transformers` model is used, the corresponding baseline is downloaded
-            from the original `bert-score` package from `BERT_score`_ if available.
-            In other cases, please specify a path to the baseline csv/tsv file, which must follow the formatting
-            of the files from `BERT_score`_.
-        baseline_path: A path to the user's own local csv/tsv file with the baseline scale.
-        baseline_url: A url path to the user's own  csv/tsv file with the baseline scale.
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
+    As input to ``forward`` and ``update`` the metric accepts the following input:
+
+    - ``preds`` (:class:`~Sequence`): An iterable of hypothesis corpus
+    - ``target`` (:class:`~Sequence`): An iterable of reference corpus
+
+    As output of ``forward`` and ``compute`` the metric returns the following output:
 
-    Returns:
-        Python dictionary containing the keys `precision`, `recall` and `f1` with corresponding values.
+    -  ``infolm`` (:class:`~torch.Tensor`): If `return_sentence_level_score=True` return a tuple with a tensor
+       with the corpus-level InfoLM score and a list of sentence-level InfoLM scores, else return a corpus-level
+       InfoLM score
+
+    Args:
+        model_name_or_path:
+            A name or a model path used to load ``transformers`` pretrained model.
+            By default the `"bert-base-uncased"` model is used.
+        temperature:
+            A temperature for calibrating language modelling. For more information, please reference `InfoLM`_ paper.
+        information_measure:
+            A name of information measure to be used. Please use one of: ['kl_divergence', 'alpha_divergence',
+            'beta_divergence', 'ab_divergence', 'renyi_divergence', 'l1_distance', 'l2_distance', 'l_infinity_distance',
+            'fisher_rao_distance']
+        idf:
+            An indication of whether normalization using inverse document frequencies should be used.
+        alpha:
+            Alpha parameter of the divergence used for alpha, AB and Rnyi divergence measures.
+        beta:
+            Beta parameter of the divergence used for beta and AB divergence measures.
+        device:
+            A device to be used for calculation.
+        max_length:
+            A maximum length of input sequences. Sequences longer than ``max_length`` are to be trimmed.
+        batch_size:
+            A batch size used for model processing.
+        num_threads:
+            A number of threads to use for a dataloader.
+        verbose:
+            An indication of whether a progress bar to be displayed during the embeddings calculation.
+        return_sentence_level_score:
+            An indication whether a sentence-level InfoLM score to be returned.
 
     Example:
-        >>> from torchmetrics.text.bert import BERTScore
-        >>> preds = ["hello there", "general kenobi"]
-        >>> target = ["hello there", "master kenobi"]
-        >>> bertscore = BERTScore()
-        >>> score = bertscore(preds, target)
-        >>> from pprint import pprint
-        >>> rounded_score = {k: [round(v, 3) for v in vv] for k, vv in score.items()}
-        >>> pprint(rounded_score)
-        {'f1': [1.0, 0.996], 'precision': [1.0, 0.996], 'recall': [1.0, 0.996]}
+        >>> from torchmetrics.text.infolm import InfoLM
+        >>> preds = ['he read the book because he was interested in world history']
+        >>> target = ['he was interested in world history because he read the book']
+        >>> infolm = InfoLM('google/bert_uncased_L-2_H-128_A-2', idf=False)
+        >>> infolm(preds, target)
+        tensor(-0.1784)
     """
 
-    is_differentiable: bool = False
-    higher_is_better: bool = True
-    full_state_update: bool = False
-
+    is_differentiable = False
+    higher_is_better = True
     preds_input_ids: List[Tensor]
     preds_attention_mask: List[Tensor]
     target_input_ids: List[Tensor]
     target_attention_mask: List[Tensor]
 
     def __init__(
         self,
-        model_name_or_path: Optional[str] = None,
-        num_layers: Optional[int] = None,
-        all_layers: bool = False,
-        model: Optional[Module] = None,
-        user_tokenizer: Optional[Any] = None,
-        user_forward_fn: Callable[[Module, Dict[str, Tensor]], Tensor] = None,
-        verbose: bool = False,
-        idf: bool = False,
+        model_name_or_path: Union[str, os.PathLike] = "bert-base-uncased",
+        temperature: float = 0.25,
+        information_measure: _ALLOWED_INFORMATION_MEASURE_LITERAL = "kl_divergence",
+        idf: bool = True,
+        alpha: Optional[float] = None,
+        beta: Optional[float] = None,
         device: Optional[Union[str, torch.device]] = None,
-        max_length: int = 512,
+        max_length: Optional[int] = None,
         batch_size: int = 64,
-        num_threads: int = 4,
-        return_hash: bool = False,
-        lang: str = "en",
-        rescale_with_baseline: bool = False,
-        baseline_path: Optional[str] = None,
-        baseline_url: Optional[str] = None,
-        **kwargs: Any,
-    ):
+        num_threads: int = 0,
+        verbose: bool = True,
+        return_sentence_level_score: bool = False,
+        **kwargs: Dict[str, Any],
+    ) -> None:
         super().__init__(**kwargs)
-        self.model_name_or_path = model_name_or_path or _DEFAULT_MODEL
-        self.num_layers = num_layers
-        self.all_layers = all_layers
-        self.model = model
-        self.user_forward_fn = user_forward_fn
-        self.verbose = verbose
+        self.model_name_or_path = model_name_or_path
+        self.temperature = temperature
+        self.information_measure = information_measure
         self.idf = idf
-        self.embedding_device = device
-        self.max_length = max_length
+        self.alpha = alpha
+        self.beta = beta
+        self._device = torch.device(device or "cpu")
         self.batch_size = batch_size
         self.num_threads = num_threads
-        self.return_hash = return_hash
-        self.lang = lang
-        self.rescale_with_baseline = rescale_with_baseline
-        self.baseline_path = baseline_path
-        self.baseline_url = baseline_url
-        self.preds: Dict[str, List[Tensor]] = {"input_ids": [], "attention_mask": []}
-        self.target: Dict[str, List[Tensor]] = {"input_ids": [], "attention_mask": []}
-
-        if user_tokenizer:
-            self.tokenizer = user_tokenizer
-            self.user_tokenizer = True
-        else:
-            if not _TRANSFORMERS_AUTO_AVAILABLE:
-                raise ModuleNotFoundError(
-                    "`BERTScore` metric with default tokenizers requires `transformers` package be installed."
-                    " Either install with `pip install transformers>=4.0` or `pip install torchmetrics[text]`."
-                )
-            if model_name_or_path is None:
-                warn(
-                    "The argument `model_name_or_path` was not specified while it is required when the default"
-                    " `transformers` model is used."
-                    f" It will use the default recommended model - {_DEFAULT_MODEL!r}."
-                )
-            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path)
-            self.user_tokenizer = False
+        self.verbose = verbose
+        self.return_sentence_level_score = return_sentence_level_score
+
+        self.tokenizer, self.model = _load_tokenizer_and_model(model_name_or_path, device)
+        self.information_measure_cls = _InformationMeasure(information_measure, alpha, beta)
+        self.max_length = max_length or self.model.config.max_length
+        self.special_tokens_map = _get_special_tokens_map(self.tokenizer)
 
         self.add_state("preds_input_ids", [], dist_reduce_fx="cat")
         self.add_state("preds_attention_mask", [], dist_reduce_fx="cat")
         self.add_state("target_input_ids", [], dist_reduce_fx="cat")
         self.add_state("target_attention_mask", [], dist_reduce_fx="cat")
 
-    def update(self, preds: List[str], target: List[str]) -> None:  # type: ignore
-        """Store predictions/references for computing BERT scores. It is necessary to store sentences in a
-        tokenized form to ensure the DDP mode working.
-
-        Args:
-            preds: An iterable of predicted sentences.
-            target: An iterable of reference sentences.
-        """
-        preds_dict = _preprocess_text(
-            preds,
-            self.tokenizer,
-            self.max_length,
-            truncation=False,
-            sort_according_length=False,
-            own_tokenizer=self.user_tokenizer,
+    def update(self, preds: Union[str, Sequence[str]], target: Union[str, Sequence[str]]) -> None:
+        """Update state with predictions and targets."""
+        preds_input_ids, preds_attention_mask, target_input_ids, target_attention_mask = _infolm_update(
+            preds, target, self.tokenizer, self.max_length
         )
-        target_dict = _preprocess_text(
-            target,
-            self.tokenizer,
-            self.max_length,
-            truncation=False,
-            sort_according_length=False,
-            own_tokenizer=self.user_tokenizer,
+        self.preds_input_ids.append(preds_input_ids)
+        self.preds_attention_mask.append(preds_attention_mask)
+        self.target_input_ids.append(target_input_ids)
+        self.target_attention_mask.append(target_attention_mask)
+
+    def compute(self) -> Union[Tensor, Tuple[Tensor, Tensor]]:
+        """Calculate selected information measure using the pre-trained language model."""
+        preds_dataloader = _get_dataloader(
+            input_ids=dim_zero_cat(self.preds_input_ids),
+            attention_mask=dim_zero_cat(self.preds_attention_mask),
+            idf=self.idf,
+            batch_size=self.batch_size,
+            num_workers=self.num_threads,
+        )
+        target_dataloader = _get_dataloader(
+            input_ids=dim_zero_cat(self.target_input_ids),
+            attention_mask=dim_zero_cat(self.target_attention_mask),
+            idf=self.idf,
+            batch_size=self.batch_size,
+            num_workers=self.num_threads,
+        )
+
+        info_lm_score = _infolm_compute(
+            self.model,
+            preds_dataloader,
+            target_dataloader,
+            self.temperature,
+            self.idf,
+            self.information_measure_cls,
+            self.special_tokens_map,
+            self.verbose,
         )
 
-        self.preds_input_ids.append(preds_dict["input_ids"])
-        self.preds_attention_mask.append(preds_dict["attention_mask"])
-        self.target_input_ids.append(target_dict["input_ids"])
-        self.target_attention_mask.append(target_dict["attention_mask"])
+        if self.return_sentence_level_score:
+            return info_lm_score.mean(), info_lm_score
+
+        return info_lm_score.mean()
 
-    def compute(self) -> Dict[str, Union[List[float], str]]:
-        """Calculate BERT scores.
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
 
-        Return:
-            Python dictionary containing the keys `precision`, `recall` and `f1` with corresponding values.
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> from torchmetrics.text.infolm import InfoLM
+            >>> metric = InfoLM('google/bert_uncased_L-2_H-128_A-2', idf=False)
+            >>> preds = ['he read the book because he was interested in world history']
+            >>> target = ['he was interested in world history because he read the book']
+            >>> metric.update(preds, target)
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> from torchmetrics.text.infolm import InfoLM
+            >>> metric = InfoLM('google/bert_uncased_L-2_H-128_A-2', idf=False)
+            >>> preds = ["this is the prediction", "there is an other sample"]
+            >>> target = ["this is the reference", "there is another one"]
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(preds, target))
+            >>> fig_, ax_ = metric.plot(values)
         """
-        return bert_score(
-            preds=_get_input_dict(self.preds_input_ids, self.preds_attention_mask),
-            target=_get_input_dict(self.target_input_ids, self.target_attention_mask),
-            model_name_or_path=self.model_name_or_path,
-            num_layers=self.num_layers,
-            all_layers=self.all_layers,
-            model=self.model,
-            user_tokenizer=self.tokenizer if self.user_tokenizer else None,
-            user_forward_fn=self.user_forward_fn,
-            verbose=self.verbose,
-            idf=self.idf,
-            device=self.embedding_device,
-            max_length=self.max_length,
-            batch_size=self.batch_size,
-            num_threads=self.num_threads,
-            return_hash=self.return_hash,
-            lang=self.lang,
-            rescale_with_baseline=self.rescale_with_baseline,
-            baseline_path=self.baseline_path,
-            baseline_url=self.baseline_url,
-        )
+        return self._plot(val, ax)
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `torchmetrics-0.9.3/torchmetrics/text/bleu.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/text/bleu.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -12,101 +12,144 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 # referenced from
 # Library Name: torchtext
 # Authors: torchtext authors and @sluks
 # Date: 2020-07-18
 # Link: https://pytorch.org/text/_modules/torchtext/data/metrics.html#bleu_score
-from typing import Any, Optional, Sequence
+from typing import Any, Optional, Sequence, Union
 
 import torch
 from torch import Tensor, tensor
 
 from torchmetrics import Metric
 from torchmetrics.functional.text.bleu import _bleu_score_compute, _bleu_score_update, _tokenize_fn
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
+
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["BLEUScore.plot"]
 
 
 class BLEUScore(Metric):
     """Calculate `BLEU score`_ of machine translated text with one or more references.
 
+    As input to ``forward`` and ``update`` the metric accepts the following input:
+
+    - ``preds`` (:class:`~Sequence`): An iterable of machine translated corpus
+    - ``target`` (:class:`~Sequence`): An iterable of iterables of reference corpus
+
+    As output of ``forward`` and ``update`` the metric returns the following output:
+
+    - ``bleu`` (:class:`~torch.Tensor`): A tensor with the BLEU Score
+
     Args:
         n_gram: Gram value ranged from 1 to 4
-        smooth: Whether or not to apply smoothing, see [2]
+        smooth: Whether or not to apply smoothing, see `Machine Translation Evolution`_
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
         weights:
             Weights used for unigrams, bigrams, etc. to calculate BLEU score.
             If not provided, uniform weights are used.
 
     Raises:
         ValueError: If a length of a list of weights is not ``None`` and not equal to ``n_gram``.
 
     Example:
-        >>> from torchmetrics import BLEUScore
+        >>> from torchmetrics.text import BLEUScore
         >>> preds = ['the cat is on the mat']
         >>> target = [['there is a cat on the mat', 'a cat is on the mat']]
-        >>> metric = BLEUScore()
-        >>> metric(preds, target)
+        >>> bleu = BLEUScore()
+        >>> bleu(preds, target)
         tensor(0.7598)
-
-    References:
-        [1] BLEU: a Method for Automatic Evaluation of Machine Translation by Papineni,
-        Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu `BLEU`_
-
-        [2] Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence
-        and Skip-Bigram Statistics by Chin-Yew Lin and Franz Josef Och `Machine Translation Evolution`_
     """
 
     is_differentiable: bool = False
     higher_is_better: bool = True
     full_state_update: bool = True
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
 
     preds_len: Tensor
     target_len: Tensor
     numerator: Tensor
     denominator: Tensor
 
     def __init__(
         self,
         n_gram: int = 4,
         smooth: bool = False,
         weights: Optional[Sequence[float]] = None,
         **kwargs: Any,
-    ):
+    ) -> None:
         super().__init__(**kwargs)
         self.n_gram = n_gram
         self.smooth = smooth
         if weights is not None and len(weights) != n_gram:
             raise ValueError(f"List of weights has different weights than `n_gram`: {len(weights)} != {n_gram}")
         self.weights = weights if weights is not None else [1.0 / n_gram] * n_gram
 
         self.add_state("preds_len", tensor(0.0), dist_reduce_fx="sum")
         self.add_state("target_len", tensor(0.0), dist_reduce_fx="sum")
         self.add_state("numerator", torch.zeros(self.n_gram), dist_reduce_fx="sum")
         self.add_state("denominator", torch.zeros(self.n_gram), dist_reduce_fx="sum")
 
-    def update(self, preds: Sequence[str], target: Sequence[Sequence[str]]) -> None:  # type: ignore
-        """Compute Precision Scores.
-
-        Args:
-            preds: An iterable of machine translated corpus
-            target: An iterable of iterables of reference corpus
-        """
+    def update(self, preds: Sequence[str], target: Sequence[Sequence[str]]) -> None:
+        """Update state with predictions and targets."""
         self.preds_len, self.target_len = _bleu_score_update(
             preds,
             target,
             self.numerator,
             self.denominator,
             self.preds_len,
             self.target_len,
             self.n_gram,
             _tokenize_fn,
         )
 
     def compute(self) -> Tensor:
-        """Calculate BLEU score.
-
-        Return:
-            Tensor with BLEU Score
-        """
+        """Calculate BLEU score."""
         return _bleu_score_compute(
             self.preds_len, self.target_len, self.numerator, self.denominator, self.n_gram, self.weights, self.smooth
         )
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> from torchmetrics.text import BLEUScore
+            >>> metric = BLEUScore()
+            >>> preds = ['the cat is on the mat']
+            >>> target = [['there is a cat on the mat', 'a cat is on the mat']]
+            >>> metric.update(preds, target)
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> from torchmetrics.text import BLEUScore
+            >>> metric = BLEUScore()
+            >>> preds = ['the cat is on the mat']
+            >>> target = [['there is a cat on the mat', 'a cat is on the mat']]
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(preds, target))
+            >>> fig_, ax_ = metric.plot(values)
+        """
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/text/cer.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/functional/detection/ciou.py`

 * *Files 27% similar despite different names*

```diff
@@ -7,84 +7,76 @@
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-
-from typing import Any, List, Union
+from typing import Optional
 
 import torch
-from torch import Tensor, tensor
-
-from torchmetrics.functional.text.cer import _cer_compute, _cer_update
-from torchmetrics.metric import Metric
-
-
-class CharErrorRate(Metric):
-    r"""Character Error Rate (CER_) is a metric of the performance of an automatic speech recognition
-    (ASR) system.
 
-    This value indicates the percentage of characters that were incorrectly predicted.
-    The lower the value, the better the performance of the ASR system with a CharErrorRate of 0 being
-    a perfect score.
-    Character error rate can then be computed as:
+from torchmetrics.utilities.imports import _TORCHVISION_AVAILABLE, _TORCHVISION_GREATER_EQUAL_0_13
 
-    .. math::
-        CharErrorRate = \frac{S + D + I}{N} = \frac{S + D + I}{S + D + C}
+if _TORCHVISION_AVAILABLE and _TORCHVISION_GREATER_EQUAL_0_13:
+    from torchvision.ops import complete_box_iou
+else:
+    complete_box_iou = None
+    __doctest_skip__ = ["complete_intersection_over_union"]
+
+__doctest_requires__ = {("complete_intersection_over_union",): ["torchvision"]}
+
+
+def _ciou_update(
+    preds: torch.Tensor, target: torch.Tensor, iou_threshold: Optional[float], replacement_val: float = 0
+) -> torch.Tensor:
+    iou = complete_box_iou(preds, target)
+    if iou_threshold is not None:
+        iou[iou < iou_threshold] = replacement_val
+    return iou
+
+
+def _ciou_compute(iou: torch.Tensor, labels_eq: bool = True) -> torch.Tensor:
+    if labels_eq:
+        return iou.diag().mean()
+    return iou.mean()
+
+
+def complete_intersection_over_union(
+    preds: torch.Tensor,
+    target: torch.Tensor,
+    iou_threshold: Optional[float] = None,
+    replacement_val: float = 0,
+    aggregate: bool = True,
+) -> torch.Tensor:
+    r"""Compute `Complete Intersection over Union <https://arxiv.org/abs/2005.03572>`_ between two sets of boxes.
 
-    where:
-        - :math:`S` is the number of substitutions,
-        - :math:`D` is the number of deletions,
-        - :math:`I` is the number of insertions,
-        - :math:`C` is the number of correct characters,
-        - :math:`N` is the number of characters in the reference (N=S+D+C).
-
-    Compute CharErrorRate score of transcribed segments against references.
+    Both sets of boxes are expected to be in (x1, y1, x2, y2) format with 0 <= x1 < x2 and 0 <= y1 < y2.
 
     Args:
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
-
-    Returns:
-        Character error rate score
-
-    Examples:
-        >>> preds = ["this is the prediction", "there is an other sample"]
-        >>> target = ["this is the reference", "there is another one"]
-        >>> metric = CharErrorRate()
-        >>> metric(preds, target)
-        tensor(0.3415)
+        preds:
+            The input tensor containing the predicted bounding boxes.
+        target:
+            The tensor containing the ground truth.
+        iou_threshold:
+            Optional IoU thresholds for evaluation. If set to `None` the threshold is ignored.
+        replacement_val:
+            Value to replace values under the threshold with.
+        aggregate:
+            Return the average value instead of the complete IoU matrix.
+
+    Example:
+        >>> import torch
+        >>> from torchmetrics.functional.detection import complete_intersection_over_union
+        >>> preds = torch.Tensor([[100, 100, 200, 200]])
+        >>> target = torch.Tensor([[110, 110, 210, 210]])
+        >>> complete_intersection_over_union(preds, target)
+        tensor(0.6724)
     """
-    is_differentiable: bool = False
-    higher_is_better: bool = False
-    full_state_update: bool = False
-
-    error: Tensor
-    total: Tensor
-
-    def __init__(
-        self,
-        **kwargs: Any,
-    ):
-        super().__init__(**kwargs)
-        self.add_state("errors", tensor(0, dtype=torch.float), dist_reduce_fx="sum")
-        self.add_state("total", tensor(0, dtype=torch.float), dist_reduce_fx="sum")
-
-    def update(self, preds: Union[str, List[str]], target: Union[str, List[str]]) -> None:  # type: ignore
-        """Store references/predictions for computing Character Error Rate scores.
-
-        Args:
-            preds: Transcription(s) to score as a string or list of strings
-            target: Reference(s) for each speech input as a string or list of strings
-        """
-        errors, total = _cer_update(preds, target)
-        self.errors += errors
-        self.total += total
-
-    def compute(self) -> Tensor:
-        """Calculate the character error rate.
-
-        Returns:
-           Character error rate score
-        """
-        return _cer_compute(self.errors, self.total)
+    if not _TORCHVISION_GREATER_EQUAL_0_13:
+        raise ModuleNotFoundError(
+            f"`{complete_intersection_over_union.__name__}` requires that `torchvision` version 0.13.0 or newer"
+            " is installed."
+            " Please install with `pip install torchvision>=0.13` or `pip install torchmetrics[detection]`."
+        )
+    iou = _ciou_update(preds, target, iou_threshold, replacement_val)
+    return _ciou_compute(iou) if aggregate else iou
```

### Comparing `torchmetrics-0.9.3/torchmetrics/text/chrf.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/text/chrf.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -21,14 +21,20 @@
 from typing import Any, Dict, Iterator, List, Optional, Sequence, Tuple, Union
 
 import torch
 from torch import Tensor, tensor
 
 from torchmetrics import Metric
 from torchmetrics.functional.text.chrf import _chrf_score_compute, _chrf_score_update, _prepare_n_grams_dicts
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
+
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["CHRFScore.plot"]
+
 
 _N_GRAM_LEVELS = ("char", "word")
 _TEXT_LEVELS = ("preds", "target", "matching")
 
 _DICT_STATES_NAMES = (
     "total_preds_char_n_grams",
     "total_preds_word_n_grams",
@@ -42,18 +48,28 @@
     Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor], Dict[int, Tensor]
 ]
 
 
 class CHRFScore(Metric):
     """Calculate `chrf score`_ of machine translated text with one or more references.
 
-    This implementation supports both ChrF score computation introduced in [1] and chrF++ score introduced
-    in `chrF++ score_`. This implementation follows the implmenetaions from https://github.com/m-popovic/chrF and
+    This implementation supports both ChrF score computation introduced in `chrF score`_ and `chrF++ score`_ introduced
+    in `chrF++ score`_. This implementation follows the implmenetaions from https://github.com/m-popovic/chrF and
     https://github.com/mjpost/sacrebleu/blob/master/sacrebleu/metrics/chrf.py.
 
+    As input to ``forward`` and ``update`` the metric accepts the following input:
+
+    - ``preds`` (:class:`~Sequence`): An iterable of hypothesis corpus
+    - ``target`` (:class:`~Sequence`): An iterable of iterables of reference corpus
+
+    As output of ``forward`` and ``compute`` the metric returns the following output:
+
+    - ``chrf`` (:class:`~torch.Tensor`): If `return_sentence_level_score=True` return a list of sentence-level
+      chrF/chrF++ scores, else return a corpus-level chrF/chrF++ score
+
     Args:
         n_char_order: A character n-gram order. If ``n_char_order=6``, the metrics refers to the official chrF/chrF++.
         n_word_order: A word n-gram order. If ``n_word_order=2``, the metric refers to the official chrF++.
             If ``n_word_order=0``, the metric is equivalent to the original ChrF.
         beta: parameter determining an importance of recall w.r.t. precision. If ``beta=1``, their importance is equal.
         lowercase: An indication whether to enable case-insesitivity.
         whitespace: An indication whether keep whitespaces during n-gram extraction.
@@ -65,43 +81,40 @@
             If ``n_char_order`` is not an integer greater than or equal to 1.
         ValueError:
             If ``n_word_order`` is not an integer greater than or equal to 0.
         ValueError:
             If ``beta`` is smaller than 0.
 
     Example:
-        >>> from torchmetrics import CHRFScore
+        >>> from torchmetrics.text import CHRFScore
         >>> preds = ['the cat is on the mat']
         >>> target = [['there is a cat on the mat', 'a cat is on the mat']]
-        >>> metric = CHRFScore()
-        >>> metric(preds, target)
+        >>> chrf = CHRFScore()
+        >>> chrf(preds, target)
         tensor(0.8640)
-
-    References:
-        [1] chrF: character n-gram F-score for automatic MT evaluation by Maja Popovi `chrF score`_
-
-        [2] chrF++: words helping character n-grams by Maja Popovi `chrF++ score`_
     """
 
     is_differentiable: bool = False
     higher_is_better: bool = True
     full_state_update: bool = True
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
 
     sentence_chrf_score: Optional[List[Tensor]] = None
 
     def __init__(
         self,
         n_char_order: int = 6,
         n_word_order: int = 2,
         beta: float = 2.0,
         lowercase: bool = False,
         whitespace: bool = False,
         return_sentence_level_score: bool = False,
         **kwargs: Any,
-    ):
+    ) -> None:
         super().__init__(**kwargs)
 
         if not isinstance(n_char_order, int) or n_char_order < 1:
             raise ValueError("Expected argument `n_char_order` to be an integer greater than or equal to 1.")
         self.n_char_order = n_char_order
         if not isinstance(n_word_order, int) or n_word_order < 0:
             raise ValueError("Expected argument `n_word_order` to be an integer greater than or equal to 0.")
@@ -120,21 +133,16 @@
             for n in range(1, n_gram_order + 1):
                 state_name = self._get_state_name(text, n_gram_level, n)
                 self.add_state(state_name, tensor(0.0), dist_reduce_fx="sum")
 
         if self.return_sentence_level_score:
             self.add_state("sentence_chrf_score", [], dist_reduce_fx="cat")
 
-    def update(self, preds: Sequence[str], target: Sequence[Sequence[str]]) -> None:  # type: ignore
-        """Compute Precision Scores.
-
-        Args:
-            preds: An iterable of hypothesis corpus.
-            target: An iterable of iterables of reference corpus.
-        """
+    def update(self, preds: Sequence[str], target: Sequence[Sequence[str]]) -> None:
+        """Update state with predictions and targets."""
         n_grams_dicts_tuple = _chrf_score_update(
             preds,
             target,
             *self._convert_states_to_dicts(),
             self.n_char_order,
             self.n_word_order,
             self.n_order,
@@ -144,48 +152,40 @@
             self.sentence_chrf_score if self.return_sentence_level_score else None,
         )
         self._update_states_from_dicts(n_grams_dicts_tuple[:-1])
         if self.sentence_chrf_score is not None:
             self.sentence_chrf_score = n_grams_dicts_tuple[-1]
 
     def compute(self) -> Union[Tensor, Tuple[Tensor, Tensor]]:
-        """Calculate chrF/chrF++ score.
-
-        Return:
-            A corpus-level chrF/chrF++ score.
-            (Optionally) A list of sentence-level chrF/chrF++ scores if `return_sentence_level_score=True`.
-        """
+        """Calculate chrF/chrF++ score."""
         if self.sentence_chrf_score is not None:
             return (
                 _chrf_score_compute(*self._convert_states_to_dicts(), self.n_order, self.beta),
                 torch.cat(self.sentence_chrf_score),
             )
         return _chrf_score_compute(*self._convert_states_to_dicts(), self.n_order, self.beta)
 
     def _convert_states_to_dicts(self) -> _DICT_STATES_TYPES:
-        """Convert global metric states to the n-gram dictionaries to be passed in `_chrf_score_update`."""
-        n_grams_dicts: Dict[str, Dict[int, Tensor]] = {
-            name: n_gram_dict
-            for name, n_gram_dict in zip(
-                _DICT_STATES_NAMES, _prepare_n_grams_dicts(self.n_char_order, self.n_word_order)
-            )
-        }
+        """Convert global metric states to the n-gram dictionaries to be passed in ``_chrf_score_update``."""
+        n_grams_dicts: Dict[str, Dict[int, Tensor]] = dict(
+            zip(_DICT_STATES_NAMES, _prepare_n_grams_dicts(self.n_char_order, self.n_word_order))
+        )
 
         for (n_gram_level, n_gram_order), text in self._get_text_n_gram_iterator():
             for n in range(1, n_gram_order + 1):
                 dict_name = self._get_dict_name(text, n_gram_level)
                 state_name = self._get_state_name(text, n_gram_level, n)
 
                 n_grams_dicts[dict_name][n] = getattr(self, state_name)
 
         return tuple(n_grams_dicts.values())  # type: ignore
 
     def _update_states_from_dicts(self, n_grams_dicts_tuple: _DICT_STATES_TYPES) -> None:
         """Update global metric states based on the n-gram dictionaries calculated on the current batch."""
-        n_grams_dicts = {name: n_gram_dict for name, n_gram_dict, in zip(_DICT_STATES_NAMES, n_grams_dicts_tuple)}
+        n_grams_dicts = dict(zip(_DICT_STATES_NAMES, n_grams_dicts_tuple))
         for (n_gram_level, n_gram_order), text in self._get_text_n_gram_iterator():
             for n in range(1, n_gram_order + 1):
                 dict_name = self._get_dict_name(text, n_gram_level)
                 state_name = self._get_state_name(text, n_gram_level, n)
 
                 setattr(self, state_name, n_grams_dicts[dict_name][n])
 
@@ -198,7 +198,50 @@
     def _get_state_name(text: str, n_gram_level: str, n: int) -> str:
         """Return a metric state name w.r.t input args."""
         return f"total_{text}_{n_gram_level}_{n}_grams"
 
     def _get_text_n_gram_iterator(self) -> Iterator[Tuple[Tuple[str, int], str]]:
         """Get iterator over char/word and reference/hypothesis/matching n-gram level."""
         return itertools.product(zip(_N_GRAM_LEVELS, [self.n_char_order, self.n_word_order]), _TEXT_LEVELS)
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> from torchmetrics.text import CHRFScore
+            >>> metric = CHRFScore()
+            >>> preds = ['the cat is on the mat']
+            >>> target = [['there is a cat on the mat', 'a cat is on the mat']]
+            >>> metric.update(preds, target)
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> from torchmetrics.text import CHRFScore
+            >>> metric = CHRFScore()
+            >>> preds = ['the cat is on the mat']
+            >>> target = [['there is a cat on the mat', 'a cat is on the mat']]
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(preds, target))
+            >>> fig_, ax_ = metric.plot(values)
+        """
+        return self._plot(val, ax)
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `torchmetrics-0.9.3/torchmetrics/text/rouge.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/text/rouge.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,50 +1,66 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, Callable, Dict, List, Sequence, Tuple, Union
+from typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union
 
 from torch import Tensor
 from typing_extensions import Literal
 
 from torchmetrics import Metric
 from torchmetrics.functional.text.rouge import (
     ALLOWED_ACCUMULATE_VALUES,
     ALLOWED_ROUGE_KEYS,
     _rouge_score_compute,
     _rouge_score_update,
 )
-from torchmetrics.utilities.imports import _NLTK_AVAILABLE
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE, _NLTK_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
+
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["ROUGEScore.plot"]
+
 
 __doctest_requires__ = {("ROUGEScore",): ["nltk"]}
 
 
 class ROUGEScore(Metric):
     """`Calculate Rouge Score`_, used for automatic summarization.
 
-    This implementation should imitate the behaviour of the `rouge-score` package `Python ROUGE Implementation`
+    This implementation should imitate the behaviour of the ``rouge-score`` package `Python ROUGE Implementation`
+
+    As input to ``forward`` and ``update`` the metric accepts the following input:
+
+    - ``preds`` (:class:`~Sequence`): An iterable of predicted sentences or a single predicted sentence
+    - ``target`` (:class:`~Sequence`): An iterable of target sentences
+      or an iterable of interables of target sentences
+      or a single target sentence
+
+    As output of ``forward`` and ``compute`` the metric returns the following output:
+
+    - ``rouge`` (:class:`~Dict`): A dictionary of tensor rouge scores for each input str rouge key
 
     Args:
         use_stemmer: Use Porter stemmer to strip word suffixes to improve matching.
         normalizer: A user's own normalizer function.
             If this is ``None``, replacing any non-alpha-numeric characters with spaces is default.
             This function must take a ``str`` and return a ``str``.
         tokenizer:
             A user's own tokenizer function. If this is ``None``, spliting by spaces is default
-            This function must take a `str` and return ``Sequence[str]``
+            This function must take a ``str`` and return ``Sequence[str]``
         accumulate:
             Useful in case of multi-reference rouge score.
 
             - ``avg`` takes the avg of all references with respect to predictions
             - ``best`` takes the best fmeasure score obtained between prediction and multiple corresponding references.
 
         rouge_keys: A list of rouge types to calculate.
@@ -73,32 +89,31 @@
 
 
     Raises:
         ValueError:
             If the python packages ``nltk`` is not installed.
         ValueError:
             If any of the ``rouge_keys`` does not belong to the allowed set of keys.
-
-    References:
-        [1] ROUGE: A Package for Automatic Evaluation of Summaries by Chin-Yew Lin `Rouge Detail`_
     """
 
     is_differentiable: bool = False
     higher_is_better: bool = True
     full_state_update: bool = True
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
 
     def __init__(
         self,
         use_stemmer: bool = False,
-        normalizer: Callable[[str], str] = None,
-        tokenizer: Callable[[str], Sequence[str]] = None,
+        normalizer: Optional[Callable[[str], str]] = None,
+        tokenizer: Optional[Callable[[str], Sequence[str]]] = None,
         accumulate: Literal["avg", "best"] = "best",
-        rouge_keys: Union[str, Tuple[str, ...]] = ("rouge1", "rouge2", "rougeL", "rougeLsum"),  # type: ignore
+        rouge_keys: Union[str, Tuple[str, ...]] = ("rouge1", "rouge2", "rougeL", "rougeLsum"),
         **kwargs: Any,
-    ):
+    ) -> None:
         super().__init__(**kwargs)
         if use_stemmer or "rougeLsum" in rouge_keys:
             if not _NLTK_AVAILABLE:
                 raise ModuleNotFoundError(
                     "Stemmer and/or `rougeLsum` requires that `nltk` is installed. Use `pip install nltk`."
                 )
             import nltk
@@ -122,23 +137,18 @@
         self.accumulate = accumulate
 
         # Adding stated dynamically to prevent IndexError during sync function as some lists can be empty.
         for rouge_key in self.rouge_keys:
             for score in ["fmeasure", "precision", "recall"]:
                 self.add_state(f"{rouge_key}_{score}", [], dist_reduce_fx=None)
 
-    def update(  # type: ignore
+    def update(
         self, preds: Union[str, Sequence[str]], target: Union[str, Sequence[str], Sequence[Sequence[str]]]
     ) -> None:
-        """Compute rouge scores.
-
-        Args:
-            preds: An iterable of predicted sentences or a single predicted sentence.
-            target: An iterable of target sentences or an iterable of target sentences or a single target sentence.
-        """
+        """Update state with predictions and targets."""
         if isinstance(target, list) and all(isinstance(tgt, str) for tgt in target):
             target = [target] if isinstance(preds, str) else [[tgt] for tgt in target]
 
         if isinstance(preds, str):
             preds = [preds]
 
         if isinstance(target, str):
@@ -155,30 +165,70 @@
         )
         for rouge_key, metrics in output.items():
             for metric in metrics:
                 for tp, value in metric.items():
                     getattr(self, f"rouge{rouge_key}_{tp}").append(value.to(self.device))
 
     def compute(self) -> Dict[str, Tensor]:
-        """Calculate (Aggregate and provide confidence intervals) ROUGE score.
-
-        Return:
-            Python dictionary of rouge scores for each input rouge key.
-        """
+        """Calculate (Aggregate and provide confidence intervals) ROUGE score."""
         update_output = {}
         for rouge_key in self.rouge_keys_values:
             for tp in ["fmeasure", "precision", "recall"]:
                 update_output[f"rouge{rouge_key}_{tp}"] = getattr(self, f"rouge{rouge_key}_{tp}")
 
         return _rouge_score_compute(update_output)
 
     def __hash__(self) -> int:
+        """Return a unique hash for the specific instance of this metric."""
         # override to hash list objects.
         # this is a bug in the upstream pytorch release.
         hash_vals = [self.__class__.__name__]
         for key in self._defaults:
             value = getattr(self, key)
             if isinstance(value, list):
                 value = tuple(value)
             hash_vals.append(value)
 
         return hash(tuple(hash_vals))
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> from torchmetrics.text.rouge import ROUGEScore
+            >>> metric = ROUGEScore()
+            >>> preds = "My name is John"
+            >>> target = "Is your name John"
+            >>> metric.update(preds, target)
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> from torchmetrics.text.rouge import ROUGEScore
+            >>> metric = ROUGEScore()
+            >>> preds = "My name is John"
+            >>> target = "Is your name John"
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(preds, target))
+            >>> fig_, ax_ = metric.plot(values)
+        """
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/text/sacre_bleu.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/regression/cosine_similarity.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,116 +1,135 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+from typing import Any, List, Optional, Sequence, Union
 
-# referenced from
-# Library Name: torchtext
-# Authors: torchtext authors and @sluks
-# Date: 2020-07-18
-# Link: https://pytorch.org/text/_modules/torchtext/data/metrics.html#bleu_score
-from typing import Any, Optional, Sequence
-
+from torch import Tensor
 from typing_extensions import Literal
 
-from torchmetrics.functional.text.bleu import _bleu_score_update
-from torchmetrics.functional.text.sacre_bleu import _SacreBLEUTokenizer
-from torchmetrics.text.bleu import BLEUScore
-from torchmetrics.utilities.imports import _REGEX_AVAILABLE
+from torchmetrics.functional.regression.cosine_similarity import _cosine_similarity_compute, _cosine_similarity_update
+from torchmetrics.metric import Metric
+from torchmetrics.utilities.data import dim_zero_cat
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
 
-AVAILABLE_TOKENIZERS = ("none", "13a", "zh", "intl", "char")
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["CosineSimilarity.plot"]
 
 
-class SacreBLEUScore(BLEUScore):
-    """Calculate `BLEU score`_ [1] of machine translated text with one or more references. This implementation
-    follows the behaviour of SacreBLEU [2] implementation from https://github.com/mjpost/sacrebleu.
+class CosineSimilarity(Metric):
+    r"""Compute the `Cosine Similarity`_.
 
-    The SacreBLEU implementation differs from the NLTK BLEU implementation in tokenization techniques.
+    .. math::
+        cos_{sim}(x,y) = \frac{x \cdot y}{||x|| \cdot ||y||} =
+        \frac{\sum_{i=1}^n x_i y_i}{\sqrt{\sum_{i=1}^n x_i^2}\sqrt{\sum_{i=1}^n y_i^2}}
 
-    Args:
-        n_gram: Gram value ranged from 1 to 4
-        smooth: Whether to apply smoothing, see [2]
-        tokenize: Tokenization technique to be used.
-            Supported tokenization: ``['none', '13a', 'zh', 'intl', 'char']``
-        lowercase:  If ``True``, BLEU score over lowercased text is calculated.
-        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
-        weights:
-            Weights used for unigrams, bigrams, etc. to calculate BLEU score.
-            If not provided, uniform weights are used.
-
-     Raises:
-        ValueError:
-            If ``tokenize`` not one of 'none', '13a', 'zh', 'intl' or 'char'
-        ValueError:
-            If ``tokenize`` is set to 'intl' and `regex` is not installed
-        ValueError:
-            If a length of a list of weights is not ``None`` and not equal to ``n_gram``.
+    where :math:`y` is a tensor of target values, and :math:`x` is a tensor of predictions.
 
+    As input to ``forward`` and ``update`` the metric accepts the following input:
 
-    Example:
-        >>> from torchmetrics import SacreBLEUScore
-        >>> preds = ['the cat is on the mat']
-        >>> target = [['there is a cat on the mat', 'a cat is on the mat']]
-        >>> metric = SacreBLEUScore()
-        >>> metric(preds, target)
-        tensor(0.7598)
-
-    References:
-        [1] BLEU: a Method for Automatic Evaluation of Machine Translation by Papineni,
-        Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu `BLEU`_
+    - ``preds`` (:class:`~torch.Tensor`): Predicted float tensor with shape ``(N,d)``
+    - ``target`` (:class:`~torch.Tensor`): Ground truth float tensor with shape ``(N,d)``
 
-        [2] A Call for Clarity in Reporting BLEU Scores by Matt Post.
+    As output of ``forward`` and ``compute`` the metric returns the following output:
 
-        [3] Automatic Evaluation of Machine Translation Quality Using Longest Common Subsequence
-        and Skip-Bigram Statistics by Chin-Yew Lin and Franz Josef Och `Machine Translation Evolution`_
-    """
+    - ``cosine_similarity`` (:class:`~torch.Tensor`): A float tensor with the cosine similarity
+
+    Args:
+        reduction: how to reduce over the batch dimension using 'sum', 'mean' or 'none' (taking the individual scores)
+        kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
-    is_differentiable: bool = False
+    Example:
+        >>> from torch import tensor
+        >>> from torchmetrics.regression import CosineSimilarity
+        >>> target = tensor([[0, 1], [1, 1]])
+        >>> preds = tensor([[0, 1], [0, 1]])
+        >>> cosine_similarity = CosineSimilarity(reduction = 'mean')
+        >>> cosine_similarity(preds, target)
+        tensor(0.8536)
+    """
+    is_differentiable: bool = True
     higher_is_better: bool = True
-    full_state_update: bool = True
+    full_state_update: bool = False
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 1.0
+
+    preds: List[Tensor]
+    target: List[Tensor]
 
     def __init__(
         self,
-        n_gram: int = 4,
-        smooth: bool = False,
-        tokenize: Literal["none", "13a", "zh", "intl", "char"] = "13a",
-        lowercase: bool = False,
-        weights: Optional[Sequence[float]] = None,
+        reduction: Literal["mean", "sum", "none", None] = "sum",
         **kwargs: Any,
-    ):
-        super().__init__(n_gram=n_gram, smooth=smooth, weights=weights, **kwargs)
-        if tokenize not in AVAILABLE_TOKENIZERS:
-            raise ValueError(f"Argument `tokenize` expected to be one of {AVAILABLE_TOKENIZERS} but got {tokenize}.")
-
-        if tokenize == "intl" and not _REGEX_AVAILABLE:
-            raise ModuleNotFoundError(
-                "`'intl'` tokenization requires that `regex` is installed."
-                " Use `pip install regex` or `pip install torchmetrics[text]`."
-            )
-        self.tokenizer = _SacreBLEUTokenizer(tokenize, lowercase)
-
-    def update(self, preds: Sequence[str], target: Sequence[Sequence[str]]) -> None:  # type: ignore
-        """Compute Precision Scores.
+    ) -> None:
+        super().__init__(**kwargs)
+        allowed_reduction = ("sum", "mean", "none", None)
+        if reduction not in allowed_reduction:
+            raise ValueError(f"Expected argument `reduction` to be one of {allowed_reduction} but got {reduction}")
+        self.reduction = reduction
+
+        self.add_state("preds", [], dist_reduce_fx="cat")
+        self.add_state("target", [], dist_reduce_fx="cat")
+
+    def update(self, preds: Tensor, target: Tensor) -> None:
+        """Update metric states with predictions and targets."""
+        preds, target = _cosine_similarity_update(preds, target)
+
+        self.preds.append(preds)
+        self.target.append(target)
+
+    def compute(self) -> Tensor:
+        """Compute metric."""
+        preds = dim_zero_cat(self.preds)
+        target = dim_zero_cat(self.target)
+        return _cosine_similarity_compute(preds, target, self.reduction)
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
 
         Args:
-            preds: An iterable of machine translated corpus
-            target: An iterable of iterables of reference corpus
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import randn
+            >>> # Example plotting a single value
+            >>> from torchmetrics.regression import CosineSimilarity
+            >>> metric = CosineSimilarity()
+            >>> metric.update(randn(10,), randn(10,))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import randn
+            >>> # Example plotting multiple values
+            >>> from torchmetrics.regression import CosineSimilarity
+            >>> metric = CosineSimilarity()
+            >>> values = []
+            >>> for _ in range(10):
+            ...     values.append(metric(randn(10,), randn(10,)))
+            >>> fig, ax = metric.plot(values)
         """
-        self.preds_len, self.target_len = _bleu_score_update(
-            preds,
-            target,
-            self.numerator,
-            self.denominator,
-            self.preds_len,
-            self.target_len,
-            self.n_gram,
-            self.tokenizer,
-        )
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/text/squad.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/text/squad.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,118 +1,165 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from typing import Any, Dict
+from typing import Any, Dict, Optional, Sequence, Union
 
 import torch
 from torch import Tensor
 
 from torchmetrics import Metric
 from torchmetrics.functional.text.squad import (
     PREDS_TYPE,
     TARGETS_TYPE,
     _squad_compute,
     _squad_input_check,
     _squad_update,
 )
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
+
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["SQuAD.plot"]
 
 
 class SQuAD(Metric):
-    """Calculate `SQuAD Metric`_ which corresponds to the scoring script for version 1 of the Stanford Question
-    Answering Dataset (SQuAD).
+    """Calculate `SQuAD Metric`_ which is a metric for evaluating question answering models.
+
+    This metric corresponds to the scoring script for version 1 of the Stanford Question Answering Dataset (SQuAD).
+
+    As input to ``forward`` and ``update`` the metric accepts the following input:
+
+    -  ``preds`` (:class:`~Dict`): A Dictionary or List of Dictionary-s that map ``id`` and ``prediction_text`` to
+       the respective values
+
+       Example ``prediction``:
+
+                .. code-block:: python
+
+                    {"prediction_text": "TorchMetrics is awesome", "id": "123"}
+
+
+    - ``target`` (:class:`~Dict`): A Dictionary or List of Dictionary-s that contain the ``answers`` and ``id`` in
+      the SQuAD Format.
+
+        Example ``target``:
+
+        .. code-block:: python
+
+            {
+                'answers': [{'answer_start': [1], 'text': ['This is a test answer']}],
+                'id': '1',
+            }
+
+        Reference SQuAD Format:
+
+        .. code-block:: python
+
+            {
+                'answers': {'answer_start': [1], 'text': ['This is a test text']},
+                'context': 'This is a test context.',
+                'id': '1',
+                'question': 'Is this a test?',
+                'title': 'train test'
+            }
+
+    As output of ``forward`` and ``compute`` the metric returns the following output:
+
+    -  ``squad`` (:class:`~Dict`): A dictionary containing the F1 score (key: "f1"),
+        and Exact match score (key: "exact_match") for the batch.
 
     Args:
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
     Example:
-        >>> from torchmetrics import SQuAD
+        >>> from torchmetrics.text import SQuAD
         >>> preds = [{"prediction_text": "1976", "id": "56e10a3be3433e1400422b22"}]
         >>> target = [{"answers": {"answer_start": [97], "text": ["1976"]}, "id": "56e10a3be3433e1400422b22"}]
         >>> squad = SQuAD()
         >>> squad(preds, target)
         {'exact_match': tensor(100.), 'f1': tensor(100.)}
-
-    References:
-        [1] SQuAD: 100,000+ Questions for Machine Comprehension of Text by Pranav Rajpurkar, Jian Zhang, Konstantin
-        Lopyrev, Percy Liang `SQuAD Metric`_ .
     """
 
     is_differentiable: bool = False
     higher_is_better: bool = True
     full_state_update: bool = False
+    plot_lower_bound: float = 0.0
+    plot_upper_bound: float = 100.0
 
     f1_score: Tensor
     exact_match: Tensor
     total: Tensor
 
     def __init__(
         self,
         **kwargs: Any,
-    ):
+    ) -> None:
         super().__init__(**kwargs)
 
         self.add_state(name="f1_score", default=torch.tensor(0, dtype=torch.float), dist_reduce_fx="sum")
         self.add_state(name="exact_match", default=torch.tensor(0, dtype=torch.float), dist_reduce_fx="sum")
         self.add_state(name="total", default=torch.tensor(0, dtype=torch.int), dist_reduce_fx="sum")
 
-    def update(self, preds: PREDS_TYPE, target: TARGETS_TYPE) -> None:  # type: ignore
-        """Compute F1 Score and Exact Match for a collection of predictions and references.
-
-        Args:
-            preds:
-                A Dictionary or List of Dictionary-s that map ``id`` and ``prediction_text`` to the respective values.
-                Example prediction:
-
-                .. code-block:: python
-
-                    {"prediction_text": "TorchMetrics is awesome", "id": "123"}
-
-            target:
-                A Dictionary or List of Dictionary-s that contain the ``answers`` and ``id`` in the SQuAD Format.
-                Example target:
-
-                .. code-block:: python
-
-                    {
-                        'answers': [{'answer_start': [1], 'text': ['This is a test answer']}],
-                        'id': '1',
-                    }
-
-                Reference SQuAD Format:
-
-                .. code-block:: python
-
-                    {
-                        'answers': {'answer_start': [1], 'text': ['This is a test text']},
-                        'context': 'This is a test context.',
-                        'id': '1',
-                        'question': 'Is this a test?',
-                        'title': 'train test'
-                    }
-
-        Raises:
-            KeyError:
-                If the required keys are missing in either predictions or targets.
-        """
+    def update(self, preds: PREDS_TYPE, target: TARGETS_TYPE) -> None:
+        """Update state with predictions and targets."""
         preds_dict, target_dict = _squad_input_check(preds, target)
         f1_score, exact_match, total = _squad_update(preds_dict, target_dict)
         self.f1_score += f1_score
         self.exact_match += exact_match
         self.total += total
 
     def compute(self) -> Dict[str, Tensor]:
-        """Aggregate the F1 Score and Exact match for the batch.
+        """Aggregate the F1 Score and Exact match for the batch."""
+        return _squad_compute(self.f1_score, self.exact_match, self.total)
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
 
-        Return:
-            Dictionary containing the F1 score, Exact match score for the batch.
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> from torchmetrics.text import SQuAD
+            >>> metric = SQuAD()
+            >>> preds = [{"prediction_text": "1976", "id": "56e10a3be3433e1400422b22"}]
+            >>> target = [{"answers": {"answer_start": [97], "text": ["1976"]}, "id": "56e10a3be3433e1400422b22"}]
+            >>> metric.update(preds, target)
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> from torchmetrics.text import SQuAD
+            >>> metric = SQuAD()
+            >>> preds = [{"prediction_text": "1976", "id": "56e10a3be3433e1400422b22"}]
+            >>> target = [{"answers": {"answer_start": [97], "text": ["1976"]}, "id": "56e10a3be3433e1400422b22"}]
+            >>> values = [ ]
+            >>> for _ in range(10):
+            ...     values.append(metric(preds, target))
+            >>> fig_, ax_ = metric.plot(values)
         """
-        return _squad_compute(self.f1_score, self.exact_match, self.total)
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/utilities/checks.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/utilities/checks.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,58 +1,65 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+import logging
+import multiprocessing
+import os
 from functools import partial
 from time import perf_counter
-from typing import Any, Dict, Mapping, Optional, Sequence, Tuple, no_type_check
+from typing import Any, Callable, Dict, Mapping, Optional, Sequence, Tuple, no_type_check
 from unittest.mock import Mock
 
 import torch
 from torch import Tensor
 
+from torchmetrics.metric import Metric
 from torchmetrics.utilities.data import select_topk, to_onehot
 from torchmetrics.utilities.enums import DataType
 
+_DOCTEST_DOWNLOAD_TIMEOUT = int(os.environ.get("DOCTEST_DOWNLOAD_TIMEOUT", 120))
+_SKIP_SLOW_DOCTEST = bool(os.environ.get("SKIP_SLOW_DOCTEST", 0))
+
 
 def _check_for_empty_tensors(preds: Tensor, target: Tensor) -> bool:
     if preds.numel() == target.numel() == 0:
         return True
     return False
 
 
 def _check_same_shape(preds: Tensor, target: Tensor) -> None:
     """Check that predictions and target have the same shape, else raise error."""
     if preds.shape != target.shape:
-        raise RuntimeError("Predictions and targets are expected to have the same shape")
+        raise RuntimeError(
+            f"Predictions and targets are expected to have the same shape, but got {preds.shape} and {target.shape}."
+        )
 
 
 def _basic_input_validation(
     preds: Tensor, target: Tensor, threshold: float, multiclass: Optional[bool], ignore_index: Optional[int]
 ) -> None:
     """Perform basic validation of inputs that does not require deducing any information of the type of inputs."""
     # Skip all other checks if both preds and target are empty tensors
     if _check_for_empty_tensors(preds, target):
         return
 
     if target.is_floating_point():
         raise ValueError("The `target` has to be an integer tensor.")
 
-    if ignore_index is None and target.min() < 0:
-        raise ValueError("The `target` has to be a non-negative tensor.")
-    elif ignore_index is not None and ignore_index >= 0 and target.min() < 0:
+    if (ignore_index is None and target.min() < 0) or (ignore_index and ignore_index >= 0 and target.min() < 0):
         raise ValueError("The `target` has to be a non-negative tensor.")
 
     preds_float = preds.is_floating_point()
     if not preds_float and preds.min() < 0:
         raise ValueError("If `preds` are integers, they have to be non-negative.")
 
     if not preds.shape[0] == target.shape[0]:
@@ -62,22 +69,23 @@
         raise ValueError("If you set `multiclass=False`, then `target` should not exceed 1.")
 
     if multiclass is False and not preds_float and preds.max() > 1:
         raise ValueError("If you set `multiclass=False` and `preds` are integers, then `preds` should not exceed 1.")
 
 
 def _check_shape_and_type_consistency(preds: Tensor, target: Tensor) -> Tuple[DataType, int]:
-    """This checks that the shape and type of inputs are consistent with each other and fall into one of the
-    allowed input types (see the documentation of docstring of ``_input_format_classification``). It does not check
-    for consistency of number of classes, other functions take care of that.
+    """Check that the shape and type of inputs are consistent with each other.
+
+    The input types needs to be one of allowed input types (see the documentation of docstring of
+    ``_input_format_classification``). It does not check for consistency of number of classes, other functions take
+    care of that.
 
     It returns the name of the case in which the inputs fall, and the implied number of classes (from the ``C`` dim for
     multi-class data, or extra dim(s) for multi-label data).
     """
-
     preds_float = preds.is_floating_point()
 
     if preds.ndim == target.ndim:
         if preds.shape != target.shape:
             raise ValueError(
                 "The `preds` and `target` should have the same shape,",
                 f" got `preds` with shape={preds.shape} and `target` with shape={target.shape}.",
@@ -105,30 +113,26 @@
             raise ValueError(
                 "If `preds` have one dimension more than `target`, the shape of `preds` should be"
                 " (N, C, ...), and the shape of `target` should be (N, ...)."
             )
 
         implied_classes = preds.shape[1] if preds.numel() > 0 else 0
 
-        if preds.ndim == 2:
-            case = DataType.MULTICLASS
-        else:
-            case = DataType.MULTIDIM_MULTICLASS
+        case = DataType.MULTICLASS if preds.ndim == 2 else DataType.MULTIDIM_MULTICLASS
     else:
         raise ValueError(
             "Either `preds` and `target` both should have the (same) shape (N, ...), or `target` should be (N, ...)"
             " and `preds` should be (N, C, ...)."
         )
 
     return case, implied_classes
 
 
 def _check_num_classes_binary(num_classes: int, multiclass: Optional[bool]) -> None:
-    """This checks that the consistency of `num_classes` with the data and `multiclass` param for binary data."""
-
+    """Check that the consistency of `num_classes` with the data and `multiclass` param for binary data."""
     if num_classes > 2:
         raise ValueError("Your data is binary, but `num_classes` is larger than 2.")
     if num_classes == 2 and not multiclass:
         raise ValueError(
             "Your data is binary and `num_classes=2`, but `multiclass` is not True."
             " Set it to True if you want to transform binary data to multi-class format."
         )
@@ -143,17 +147,15 @@
 def _check_num_classes_mc(
     preds: Tensor,
     target: Tensor,
     num_classes: int,
     multiclass: Optional[bool],
     implied_classes: int,
 ) -> None:
-    """This checks that the consistency of `num_classes` with the data and `multiclass` param for (multi-
-    dimensional) multi-class data."""
-
+    """Check consistency of `num_classes`, data and `multiclass` param for (multi-dimensional) multi-class data."""
     if num_classes == 1 and multiclass is not False:
         raise ValueError(
             "You have set `num_classes=1`, but predictions are integers."
             " If you want to convert (multi-dimensional) multi-class data with 2 classes"
             " to binary/multi-label, set `multiclass=False`."
         )
     if num_classes > 1:
@@ -168,17 +170,15 @@
         if target.numel() > 0 and num_classes <= target.max():
             raise ValueError("The highest label in `target` should be smaller than `num_classes`.")
         if preds.shape != target.shape and num_classes != implied_classes:
             raise ValueError("The size of C dimension of `preds` does not match `num_classes`.")
 
 
 def _check_num_classes_ml(num_classes: int, multiclass: Optional[bool], implied_classes: int) -> None:
-    """This checks that the consistency of ``num_classes`` with the data and ``multiclass`` param for multi-label
-    data."""
-
+    """Check that the consistency of ``num_classes`` with the data and ``multiclass`` param for multi-label data."""
     if multiclass and num_classes != 2:
         raise ValueError(
             "Your have set `multiclass=True`, but `num_classes` is not equal to 2."
             " If you are trying to transform multi-label data to 2 class multi-dimensional"
             " multi-class, you should set `num_classes` to either 2 or None."
         )
     if not multiclass and num_classes != implied_classes:
@@ -208,15 +208,15 @@
     target: Tensor,
     threshold: float,
     num_classes: Optional[int],
     multiclass: Optional[bool],
     top_k: Optional[int],
     ignore_index: Optional[int] = None,
 ) -> DataType:
-    """Performs error checking on inputs for classification.
+    """Perform error checking on inputs for classification.
 
     This ensures that preds and target take one of the shape/type combinations that are
     specified in ``_input_format_classification`` docstring. It also checks the cases of
     over-rides with ``multiclass`` by checking (for multi-class and multi-dim multi-class
     cases) that there are only up to 2 distinct labels.
 
     In case where preds are floats (probabilities), it is checked whether they are in ``[0,1]`` interval.
@@ -253,21 +253,21 @@
 
             Should be left unset (``None``) for inputs with label predictions.
         multiclass:
             Used only in certain special cases, where you want to treat inputs as a different type
             than what they appear to be. See the parameter's
             :ref:`documentation section <pages/overview:using the multiclass parameter>`
             for a more detailed explanation and examples.
+        ignore_index: ignore predictions where targets are equal to this number
 
 
     Return:
         case: The case the inputs fall in, one of 'binary', 'multi-class', 'multi-label' or
             'multi-dim multi-class'
     """
-
     # Basic validation (that does not need case/type information)
     _basic_input_validation(preds, target, threshold, multiclass, ignore_index)
 
     # Check that shape/types fall into one of the cases
     case, implied_classes = _check_shape_and_type_consistency(preds, target)
 
     # Check consistency with the `C` dimension in case of multi-class data
@@ -320,25 +320,25 @@
     ignore_index: Optional[int] = None,
 ) -> Tuple[Tensor, Tensor, DataType]:
     """Convert preds and target tensors into common format.
 
     Preds and targets are supposed to fall into one of these categories (and are
     validated to make sure this is the case):
 
-    * Both preds and target are of shape ``(N,)``, and both are integers (multi-class)
-    * Both preds and target are of shape ``(N,)``, and target is binary, while preds
-      are a float (binary)
-    * preds are of shape ``(N, C)`` and are floats, and target is of shape ``(N,)`` and
-      is integer (multi-class)
-    * preds and target are of shape ``(N, ...)``, target is binary and preds is a float
-      (multi-label)
-    * preds are of shape ``(N, C, ...)`` and are floats, target is of shape ``(N, ...)``
-      and is integer (multi-dimensional multi-class)
-    * preds and target are of shape ``(N, ...)`` both are integers (multi-dimensional
-      multi-class)
+        * Both preds and target are of shape ``(N,)``, and both are integers (multi-class)
+        * Both preds and target are of shape ``(N,)``, and target is binary, while preds
+          are a float (binary)
+        * preds are of shape ``(N, C)`` and are floats, and target is of shape ``(N,)`` and
+          is integer (multi-class)
+        * preds and target are of shape ``(N, ...)``, target is binary and preds is a float
+          (multi-label)
+        * preds are of shape ``(N, C, ...)`` and are floats, target is of shape ``(N, ...)``
+          and is integer (multi-dimensional multi-class)
+        * preds and target are of shape ``(N, ...)`` both are integers (multi-dimensional
+          multi-class)
 
     To avoid ambiguities, all dimensions of size 1, except the first one, are squeezed out.
 
     The returned output tensors will be binary tensors of the same shape, either ``(N, C)``
     of ``(N, C, X)``, the details for each case are described below. The function also returns
     a ``case`` string, which describes which of the above cases the inputs belonged to - regardless
     of whether this was "overridden" by other settings (like ``multiclass``).
@@ -388,14 +388,15 @@
 
             Should be left unset (``None``) for all other types of inputs.
         multiclass:
             Used only in certain special cases, where you want to treat inputs as a different type
             than what they appear to be. See the parameter's
             :ref:`documentation section <pages/overview:using the multiclass parameter>`
             for a more detailed explanation and examples.
+        ignore_index: ignore predictions where targets are equal to this number
 
     Returns:
         preds: binary tensor of shape ``(N, C)`` or ``(N, C, X)``
         target: binary tensor of shape ``(N, C)`` or ``(N, C, X)``
         case: The case the inputs fall in, one of ``'binary'``, ``'multi-class'``, ``'multi-label'`` or
             ``'multi-dim multi-class'``
     """
@@ -425,18 +426,18 @@
         preds = select_topk(preds, top_k)
 
     if case in (DataType.MULTICLASS, DataType.MULTIDIM_MULTICLASS) or multiclass:
         if preds.is_floating_point():
             num_classes = preds.shape[1]
             preds = select_topk(preds, top_k or 1)
         else:
-            num_classes = num_classes if num_classes else max(preds.max(), target.max()) + 1
+            num_classes = num_classes or int(max(preds.max().item(), target.max().item()) + 1)
             preds = to_onehot(preds, max(2, num_classes))
 
-        target = to_onehot(target, max(2, num_classes))  # type: ignore
+        target = to_onehot(target, max(2, num_classes))
 
         if multiclass is False:
             preds, target = preds[:, 1, ...], target[:, 1, ...]
 
     if not _check_for_empty_tensors(preds, target):
         if (case in (DataType.MULTICLASS, DataType.MULTIDIM_MULTICLASS) and multiclass is not False) or multiclass:
             target = target.reshape(target.shape[0], target.shape[1], -1)
@@ -540,14 +541,15 @@
 ) -> Tuple[Tensor, Tensor, Tensor]:
     """Check ``indexes``, ``preds`` and ``target`` tensors are of the same shape and of the correct data type.
 
     Args:
         indexes: tensor with queries indexes
         preds: tensor with scores/logits
         target: tensor with ground true labels
+        allow_non_binary_target: whether to allow target to contain non-binary values
         ignore_index: ignore predictions where targets are equal to this number
 
     Raises:
         ValueError:
             If ``preds`` and ``target`` don't have the same shape, if they are empty or not of the correct ``dtypes``.
 
     Returns:
@@ -605,78 +607,82 @@
 
     target = target.float() if target.is_floating_point() else target.long()
     preds = preds.float()
 
     return preds.flatten(), target.flatten()
 
 
-def _allclose_recursive(res1: Any, res2: Any, atol: float = 1e-8) -> bool:
-    """Utility function for recursively asserting that two results are within a certain tolerance."""
+def _allclose_recursive(res1: Any, res2: Any, atol: float = 1e-6) -> bool:
+    """Recursively asserting that two results are within a certain tolerance."""
     # single output compare
     if isinstance(res1, Tensor):
         return torch.allclose(res1, res2, atol=atol)
-    elif isinstance(res1, str):
+    if isinstance(res1, str):
         return res1 == res2
-    elif isinstance(res1, Sequence):
+    if isinstance(res1, Sequence):
         return all(_allclose_recursive(r1, r2) for r1, r2 in zip(res1, res2))
-    elif isinstance(res1, Mapping):
-        return all(_allclose_recursive(res1[k], res2[k]) for k in res1.keys())
+    if isinstance(res1, Mapping):
+        return all(_allclose_recursive(res1[k], res2[k]) for k in res1)
     return res1 == res2
 
 
 @no_type_check
 def check_forward_full_state_property(
-    metric_class,
-    init_args: Dict[str, Any] = {},
-    input_args: Dict[str, Any] = {},
+    metric_class: Metric,
+    init_args: Optional[Dict[str, Any]] = None,
+    input_args: Optional[Dict[str, Any]] = None,
     num_update_to_compare: Sequence[int] = [10, 100, 1000],
     reps: int = 5,
-) -> bool:
-    """Utility function for checking if the new ``full_state_update`` property can safely be set to ``False`` which
-    will for most metrics results in a speedup when using ``forward``.
+) -> None:
+    """Check if the new ``full_state_update`` property works as intended.
+
+    This function checks if the property can safely be set to ``False`` which will for most metrics results in a
+    speedup when using ``forward``.
 
     Args:
         metric_class: metric class object that should be checked
         init_args: dict containing arguments for initializing the metric class
         input_args: dict containing arguments to pass to ``forward``
         num_update_to_compare: if we successfully detech that the flag is safe to set to ``False``
             we will run some speedup test. This arg should be a list of integers for how many
             steps to compare over.
         reps: number of repetitions of speedup test
 
     Example (states in ``update`` are independent, save to set ``full_state_update=False``)
-        >>> from torchmetrics import ConfusionMatrix
-        >>> check_forward_full_state_property(
-        ...     ConfusionMatrix,
+        >>> from torchmetrics.classification import MulticlassConfusionMatrix
+        >>> check_forward_full_state_property(  # doctest: +ELLIPSIS
+        ...     MulticlassConfusionMatrix,
         ...     init_args = {'num_classes': 3},
-        ...     input_args = {'preds': torch.randint(3, (10,)), 'target': torch.randint(3, (10,))},
-        ... )  # doctest: +ELLIPSIS
+        ...     input_args = {'preds': torch.randint(3, (100,)), 'target': torch.randint(3, (100,))},
+        ... )
         Full state for 10 steps took: ...
         Partial state for 10 steps took: ...
         Full state for 100 steps took: ...
         Partial state for 100 steps took: ...
         Full state for 1000 steps took: ...
         Partial state for 1000 steps took: ...
         Recommended setting `full_state_update=False`
 
     Example (states in ``update`` are dependend meaning that ``full_state_update=True``):
-        >>> from torchmetrics import ConfusionMatrix
-        >>> class MyMetric(ConfusionMatrix):
+        >>> from torchmetrics.classification import MulticlassConfusionMatrix
+        >>> class MyMetric(MulticlassConfusionMatrix):
         ...     def update(self, preds, target):
         ...         super().update(preds, target)
         ...         # by construction make future states dependent on prior states
         ...         if self.confmat.sum() > 20:
         ...             self.reset()
         >>> check_forward_full_state_property(
         ...     MyMetric,
         ...     init_args = {'num_classes': 3},
         ...     input_args = {'preds': torch.randint(3, (10,)), 'target': torch.randint(3, (10,))},
         ... )
         Recommended setting `full_state_update=True`
     """
+    init_args = init_args or {}
+    input_args = input_args or {}
 
     class FullState(metric_class):
         full_state_update = True
 
     class PartState(metric_class):
         full_state_update = False
 
@@ -721,14 +727,15 @@
 
     for t in range(len(num_update_to_compare)):
         print(f"Full state for {num_update_to_compare[t]} steps took: {mean[0, t]}+-{std[0, t]:0.3f}")
         print(f"Partial state for {num_update_to_compare[t]} steps took: {mean[1, t]:0.3f}+-{std[1, t]:0.3f}")
 
     faster = (mean[1, -1] < mean[0, -1]).item()  # if faster on average, we recommend upgrading
     print(f"Recommended setting `full_state_update={not faster}`")
+    return
 
 
 def is_overridden(method_name: str, instance: object, parent: object) -> bool:
     """Check if a method has been overridden by an instance compared to its parent class."""
     instance_attr = getattr(instance, method_name, None)
     if instance_attr is None:
         return False
@@ -746,7 +753,38 @@
         return False
 
     parent_attr = getattr(parent, method_name, None)
     if parent_attr is None:
         raise ValueError("The parent should define the method")
 
     return instance_attr.__code__ != parent_attr.__code__
+
+
+def _try_proceed_with_timeout(fn: Callable, timeout: int = _DOCTEST_DOWNLOAD_TIMEOUT) -> bool:
+    """Check if a certain function is taking too long to execute.
+
+    Function will only be executed if running inside a doctest context. Currently does not support Windows.
+
+    Args:
+        fn: function to check
+        timeout: timeout for function
+
+    Returns:
+        Bool indicating if the function finished within the specified timeout
+    """
+    # source: https://stackoverflow.com/a/14924210/4521646
+    proc = multiprocessing.Process(target=fn)
+    logging.debug(f"try to run `{fn.__name__}` for {timeout}s...")
+    proc.start()
+    # Wait for N seconds or until process finishes
+    proc.join(timeout)
+    # If thread is still active
+    if not proc.is_alive():
+        return True
+
+    logging.warning(f"running `{fn.__name__}`... let's kill it...")
+    # Terminate - may not work if process is stuck for good
+    proc.terminate()
+    # OR Kill - will work for sure, no chance for process to finish nicely however
+    # p.kill()
+    proc.join()
+    return False
```

### Comparing `torchmetrics-0.9.3/torchmetrics/utilities/data.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/utilities/data.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,45 +1,38 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+import sys
 from typing import Any, Callable, Dict, List, Mapping, Optional, Sequence, Union
 
+import numpy as np
 import torch
-from torch import Tensor, tensor
-
-from torchmetrics.utilities.imports import _TORCH_GREATER_EQUAL_1_6, _TORCH_GREATER_EQUAL_1_7, _TORCH_GREATER_EQUAL_1_8
-
-if _TORCH_GREATER_EQUAL_1_8:
-    deterministic = torch.are_deterministic_algorithms_enabled
-elif _TORCH_GREATER_EQUAL_1_7:
-    deterministic = torch.is_deterministic
-elif _TORCH_GREATER_EQUAL_1_6:
-    deterministic = torch._is_deterministic
-else:
-
-    def deterministic() -> bool:
-        return True
+from torch import Tensor
 
+from torchmetrics.utilities.exceptions import TorchMetricsUserWarning
+from torchmetrics.utilities.imports import _TORCH_GREATER_EQUAL_1_12, _XLA_AVAILABLE
+from torchmetrics.utilities.prints import rank_zero_warn
 
 METRIC_EPS = 1e-6
 
 
 def dim_zero_cat(x: Union[Tensor, List[Tensor]]) -> Tensor:
     """Concatenation along the zero dimension."""
-    x = x if isinstance(x, (list, tuple)) else [x]
+    if isinstance(x, torch.Tensor):
+        return x
     x = [y.unsqueeze(0) if y.numel() == 1 and y.ndim == 0 else y for y in x]
     if not x:  # empty list
         raise ValueError("No samples to concatenate")
     return torch.cat(x, dim=0)
 
 
 def dim_zero_sum(x: Tensor) -> Tensor:
@@ -79,15 +72,15 @@
     return new_dict
 
 
 def to_onehot(
     label_tensor: Tensor,
     num_classes: Optional[int] = None,
 ) -> Tensor:
-    """Converts a dense label tensor to one-hot format.
+    """Convert  a dense label tensor to one-hot format.
 
     Args:
         label_tensor: dense label tensor, with shape [N, d1, d2, ...]
         num_classes: number of classes C
 
     Returns:
         A sparse label tensor with shape [N, C, d1, d2, ...]
@@ -136,15 +129,15 @@
         topk_tensor = zeros.scatter(dim, prob_tensor.argmax(dim=dim, keepdim=True), 1.0)
     else:
         topk_tensor = zeros.scatter(dim, prob_tensor.topk(k=topk, dim=dim).indices, 1.0)
     return topk_tensor.int()
 
 
 def to_categorical(x: Tensor, argmax_dim: int = 1) -> Tensor:
-    """Converts a tensor of probabilities to a dense label tensor.
+    """Convert  a tensor of probabilities to a dense label tensor.
 
     Args:
         x: probabilities to get the categorical label [N, d1, d2, ...]
         argmax_dim: dimension to apply
 
     Return:
         A tensor with categorical labels [N, d2, ...]
@@ -203,69 +196,83 @@
     if isinstance(data, Sequence) and not isinstance(data, str):
         return elem_type([apply_to_collection(d, dtype, function, *args, **kwargs) for d in data])
 
     # data is neither of dtype, nor a collection
     return data
 
 
-def get_group_indexes(indexes: Tensor) -> List[Tensor]:
-    """Given an integer ``indexes``, return indexes for each different value in ``indexes``.
-
-    Args:
-        indexes:
-
-    Return:
-        A list of integer ``torch.Tensor``s
-
-    Example:
-        >>> indexes = torch.tensor([0, 0, 0, 1, 1, 1, 1])
-        >>> get_group_indexes(indexes)
-        [tensor([0, 1, 2]), tensor([3, 4, 5, 6])]
-    """
-
-    res: dict = {}
-    for i, _id in enumerate(indexes):
-        _id = _id.item()
-        if _id in res:
-            res[_id] += [i]
-        else:
-            res[_id] = [i]
-
-    return [tensor(x, dtype=torch.long) for x in res.values()]
-
-
 def _squeeze_scalar_element_tensor(x: Tensor) -> Tensor:
     return x.squeeze() if x.numel() == 1 else x
 
 
 def _squeeze_if_scalar(data: Any) -> Any:
     return apply_to_collection(data, Tensor, _squeeze_scalar_element_tensor)
 
 
 def _bincount(x: Tensor, minlength: Optional[int] = None) -> Tensor:
-    """``torch.bincount`` currently does not support deterministic mode on GPU.
+    """Implement custom bincount.
+
+    PyTorch currently does not support ``torch.bincount`` for:
+
+        - deterministic mode on GPU.
+        - MPS devices
 
     This implementation fallback to a for-loop counting occurrences in that case.
 
     Args:
         x: tensor to count
         minlength: minimum length to count
 
     Returns:
         Number of occurrences for each unique element in x
+
+    Example:
+        >>> x = torch.tensor([0,0,0,1,1,2,2,2,2])
+        >>> _bincount(x, minlength=3)
+        tensor([3, 2, 4])
+
     """
-    if x.is_cuda and deterministic():
-        if minlength is None:
-            minlength = len(torch.unique(x))
+    if minlength is None:
+        minlength = len(torch.unique(x))
+    if torch.are_deterministic_algorithms_enabled() or _XLA_AVAILABLE or _TORCH_GREATER_EQUAL_1_12 and x.is_mps:
         output = torch.zeros(minlength, device=x.device, dtype=torch.long)
         for i in range(minlength):
             output[i] = (x == i).sum()
         return output
-    else:
-        return torch.bincount(x, minlength=minlength)
+    return torch.bincount(x, minlength=minlength)
+
+
+def _cumsum(x: Tensor, dim: Optional[int] = 0, dtype: Optional[torch.dtype] = None) -> Tensor:
+    if torch.are_deterministic_algorithms_enabled() and x.is_cuda and x.is_floating_point() and sys.platform != "win32":
+        rank_zero_warn(
+            "You are trying to use a metric in deterministic mode on GPU that uses `torch.cumsum` which is currently"
+            "not supported. Instead the tensor will be casted to CPU, compute the `cumsum` and then casted back to GPU"
+            "Expect some slowdowns.",
+            TorchMetricsUserWarning,
+        )
+        return x.cpu().cumsum(dim=dim, dtype=dtype).cuda()
+    return torch.cumsum(x, dim=dim, dtype=dtype)
+
+
+def _flexible_bincount(x: Tensor) -> Tensor:
+    """Similar to `_bincount`, but works also with tensor that do not contain continuous values.
+
+    Args:
+        x: tensor to count
+
+    Returns:
+        Number of occurrences for each unique element in x
+    """
+    # make sure elements in x start from 0
+    x = x - x.min()
+    unique_x = torch.unique(x)
+
+    output = _bincount(x, minlength=torch.max(unique_x) + 1)  # type: ignore[arg-type]
+    # remove zeros from output tensor
+    return output[unique_x]
 
 
 def allclose(tensor1: Tensor, tensor2: Tensor) -> bool:
-    """Wrapper of torch.allclose that is robust towards dtype difference."""
+    """Wrap torch.allclose to be robust towards dtype difference."""
     if tensor1.dtype != tensor2.dtype:
         tensor2 = tensor2.to(dtype=tensor1.dtype)
     return torch.allclose(tensor1, tensor2)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/utilities/distributed.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/utilities/distributed.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -10,16 +10,16 @@
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from typing import Any, List, Optional
 
 import torch
-import torch.nn.functional as F
 from torch import Tensor
+from torch.nn import functional as F  # noqa: N812
 from typing_extensions import Literal
 
 
 def reduce(x: Tensor, reduction: Literal["elementwise_mean", "sum", "none", None]) -> Tensor:
     """Reduces a given tensor by a given reduction method.
 
     Args:
@@ -43,19 +43,18 @@
 
 def class_reduce(
     num: Tensor,
     denom: Tensor,
     weights: Tensor,
     class_reduction: Literal["micro", "macro", "weighted", "none", None] = "none",
 ) -> Tensor:
-    """
-    Function used to reduce classification metrics of the form ``num / denom * weights``.
-    For example for calculating standard accuracy the num would be number of
-    true positives per class, denom would be the support per class, and weights
-    would be a tensor of 1s
+    """Reduce classification metrics of the form ``num / denom * weights``.
+
+    For example for calculating standard accuracy the num would be number of true positives per class, denom would be
+    the support per class, and weights would be a tensor of 1s.
 
     Args:
         num: numerator tensor
         denom: denominator tensor
         weights: weights for each class
         class_reduction: reduction method for multiclass problems:
 
@@ -66,18 +65,15 @@
 
     Raises:
         ValueError:
             If ``class_reduction`` is none of ``"micro"``, ``"macro"``, ``"weighted"``, ``"none"`` or ``None``.
 
     """
     valid_reduction = ("micro", "macro", "weighted", "none", None)
-    if class_reduction == "micro":
-        fraction = torch.sum(num) / torch.sum(denom)
-    else:
-        fraction = num / denom
+    fraction = torch.sum(num) / torch.sum(denom) if class_reduction == "micro" else num / denom
 
     # We need to take care of instances where the denom can be 0
     # for some (or all) classes which will produce nans
     fraction[fraction != fraction] = 0
 
     if class_reduction == "micro":
         return fraction
@@ -96,15 +92,16 @@
 def _simple_gather_all_tensors(result: Tensor, group: Any, world_size: int) -> List[Tensor]:
     gathered_result = [torch.zeros_like(result) for _ in range(world_size)]
     torch.distributed.all_gather(gathered_result, result, group)
     return gathered_result
 
 
 def gather_all_tensors(result: Tensor, group: Optional[Any] = None) -> List[Tensor]:
-    """Function to gather all tensors from several ddp processes onto a list that is broadcasted to all processes.
+    """Gather all tensors from several ddp processes onto a list that is broadcasted to all processes.
+
     Works on tensors that have the same number of dimensions, but where each dimension may differ. In this case
     tensors are padded, gathered and then trimmed to secure equal workload for all processes.
 
     Args:
         result: the value to sync
         group: the process group to gather results from. Defaults to all processes (world)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/utilities/exceptions.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/utilities/exceptions.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,8 +1,8 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
@@ -11,7 +11,11 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
 
 class TorchMetricsUserError(Exception):
     """Error used to inform users of a wrong combination of Metric API calls."""
+
+
+class TorchMetricsUserWarning(Warning):
+    """Error used to inform users of specific warnings due to the torchmetrics API."""
```

### Comparing `torchmetrics-0.9.3/torchmetrics/wrappers/__init__.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/wrappers/__init__.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,18 +1,26 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-from torchmetrics.wrappers.bootstrapping import BootStrapper  # noqa: F401
-from torchmetrics.wrappers.classwise import ClasswiseWrapper  # noqa: F401
-from torchmetrics.wrappers.minmax import MinMaxMetric  # noqa: F401
-from torchmetrics.wrappers.multioutput import MultioutputWrapper  # noqa: F401
-from torchmetrics.wrappers.tracker import MetricTracker  # noqa: F401
+from torchmetrics.wrappers.bootstrapping import BootStrapper
+from torchmetrics.wrappers.classwise import ClasswiseWrapper
+from torchmetrics.wrappers.minmax import MinMaxMetric
+from torchmetrics.wrappers.multioutput import MultioutputWrapper
+from torchmetrics.wrappers.tracker import MetricTracker
+
+__all__ = [
+    "BootStrapper",
+    "ClasswiseWrapper",
+    "MinMaxMetric",
+    "MultioutputWrapper",
+    "MetricTracker",
+]
```

### Comparing `torchmetrics-0.9.3/torchmetrics/wrappers/bootstrapping.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/wrappers/bootstrapping.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,30 +1,34 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
 from copy import deepcopy
-from typing import Any, Dict, Optional, Union
+from typing import Any, Dict, Optional, Sequence, Union
 
 import torch
 from torch import Tensor
 from torch.nn import ModuleList
 
 from torchmetrics.metric import Metric
 from torchmetrics.utilities import apply_to_collection
-from torchmetrics.utilities.imports import _TORCH_GREATER_EQUAL_1_7
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
+
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["BootStrapper.plot"]
 
 
 def _bootstrap_sampler(
     size: int,
     sampling_strategy: str = "poisson",
 ) -> Tensor:
     """Resample a tensor along its first dimension with replacement.
@@ -43,16 +47,15 @@
     if sampling_strategy == "multinomial":
         idx = torch.multinomial(torch.ones(size), num_samples=size, replacement=True)
         return idx
     raise ValueError("Unknown sampling strategy")
 
 
 class BootStrapper(Metric):
-    r"""
-    Using `Turn a Metric into a Bootstrapped`_
+    r"""Using `Turn a Metric into a Bootstrapped`_.
 
     That can automate the process of getting confidence intervals for metric values. This wrapper
     class basically keeps multiple copies of the same base metric in memory and whenever ``update`` or
     ``forward`` is called, all input tensors are resampled (with replacement) along the first dimension.
 
     Args:
         base_metric: base metric class to wrap
@@ -67,24 +70,25 @@
             will be given by :math:`n\sim Poisson(\lambda=1)`, which approximates the true bootstrap distribution
             when the number of samples is large. If ``'multinomial'`` is chosen, we will apply true bootstrapping
             at the batch level to approximate bootstrapping over the hole dataset.
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
     Example::
         >>> from pprint import pprint
-        >>> from torchmetrics import Accuracy, BootStrapper
+        >>> from torchmetrics.wrappers import BootStrapper
+        >>> from torchmetrics.classification import MulticlassAccuracy
         >>> _ = torch.manual_seed(123)
-        >>> base_metric = Accuracy()
+        >>> base_metric = MulticlassAccuracy(num_classes=5, average='micro')
         >>> bootstrap = BootStrapper(base_metric, num_bootstraps=20)
         >>> bootstrap.update(torch.randint(5, (20,)), torch.randint(5, (20,)))
         >>> output = bootstrap.compute()
         >>> pprint(output)
         {'mean': tensor(0.2205), 'std': tensor(0.0859)}
-
     """
+    full_state_update: Optional[bool] = True
 
     def __init__(
         self,
         base_metric: Metric,
         num_bootstraps: int = 10,
         mean: bool = True,
         std: bool = True,
@@ -100,29 +104,27 @@
             )
 
         self.metrics = ModuleList([deepcopy(base_metric) for _ in range(num_bootstraps)])
         self.num_bootstraps = num_bootstraps
 
         self.mean = mean
         self.std = std
-        if quantile is not None and not _TORCH_GREATER_EQUAL_1_7:
-            raise ValueError("quantile argument can only be used with pytorch v1.7 or higher")
         self.quantile = quantile
         self.raw = raw
 
         allowed_sampling = ("poisson", "multinomial")
         if sampling_strategy not in allowed_sampling:
             raise ValueError(
                 f"Expected argument ``sampling_strategy`` to be one of {allowed_sampling}"
                 f" but recieved {sampling_strategy}"
             )
         self.sampling_strategy = sampling_strategy
 
     def update(self, *args: Any, **kwargs: Any) -> None:
-        """Updates the state of the base metric.
+        """Update the state of the base metric.
 
         Any tensor passed in will be bootstrapped along dimension 0.
         """
         for idx in range(self.num_bootstraps):
             args_sizes = apply_to_collection(args, Tensor, len)
             kwargs_sizes = list(apply_to_collection(kwargs, Tensor, len))
             if len(args_sizes) > 0:
@@ -133,15 +135,15 @@
                 raise ValueError("None of the input contained tensors, so could not determine the sampling size")
             sample_idx = _bootstrap_sampler(size, sampling_strategy=self.sampling_strategy).to(self.device)
             new_args = apply_to_collection(args, Tensor, torch.index_select, dim=0, index=sample_idx)
             new_kwargs = apply_to_collection(kwargs, Tensor, torch.index_select, dim=0, index=sample_idx)
             self.metrics[idx].update(*new_args, **new_kwargs)
 
     def compute(self) -> Dict[str, Tensor]:
-        """Computes the bootstrapped metric values.
+        """Compute the bootstrapped metric values.
 
         Always returns a dict of tensors, which can contain the following keys: ``mean``, ``std``, ``quantile`` and
         ``raw`` depending on how the class was initialized.
         """
         computed_vals = torch.stack([m.compute() for m in self.metrics], dim=0)
         output_dict = {}
         if self.mean:
@@ -149,7 +151,50 @@
         if self.std:
             output_dict["std"] = computed_vals.std(dim=0)
         if self.quantile is not None:
             output_dict["quantile"] = torch.quantile(computed_vals, self.quantile)
         if self.raw:
             output_dict["raw"] = computed_vals
         return output_dict
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> import torch
+            >>> from torchmetrics.wrappers import BootStrapper
+            >>> from torchmetrics.regression import MeanSquaredError
+            >>> metric = BootStrapper(MeanSquaredError(), num_bootstraps=20)
+            >>> metric.update(torch.randn(100,), torch.randn(100,))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> import torch
+            >>> from torchmetrics.wrappers import BootStrapper
+            >>> from torchmetrics.regression import MeanSquaredError
+            >>> metric = BootStrapper(MeanSquaredError(), num_bootstraps=20)
+            >>> values = [ ]
+            >>> for _ in range(3):
+            ...     values.append(metric(torch.randn(100,), torch.randn(100,)))
+            >>> fig_, ax_ = metric.plot(values)
+        """
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/wrappers/minmax.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/regression/mae.py`

 * *Files 24% similar despite different names*

```diff
@@ -1,102 +1,122 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+from typing import Any, Optional, Sequence, Union
 
-from typing import Any, Dict, Union
-
-import torch
-from torch import Tensor
+from torch import Tensor, tensor
 
+from torchmetrics.functional.regression.mae import _mean_absolute_error_compute, _mean_absolute_error_update
 from torchmetrics.metric import Metric
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
+
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["MeanAbsoluteError.plot"]
+
+
+class MeanAbsoluteError(Metric):
+    r"""`Compute Mean Absolute Error`_ (MAE).
+
+    .. math:: \text{MAE} = \frac{1}{N}\sum_i^N | y_i - \hat{y_i} |
+
+    Where :math:`y` is a tensor of target values, and :math:`\hat{y}` is a tensor of predictions.
+
+    As input to ``forward`` and ``update`` the metric accepts the following input:
+
+    - ``preds`` (:class:`~torch.Tensor`): Predictions from model
+    - ``target`` (:class:`~torch.Tensor`): Ground truth values
 
+    As output of ``forward`` and ``compute`` the metric returns the following output:
 
-class MinMaxMetric(Metric):
-    """Wrapper Metric that tracks both the minimum and maximum of a scalar/tensor across an experiment. The min/max
-    value will be updated each time ``.compute`` is called.
+    - ``mean_absolute_error`` (:class:`~torch.Tensor`): A tensor with the mean absolute error over the state
 
     Args:
-        base_metric:
-            The metric of which you want to keep track of its maximum and minimum values.
         kwargs: Additional keyword arguments, see :ref:`Metric kwargs` for more info.
 
-    Raises:
-        ValueError
-            If ``base_metric` argument is not a subclasses instance of ``torchmetrics.Metric``
-
-    Example::
-        >>> import torch
-        >>> from torchmetrics import Accuracy
-        >>> from pprint import pprint
-        >>> base_metric = Accuracy()
-        >>> minmax_metric = MinMaxMetric(base_metric)
-        >>> preds_1 = torch.Tensor([[0.1, 0.9], [0.2, 0.8]])
-        >>> preds_2 = torch.Tensor([[0.9, 0.1], [0.2, 0.8]])
-        >>> labels = torch.Tensor([[0, 1], [0, 1]]).long()
-        >>> pprint(minmax_metric(preds_1, labels))
-        {'max': tensor(1.), 'min': tensor(1.), 'raw': tensor(1.)}
-        >>> pprint(minmax_metric.compute())
-        {'max': tensor(1.), 'min': tensor(1.), 'raw': tensor(1.)}
-        >>> minmax_metric.update(preds_2, labels)
-        >>> pprint(minmax_metric.compute())
-        {'max': tensor(1.), 'min': tensor(0.7500), 'raw': tensor(0.7500)}
+    Example:
+        >>> from torch import tensor
+        >>> from torchmetrics.regression import MeanAbsoluteError
+        >>> target = tensor([3.0, -0.5, 2.0, 7.0])
+        >>> preds = tensor([2.5, 0.0, 2.0, 8.0])
+        >>> mean_absolute_error = MeanAbsoluteError()
+        >>> mean_absolute_error(preds, target)
+        tensor(0.5000)
     """
+    is_differentiable: bool = True
+    higher_is_better: bool = False
+    full_state_update: bool = False
+    plot_lower_bound: float = 0.0
 
-    min_val: Tensor
-    max_val: Tensor
+    sum_abs_error: Tensor
+    total: Tensor
 
     def __init__(
         self,
-        base_metric: Metric,
         **kwargs: Any,
     ) -> None:
         super().__init__(**kwargs)
-        if not isinstance(base_metric, Metric):
-            raise ValueError(
-                f"Expected base metric to be an instance of `torchmetrics.Metric` but received {base_metric}"
-            )
-        self._base_metric = base_metric
-        self.min_val = torch.tensor(float("inf"))
-        self.max_val = torch.tensor(float("-inf"))
-
-    def update(self, *args: Any, **kwargs: Any) -> None:  # type: ignore
-        """Updates the underlying metric."""
-        self._base_metric.update(*args, **kwargs)
 
-    def compute(self) -> Dict[str, Tensor]:  # type: ignore
-        """Computes the underlying metric as well as max and min values for this metric.
+        self.add_state("sum_abs_error", default=tensor(0.0), dist_reduce_fx="sum")
+        self.add_state("total", default=tensor(0), dist_reduce_fx="sum")
 
-        Returns a dictionary that consists of the computed value (``raw``), as well as the minimum (``min``) and maximum
-        (``max``) values.
+    def update(self, preds: Tensor, target: Tensor) -> None:
+        """Update state with predictions and targets."""
+        sum_abs_error, n_obs = _mean_absolute_error_update(preds, target)
+
+        self.sum_abs_error += sum_abs_error
+        self.total += n_obs
+
+    def compute(self) -> Tensor:
+        """Compute mean absolute error over state."""
+        return _mean_absolute_error_compute(self.sum_abs_error, self.total)
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import randn
+            >>> # Example plotting a single value
+            >>> from torchmetrics.regression import MeanAbsoluteError
+            >>> metric = MeanAbsoluteError()
+            >>> metric.update(randn(10,), randn(10,))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> from torch import randn
+            >>> # Example plotting multiple values
+            >>> from torchmetrics.regression import MeanAbsoluteError
+            >>> metric = MeanAbsoluteError()
+            >>> values = []
+            >>> for _ in range(10):
+            ...     values.append(metric(randn(10,), randn(10,)))
+            >>> fig, ax = metric.plot(values)
         """
-        val = self._base_metric.compute()
-        if not self._is_suitable_val(val):
-            raise RuntimeError(
-                f"Returned value from base metric should be a scalar (int, float or tensor of size 1, but got {val}"
-            )
-        self.max_val = val if self.max_val.to(val.device) < val else self.max_val.to(val.device)
-        self.min_val = val if self.min_val.to(val.device) > val else self.min_val.to(val.device)
-        return {"raw": val, "max": self.max_val, "min": self.min_val}
-
-    def reset(self) -> None:
-        """Sets ``max_val`` and ``min_val`` to the initialization bounds and resets the base metric."""
-        super().reset()
-        self._base_metric.reset()
-
-    @staticmethod
-    def _is_suitable_val(val: Union[int, float, Tensor]) -> bool:
-        """Utility function that checks whether min/max value."""
-        if isinstance(val, (int, float)):
-            return True
-        if isinstance(val, Tensor):
-            return val.numel() == 1
-        return False
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/wrappers/multioutput.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/wrappers/multioutput.py`

 * *Files 22% similar despite different names*

```diff
@@ -1,16 +1,21 @@
 from copy import deepcopy
-from typing import Any, List, Tuple
+from typing import Any, Callable, List, Optional, Sequence, Tuple, Union
 
 import torch
 from torch import Tensor
 from torch.nn import ModuleList
 
 from torchmetrics import Metric
 from torchmetrics.utilities import apply_to_collection
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE
+
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["MultioutputWrapper.plot"]
 
 
 def _get_nan_indices(*tensors: Tensor) -> Tensor:
     """Get indices of rows along dim 0 which have NaN values."""
     if len(tensors) == 0:
         raise ValueError("Must pass at least one tensor as argument")
     sentinel = tensors[0]
@@ -52,46 +57,35 @@
             represents the length of the batch or dataset being passed in.
         squeeze_outputs:
             If ``True``, will squeeze the 1-item dimensions left after ``index_select`` is applied.
             This is sometimes unnecessary but harmless for metrics such as `R2Score` but useful
             for certain classification metrics that can't handle additional 1-item dimensions.
 
     Example:
-
          >>> # Mimic R2Score in `multioutput`, `raw_values` mode:
          >>> import torch
-         >>> from torchmetrics import MultioutputWrapper, R2Score
+         >>> from torchmetrics.wrappers import MultioutputWrapper
+         >>> from torchmetrics.regression import R2Score
          >>> target = torch.tensor([[0.5, 1], [-1, 1], [7, -6]])
          >>> preds = torch.tensor([[0, 2], [-1, 2], [8, -5]])
          >>> r2score = MultioutputWrapper(R2Score(), 2)
          >>> r2score(preds, target)
-         [tensor(0.9654), tensor(0.9082)]
-         >>> # Classification metric where prediction and label tensors have different shapes.
-         >>> from torchmetrics import BinnedAveragePrecision
-         >>> target = torch.tensor([[1, 2], [2, 0], [1, 2]])
-         >>> preds = torch.tensor([
-         ...     [[.1, .8], [.8, .05], [.1, .15]],
-         ...     [[.1, .1], [.2, .3], [.7, .6]],
-         ...     [[.002, .4], [.95, .45], [.048, .15]]
-         ... ])
-         >>> binned_avg_precision = MultioutputWrapper(BinnedAveragePrecision(3, thresholds=5), 2)
-         >>> binned_avg_precision(preds, target)
-         [[tensor(-0.), tensor(1.0000), tensor(1.0000)], [tensor(0.3333), tensor(-0.), tensor(0.6667)]]
+         tensor([0.9654, 0.9082])
     """
 
     is_differentiable = False
 
     def __init__(
         self,
         base_metric: Metric,
         num_outputs: int,
         output_dim: int = -1,
         remove_nans: bool = True,
         squeeze_outputs: bool = True,
-    ):
+    ) -> None:
         super().__init__()
         self.metrics = ModuleList([deepcopy(base_metric) for _ in range(num_outputs)])
         self.output_dim = output_dim
         self.remove_nans = remove_nans
         self.squeeze_outputs = squeeze_outputs
 
     def _get_args_kwargs_by_output(self, *args: Tensor, **kwargs: Tensor) -> List[Tuple[Tensor, Tensor]]:
@@ -108,38 +102,91 @@
                 args_kwargs = selected_args + tuple(selected_kwargs.values())
                 nan_idxs = _get_nan_indices(*args_kwargs)
                 selected_args = [arg[~nan_idxs] for arg in selected_args]
                 selected_kwargs = {k: v[~nan_idxs] for k, v in selected_kwargs.items()}
 
             if self.squeeze_outputs:
                 selected_args = [arg.squeeze(self.output_dim) for arg in selected_args]
+                selected_kwargs = {k: v.squeeze(self.output_dim) for k, v in selected_kwargs.items()}
             args_kwargs_by_output.append((selected_args, selected_kwargs))
         return args_kwargs_by_output
 
     def update(self, *args: Any, **kwargs: Any) -> None:
         """Update each underlying metric with the corresponding output."""
         reshaped_args_kwargs = self._get_args_kwargs_by_output(*args, **kwargs)
         for metric, (selected_args, selected_kwargs) in zip(self.metrics, reshaped_args_kwargs):
             metric.update(*selected_args, **selected_kwargs)
 
-    def compute(self) -> List[Tensor]:
+    def compute(self) -> Tensor:
         """Compute metrics."""
-        return [m.compute() for m in self.metrics]
+        return torch.stack([m.compute() for m in self.metrics], 0)
 
     @torch.jit.unused
     def forward(self, *args: Any, **kwargs: Any) -> Any:
         """Call underlying forward methods and aggregate the results if they're non-null.
 
         We override this method to ensure that state variables get copied over on the underlying metrics.
         """
         results = []
         reshaped_args_kwargs = self._get_args_kwargs_by_output(*args, **kwargs)
         for metric, (selected_args, selected_kwargs) in zip(self.metrics, reshaped_args_kwargs):
             results.append(metric(*selected_args, **selected_kwargs))
         if results[0] is None:
             return None
-        return results
+        return torch.stack(results, 0)
 
     def reset(self) -> None:
         """Reset all underlying metrics."""
         for metric in self.metrics:
             metric.reset()
+        super().reset()
+
+    def _wrap_update(self, update: Callable) -> Callable:
+        """Overwrite to do nothing."""
+        return update
+
+    def _wrap_compute(self, compute: Callable) -> Callable:
+        """Overwrite to do nothing."""
+        return compute
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> import torch
+            >>> from torchmetrics.wrappers import MultioutputWrapper
+            >>> from torchmetrics.regression import R2Score
+            >>> metric = MultioutputWrapper(R2Score(), 2)
+            >>> metric.update(torch.randn(20, 2), torch.randn(20, 2))
+            >>> fig_, ax_ = metric.plot()
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting multiple values
+            >>> import torch
+            >>> from torchmetrics.wrappers import MultioutputWrapper
+            >>> from torchmetrics.regression import R2Score
+            >>> metric = MultioutputWrapper(R2Score(), 2)
+            >>> values = [ ]
+            >>> for _ in range(3):
+            ...     values.append(metric(torch.randn(20, 2), torch.randn(20, 2)))
+            >>> fig_, ax_ = metric.plot(values)
+        """
+        return self._plot(val, ax)
```

### Comparing `torchmetrics-0.9.3/torchmetrics/wrappers/tracker.py` & `torchmetrics-1.0.0rc0/src/torchmetrics/wrappers/tracker.py`

 * *Files 27% similar despite different names*

```diff
@@ -1,53 +1,65 @@
-# Copyright The PyTorch Lightning team.
+# Copyright The Lightning team.
 #
 # Licensed under the Apache License, Version 2.0 (the "License");
 # you may not use this file except in compliance with the License.
 # You may obtain a copy of the License at
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
-import warnings
 from copy import deepcopy
-from typing import Any, Dict, List, Tuple, Union
+from typing import Any, Dict, List, Optional, Sequence, Tuple, Union
 
 import torch
 from torch import Tensor
 from torch.nn import ModuleList
 
 from torchmetrics.collections import MetricCollection
 from torchmetrics.metric import Metric
+from torchmetrics.utilities.imports import _MATPLOTLIB_AVAILABLE
+from torchmetrics.utilities.plot import _AX_TYPE, _PLOT_OUT_TYPE, plot_single_or_multi_val
+from torchmetrics.utilities.prints import rank_zero_warn
+
+if not _MATPLOTLIB_AVAILABLE:
+    __doctest_skip__ = ["MetricTracker.plot"]
 
 
 class MetricTracker(ModuleList):
-    """A wrapper class that can help keeping track of a metric or metric collection over time and implement useful
-    methods. The wrapper implements the standard ``.update()``, ``.compute()``, ``.reset()`` methods that just
+    """A wrapper class that can help keeping track of a metric or metric collection over time.
+
+    The wrapper implements the standard ``.update()``, ``.compute()``, ``.reset()`` methods that just
     calls corresponding method of the currently tracked metric. However, the following additional methods are
     provided:
 
         -``MetricTracker.n_steps``: number of metrics being tracked
         -``MetricTracker.increment()``: initialize a new metric for being tracked
         -``MetricTracker.compute_all()``: get the metric value for all steps
         -``MetricTracker.best_metric()``: returns the best value
 
+    Out of the box, this wrapper class fully supports that the base metric being tracked is a single `Metric`, a
+    `MetricCollection` or another `MetricWrapper` wrapped around a metric. However, multiple layers of nesting, such
+    as using a `Metric` inside a `MetricWrapper` inside a `MetricCollection` is not fully supported, especially the
+    `.best_metric` method that cannot auto compute the best metric and index for such nested structures.
+
     Args:
         metric: instance of a ``torchmetrics.Metric`` or ``torchmetrics.MetricCollection``
             to keep track of at each timestep.
         maximize: either single bool or list of bool indicating if higher metric values are
             better (``True``) or lower is better (``False``).
 
     Example (single metric):
-        >>> from torchmetrics import Accuracy, MetricTracker
+        >>> from torchmetrics.wrappers import MetricTracker
+        >>> from torchmetrics.classification import MulticlassAccuracy
         >>> _ = torch.manual_seed(42)
-        >>> tracker = MetricTracker(Accuracy(num_classes=10))
+        >>> tracker = MetricTracker(MulticlassAccuracy(num_classes=10, average='micro'))
         >>> for epoch in range(5):
         ...     tracker.increment()
         ...     for batch_idx in range(5):
         ...         preds, target = torch.randint(10, (100,)), torch.randint(10, (100,))
         ...         tracker.update(preds, target)
         ...     print(f"current acc={tracker.compute()}")
         current acc=0.1120000034570694
@@ -60,15 +72,17 @@
         0.1260...
         >>> which_epoch
         2
         >>> tracker.compute_all()
         tensor([0.1120, 0.0880, 0.1260, 0.0800, 0.1020])
 
     Example (multiple metrics using MetricCollection):
-        >>> from torchmetrics import MetricTracker, MetricCollection, MeanSquaredError, ExplainedVariance
+        >>> from torchmetrics.wrappers import MetricTracker
+        >>> from torchmetrics import MetricCollection
+        >>> from torchmetrics.regression import MeanSquaredError, ExplainedVariance
         >>> _ = torch.manual_seed(42)
         >>> tracker = MetricTracker(MetricCollection([MeanSquaredError(), ExplainedVariance()]), maximize=[False, True])
         >>> for epoch in range(5):
         ...     tracker.increment()
         ...     for batch_idx in range(5):
         ...         preds, target = torch.randn(100), torch.randn(100)
         ...         tracker.update(preds, target)
@@ -98,116 +112,197 @@
                 f" `Metric` or `MetricCollection` but got {metric}"
             )
         self._base_metric = metric
         if not isinstance(maximize, (bool, list)):
             raise ValueError("Argument `maximize` should either be a single bool or list of bool")
         if isinstance(maximize, list) and isinstance(metric, MetricCollection) and len(maximize) != len(metric):
             raise ValueError("The len of argument `maximize` should match the length of the metric collection")
+        if isinstance(metric, Metric) and not isinstance(maximize, bool):
+            raise ValueError("Argument `maximize` should be a single bool when `metric` is a single Metric")
         self.maximize = maximize
 
         self._increment_called = False
 
     @property
     def n_steps(self) -> int:
         """Returns the number of times the tracker has been incremented."""
         return len(self) - 1  # subtract the base metric
 
     def increment(self) -> None:
-        """Creates a new instance of the input metric that will be updated next."""
+        """Create a new instance of the input metric that will be updated next."""
         self._increment_called = True
         self.append(deepcopy(self._base_metric))
 
-    def forward(self, *args, **kwargs) -> None:  # type: ignore
-        """Calls forward of the current metric being tracked."""
+    def forward(self, *args: Any, **kwargs: Any) -> None:
+        """Call forward of the current metric being tracked."""
         self._check_for_increment("forward")
         return self[-1](*args, **kwargs)
 
-    def update(self, *args, **kwargs) -> None:  # type: ignore
-        """Updates the current metric being tracked."""
+    def update(self, *args: Any, **kwargs: Any) -> None:
+        """Update the current metric being tracked."""
         self._check_for_increment("update")
         self[-1].update(*args, **kwargs)
 
     def compute(self) -> Any:
         """Call compute of the current metric being tracked."""
         self._check_for_increment("compute")
         return self[-1].compute()
 
-    def compute_all(self) -> Tensor:
-        """Compute the metric value for all tracked metrics."""
+    def compute_all(self) -> Any:
+        """Compute the metric value for all tracked metrics.
+
+        Return:
+            By default will try stacking the results from all increaments into a single tensor if the tracked base
+            object is a single metric. If a metric collection is provided a dict of stacked tensors will be returned.
+            If the stacking process fails a list of the computed results will be returned.
+
+        Raises:
+            ValueError:
+                If `self.increment` have not been called before this method is called.
+        """
         self._check_for_increment("compute_all")
         # The i!=0 accounts for the self._base_metric should be ignored
         res = [metric.compute() for i, metric in enumerate(self) if i != 0]
-        if isinstance(self._base_metric, MetricCollection):
-            keys = res[0].keys()
-            return {k: torch.stack([r[k] for r in res], dim=0) for k in keys}
-        return torch.stack(res, dim=0)
+        try:
+            if isinstance(res[0], dict):
+                keys = res[0].keys()
+                return {k: torch.stack([r[k] for r in res], dim=0) for k in keys}
+            if isinstance(res[0], list):
+                return torch.stack([torch.stack(r, dim=0) for r in res], 0)
+            return torch.stack(res, dim=0)
+        except TypeError:  # fallback solution to just return as it is if we cannot succesfully stack
+            return res
 
     def reset(self) -> None:
-        """Resets the current metric being tracked."""
+        """Reset the current metric being tracked."""
         self[-1].reset()
 
     def reset_all(self) -> None:
-        """Resets all metrics being tracked."""
+        """Reset all metrics being tracked."""
         for metric in self:
             metric.reset()
 
     def best_metric(
         self, return_step: bool = False
     ) -> Union[
         None,
         float,
-        Tuple[int, float],
+        Tuple[float, int],
         Tuple[None, None],
         Dict[str, Union[float, None]],
-        Tuple[Dict[str, Union[int, None]], Dict[str, Union[float, None]]],
+        Tuple[Dict[str, Union[float, None]], Dict[str, Union[int, None]]],
     ]:
-        """Returns the highest metric out of all tracked.
+        """Return the highest metric out of all tracked.
 
         Args:
             return_step: If ``True`` will also return the step with the highest metric value.
 
         Returns:
-            The best metric value, and optionally the time-step.
+            Either a single value or a tuple, depends on the value of ``return_step`` and the object being tracked.
+
+            - If a single metric is being tracked and ``return_step=False`` then a single tensor will be returned
+            - If a single metric is being tracked and ``return_step=True`` then a 2-element tuple will be returned,
+              where the first value is optimal value and second value is the corresponding optimal step
+            - If a metric collection is being tracked and ``return_step=False`` then a single dict will be returned,
+              where keys correspond to the different values of the collection and the values are the optimal metric
+              value
+            - If a metric collection is being bracked and ``return_step=True`` then a 2-element tuple will be returned
+              where each is a dict, with keys corresponding to the different values of th collection and the values
+              of the first dict being the optimal values and the values of the second dict being the optimal step
+
+            In addtion the value in all cases may be ``None`` if the underlying metric does have a proper defined way
+            of being optimal or in the case where a nested structure of metrics are being tracked.
         """
+        res = self.compute_all()
+        if isinstance(res, list):
+            rank_zero_warn(
+                "Encounted nested structure. You are probably using a metric collection inside a metric collection, or"
+                " a metric wrapper inside a metric collection, which is not supported by `.best_metric()` method."
+                "Returning `None` instead. Please consider "
+            )
+            if return_step:
+                return None, None
+            return None
+
         if isinstance(self._base_metric, Metric):
             fn = torch.max if self.maximize else torch.min
             try:
-                idx, best = fn(self.compute_all(), 0)
+                value, idx = fn(res, 0)
                 if return_step:
-                    return idx.item(), best.item()
-                return best.item()
-            except ValueError as error:
-                warnings.warn(
+                    return value.item(), idx.item()
+                return value.item()
+            except (ValueError, RuntimeError) as error:
+                rank_zero_warn(
                     f"Encountered the following error when trying to get the best metric: {error}"
                     "this is probably due to the 'best' not being defined for this metric."
                     "Returning `None` instead.",
                     UserWarning,
                 )
                 if return_step:
                     return None, None
                 return None
 
         else:  # this is a metric collection
-            res = self.compute_all()
             maximize = self.maximize if isinstance(self.maximize, list) else len(res) * [self.maximize]
-            idx, best = {}, {}
+            value, idx = {}, {}
             for i, (k, v) in enumerate(res.items()):
                 try:
                     fn = torch.max if maximize[i] else torch.min
                     out = fn(v, 0)
-                    idx[k], best[k] = out[0].item(), out[1].item()
-                except ValueError as error:
-                    warnings.warn(
+                    value[k], idx[k] = out[0].item(), out[1].item()
+                except (ValueError, RuntimeError) as error:
+                    rank_zero_warn(
                         f"Encountered the following error when trying to get the best metric for metric {k}:"
                         f"{error} this is probably due to the 'best' not being defined for this metric."
                         "Returning `None` instead.",
                         UserWarning,
                     )
-                    idx[k], best[k] = None, None
+                    value[k], idx[k] = None, None
 
             if return_step:
-                return idx, best
-            return best
+                return value, idx
+            return value
 
     def _check_for_increment(self, method: str) -> None:
+        """Check that a metric that can be updated/used for computations has been intialized."""
         if not self._increment_called:
-            raise ValueError(f"`{method}` cannot be called before `.increment()` has been called")
+            raise ValueError(f"`{method}` cannot be called before `.increment()` has been called.")
+
+    def plot(
+        self, val: Optional[Union[Tensor, Sequence[Tensor]]] = None, ax: Optional[_AX_TYPE] = None
+    ) -> _PLOT_OUT_TYPE:
+        """Plot a single or multiple values from the metric.
+
+        Args:
+            val: Either a single result from calling `metric.forward` or `metric.compute` or a list of these results.
+                If no value is provided, will automatically call `metric.compute` and plot that result.
+            ax: An matplotlib axis object. If provided will add plot to that axis
+
+        Returns:
+            Figure and Axes object
+
+        Raises:
+            ModuleNotFoundError:
+                If `matplotlib` is not installed
+
+        .. plot::
+            :scale: 75
+
+            >>> # Example plotting a single value
+            >>> import torch
+            >>> from torchmetrics.wrappers import MetricTracker
+            >>> from torchmetrics.classification import BinaryAccuracy
+            >>> tracker = MetricTracker(BinaryAccuracy())
+            >>> for epoch in range(5):
+            ...     tracker.increment()
+            ...     for batch_idx in range(5):
+            ...         tracker.update(torch.randint(2, (10,)), torch.randint(2, (10,)))
+            >>> fig_, ax_ = tracker.plot()  # plot all epochs
+
+        """
+        val = val if val is not None else self.compute_all()
+        fig, ax = plot_single_or_multi_val(
+            val,
+            ax=ax,
+            name=self.__class__.__name__,
+        )
+        return fig, ax
```

### Comparing `torchmetrics-0.9.3/torchmetrics.egg-info/PKG-INFO` & `torchmetrics-1.0.0rc0/src/torchmetrics.egg-info/PKG-INFO`

 * *Files 9% similar despite different names*

```diff
@@ -1,81 +1,83 @@
 Metadata-Version: 2.1
 Name: torchmetrics
-Version: 0.9.3
+Version: 1.0.0rc0
 Summary: PyTorch native Metrics
-Home-page: https://github.com/Lightning-AI/metrics
-Download-URL: https://github.com/Lightning-AI/metrics/archive/master.zip
+Home-page: https://github.com/Lightning-AI/torchmetrics
+Download-URL: https://github.com/Lightning-AI/torchmetrics/archive/master.zip
 Author: Lightning-AI et al.
 Author-email: name@pytorchlightning.ai
 License: Apache-2.0
-Project-URL: Bug Tracker, https://github.com/Lightning-AI/metrics/issues
+Project-URL: Bug Tracker, https://github.com/Lightning-AI/torchmetrics/issues
 Project-URL: Documentation, https://torchmetrics.rtfd.io/en/latest/
-Project-URL: Source Code, https://github.com/Lightning-AI/metrics
+Project-URL: Source Code, https://github.com/Lightning-AI/torchmetrics
 Keywords: deep learning,machine learning,pytorch,metrics,AI
 Classifier: Environment :: Console
 Classifier: Natural Language :: English
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Intended Audience :: Developers
 Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
 Classifier: Topic :: Scientific/Engineering :: Image Recognition
 Classifier: Topic :: Scientific/Engineering :: Information Analysis
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
 Classifier: Programming Language :: Python :: 3
-Classifier: Programming Language :: Python :: 3.7
 Classifier: Programming Language :: Python :: 3.8
 Classifier: Programming Language :: Python :: 3.9
-Requires-Python: >=3.7
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
+Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 Provides-Extra: audio
 Provides-Extra: detection
-Provides-Extra: docs
 Provides-Extra: image
-Provides-Extra: integrate
+Provides-Extra: multimodal
 Provides-Extra: test
 Provides-Extra: text
+Provides-Extra: typing
+Provides-Extra: visual
 Provides-Extra: all
+Provides-Extra: dev
 License-File: LICENSE
 
 <div align="center">
 
-<img src="https://github.com/Lightning-AI/metrics/raw/v0.9.3/docs/source/_static/images/logo.png" width="400px">
+<img src="https://github.com/Lightning-AI/torchmetrics/raw/v1.0.0.rc0/docs/source/_static/images/logo.png" width="400px">
 
 **Machine learning metrics for distributed, scalable PyTorch applications.**
 
 ______________________________________________________________________
 
 <p align="center">
   <a href="#what-is-torchmetrics">What is Torchmetrics</a> 
   <a href="#implementing-your-own-module-metric">Implementing a metric</a> 
   <a href="#build-in-metrics">Built-in metrics</a> 
-  <a href="https://torchmetrics.readthedocs.io/en/v0.9.3">Docs</a> 
+  <a href="https://torchmetrics.readthedocs.io/en/v1.0.0.rc0">Docs</a> 
   <a href="#community">Community</a> 
   <a href="#license">License</a>
 </p>
 
 ______________________________________________________________________
 
 [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/torchmetrics)](https://pypi.org/project/torchmetrics/)
 [![PyPI Status](https://badge.fury.io/py/torchmetrics.svg)](https://badge.fury.io/py/torchmetrics)
 [![PyPI Status](https://pepy.tech/badge/torchmetrics)](https://pepy.tech/project/torchmetrics)
 [![Conda](https://img.shields.io/conda/v/conda-forge/torchmetrics?label=conda&color=success)](https://anaconda.org/conda-forge/torchmetrics)
 ![Conda](https://img.shields.io/conda/dn/conda-forge/torchmetrics)
-[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/Lightning-AI/metrics/blob/master/LICENSE)
+[![license](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://github.com/Lightning-AI/torchmetrics/blob/master/LICENSE)
 
-[![CI testing - complete](https://github.com/Lightning-AI/metrics/actions/workflows/ci_test-full.yml/badge.svg?event=push)](https://github.com/Lightning-AI/metrics/actions/workflows/ci_test-full.yml)
-[![PyTorch & Conda](https://github.com/Lightning-AI/metrics/actions/workflows/ci_test-conda.yml/badge.svg?tag=v0.9.3)](https://github.com/Lightning-AI/metrics/actions/workflows/ci_test-conda.yml)
-[![Build Status](https://dev.azure.com/Lightning-AI/Metrics/_apis/build/status/Lightning-AI.metrics?branchName=refs%2Ftags%2Fv0.9.3)](https://dev.azure.com/Lightning-AI/Metrics/_build/latest?definitionId=2&branchName=refs%2Ftags%2Fv0.9.3)
-[![codecov](https://codecov.io/gh/Lightning-AI/metrics/release/v0.9.3/graph/badge.svg?token=NER6LPI3HS)](https://codecov.io/gh/Lightning-AI/metrics)
+[![CI testing - complete](https://github.com/Lightning-AI/torchmetrics/actions/workflows/ci-tests-full.yml/badge.svg?event=push)](https://github.com/Lightning-AI/torchmetrics/actions/workflows/ci-tests-full.yml)
+[![Build Status](https://dev.azure.com/Lightning-AI/Metrics/_apis/build/status%2FTM.unittests?branchName=refs%2Ftags%2Fv1.0.0.rc0)](https://dev.azure.com/Lightning-AI/Metrics/_build/latest?definitionId=2&branchName=refs%2Ftags%2Fv1.0.0.rc0)
+[![codecov](https://codecov.io/gh/Lightning-AI/torchmetrics/release/v1.0.0.rc0/graph/badge.svg?token=NER6LPI3HS)](https://codecov.io/gh/Lightning-AI/torchmetrics)
+[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/Lightning-AI/torchmetrics/master.svg)](https://results.pre-commit.ci/latest/github/Lightning-AI/torchmetrics/master)
 
-[![Slack](https://img.shields.io/badge/slack-chat-green.svg?logo=slack)](https://www.pytorchlightning.ai/community)
 [![Documentation Status](https://readthedocs.org/projects/torchmetrics/badge/?version=latest)](https://torchmetrics.readthedocs.io/en/latest/?badge=latest)
+[![Discord](https://img.shields.io/discord/1077906959069626439?style=plastic)](https://discord.gg/VptPCZkGNa)
 [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5844769.svg)](https://doi.org/10.5281/zenodo.5844769)
 [![JOSS status](https://joss.theoj.org/papers/561d9bb59b400158bc8204e2639dca43/status.svg)](https://joss.theoj.org/papers/561d9bb59b400158bc8204e2639dca43)
-[![pre-commit.ci status](https://results.pre-commit.ci/badge/github/Lightning-AI/metrics/master.svg)](https://results.pre-commit.ci/latest/github/Lightning-AI/metrics/master)
 
 ______________________________________________________________________
 
 </div>
 
 ## Installation
 
@@ -94,45 +96,45 @@
 conda install -c conda-forge torchmetrics
 ```
 
 Pip from source
 
 ```bash
 # with git
-pip install git+https://github.com/Lightning-AI/metrics.git@release/latest
+pip install git+https://github.com/Lightning-AI/torchmetrics.git@release/stable
 ```
 
 Pip from archive
 
 ```bash
-pip install https://github.com/Lightning-AI/metrics/archive/refs/heads/release/latest.zip
+pip install https://github.com/Lightning-AI/torchmetrics/archive/refs/heads/release/stable.zip
 ```
 
 Extra dependencies for specialized metrics:
 
 ```bash
 pip install torchmetrics[audio]
 pip install torchmetrics[image]
 pip install torchmetrics[text]
 pip install torchmetrics[all]  # install all of the above
 ```
 
 Install latest developer version
 
 ```bash
-pip install https://github.com/Lightning-AI/metrics/archive/master.zip
+pip install https://github.com/Lightning-AI/torchmetrics/archive/master.zip
 ```
 
 </details>
 
 ______________________________________________________________________
 
 ## What is TorchMetrics
 
-TorchMetrics is a collection of 80+ PyTorch metrics implementations and an easy-to-use API to create custom metrics. It offers:
+TorchMetrics is a collection of 100+ PyTorch metrics implementations and an easy-to-use API to create custom metrics. It offers:
 
 - A standardized interface to increase reproducibility
 - Reduces boilerplate
 - Automatic accumulation over batches
 - Metrics optimized for distributed-training
 - Automatic synchronization between multiple devices
 
@@ -158,15 +160,15 @@
 ```python
 import torch
 
 # import our library
 import torchmetrics
 
 # initialize metric
-metric = torchmetrics.Accuracy()
+metric = torchmetrics.classification.Accuracy(task="multiclass", num_classes=5)
 
 # move the metric to device you want computations to take place
 device = "cuda" if torch.cuda.is_available() else "cpu"
 metric.to(device)
 
 n_batches = 10
 for i in range(n_batches):
@@ -204,30 +206,29 @@
     os.environ["MASTER_ADDR"] = "localhost"
     os.environ["MASTER_PORT"] = "12355"
 
     # create default process group
     dist.init_process_group("gloo", rank=rank, world_size=world_size)
 
     # initialize model
-    metric = torchmetrics.Accuracy()
+    metric = torchmetrics.classification.Accuracy(task="multiclass", num_classes=5)
 
     # define a model and append your metric to it
     # this allows metric states to be placed on correct accelerators when
     # .to(device) is called on the model
     model = nn.Linear(10, 10)
     model.metric = metric
     model = model.to(rank)
 
     # initialize DDP
     model = DDP(model, device_ids=[rank])
 
     n_epochs = 5
     # this shows iteration over multiple training epochs
     for n in range(n_epochs):
-
         # this will be replaced by a DataLoader with a DistributedSampler
         n_batches = 10
         for i in range(n_batches):
             # simulate a classification problem
             preds = torch.randn(10, 5).softmax(dim=-1)
             target = torch.randint(5, (10,))
 
@@ -298,47 +299,96 @@
 # import our library
 import torchmetrics
 
 # simulate a classification problem
 preds = torch.randn(10, 5).softmax(dim=-1)
 target = torch.randint(5, (10,))
 
-acc = torchmetrics.functional.accuracy(preds, target)
+acc = torchmetrics.functional.classification.multiclass_accuracy(
+    preds, target, num_classes=5
+)
 ```
 
 ### Covered domains and example metrics
 
-We currently have implemented metrics within the following domains:
+In total TorchMetrics contains [100+ metrics](https://torchmetrics.readthedocs.io/en/v1.0.0.rc0all-metrics.html), which
+convers the following domains:
 
 - Audio
 - Classification
 - Detection
 - Information Retrieval
 - Image
+- Multimodal (Image-Text)
+- Nominal
 - Regression
 - Text
 
-In total TorchMetrics contains [80+ metrics](https://torchmetrics.readthedocs.io/en/v0.9.3all-metrics.html)!
+Each domain may require some additional dependencies which can be installed with `pip install torchmetrics[audio]`,
+`pip install torchmetrics['image']` etc.
+
+### Additional features
+
+#### Plotting
+
+Visualization of metrics can be important to help understand what is going on with your machine learning algorithms.
+Torchmetrics have build-in plotting support (install dependencies with `pip install torchmetrics[visual]`) for nearly
+all modular metrics through the `.plot` method. Simply call the method to get a simple visualization of any metric!
+
+```python
+import torch
+from torchmetrics.classification import MulticlassAccuracy, MulticlassConfusionMatrix
+
+num_classes = 3
+
+# this will generate two distributions that comes more similar as iterations increase
+w = torch.randn(num_classes)
+target = lambda it: torch.multinomial((it * w).softmax(dim=-1), 100, replacement=True)
+preds = lambda it: torch.multinomial((it * w).softmax(dim=-1), 100, replacement=True)
+
+acc = MulticlassAccuracy(num_classes=num_classes, average="micro")
+acc_per_class = MulticlassAccuracy(num_classes=num_classes, average=None)
+confmat = MulticlassConfusionMatrix(num_classes=num_classes)
+
+# plot single value
+for i in range(5):
+    acc_per_class.update(preds(i), target(i))
+    confmat.update(preds(i), target(i))
+fig1, ax1 = acc_per_class.plot()
+fig2, ax2 = confmat.plot()
+
+# plot multiple values
+values = []
+for i in range(10):
+    values.append(acc(preds(i), target(i)))
+fig3, ax3 = acc.plot(values)
+```
+
+<p align="center">
+  <img src="https://github.com/Lightning-AI/torchmetrics/raw/v1.0.0.rc0/docs/source/_static/images/plot_example.png" width="1000">
+</p>
+
+For examples of plotting different metrics try running [this example file](examples/plotting.py).
 
 ## Contribute!
 
 The lightning + TorchMetrics team is hard at work adding even more metrics.
 But we're looking for incredible contributors like you to submit new metrics
 and improve existing ones!
 
-Join our [Slack](https://www.pytorchlightning.ai/community) to get help become a contributor!
+Join our [Slack](https://www.pytorchlightning.ai/community) to get help with becoming a contributor!
 
 ## Community
 
 For help or questions, join our huge community on [Slack](https://www.pytorchlightning.ai/community)!
 
 ## Citation
 
 Were excited to continue the strong legacy of open source software and have been inspired
 over the years by Caffe, Theano, Keras, PyTorch, torchbearer, ignite, sklearn and fast.ai.
 
-If you want to cite this framework feel free to use GitHub's built-in citation option to generate a bibtex or APA-Style citation based on [this file](https://github.com/Lightning-AI/metrics/blob/master/CITATION.cff) (but only if you loved it ).
+If you want to cite this framework feel free to use GitHub's built-in citation option to generate a bibtex or APA-Style citation based on [this file](https://github.com/Lightning-AI/torchmetrics/blob/master/CITATION.cff) (but only if you loved it ).
 
 ## License
 
 Please observe the Apache 2.0 license that is listed in this repository.
 In addition, the Lightning framework is Patent Pending.
```

### Comparing `torchmetrics-0.9.3/torchmetrics.egg-info/SOURCES.txt` & `torchmetrics-1.0.0rc0/src/torchmetrics.egg-info/SOURCES.txt`

 * *Files 27% similar despite different names*

```diff
@@ -1,211 +1,285 @@
 CHANGELOG.md
 CITATION.cff
 LICENSE
 MANIFEST.in
 README.md
 requirements.txt
-setup.cfg
 setup.py
 requirements/audio.txt
 requirements/audio_test.txt
+requirements/classification_test.txt
 requirements/detection.txt
 requirements/detection_test.txt
 requirements/devel.txt
 requirements/docs.txt
+requirements/doctest.txt
 requirements/image.txt
 requirements/image_test.txt
 requirements/integrate.txt
+requirements/multimodal.txt
+requirements/nominal_test.txt
 requirements/test.txt
 requirements/text.txt
 requirements/text_test.txt
-tm_examples/bert_score-own_model.py
-tm_examples/detection_map.py
-tm_examples/rouge_score-own_normalizer_and_tokenizer.py
-torchmetrics/__about__.py
-torchmetrics/__init__.py
-torchmetrics/aggregation.py
-torchmetrics/collections.py
-torchmetrics/metric.py
-torchmetrics/py.typed
-torchmetrics/setup_tools.py
-torchmetrics.egg-info/PKG-INFO
-torchmetrics.egg-info/SOURCES.txt
-torchmetrics.egg-info/dependency_links.txt
-torchmetrics.egg-info/not-zip-safe
-torchmetrics.egg-info/requires.txt
-torchmetrics.egg-info/top_level.txt
-torchmetrics/audio/__init__.py
-torchmetrics/audio/pesq.py
-torchmetrics/audio/pit.py
-torchmetrics/audio/sdr.py
-torchmetrics/audio/snr.py
-torchmetrics/audio/stoi.py
-torchmetrics/classification/__init__.py
-torchmetrics/classification/accuracy.py
-torchmetrics/classification/auc.py
-torchmetrics/classification/auroc.py
-torchmetrics/classification/avg_precision.py
-torchmetrics/classification/binned_precision_recall.py
-torchmetrics/classification/calibration_error.py
-torchmetrics/classification/cohen_kappa.py
-torchmetrics/classification/confusion_matrix.py
-torchmetrics/classification/dice.py
-torchmetrics/classification/f_beta.py
-torchmetrics/classification/hamming.py
-torchmetrics/classification/hinge.py
-torchmetrics/classification/jaccard.py
-torchmetrics/classification/kl_divergence.py
-torchmetrics/classification/matthews_corrcoef.py
-torchmetrics/classification/precision_recall.py
-torchmetrics/classification/precision_recall_curve.py
-torchmetrics/classification/ranking.py
-torchmetrics/classification/roc.py
-torchmetrics/classification/specificity.py
-torchmetrics/classification/stat_scores.py
-torchmetrics/detection/__init__.py
-torchmetrics/detection/mean_ap.py
-torchmetrics/functional/__init__.py
-torchmetrics/functional/audio/__init__.py
-torchmetrics/functional/audio/pesq.py
-torchmetrics/functional/audio/pit.py
-torchmetrics/functional/audio/sdr.py
-torchmetrics/functional/audio/snr.py
-torchmetrics/functional/audio/stoi.py
-torchmetrics/functional/classification/__init__.py
-torchmetrics/functional/classification/accuracy.py
-torchmetrics/functional/classification/auc.py
-torchmetrics/functional/classification/auroc.py
-torchmetrics/functional/classification/average_precision.py
-torchmetrics/functional/classification/calibration_error.py
-torchmetrics/functional/classification/cohen_kappa.py
-torchmetrics/functional/classification/confusion_matrix.py
-torchmetrics/functional/classification/dice.py
-torchmetrics/functional/classification/f_beta.py
-torchmetrics/functional/classification/hamming.py
-torchmetrics/functional/classification/hinge.py
-torchmetrics/functional/classification/jaccard.py
-torchmetrics/functional/classification/kl_divergence.py
-torchmetrics/functional/classification/matthews_corrcoef.py
-torchmetrics/functional/classification/precision_recall.py
-torchmetrics/functional/classification/precision_recall_curve.py
-torchmetrics/functional/classification/ranking.py
-torchmetrics/functional/classification/roc.py
-torchmetrics/functional/classification/specificity.py
-torchmetrics/functional/classification/stat_scores.py
-torchmetrics/functional/image/__init__.py
-torchmetrics/functional/image/d_lambda.py
-torchmetrics/functional/image/ergas.py
-torchmetrics/functional/image/gradients.py
-torchmetrics/functional/image/helper.py
-torchmetrics/functional/image/psnr.py
-torchmetrics/functional/image/sam.py
-torchmetrics/functional/image/ssim.py
-torchmetrics/functional/image/uqi.py
-torchmetrics/functional/pairwise/__init__.py
-torchmetrics/functional/pairwise/cosine.py
-torchmetrics/functional/pairwise/euclidean.py
-torchmetrics/functional/pairwise/helpers.py
-torchmetrics/functional/pairwise/linear.py
-torchmetrics/functional/pairwise/manhattan.py
-torchmetrics/functional/regression/__init__.py
-torchmetrics/functional/regression/cosine_similarity.py
-torchmetrics/functional/regression/explained_variance.py
-torchmetrics/functional/regression/log_mse.py
-torchmetrics/functional/regression/mae.py
-torchmetrics/functional/regression/mape.py
-torchmetrics/functional/regression/mse.py
-torchmetrics/functional/regression/pearson.py
-torchmetrics/functional/regression/r2.py
-torchmetrics/functional/regression/spearman.py
-torchmetrics/functional/regression/symmetric_mape.py
-torchmetrics/functional/regression/tweedie_deviance.py
-torchmetrics/functional/regression/wmape.py
-torchmetrics/functional/retrieval/__init__.py
-torchmetrics/functional/retrieval/average_precision.py
-torchmetrics/functional/retrieval/fall_out.py
-torchmetrics/functional/retrieval/hit_rate.py
-torchmetrics/functional/retrieval/ndcg.py
-torchmetrics/functional/retrieval/precision.py
-torchmetrics/functional/retrieval/precision_recall_curve.py
-torchmetrics/functional/retrieval/r_precision.py
-torchmetrics/functional/retrieval/recall.py
-torchmetrics/functional/retrieval/reciprocal_rank.py
-torchmetrics/functional/text/__init__.py
-torchmetrics/functional/text/bert.py
-torchmetrics/functional/text/bleu.py
-torchmetrics/functional/text/cer.py
-torchmetrics/functional/text/chrf.py
-torchmetrics/functional/text/eed.py
-torchmetrics/functional/text/helper.py
-torchmetrics/functional/text/mer.py
-torchmetrics/functional/text/rouge.py
-torchmetrics/functional/text/sacre_bleu.py
-torchmetrics/functional/text/squad.py
-torchmetrics/functional/text/ter.py
-torchmetrics/functional/text/wer.py
-torchmetrics/functional/text/wil.py
-torchmetrics/functional/text/wip.py
-torchmetrics/image/__init__.py
-torchmetrics/image/d_lambda.py
-torchmetrics/image/ergas.py
-torchmetrics/image/fid.py
-torchmetrics/image/inception.py
-torchmetrics/image/kid.py
-torchmetrics/image/lpip.py
-torchmetrics/image/psnr.py
-torchmetrics/image/sam.py
-torchmetrics/image/ssim.py
-torchmetrics/image/uqi.py
-torchmetrics/regression/__init__.py
-torchmetrics/regression/cosine_similarity.py
-torchmetrics/regression/explained_variance.py
-torchmetrics/regression/log_mse.py
-torchmetrics/regression/mae.py
-torchmetrics/regression/mape.py
-torchmetrics/regression/mse.py
-torchmetrics/regression/pearson.py
-torchmetrics/regression/r2.py
-torchmetrics/regression/spearman.py
-torchmetrics/regression/symmetric_mape.py
-torchmetrics/regression/tweedie_deviance.py
-torchmetrics/regression/wmape.py
-torchmetrics/retrieval/__init__.py
-torchmetrics/retrieval/average_precision.py
-torchmetrics/retrieval/base.py
-torchmetrics/retrieval/fall_out.py
-torchmetrics/retrieval/hit_rate.py
-torchmetrics/retrieval/ndcg.py
-torchmetrics/retrieval/precision.py
-torchmetrics/retrieval/precision_recall_curve.py
-torchmetrics/retrieval/r_precision.py
-torchmetrics/retrieval/recall.py
-torchmetrics/retrieval/reciprocal_rank.py
-torchmetrics/text/__init__.py
-torchmetrics/text/bert.py
-torchmetrics/text/bleu.py
-torchmetrics/text/cer.py
-torchmetrics/text/chrf.py
-torchmetrics/text/eed.py
-torchmetrics/text/mer.py
-torchmetrics/text/rouge.py
-torchmetrics/text/sacre_bleu.py
-torchmetrics/text/squad.py
-torchmetrics/text/ter.py
-torchmetrics/text/wer.py
-torchmetrics/text/wil.py
-torchmetrics/text/wip.py
-torchmetrics/utilities/__init__.py
-torchmetrics/utilities/checks.py
-torchmetrics/utilities/compute.py
-torchmetrics/utilities/data.py
-torchmetrics/utilities/distributed.py
-torchmetrics/utilities/enums.py
-torchmetrics/utilities/exceptions.py
-torchmetrics/utilities/imports.py
-torchmetrics/utilities/prints.py
-torchmetrics/wrappers/__init__.py
-torchmetrics/wrappers/bootstrapping.py
-torchmetrics/wrappers/classwise.py
-torchmetrics/wrappers/minmax.py
-torchmetrics/wrappers/multioutput.py
-torchmetrics/wrappers/tracker.py
+requirements/typing.txt
+requirements/visual.txt
+src/torchmetrics/__about__.py
+src/torchmetrics/__init__.py
+src/torchmetrics/aggregation.py
+src/torchmetrics/collections.py
+src/torchmetrics/metric.py
+src/torchmetrics/py.typed
+src/torchmetrics.egg-info/PKG-INFO
+src/torchmetrics.egg-info/SOURCES.txt
+src/torchmetrics.egg-info/dependency_links.txt
+src/torchmetrics.egg-info/not-zip-safe
+src/torchmetrics.egg-info/requires.txt
+src/torchmetrics.egg-info/top_level.txt
+src/torchmetrics/audio/__init__.py
+src/torchmetrics/audio/_deprecated.py
+src/torchmetrics/audio/pesq.py
+src/torchmetrics/audio/pit.py
+src/torchmetrics/audio/sdr.py
+src/torchmetrics/audio/snr.py
+src/torchmetrics/audio/stoi.py
+src/torchmetrics/classification/__init__.py
+src/torchmetrics/classification/accuracy.py
+src/torchmetrics/classification/auroc.py
+src/torchmetrics/classification/average_precision.py
+src/torchmetrics/classification/calibration_error.py
+src/torchmetrics/classification/cohen_kappa.py
+src/torchmetrics/classification/confusion_matrix.py
+src/torchmetrics/classification/dice.py
+src/torchmetrics/classification/exact_match.py
+src/torchmetrics/classification/f_beta.py
+src/torchmetrics/classification/group_fairness.py
+src/torchmetrics/classification/hamming.py
+src/torchmetrics/classification/hinge.py
+src/torchmetrics/classification/jaccard.py
+src/torchmetrics/classification/matthews_corrcoef.py
+src/torchmetrics/classification/precision_fixed_recall.py
+src/torchmetrics/classification/precision_recall.py
+src/torchmetrics/classification/precision_recall_curve.py
+src/torchmetrics/classification/ranking.py
+src/torchmetrics/classification/recall_fixed_precision.py
+src/torchmetrics/classification/roc.py
+src/torchmetrics/classification/specificity.py
+src/torchmetrics/classification/specificity_sensitivity.py
+src/torchmetrics/classification/stat_scores.py
+src/torchmetrics/detection/__init__.py
+src/torchmetrics/detection/_deprecated.py
+src/torchmetrics/detection/ciou.py
+src/torchmetrics/detection/diou.py
+src/torchmetrics/detection/giou.py
+src/torchmetrics/detection/helpers.py
+src/torchmetrics/detection/iou.py
+src/torchmetrics/detection/mean_ap.py
+src/torchmetrics/detection/panoptic_qualities.py
+src/torchmetrics/functional/__init__.py
+src/torchmetrics/functional/audio/__init__.py
+src/torchmetrics/functional/audio/_deprecated.py
+src/torchmetrics/functional/audio/pesq.py
+src/torchmetrics/functional/audio/pit.py
+src/torchmetrics/functional/audio/sdr.py
+src/torchmetrics/functional/audio/snr.py
+src/torchmetrics/functional/audio/stoi.py
+src/torchmetrics/functional/classification/__init__.py
+src/torchmetrics/functional/classification/accuracy.py
+src/torchmetrics/functional/classification/auroc.py
+src/torchmetrics/functional/classification/average_precision.py
+src/torchmetrics/functional/classification/calibration_error.py
+src/torchmetrics/functional/classification/cohen_kappa.py
+src/torchmetrics/functional/classification/confusion_matrix.py
+src/torchmetrics/functional/classification/dice.py
+src/torchmetrics/functional/classification/exact_match.py
+src/torchmetrics/functional/classification/f_beta.py
+src/torchmetrics/functional/classification/group_fairness.py
+src/torchmetrics/functional/classification/hamming.py
+src/torchmetrics/functional/classification/hinge.py
+src/torchmetrics/functional/classification/jaccard.py
+src/torchmetrics/functional/classification/matthews_corrcoef.py
+src/torchmetrics/functional/classification/precision_fixed_recall.py
+src/torchmetrics/functional/classification/precision_recall.py
+src/torchmetrics/functional/classification/precision_recall_curve.py
+src/torchmetrics/functional/classification/ranking.py
+src/torchmetrics/functional/classification/recall_fixed_precision.py
+src/torchmetrics/functional/classification/roc.py
+src/torchmetrics/functional/classification/specificity.py
+src/torchmetrics/functional/classification/specificity_sensitivity.py
+src/torchmetrics/functional/classification/stat_scores.py
+src/torchmetrics/functional/detection/__init__.py
+src/torchmetrics/functional/detection/_deprecated.py
+src/torchmetrics/functional/detection/_panoptic_quality_common.py
+src/torchmetrics/functional/detection/ciou.py
+src/torchmetrics/functional/detection/diou.py
+src/torchmetrics/functional/detection/giou.py
+src/torchmetrics/functional/detection/iou.py
+src/torchmetrics/functional/detection/panoptic_qualities.py
+src/torchmetrics/functional/image/__init__.py
+src/torchmetrics/functional/image/_deprecated.py
+src/torchmetrics/functional/image/d_lambda.py
+src/torchmetrics/functional/image/ergas.py
+src/torchmetrics/functional/image/gradients.py
+src/torchmetrics/functional/image/helper.py
+src/torchmetrics/functional/image/lpips.py
+src/torchmetrics/functional/image/psnr.py
+src/torchmetrics/functional/image/psnrb.py
+src/torchmetrics/functional/image/rase.py
+src/torchmetrics/functional/image/rmse_sw.py
+src/torchmetrics/functional/image/sam.py
+src/torchmetrics/functional/image/ssim.py
+src/torchmetrics/functional/image/tv.py
+src/torchmetrics/functional/image/uqi.py
+src/torchmetrics/functional/image/lpips_models/alex.pth
+src/torchmetrics/functional/image/lpips_models/squeeze.pth
+src/torchmetrics/functional/image/lpips_models/vgg.pth
+src/torchmetrics/functional/multimodal/__init__.py
+src/torchmetrics/functional/multimodal/clip_score.py
+src/torchmetrics/functional/nominal/__init__.py
+src/torchmetrics/functional/nominal/cramers.py
+src/torchmetrics/functional/nominal/pearson.py
+src/torchmetrics/functional/nominal/theils_u.py
+src/torchmetrics/functional/nominal/tschuprows.py
+src/torchmetrics/functional/nominal/utils.py
+src/torchmetrics/functional/pairwise/__init__.py
+src/torchmetrics/functional/pairwise/cosine.py
+src/torchmetrics/functional/pairwise/euclidean.py
+src/torchmetrics/functional/pairwise/helpers.py
+src/torchmetrics/functional/pairwise/linear.py
+src/torchmetrics/functional/pairwise/manhattan.py
+src/torchmetrics/functional/pairwise/minkowski.py
+src/torchmetrics/functional/regression/__init__.py
+src/torchmetrics/functional/regression/concordance.py
+src/torchmetrics/functional/regression/cosine_similarity.py
+src/torchmetrics/functional/regression/explained_variance.py
+src/torchmetrics/functional/regression/kendall.py
+src/torchmetrics/functional/regression/kl_divergence.py
+src/torchmetrics/functional/regression/log_cosh.py
+src/torchmetrics/functional/regression/log_mse.py
+src/torchmetrics/functional/regression/mae.py
+src/torchmetrics/functional/regression/mape.py
+src/torchmetrics/functional/regression/minkowski.py
+src/torchmetrics/functional/regression/mse.py
+src/torchmetrics/functional/regression/pearson.py
+src/torchmetrics/functional/regression/r2.py
+src/torchmetrics/functional/regression/spearman.py
+src/torchmetrics/functional/regression/symmetric_mape.py
+src/torchmetrics/functional/regression/tweedie_deviance.py
+src/torchmetrics/functional/regression/utils.py
+src/torchmetrics/functional/regression/wmape.py
+src/torchmetrics/functional/retrieval/__init__.py
+src/torchmetrics/functional/retrieval/_deprecated.py
+src/torchmetrics/functional/retrieval/average_precision.py
+src/torchmetrics/functional/retrieval/fall_out.py
+src/torchmetrics/functional/retrieval/hit_rate.py
+src/torchmetrics/functional/retrieval/ndcg.py
+src/torchmetrics/functional/retrieval/precision.py
+src/torchmetrics/functional/retrieval/precision_recall_curve.py
+src/torchmetrics/functional/retrieval/r_precision.py
+src/torchmetrics/functional/retrieval/recall.py
+src/torchmetrics/functional/retrieval/reciprocal_rank.py
+src/torchmetrics/functional/text/__init__.py
+src/torchmetrics/functional/text/_deprecated.py
+src/torchmetrics/functional/text/bert.py
+src/torchmetrics/functional/text/bleu.py
+src/torchmetrics/functional/text/cer.py
+src/torchmetrics/functional/text/chrf.py
+src/torchmetrics/functional/text/eed.py
+src/torchmetrics/functional/text/helper.py
+src/torchmetrics/functional/text/helper_embedding_metric.py
+src/torchmetrics/functional/text/infolm.py
+src/torchmetrics/functional/text/mer.py
+src/torchmetrics/functional/text/perplexity.py
+src/torchmetrics/functional/text/rouge.py
+src/torchmetrics/functional/text/sacre_bleu.py
+src/torchmetrics/functional/text/squad.py
+src/torchmetrics/functional/text/ter.py
+src/torchmetrics/functional/text/wer.py
+src/torchmetrics/functional/text/wil.py
+src/torchmetrics/functional/text/wip.py
+src/torchmetrics/image/__init__.py
+src/torchmetrics/image/_deprecated.py
+src/torchmetrics/image/d_lambda.py
+src/torchmetrics/image/ergas.py
+src/torchmetrics/image/fid.py
+src/torchmetrics/image/inception.py
+src/torchmetrics/image/kid.py
+src/torchmetrics/image/lpip.py
+src/torchmetrics/image/psnr.py
+src/torchmetrics/image/psnrb.py
+src/torchmetrics/image/rase.py
+src/torchmetrics/image/rmse_sw.py
+src/torchmetrics/image/sam.py
+src/torchmetrics/image/ssim.py
+src/torchmetrics/image/tv.py
+src/torchmetrics/image/uqi.py
+src/torchmetrics/multimodal/__init__.py
+src/torchmetrics/multimodal/clip_score.py
+src/torchmetrics/nominal/__init__.py
+src/torchmetrics/nominal/cramers.py
+src/torchmetrics/nominal/pearson.py
+src/torchmetrics/nominal/theils_u.py
+src/torchmetrics/nominal/tschuprows.py
+src/torchmetrics/regression/__init__.py
+src/torchmetrics/regression/concordance.py
+src/torchmetrics/regression/cosine_similarity.py
+src/torchmetrics/regression/explained_variance.py
+src/torchmetrics/regression/kendall.py
+src/torchmetrics/regression/kl_divergence.py
+src/torchmetrics/regression/log_cosh.py
+src/torchmetrics/regression/log_mse.py
+src/torchmetrics/regression/mae.py
+src/torchmetrics/regression/mape.py
+src/torchmetrics/regression/minkowski.py
+src/torchmetrics/regression/mse.py
+src/torchmetrics/regression/pearson.py
+src/torchmetrics/regression/r2.py
+src/torchmetrics/regression/spearman.py
+src/torchmetrics/regression/symmetric_mape.py
+src/torchmetrics/regression/tweedie_deviance.py
+src/torchmetrics/regression/wmape.py
+src/torchmetrics/retrieval/__init__.py
+src/torchmetrics/retrieval/_deprecated.py
+src/torchmetrics/retrieval/average_precision.py
+src/torchmetrics/retrieval/base.py
+src/torchmetrics/retrieval/fall_out.py
+src/torchmetrics/retrieval/hit_rate.py
+src/torchmetrics/retrieval/ndcg.py
+src/torchmetrics/retrieval/precision.py
+src/torchmetrics/retrieval/precision_recall_curve.py
+src/torchmetrics/retrieval/r_precision.py
+src/torchmetrics/retrieval/recall.py
+src/torchmetrics/retrieval/reciprocal_rank.py
+src/torchmetrics/text/__init__.py
+src/torchmetrics/text/_deprecated.py
+src/torchmetrics/text/bert.py
+src/torchmetrics/text/bleu.py
+src/torchmetrics/text/cer.py
+src/torchmetrics/text/chrf.py
+src/torchmetrics/text/eed.py
+src/torchmetrics/text/infolm.py
+src/torchmetrics/text/mer.py
+src/torchmetrics/text/perplexity.py
+src/torchmetrics/text/rouge.py
+src/torchmetrics/text/sacre_bleu.py
+src/torchmetrics/text/squad.py
+src/torchmetrics/text/ter.py
+src/torchmetrics/text/wer.py
+src/torchmetrics/text/wil.py
+src/torchmetrics/text/wip.py
+src/torchmetrics/utilities/__init__.py
+src/torchmetrics/utilities/checks.py
+src/torchmetrics/utilities/compute.py
+src/torchmetrics/utilities/data.py
+src/torchmetrics/utilities/distributed.py
+src/torchmetrics/utilities/enums.py
+src/torchmetrics/utilities/exceptions.py
+src/torchmetrics/utilities/imports.py
+src/torchmetrics/utilities/plot.py
+src/torchmetrics/utilities/prints.py
+src/torchmetrics/wrappers/__init__.py
+src/torchmetrics/wrappers/bootstrapping.py
+src/torchmetrics/wrappers/classwise.py
+src/torchmetrics/wrappers/minmax.py
+src/torchmetrics/wrappers/multioutput.py
+src/torchmetrics/wrappers/tracker.py
```

