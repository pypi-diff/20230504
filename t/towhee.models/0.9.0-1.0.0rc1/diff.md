# Comparing `tmp/towhee.models-0.9.0-py3-none-any.whl.zip` & `tmp/towhee.models-1.0.0rc1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,282 +1,282 @@
-Zip file size: 1799566 bytes, number of entries: 280
--rw-r--r--  2.0 unx     8596 b- defN 22-Dec-02 13:07 towhee/models/README.md
--rw-r--r--  2.0 unx     8570 b- defN 22-Dec-02 13:07 towhee/models/README_CN.md
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/__init__.py
--rw-r--r--  2.0 unx      678 b- defN 22-Dec-02 13:07 towhee/models/acar_net/__init__.py
--rw-r--r--  2.0 unx    10305 b- defN 22-Dec-02 13:07 towhee/models/acar_net/backbone.py
--rw-r--r--  2.0 unx     9275 b- defN 22-Dec-02 13:07 towhee/models/acar_net/head.py
--rw-r--r--  2.0 unx     3613 b- defN 22-Dec-02 13:07 towhee/models/acar_net/model.py
--rw-r--r--  2.0 unx     4102 b- defN 22-Dec-02 13:07 towhee/models/acar_net/neck.py
--rw-r--r--  2.0 unx     1618 b- defN 22-Dec-02 13:07 towhee/models/acar_net/utils.py
--rw-r--r--  2.0 unx      709 b- defN 22-Dec-02 13:07 towhee/models/action_clip/__init__.py
--rw-r--r--  2.0 unx     4305 b- defN 22-Dec-02 13:07 towhee/models/action_clip/action_clip.py
--rw-r--r--  2.0 unx     2039 b- defN 22-Dec-02 13:07 towhee/models/action_clip/action_clip_utils.py
--rw-r--r--  2.0 unx     1804 b- defN 22-Dec-02 13:07 towhee/models/action_clip/text_prompt.py
--rw-r--r--  2.0 unx     8805 b- defN 22-Dec-02 13:07 towhee/models/action_clip/visual_prompt.py
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/allinone/__init__.py
--rw-r--r--  2.0 unx     1289 b- defN 22-Dec-02 13:07 towhee/models/allinone/allinone.py
--rw-r--r--  2.0 unx      622 b- defN 22-Dec-02 13:07 towhee/models/bridgeformer/__init__.py
--rw-r--r--  2.0 unx     3300 b- defN 22-Dec-02 13:07 towhee/models/bridgeformer/bridge_former.py
--rw-r--r--  2.0 unx    10421 b- defN 22-Dec-02 13:07 towhee/models/bridgeformer/bridge_former_training.py
--rw-r--r--  2.0 unx    16155 b- defN 22-Dec-02 13:07 towhee/models/bridgeformer/bridge_former_training_block.py
--rw-r--r--  2.0 unx     1560 b- defN 22-Dec-02 13:07 towhee/models/clip/README.md
--rw-r--r--  2.0 unx      639 b- defN 22-Dec-02 13:07 towhee/models/clip/__init__.py
--rw-r--r--  2.0 unx    20204 b- defN 22-Dec-02 13:07 towhee/models/clip/auxilary.py
--rw-r--r--  2.0 unx  1356917 b- defN 22-Dec-02 13:07 towhee/models/clip/bpe_simple_vocab_16e6.txt.gz
--rw-r--r--  2.0 unx    30031 b- defN 22-Dec-02 13:07 towhee/models/clip/clip.py
--rw-r--r--  2.0 unx     9935 b- defN 22-Dec-02 13:07 towhee/models/clip/clip_utils.py
--rw-r--r--  2.0 unx     5618 b- defN 22-Dec-02 13:07 towhee/models/clip/simple_tokenizer.py
--rw-r--r--  2.0 unx       46 b- defN 22-Dec-02 13:07 towhee/models/clip4clip/__init__.py
--rw-r--r--  2.0 unx    10131 b- defN 22-Dec-02 13:07 towhee/models/clip4clip/clip4clip.py
--rw-r--r--  2.0 unx     2605 b- defN 22-Dec-02 13:07 towhee/models/clip4clip/until_module.py
--rw-r--r--  2.0 unx     1836 b- defN 22-Dec-02 13:07 towhee/models/clip4clip/utils.py
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/coca/__init__.py
--rw-r--r--  2.0 unx    11514 b- defN 22-Dec-02 13:07 towhee/models/coca/coca.py
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/coformer/__init__.py
--rw-r--r--  2.0 unx     5167 b- defN 22-Dec-02 13:07 towhee/models/coformer/backbone.py
--rw-r--r--  2.0 unx    10222 b- defN 22-Dec-02 13:07 towhee/models/coformer/coformer.py
--rw-r--r--  2.0 unx    10008 b- defN 22-Dec-02 13:07 towhee/models/coformer/config.py
--rw-r--r--  2.0 unx    15471 b- defN 22-Dec-02 13:07 towhee/models/coformer/transformer.py
--rw-r--r--  2.0 unx     2919 b- defN 22-Dec-02 13:07 towhee/models/coformer/utils.py
--rw-r--r--  2.0 unx      630 b- defN 22-Dec-02 13:07 towhee/models/collaborative_experts/__init__.py
--rw-r--r--  2.0 unx    60484 b- defN 22-Dec-02 13:07 towhee/models/collaborative_experts/collaborative_experts.py
--rw-r--r--  2.0 unx     3587 b- defN 22-Dec-02 13:07 towhee/models/collaborative_experts/net_vlad.py
--rw-r--r--  2.0 unx     1612 b- defN 22-Dec-02 13:07 towhee/models/collaborative_experts/util.py
--rw-r--r--  2.0 unx      661 b- defN 22-Dec-02 13:07 towhee/models/convnext/__init__.py
--rw-r--r--  2.0 unx     3245 b- defN 22-Dec-02 13:07 towhee/models/convnext/configs.py
--rw-r--r--  2.0 unx     4425 b- defN 22-Dec-02 13:07 towhee/models/convnext/convnext.py
--rw-r--r--  2.0 unx     4107 b- defN 22-Dec-02 13:07 towhee/models/convnext/utils.py
--rw-r--r--  2.0 unx      614 b- defN 22-Dec-02 13:07 towhee/models/cvnet/__init__.py
--rw-r--r--  2.0 unx     4665 b- defN 22-Dec-02 13:07 towhee/models/cvnet/cvnet.py
--rw-r--r--  2.0 unx     6640 b- defN 22-Dec-02 13:07 towhee/models/cvnet/cvnet_block.py
--rw-r--r--  2.0 unx     2439 b- defN 22-Dec-02 13:07 towhee/models/cvnet/cvnet_utils.py
--rw-r--r--  2.0 unx     8106 b- defN 22-Dec-02 13:07 towhee/models/cvnet/resnet.py
--rw-r--r--  2.0 unx       19 b- defN 22-Dec-02 13:07 towhee/models/drl/__init__.py
--rw-r--r--  2.0 unx    34544 b- defN 22-Dec-02 13:07 towhee/models/drl/drl.py
--rw-r--r--  2.0 unx     7936 b- defN 22-Dec-02 13:07 towhee/models/drl/module_cross.py
--rw-r--r--  2.0 unx     4891 b- defN 22-Dec-02 13:07 towhee/models/drl/until_module.py
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/embedding/__init__.py
--rw-r--r--  2.0 unx     2061 b- defN 22-Dec-02 13:07 towhee/models/embedding/embedding_extractor.py
--rw-r--r--  2.0 unx      623 b- defN 22-Dec-02 13:07 towhee/models/frozen_in_time/__init__.py
--rw-r--r--  2.0 unx    14921 b- defN 22-Dec-02 13:07 towhee/models/frozen_in_time/frozen_in_time.py
--rw-r--r--  2.0 unx     2394 b- defN 22-Dec-02 13:07 towhee/models/frozen_in_time/frozen_utils.py
--rw-r--r--  2.0 unx    15910 b- defN 22-Dec-02 13:07 towhee/models/frozen_in_time/frozen_video_transformer.py
--rw-r--r--  2.0 unx      659 b- defN 22-Dec-02 13:07 towhee/models/hornet/__init__.py
--rw-r--r--  2.0 unx     4309 b- defN 22-Dec-02 13:07 towhee/models/hornet/configs.py
--rw-r--r--  2.0 unx     5036 b- defN 22-Dec-02 13:07 towhee/models/hornet/hornet.py
--rw-r--r--  2.0 unx     5983 b- defN 22-Dec-02 13:07 towhee/models/hornet/utils.py
--rw-r--r--  2.0 unx      612 b- defN 22-Dec-02 13:07 towhee/models/isc/__init__.py
--rw-r--r--  2.0 unx     3341 b- defN 22-Dec-02 13:07 towhee/models/isc/isc.py
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/layers/__init__.py
--rw-r--r--  2.0 unx     1638 b- defN 22-Dec-02 13:07 towhee/models/layers/aspp.py
--rw-r--r--  2.0 unx     6693 b- defN 22-Dec-02 13:07 towhee/models/layers/attention.py
--rw-r--r--  2.0 unx     5783 b- defN 22-Dec-02 13:07 towhee/models/layers/cond_conv2d.py
--rw-r--r--  2.0 unx     2706 b- defN 22-Dec-02 13:07 towhee/models/layers/conv2d_same.py
--rw-r--r--  2.0 unx     4180 b- defN 22-Dec-02 13:07 towhee/models/layers/conv2d_separable.py
--rw-r--r--  2.0 unx     4354 b- defN 22-Dec-02 13:07 towhee/models/layers/conv4d.py
--rw-r--r--  2.0 unx     5125 b- defN 22-Dec-02 13:07 towhee/models/layers/conv_bn_activation.py
--rw-r--r--  2.0 unx     1918 b- defN 22-Dec-02 13:07 towhee/models/layers/convmlp.py
--rw-r--r--  2.0 unx     4369 b- defN 22-Dec-02 13:07 towhee/models/layers/cross_attention.py
--rw-r--r--  2.0 unx     5964 b- defN 22-Dec-02 13:07 towhee/models/layers/dropblock2d.py
--rw-r--r--  2.0 unx     2200 b- defN 22-Dec-02 13:07 towhee/models/layers/droppath.py
--rw-r--r--  2.0 unx     2175 b- defN 22-Dec-02 13:07 towhee/models/layers/ffn.py
--rw-r--r--  2.0 unx     2052 b- defN 22-Dec-02 13:07 towhee/models/layers/gatedmlp.py
--rw-r--r--  2.0 unx    10670 b- defN 22-Dec-02 13:07 towhee/models/layers/layers_with_relprop.py
--rw-r--r--  2.0 unx     4330 b- defN 22-Dec-02 13:07 towhee/models/layers/mbconv.py
--rw-r--r--  2.0 unx     2971 b- defN 22-Dec-02 13:07 towhee/models/layers/mixed_conv2d.py
--rw-r--r--  2.0 unx     2566 b- defN 22-Dec-02 13:07 towhee/models/layers/mlp.py
--rw-r--r--  2.0 unx    13234 b- defN 22-Dec-02 13:07 towhee/models/layers/multi_scale_attention.py
--rw-r--r--  2.0 unx     6651 b- defN 22-Dec-02 13:07 towhee/models/layers/multi_scale_transformer_block.py
--rw-r--r--  2.0 unx     3312 b- defN 22-Dec-02 13:07 towhee/models/layers/netvlad.py
--rw-r--r--  2.0 unx     5460 b- defN 22-Dec-02 13:07 towhee/models/layers/non_local.py
--rw-r--r--  2.0 unx     5759 b- defN 22-Dec-02 13:07 towhee/models/layers/padding_functions.py
--rw-r--r--  2.0 unx     4015 b- defN 22-Dec-02 13:07 towhee/models/layers/patch_embed2d.py
--rw-r--r--  2.0 unx     3033 b- defN 22-Dec-02 13:07 towhee/models/layers/patch_embed3d.py
--rw-r--r--  2.0 unx     2458 b- defN 22-Dec-02 13:07 towhee/models/layers/patch_merging.py
--rw-r--r--  2.0 unx     2423 b- defN 22-Dec-02 13:07 towhee/models/layers/patch_merging3d.py
--rw-r--r--  2.0 unx     3343 b- defN 22-Dec-02 13:07 towhee/models/layers/pool_attention.py
--rw-r--r--  2.0 unx     3084 b- defN 22-Dec-02 13:07 towhee/models/layers/position_encoding.py
--rw-r--r--  2.0 unx     4749 b- defN 22-Dec-02 13:07 towhee/models/layers/relative_self_attention.py
--rw-r--r--  2.0 unx     2028 b- defN 22-Dec-02 13:07 towhee/models/layers/resnet_basic_3d_module.py
--rw-r--r--  2.0 unx     3273 b- defN 22-Dec-02 13:07 towhee/models/layers/sam.py
--rw-r--r--  2.0 unx     1500 b- defN 22-Dec-02 13:07 towhee/models/layers/sequence_pool.py
--rw-r--r--  2.0 unx     3663 b- defN 22-Dec-02 13:07 towhee/models/layers/spatial_temporal_cls_positional_encoding.py
--rw-r--r--  2.0 unx      639 b- defN 22-Dec-02 13:07 towhee/models/layers/spp.py
--rw-r--r--  2.0 unx     5347 b- defN 22-Dec-02 13:07 towhee/models/layers/swin_transformer_block3d.py
--rw-r--r--  2.0 unx     1964 b- defN 22-Dec-02 13:07 towhee/models/layers/temporal_cg_avgpool3d.py
--rw-r--r--  2.0 unx     1964 b- defN 22-Dec-02 13:07 towhee/models/layers/tf_avgpool3d.py
--rw-r--r--  2.0 unx     2351 b- defN 22-Dec-02 13:07 towhee/models/layers/time2vec.py
--rw-r--r--  2.0 unx     2054 b- defN 22-Dec-02 13:07 towhee/models/layers/transformer_encoder.py
--rw-r--r--  2.0 unx     3223 b- defN 22-Dec-02 13:07 towhee/models/layers/vision_transformer_basic_head.py
--rw-r--r--  2.0 unx     8064 b- defN 22-Dec-02 13:07 towhee/models/layers/window_attention.py
--rw-r--r--  2.0 unx     4765 b- defN 22-Dec-02 13:07 towhee/models/layers/window_attention3d.py
--rw-r--r--  2.0 unx     1202 b- defN 22-Dec-02 13:07 towhee/models/layers/activations/__init__.py
--rw-r--r--  2.0 unx     1312 b- defN 22-Dec-02 13:07 towhee/models/layers/activations/gelu.py
--rw-r--r--  2.0 unx     1393 b- defN 22-Dec-02 13:07 towhee/models/layers/activations/hardmish.py
--rw-r--r--  2.0 unx     1482 b- defN 22-Dec-02 13:07 towhee/models/layers/activations/hardsigmoid.py
--rw-r--r--  2.0 unx     1424 b- defN 22-Dec-02 13:07 towhee/models/layers/activations/hardswish.py
--rw-r--r--  2.0 unx     1328 b- defN 22-Dec-02 13:07 towhee/models/layers/activations/mish.py
--rw-r--r--  2.0 unx     1320 b- defN 22-Dec-02 13:07 towhee/models/layers/activations/prelu.py
--rw-r--r--  2.0 unx     1378 b- defN 22-Dec-02 13:07 towhee/models/layers/activations/sigmoid.py
--rw-r--r--  2.0 unx      815 b- defN 22-Dec-02 13:07 towhee/models/layers/activations/swiglu.py
--rw-r--r--  2.0 unx     1287 b- defN 22-Dec-02 13:07 towhee/models/layers/activations/swish.py
--rw-r--r--  2.0 unx     1290 b- defN 22-Dec-02 13:07 towhee/models/layers/activations/tanh.py
--rw-r--r--  2.0 unx       26 b- defN 22-Dec-02 13:07 towhee/models/lightning_dot/__init__.py
--rw-r--r--  2.0 unx     5800 b- defN 22-Dec-02 13:07 towhee/models/lightning_dot/bi_encoder.py
--rw-r--r--  2.0 unx      593 b- defN 22-Dec-02 13:07 towhee/models/loss/__init__.py
--rw-r--r--  2.0 unx     2467 b- defN 22-Dec-02 13:07 towhee/models/loss/focal_loss.py
--rw-r--r--  2.0 unx      616 b- defN 22-Dec-02 13:07 towhee/models/max_vit/__init__.py
--rw-r--r--  2.0 unx     1802 b- defN 22-Dec-02 13:07 towhee/models/max_vit/configs.py
--rw-r--r--  2.0 unx     8066 b- defN 22-Dec-02 13:07 towhee/models/max_vit/max_vit.py
--rw-r--r--  2.0 unx    11404 b- defN 22-Dec-02 13:07 towhee/models/max_vit/max_vit_block.py
--rw-r--r--  2.0 unx     5382 b- defN 22-Dec-02 13:07 towhee/models/max_vit/max_vit_utils.py
--rw-r--r--  2.0 unx      702 b- defN 22-Dec-02 13:07 towhee/models/mcprop/__init__.py
--rw-r--r--  2.0 unx     3053 b- defN 22-Dec-02 13:07 towhee/models/mcprop/depthaggregator.py
--rw-r--r--  2.0 unx     2388 b- defN 22-Dec-02 13:07 towhee/models/mcprop/featurefusion.py
--rw-r--r--  2.0 unx     1413 b- defN 22-Dec-02 13:07 towhee/models/mcprop/imageextractor.py
--rw-r--r--  2.0 unx     3070 b- defN 22-Dec-02 13:07 towhee/models/mcprop/loss.py
--rw-r--r--  2.0 unx     6296 b- defN 22-Dec-02 13:07 towhee/models/mcprop/matching.py
--rw-r--r--  2.0 unx     1399 b- defN 22-Dec-02 13:07 towhee/models/mcprop/textextractor.py
--rw-r--r--  2.0 unx     2029 b- defN 22-Dec-02 13:07 towhee/models/mcprop/transformerpooling.py
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/mdmmt/__init__.py
--rw-r--r--  2.0 unx    15492 b- defN 22-Dec-02 13:07 towhee/models/mdmmt/bert_mmt.py
--rw-r--r--  2.0 unx    13512 b- defN 22-Dec-02 13:07 towhee/models/mdmmt/mmt.py
--rw-r--r--  2.0 unx     1602 b- defN 22-Dec-02 13:07 towhee/models/metaformer/addpositionembed.py
--rw-r--r--  2.0 unx     2453 b- defN 22-Dec-02 13:07 towhee/models/metaformer/attention.py
--rw-r--r--  2.0 unx     1803 b- defN 22-Dec-02 13:07 towhee/models/metaformer/basicblocks.py
--rw-r--r--  2.0 unx    10807 b- defN 22-Dec-02 13:07 towhee/models/metaformer/metaformer.py
--rw-r--r--  2.0 unx     3109 b- defN 22-Dec-02 13:07 towhee/models/metaformer/metaformerblock.py
--rw-r--r--  2.0 unx     1684 b- defN 22-Dec-02 13:07 towhee/models/metaformer/spatialfc.py
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/movinet/__init__.py
--rw-r--r--  2.0 unx    27214 b- defN 22-Dec-02 13:07 towhee/models/movinet/config.py
--rw-r--r--  2.0 unx     8063 b- defN 22-Dec-02 13:07 towhee/models/movinet/movinet.py
--rw-r--r--  2.0 unx    13711 b- defN 22-Dec-02 13:07 towhee/models/movinet/movinet_block.py
--rw-r--r--  2.0 unx       21 b- defN 22-Dec-02 13:07 towhee/models/mpvit/__init__.py
--rw-r--r--  2.0 unx    28795 b- defN 22-Dec-02 13:07 towhee/models/mpvit/mpvit.py
--rw-r--r--  2.0 unx       47 b- defN 22-Dec-02 13:07 towhee/models/multiscale_vision_transformers/__init__.py
--rw-r--r--  2.0 unx     4509 b- defN 22-Dec-02 13:07 towhee/models/multiscale_vision_transformers/create_mvit.py
--rw-r--r--  2.0 unx    34605 b- defN 22-Dec-02 13:07 towhee/models/multiscale_vision_transformers/mvit.py
--rw-r--r--  2.0 unx      613 b- defN 22-Dec-02 13:07 towhee/models/nnfp/__init__.py
--rw-r--r--  2.0 unx     5235 b- defN 22-Dec-02 13:07 towhee/models/nnfp/nnfp.py
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/omnivore/__init__.py
--rw-r--r--  2.0 unx    15170 b- defN 22-Dec-02 13:07 towhee/models/omnivore/omnivore.py
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/perceiver/__init__.py
--rw-r--r--  2.0 unx     1446 b- defN 22-Dec-02 13:07 towhee/models/perceiver/create_cross_attention.py
--rw-r--r--  2.0 unx     1277 b- defN 22-Dec-02 13:07 towhee/models/perceiver/create_self_attention.py
--rw-r--r--  2.0 unx      979 b- defN 22-Dec-02 13:07 towhee/models/perceiver/create_self_attention_block.py
--rw-r--r--  2.0 unx     1233 b- defN 22-Dec-02 13:07 towhee/models/perceiver/cross_attention.py
--rw-r--r--  2.0 unx      344 b- defN 22-Dec-02 13:07 towhee/models/perceiver/mlp.py
--rw-r--r--  2.0 unx     1435 b- defN 22-Dec-02 13:07 towhee/models/perceiver/multi_head_attention.py
--rw-r--r--  2.0 unx      667 b- defN 22-Dec-02 13:07 towhee/models/perceiver/residual.py
--rw-r--r--  2.0 unx     1234 b- defN 22-Dec-02 13:07 towhee/models/perceiver/self_attention.py
--rw-r--r--  2.0 unx      434 b- defN 22-Dec-02 13:07 towhee/models/perceiver/sequential.py
--rw-r--r--  2.0 unx     1767 b- defN 22-Dec-02 13:07 towhee/models/poolformer/basic_blocks.py
--rw-r--r--  2.0 unx      978 b- defN 22-Dec-02 13:07 towhee/models/poolformer/groupnorm.py
--rw-r--r--  2.0 unx     1492 b- defN 22-Dec-02 13:07 towhee/models/poolformer/layernormchannel.py
--rw-r--r--  2.0 unx     1839 b- defN 22-Dec-02 13:07 towhee/models/poolformer/mlp.py
--rw-r--r--  2.0 unx     1558 b- defN 22-Dec-02 13:07 towhee/models/poolformer/patchembed.py
--rw-r--r--  2.0 unx    10490 b- defN 22-Dec-02 13:07 towhee/models/poolformer/poolformer.py
--rw-r--r--  2.0 unx     3034 b- defN 22-Dec-02 13:07 towhee/models/poolformer/poolformerblock.py
--rw-r--r--  2.0 unx     1122 b- defN 22-Dec-02 13:07 towhee/models/poolformer/pooling.py
--rw-r--r--  2.0 unx      661 b- defN 22-Dec-02 13:07 towhee/models/replknet/__init__.py
--rw-r--r--  2.0 unx     2718 b- defN 22-Dec-02 13:07 towhee/models/replknet/configs.py
--rw-r--r--  2.0 unx     8942 b- defN 22-Dec-02 13:07 towhee/models/replknet/replknet.py
--rw-r--r--  2.0 unx    10184 b- defN 22-Dec-02 13:07 towhee/models/replknet/utils.py
--rw-r--r--  2.0 unx      637 b- defN 22-Dec-02 13:07 towhee/models/repmlp/__init__.py
--rw-r--r--  2.0 unx     8448 b- defN 22-Dec-02 13:07 towhee/models/repmlp/blocks.py
--rw-r--r--  2.0 unx     2339 b- defN 22-Dec-02 13:07 towhee/models/repmlp/configs.py
--rw-r--r--  2.0 unx     7259 b- defN 22-Dec-02 13:07 towhee/models/repmlp/repmlp.py
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/retina_face/__init__.py
--rw-r--r--  2.0 unx     1503 b- defN 22-Dec-02 13:07 towhee/models/retina_face/configs.py
--rw-r--r--  2.0 unx     2552 b- defN 22-Dec-02 13:07 towhee/models/retina_face/heads.py
--rw-r--r--  2.0 unx     2057 b- defN 22-Dec-02 13:07 towhee/models/retina_face/mobilenet_v1.py
--rw-r--r--  2.0 unx     2194 b- defN 22-Dec-02 13:07 towhee/models/retina_face/prior_box.py
--rw-r--r--  2.0 unx     6531 b- defN 22-Dec-02 13:07 towhee/models/retina_face/retinaface.py
--rw-r--r--  2.0 unx     2350 b- defN 22-Dec-02 13:07 towhee/models/retina_face/retinaface_fpn.py
--rw-r--r--  2.0 unx     2140 b- defN 22-Dec-02 13:07 towhee/models/retina_face/ssh.py
--rw-r--r--  2.0 unx     5194 b- defN 22-Dec-02 13:07 towhee/models/retina_face/utils.py
--rw-r--r--  2.0 unx      672 b- defN 22-Dec-02 13:07 towhee/models/shunted_transformer/__init__.py
--rw-r--r--  2.0 unx     2134 b- defN 22-Dec-02 13:07 towhee/models/shunted_transformer/configs.py
--rw-r--r--  2.0 unx     5411 b- defN 22-Dec-02 13:07 towhee/models/shunted_transformer/shunted_transformer.py
--rw-r--r--  2.0 unx    10283 b- defN 22-Dec-02 13:07 towhee/models/shunted_transformer/utils.py
--rw-r--r--  2.0 unx      612 b- defN 22-Dec-02 13:07 towhee/models/svt/__init__.py
--rw-r--r--  2.0 unx     2925 b- defN 22-Dec-02 13:07 towhee/models/svt/svt.py
--rw-r--r--  2.0 unx     1375 b- defN 22-Dec-02 13:07 towhee/models/svt/svt_utils.py
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/swin_transformer/__init__.py
--rw-r--r--  2.0 unx     3558 b- defN 22-Dec-02 13:07 towhee/models/swin_transformer/basic_layer.py
--rw-r--r--  2.0 unx     9904 b- defN 22-Dec-02 13:07 towhee/models/swin_transformer/configs.py
--rw-r--r--  2.0 unx     7134 b- defN 22-Dec-02 13:07 towhee/models/swin_transformer/model.py
--rw-r--r--  2.0 unx     6114 b- defN 22-Dec-02 13:07 towhee/models/swin_transformer/swin_transformer_block.py
--rw-r--r--  2.0 unx      620 b- defN 22-Dec-02 13:07 towhee/models/timesformer/__init__.py
--rw-r--r--  2.0 unx     9537 b- defN 22-Dec-02 13:07 towhee/models/timesformer/timesformer.py
--rw-r--r--  2.0 unx     6249 b- defN 22-Dec-02 13:07 towhee/models/timesformer/timesformer_block.py
--rw-r--r--  2.0 unx     9090 b- defN 22-Dec-02 13:07 towhee/models/timesformer/timesformer_utils.py
--rw-r--r--  2.0 unx      638 b- defN 22-Dec-02 13:07 towhee/models/transrac/__init__.py
--rw-r--r--  2.0 unx     5724 b- defN 22-Dec-02 13:07 towhee/models/transrac/transrac.py
--rw-r--r--  2.0 unx     3378 b- defN 22-Dec-02 13:07 towhee/models/transrac/utils.py
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/tsm/__init__.py
--rw-r--r--  2.0 unx     4683 b- defN 22-Dec-02 13:07 towhee/models/tsm/config.py
--rw-r--r--  2.0 unx     5394 b- defN 22-Dec-02 13:07 towhee/models/tsm/mobilenet_v2.py
--rw-r--r--  2.0 unx     5815 b- defN 22-Dec-02 13:07 towhee/models/tsm/temporal_shift.py
--rw-r--r--  2.0 unx    13942 b- defN 22-Dec-02 13:07 towhee/models/tsm/tsm.py
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/uniformer/__init__.py
--rw-r--r--  2.0 unx     4337 b- defN 22-Dec-02 13:07 towhee/models/uniformer/config.py
--rw-r--r--  2.0 unx    21976 b- defN 22-Dec-02 13:07 towhee/models/uniformer/uniformer.py
--rw-r--r--  2.0 unx       89 b- defN 22-Dec-02 13:07 towhee/models/utils/__init__.py
--rw-r--r--  2.0 unx     4633 b- defN 22-Dec-02 13:07 towhee/models/utils/audio_preprocess.py
--rw-r--r--  2.0 unx     1488 b- defN 22-Dec-02 13:07 towhee/models/utils/basic_ops.py
--rw-r--r--  2.0 unx      808 b- defN 22-Dec-02 13:07 towhee/models/utils/causal_module.py
--rw-r--r--  2.0 unx     4368 b- defN 22-Dec-02 13:07 towhee/models/utils/create_act.py
--rw-r--r--  2.0 unx     2159 b- defN 22-Dec-02 13:07 towhee/models/utils/create_conv2d.py
--rw-r--r--  2.0 unx     1328 b- defN 22-Dec-02 13:07 towhee/models/utils/create_conv2d_pad.py
--rw-r--r--  2.0 unx     1664 b- defN 22-Dec-02 13:07 towhee/models/utils/create_model.py
--rw-r--r--  2.0 unx     3929 b- defN 22-Dec-02 13:07 towhee/models/utils/create_resnet_basic_3d_module.py
--rw-r--r--  2.0 unx     3658 b- defN 22-Dec-02 13:07 towhee/models/utils/download.py
--rw-r--r--  2.0 unx     1208 b- defN 22-Dec-02 13:07 towhee/models/utils/fuse_bn.py
--rw-r--r--  2.0 unx     1153 b- defN 22-Dec-02 13:07 towhee/models/utils/gelu_ignore_parameters.py
--rw-r--r--  2.0 unx     1370 b- defN 22-Dec-02 13:07 towhee/models/utils/general_utils.py
--rw-r--r--  2.0 unx     1720 b- defN 22-Dec-02 13:07 towhee/models/utils/get_relative_position_index.py
--rw-r--r--  2.0 unx      958 b- defN 22-Dec-02 13:07 towhee/models/utils/get_window_size.py
--rw-r--r--  2.0 unx     2642 b- defN 22-Dec-02 13:07 towhee/models/utils/init_vit_weights.py
--rw-r--r--  2.0 unx     1439 b- defN 22-Dec-02 13:07 towhee/models/utils/pretrained_utils.py
--rw-r--r--  2.0 unx     1556 b- defN 22-Dec-02 13:07 towhee/models/utils/round_width.py
--rw-r--r--  2.0 unx     8505 b- defN 22-Dec-02 13:07 towhee/models/utils/video_transforms.py
--rw-r--r--  2.0 unx     3957 b- defN 22-Dec-02 13:07 towhee/models/utils/weight_init.py
--rw-r--r--  2.0 unx      527 b- defN 22-Dec-02 13:07 towhee/models/utils/window_partition.py
--rw-r--r--  2.0 unx      891 b- defN 22-Dec-02 13:07 towhee/models/utils/window_partition3d.py
--rw-r--r--  2.0 unx      690 b- defN 22-Dec-02 13:07 towhee/models/utils/window_reverse.py
--rw-r--r--  2.0 unx      887 b- defN 22-Dec-02 13:07 towhee/models/utils/window_reverse3d.py
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/vggish/__init__.py
--rw-r--r--  2.0 unx     2049 b- defN 22-Dec-02 13:07 towhee/models/vggish/torch_vggish.py
--rw-r--r--  2.0 unx      824 b- defN 22-Dec-02 13:07 towhee/models/video_swin_transformer/__init__.py
--rw-r--r--  2.0 unx     1643 b- defN 22-Dec-02 13:07 towhee/models/video_swin_transformer/compute_mask.py
--rw-r--r--  2.0 unx     3992 b- defN 22-Dec-02 13:07 towhee/models/video_swin_transformer/get_configs.py
--rw-r--r--  2.0 unx    14573 b- defN 22-Dec-02 13:07 towhee/models/video_swin_transformer/video_swin_transformer.py
--rw-r--r--  2.0 unx     3925 b- defN 22-Dec-02 13:07 towhee/models/video_swin_transformer/video_swin_transformer_block.py
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/violet/__init__.py
--rw-r--r--  2.0 unx     3916 b- defN 22-Dec-02 13:07 towhee/models/violet/violet.py
--rw-r--r--  2.0 unx     1007 b- defN 22-Dec-02 13:07 towhee/models/vis4mer/__init__.py
--rw-r--r--  2.0 unx     1471 b- defN 22-Dec-02 13:07 towhee/models/vis4mer/activation.py
--rw-r--r--  2.0 unx     1944 b- defN 22-Dec-02 13:07 towhee/models/vis4mer/get_initializer.py
--rw-r--r--  2.0 unx     2347 b- defN 22-Dec-02 13:07 towhee/models/vis4mer/linearactivation.py
--rw-r--r--  2.0 unx     1662 b- defN 22-Dec-02 13:07 towhee/models/vis4mer/transposelinear.py
--rw-r--r--  2.0 unx    10305 b- defN 22-Dec-02 13:07 towhee/models/vis4mer/utils.py
--rw-r--r--  2.0 unx        0 b- defN 22-Dec-02 13:07 towhee/models/visualization/__init__.py
--rw-r--r--  2.0 unx     8524 b- defN 22-Dec-02 13:07 towhee/models/visualization/clip_visualization.py
--rw-r--r--  2.0 unx     3193 b- defN 22-Dec-02 13:07 towhee/models/visualization/embedding_visualization.py
--rw-r--r--  2.0 unx     7065 b- defN 22-Dec-02 13:07 towhee/models/visualization/transformer_visualization.py
--rw-r--r--  2.0 unx      612 b- defN 22-Dec-02 13:07 towhee/models/vit/__init__.py
--rw-r--r--  2.0 unx    10295 b- defN 22-Dec-02 13:07 towhee/models/vit/vit.py
--rw-r--r--  2.0 unx     3878 b- defN 22-Dec-02 13:07 towhee/models/vit/vit_block.py
--rw-r--r--  2.0 unx     1868 b- defN 22-Dec-02 13:07 towhee/models/vit/vit_utils.py
--rw-r--r--  2.0 unx      617 b- defN 22-Dec-02 13:07 towhee/models/wave_vit/__init__.py
--rw-r--r--  2.0 unx    10014 b- defN 22-Dec-02 13:07 towhee/models/wave_vit/wave_vit.py
--rw-r--r--  2.0 unx     9731 b- defN 22-Dec-02 13:07 towhee/models/wave_vit/wave_vit_block.py
--rw-r--r--  2.0 unx     6361 b- defN 22-Dec-02 13:07 towhee/models/wave_vit/wave_vit_utils.py
--rw-r--r--  2.0 unx    11357 b- defN 22-Dec-02 13:26 towhee.models-0.9.0.dist-info/LICENSE
--rw-r--r--  2.0 unx    13931 b- defN 22-Dec-02 13:26 towhee.models-0.9.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 22-Dec-02 13:26 towhee.models-0.9.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       55 b- defN 22-Dec-02 13:26 towhee.models-0.9.0.dist-info/entry_points.txt
--rw-r--r--  2.0 unx        7 b- defN 22-Dec-02 13:26 towhee.models-0.9.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx    26302 b- defN 22-Dec-02 13:26 towhee.models-0.9.0.dist-info/RECORD
-280 files, 2676445 bytes uncompressed, 1757338 bytes compressed:  34.3%
+Zip file size: 1800462 bytes, number of entries: 280
+-rw-r--r--  2.0 unx     8596 b- defN 23-Mar-21 08:26 towhee/models/README.md
+-rw-r--r--  2.0 unx     8570 b- defN 23-Mar-21 08:26 towhee/models/README_CN.md
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/__init__.py
+-rw-r--r--  2.0 unx      678 b- defN 23-Mar-21 08:26 towhee/models/acar_net/__init__.py
+-rw-r--r--  2.0 unx    10305 b- defN 23-Mar-21 08:26 towhee/models/acar_net/backbone.py
+-rw-r--r--  2.0 unx     9275 b- defN 23-Mar-21 08:26 towhee/models/acar_net/head.py
+-rw-r--r--  2.0 unx     3613 b- defN 23-Mar-21 08:26 towhee/models/acar_net/model.py
+-rw-r--r--  2.0 unx     4102 b- defN 23-Mar-21 08:26 towhee/models/acar_net/neck.py
+-rw-r--r--  2.0 unx     1618 b- defN 23-Mar-21 08:26 towhee/models/acar_net/utils.py
+-rw-r--r--  2.0 unx      709 b- defN 23-Mar-21 08:26 towhee/models/action_clip/__init__.py
+-rw-r--r--  2.0 unx     4305 b- defN 23-Mar-21 08:26 towhee/models/action_clip/action_clip.py
+-rw-r--r--  2.0 unx     2039 b- defN 23-Mar-21 08:26 towhee/models/action_clip/action_clip_utils.py
+-rw-r--r--  2.0 unx     1804 b- defN 23-Mar-21 08:26 towhee/models/action_clip/text_prompt.py
+-rw-r--r--  2.0 unx     8805 b- defN 23-Mar-21 08:26 towhee/models/action_clip/visual_prompt.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/allinone/__init__.py
+-rw-r--r--  2.0 unx     1289 b- defN 23-Mar-21 08:26 towhee/models/allinone/allinone.py
+-rw-r--r--  2.0 unx      622 b- defN 23-Mar-21 08:26 towhee/models/bridgeformer/__init__.py
+-rw-r--r--  2.0 unx     3300 b- defN 23-Mar-21 08:26 towhee/models/bridgeformer/bridge_former.py
+-rw-r--r--  2.0 unx    10421 b- defN 23-Mar-21 08:26 towhee/models/bridgeformer/bridge_former_training.py
+-rw-r--r--  2.0 unx    16155 b- defN 23-Mar-21 08:26 towhee/models/bridgeformer/bridge_former_training_block.py
+-rw-r--r--  2.0 unx     1560 b- defN 23-Mar-21 08:26 towhee/models/clip/README.md
+-rw-r--r--  2.0 unx      639 b- defN 23-Mar-21 08:26 towhee/models/clip/__init__.py
+-rw-r--r--  2.0 unx    20204 b- defN 23-Mar-21 08:26 towhee/models/clip/auxilary.py
+-rw-r--r--  2.0 unx  1356917 b- defN 23-Mar-21 08:26 towhee/models/clip/bpe_simple_vocab_16e6.txt.gz
+-rw-r--r--  2.0 unx    30031 b- defN 23-Mar-21 08:26 towhee/models/clip/clip.py
+-rw-r--r--  2.0 unx     9935 b- defN 23-Mar-21 08:26 towhee/models/clip/clip_utils.py
+-rw-r--r--  2.0 unx     5618 b- defN 23-Mar-21 08:26 towhee/models/clip/simple_tokenizer.py
+-rw-r--r--  2.0 unx       46 b- defN 23-Mar-21 08:26 towhee/models/clip4clip/__init__.py
+-rw-r--r--  2.0 unx    10156 b- defN 23-Mar-21 08:26 towhee/models/clip4clip/clip4clip.py
+-rw-r--r--  2.0 unx     2605 b- defN 23-Mar-21 08:26 towhee/models/clip4clip/until_module.py
+-rw-r--r--  2.0 unx     1837 b- defN 23-Mar-21 08:26 towhee/models/clip4clip/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/coca/__init__.py
+-rw-r--r--  2.0 unx    11514 b- defN 23-Mar-21 08:26 towhee/models/coca/coca.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/coformer/__init__.py
+-rw-r--r--  2.0 unx     5167 b- defN 23-Mar-21 08:26 towhee/models/coformer/backbone.py
+-rw-r--r--  2.0 unx    10222 b- defN 23-Mar-21 08:26 towhee/models/coformer/coformer.py
+-rw-r--r--  2.0 unx    10008 b- defN 23-Mar-21 08:26 towhee/models/coformer/config.py
+-rw-r--r--  2.0 unx    15471 b- defN 23-Mar-21 08:26 towhee/models/coformer/transformer.py
+-rw-r--r--  2.0 unx     2919 b- defN 23-Mar-21 08:26 towhee/models/coformer/utils.py
+-rw-r--r--  2.0 unx      630 b- defN 23-Mar-21 08:26 towhee/models/collaborative_experts/__init__.py
+-rw-r--r--  2.0 unx    60484 b- defN 23-Mar-21 08:26 towhee/models/collaborative_experts/collaborative_experts.py
+-rw-r--r--  2.0 unx     3587 b- defN 23-Mar-21 08:26 towhee/models/collaborative_experts/net_vlad.py
+-rw-r--r--  2.0 unx     1612 b- defN 23-Mar-21 08:26 towhee/models/collaborative_experts/util.py
+-rw-r--r--  2.0 unx      661 b- defN 23-Mar-21 08:26 towhee/models/convnext/__init__.py
+-rw-r--r--  2.0 unx     3245 b- defN 23-Mar-21 08:26 towhee/models/convnext/configs.py
+-rw-r--r--  2.0 unx     4425 b- defN 23-Mar-21 08:26 towhee/models/convnext/convnext.py
+-rw-r--r--  2.0 unx     4107 b- defN 23-Mar-21 08:26 towhee/models/convnext/utils.py
+-rw-r--r--  2.0 unx      614 b- defN 23-Mar-21 08:26 towhee/models/cvnet/__init__.py
+-rw-r--r--  2.0 unx     4665 b- defN 23-Mar-21 08:26 towhee/models/cvnet/cvnet.py
+-rw-r--r--  2.0 unx     6640 b- defN 23-Mar-21 08:26 towhee/models/cvnet/cvnet_block.py
+-rw-r--r--  2.0 unx     2439 b- defN 23-Mar-21 08:26 towhee/models/cvnet/cvnet_utils.py
+-rw-r--r--  2.0 unx     8106 b- defN 23-Mar-21 08:26 towhee/models/cvnet/resnet.py
+-rw-r--r--  2.0 unx       19 b- defN 23-Mar-21 08:26 towhee/models/drl/__init__.py
+-rw-r--r--  2.0 unx    34568 b- defN 23-Mar-21 08:26 towhee/models/drl/drl.py
+-rw-r--r--  2.0 unx     7936 b- defN 23-Mar-21 08:26 towhee/models/drl/module_cross.py
+-rw-r--r--  2.0 unx     4891 b- defN 23-Mar-21 08:26 towhee/models/drl/until_module.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/embedding/__init__.py
+-rw-r--r--  2.0 unx     2061 b- defN 23-Mar-21 08:26 towhee/models/embedding/embedding_extractor.py
+-rw-r--r--  2.0 unx      623 b- defN 23-Mar-21 08:26 towhee/models/frozen_in_time/__init__.py
+-rw-r--r--  2.0 unx    14921 b- defN 23-Mar-21 08:26 towhee/models/frozen_in_time/frozen_in_time.py
+-rw-r--r--  2.0 unx     2394 b- defN 23-Mar-21 08:26 towhee/models/frozen_in_time/frozen_utils.py
+-rw-r--r--  2.0 unx    15910 b- defN 23-Mar-21 08:26 towhee/models/frozen_in_time/frozen_video_transformer.py
+-rw-r--r--  2.0 unx      659 b- defN 23-Mar-21 08:26 towhee/models/hornet/__init__.py
+-rw-r--r--  2.0 unx     4309 b- defN 23-Mar-21 08:26 towhee/models/hornet/configs.py
+-rw-r--r--  2.0 unx     5036 b- defN 23-Mar-21 08:26 towhee/models/hornet/hornet.py
+-rw-r--r--  2.0 unx     5983 b- defN 23-Mar-21 08:26 towhee/models/hornet/utils.py
+-rw-r--r--  2.0 unx      612 b- defN 23-Mar-21 08:26 towhee/models/isc/__init__.py
+-rw-r--r--  2.0 unx     3470 b- defN 23-Mar-21 08:26 towhee/models/isc/isc.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/layers/__init__.py
+-rw-r--r--  2.0 unx     1638 b- defN 23-Mar-21 08:26 towhee/models/layers/aspp.py
+-rw-r--r--  2.0 unx     6693 b- defN 23-Mar-21 08:26 towhee/models/layers/attention.py
+-rw-r--r--  2.0 unx     5783 b- defN 23-Mar-21 08:26 towhee/models/layers/cond_conv2d.py
+-rw-r--r--  2.0 unx     2706 b- defN 23-Mar-21 08:26 towhee/models/layers/conv2d_same.py
+-rw-r--r--  2.0 unx     4180 b- defN 23-Mar-21 08:26 towhee/models/layers/conv2d_separable.py
+-rw-r--r--  2.0 unx     4354 b- defN 23-Mar-21 08:26 towhee/models/layers/conv4d.py
+-rw-r--r--  2.0 unx     5125 b- defN 23-Mar-21 08:26 towhee/models/layers/conv_bn_activation.py
+-rw-r--r--  2.0 unx     1918 b- defN 23-Mar-21 08:26 towhee/models/layers/convmlp.py
+-rw-r--r--  2.0 unx     4369 b- defN 23-Mar-21 08:26 towhee/models/layers/cross_attention.py
+-rw-r--r--  2.0 unx     5964 b- defN 23-Mar-21 08:26 towhee/models/layers/dropblock2d.py
+-rw-r--r--  2.0 unx     2200 b- defN 23-Mar-21 08:26 towhee/models/layers/droppath.py
+-rw-r--r--  2.0 unx     2175 b- defN 23-Mar-21 08:26 towhee/models/layers/ffn.py
+-rw-r--r--  2.0 unx     2052 b- defN 23-Mar-21 08:26 towhee/models/layers/gatedmlp.py
+-rw-r--r--  2.0 unx    10670 b- defN 23-Mar-21 08:26 towhee/models/layers/layers_with_relprop.py
+-rw-r--r--  2.0 unx     4330 b- defN 23-Mar-21 08:26 towhee/models/layers/mbconv.py
+-rw-r--r--  2.0 unx     2971 b- defN 23-Mar-21 08:26 towhee/models/layers/mixed_conv2d.py
+-rw-r--r--  2.0 unx     2566 b- defN 23-Mar-21 08:26 towhee/models/layers/mlp.py
+-rw-r--r--  2.0 unx    13234 b- defN 23-Mar-21 08:26 towhee/models/layers/multi_scale_attention.py
+-rw-r--r--  2.0 unx     6651 b- defN 23-Mar-21 08:26 towhee/models/layers/multi_scale_transformer_block.py
+-rw-r--r--  2.0 unx     3312 b- defN 23-Mar-21 08:26 towhee/models/layers/netvlad.py
+-rw-r--r--  2.0 unx     5460 b- defN 23-Mar-21 08:26 towhee/models/layers/non_local.py
+-rw-r--r--  2.0 unx     5759 b- defN 23-Mar-21 08:26 towhee/models/layers/padding_functions.py
+-rw-r--r--  2.0 unx     4015 b- defN 23-Mar-21 08:26 towhee/models/layers/patch_embed2d.py
+-rw-r--r--  2.0 unx     3033 b- defN 23-Mar-21 08:26 towhee/models/layers/patch_embed3d.py
+-rw-r--r--  2.0 unx     2458 b- defN 23-Mar-21 08:26 towhee/models/layers/patch_merging.py
+-rw-r--r--  2.0 unx     2423 b- defN 23-Mar-21 08:26 towhee/models/layers/patch_merging3d.py
+-rw-r--r--  2.0 unx     3343 b- defN 23-Mar-21 08:26 towhee/models/layers/pool_attention.py
+-rw-r--r--  2.0 unx     3084 b- defN 23-Mar-21 08:26 towhee/models/layers/position_encoding.py
+-rw-r--r--  2.0 unx     4749 b- defN 23-Mar-21 08:26 towhee/models/layers/relative_self_attention.py
+-rw-r--r--  2.0 unx     2028 b- defN 23-Mar-21 08:26 towhee/models/layers/resnet_basic_3d_module.py
+-rw-r--r--  2.0 unx     3273 b- defN 23-Mar-21 08:26 towhee/models/layers/sam.py
+-rw-r--r--  2.0 unx     1500 b- defN 23-Mar-21 08:26 towhee/models/layers/sequence_pool.py
+-rw-r--r--  2.0 unx     3663 b- defN 23-Mar-21 08:26 towhee/models/layers/spatial_temporal_cls_positional_encoding.py
+-rw-r--r--  2.0 unx      639 b- defN 23-Mar-21 08:26 towhee/models/layers/spp.py
+-rw-r--r--  2.0 unx     5347 b- defN 23-Mar-21 08:26 towhee/models/layers/swin_transformer_block3d.py
+-rw-r--r--  2.0 unx     1964 b- defN 23-Mar-21 08:26 towhee/models/layers/temporal_cg_avgpool3d.py
+-rw-r--r--  2.0 unx     1964 b- defN 23-Mar-21 08:26 towhee/models/layers/tf_avgpool3d.py
+-rw-r--r--  2.0 unx     2351 b- defN 23-Mar-21 08:26 towhee/models/layers/time2vec.py
+-rw-r--r--  2.0 unx     2054 b- defN 23-Mar-21 08:26 towhee/models/layers/transformer_encoder.py
+-rw-r--r--  2.0 unx     3223 b- defN 23-Mar-21 08:26 towhee/models/layers/vision_transformer_basic_head.py
+-rw-r--r--  2.0 unx     8064 b- defN 23-Mar-21 08:26 towhee/models/layers/window_attention.py
+-rw-r--r--  2.0 unx     4765 b- defN 23-Mar-21 08:26 towhee/models/layers/window_attention3d.py
+-rw-r--r--  2.0 unx     1202 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/__init__.py
+-rw-r--r--  2.0 unx     1312 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/gelu.py
+-rw-r--r--  2.0 unx     1393 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/hardmish.py
+-rw-r--r--  2.0 unx     1482 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/hardsigmoid.py
+-rw-r--r--  2.0 unx     1424 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/hardswish.py
+-rw-r--r--  2.0 unx     1328 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/mish.py
+-rw-r--r--  2.0 unx     1320 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/prelu.py
+-rw-r--r--  2.0 unx     1378 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/sigmoid.py
+-rw-r--r--  2.0 unx      815 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/swiglu.py
+-rw-r--r--  2.0 unx     1287 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/swish.py
+-rw-r--r--  2.0 unx     1290 b- defN 23-Mar-21 08:26 towhee/models/layers/activations/tanh.py
+-rw-r--r--  2.0 unx       26 b- defN 23-Mar-21 08:26 towhee/models/lightning_dot/__init__.py
+-rw-r--r--  2.0 unx     5800 b- defN 23-Mar-21 08:26 towhee/models/lightning_dot/bi_encoder.py
+-rw-r--r--  2.0 unx      593 b- defN 23-Mar-21 08:26 towhee/models/loss/__init__.py
+-rw-r--r--  2.0 unx     2467 b- defN 23-Mar-21 08:26 towhee/models/loss/focal_loss.py
+-rw-r--r--  2.0 unx      616 b- defN 23-Mar-21 08:26 towhee/models/max_vit/__init__.py
+-rw-r--r--  2.0 unx     1802 b- defN 23-Mar-21 08:26 towhee/models/max_vit/configs.py
+-rw-r--r--  2.0 unx     8066 b- defN 23-Mar-21 08:26 towhee/models/max_vit/max_vit.py
+-rw-r--r--  2.0 unx    11404 b- defN 23-Mar-21 08:26 towhee/models/max_vit/max_vit_block.py
+-rw-r--r--  2.0 unx     5382 b- defN 23-Mar-21 08:26 towhee/models/max_vit/max_vit_utils.py
+-rw-r--r--  2.0 unx      702 b- defN 23-Mar-21 08:26 towhee/models/mcprop/__init__.py
+-rw-r--r--  2.0 unx     3053 b- defN 23-Mar-21 08:26 towhee/models/mcprop/depthaggregator.py
+-rw-r--r--  2.0 unx     2388 b- defN 23-Mar-21 08:26 towhee/models/mcprop/featurefusion.py
+-rw-r--r--  2.0 unx     1413 b- defN 23-Mar-21 08:26 towhee/models/mcprop/imageextractor.py
+-rw-r--r--  2.0 unx     3070 b- defN 23-Mar-21 08:26 towhee/models/mcprop/loss.py
+-rw-r--r--  2.0 unx     6296 b- defN 23-Mar-21 08:26 towhee/models/mcprop/matching.py
+-rw-r--r--  2.0 unx     1399 b- defN 23-Mar-21 08:26 towhee/models/mcprop/textextractor.py
+-rw-r--r--  2.0 unx     2029 b- defN 23-Mar-21 08:26 towhee/models/mcprop/transformerpooling.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/mdmmt/__init__.py
+-rw-r--r--  2.0 unx    15492 b- defN 23-Mar-21 08:26 towhee/models/mdmmt/bert_mmt.py
+-rw-r--r--  2.0 unx    13512 b- defN 23-Mar-21 08:26 towhee/models/mdmmt/mmt.py
+-rw-r--r--  2.0 unx     1602 b- defN 23-Mar-21 08:26 towhee/models/metaformer/addpositionembed.py
+-rw-r--r--  2.0 unx     2453 b- defN 23-Mar-21 08:26 towhee/models/metaformer/attention.py
+-rw-r--r--  2.0 unx     1803 b- defN 23-Mar-21 08:26 towhee/models/metaformer/basicblocks.py
+-rw-r--r--  2.0 unx    10807 b- defN 23-Mar-21 08:26 towhee/models/metaformer/metaformer.py
+-rw-r--r--  2.0 unx     3109 b- defN 23-Mar-21 08:26 towhee/models/metaformer/metaformerblock.py
+-rw-r--r--  2.0 unx     1684 b- defN 23-Mar-21 08:26 towhee/models/metaformer/spatialfc.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/movinet/__init__.py
+-rw-r--r--  2.0 unx    27214 b- defN 23-Mar-21 08:26 towhee/models/movinet/config.py
+-rw-r--r--  2.0 unx     8063 b- defN 23-Mar-21 08:26 towhee/models/movinet/movinet.py
+-rw-r--r--  2.0 unx    13711 b- defN 23-Mar-21 08:26 towhee/models/movinet/movinet_block.py
+-rw-r--r--  2.0 unx       21 b- defN 23-Mar-21 08:26 towhee/models/mpvit/__init__.py
+-rw-r--r--  2.0 unx    28795 b- defN 23-Mar-21 08:26 towhee/models/mpvit/mpvit.py
+-rw-r--r--  2.0 unx       47 b- defN 23-Mar-21 08:26 towhee/models/multiscale_vision_transformers/__init__.py
+-rw-r--r--  2.0 unx     4509 b- defN 23-Mar-21 08:26 towhee/models/multiscale_vision_transformers/create_mvit.py
+-rw-r--r--  2.0 unx    34605 b- defN 23-Mar-21 08:26 towhee/models/multiscale_vision_transformers/mvit.py
+-rw-r--r--  2.0 unx      613 b- defN 23-Mar-21 08:26 towhee/models/nnfp/__init__.py
+-rw-r--r--  2.0 unx     5235 b- defN 23-Mar-21 08:26 towhee/models/nnfp/nnfp.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/omnivore/__init__.py
+-rw-r--r--  2.0 unx    15170 b- defN 23-Mar-21 08:26 towhee/models/omnivore/omnivore.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/perceiver/__init__.py
+-rw-r--r--  2.0 unx     1446 b- defN 23-Mar-21 08:26 towhee/models/perceiver/create_cross_attention.py
+-rw-r--r--  2.0 unx     1277 b- defN 23-Mar-21 08:26 towhee/models/perceiver/create_self_attention.py
+-rw-r--r--  2.0 unx      979 b- defN 23-Mar-21 08:26 towhee/models/perceiver/create_self_attention_block.py
+-rw-r--r--  2.0 unx     1233 b- defN 23-Mar-21 08:26 towhee/models/perceiver/cross_attention.py
+-rw-r--r--  2.0 unx      344 b- defN 23-Mar-21 08:26 towhee/models/perceiver/mlp.py
+-rw-r--r--  2.0 unx     1435 b- defN 23-Mar-21 08:26 towhee/models/perceiver/multi_head_attention.py
+-rw-r--r--  2.0 unx      667 b- defN 23-Mar-21 08:26 towhee/models/perceiver/residual.py
+-rw-r--r--  2.0 unx     1234 b- defN 23-Mar-21 08:26 towhee/models/perceiver/self_attention.py
+-rw-r--r--  2.0 unx      434 b- defN 23-Mar-21 08:26 towhee/models/perceiver/sequential.py
+-rw-r--r--  2.0 unx     1767 b- defN 23-Mar-21 08:26 towhee/models/poolformer/basic_blocks.py
+-rw-r--r--  2.0 unx      978 b- defN 23-Mar-21 08:26 towhee/models/poolformer/groupnorm.py
+-rw-r--r--  2.0 unx     1492 b- defN 23-Mar-21 08:26 towhee/models/poolformer/layernormchannel.py
+-rw-r--r--  2.0 unx     1839 b- defN 23-Mar-21 08:26 towhee/models/poolformer/mlp.py
+-rw-r--r--  2.0 unx     1558 b- defN 23-Mar-21 08:26 towhee/models/poolformer/patchembed.py
+-rw-r--r--  2.0 unx    10490 b- defN 23-Mar-21 08:26 towhee/models/poolformer/poolformer.py
+-rw-r--r--  2.0 unx     3034 b- defN 23-Mar-21 08:26 towhee/models/poolformer/poolformerblock.py
+-rw-r--r--  2.0 unx     1122 b- defN 23-Mar-21 08:26 towhee/models/poolformer/pooling.py
+-rw-r--r--  2.0 unx      661 b- defN 23-Mar-21 08:26 towhee/models/replknet/__init__.py
+-rw-r--r--  2.0 unx     2718 b- defN 23-Mar-21 08:26 towhee/models/replknet/configs.py
+-rw-r--r--  2.0 unx     8942 b- defN 23-Mar-21 08:26 towhee/models/replknet/replknet.py
+-rw-r--r--  2.0 unx    10184 b- defN 23-Mar-21 08:26 towhee/models/replknet/utils.py
+-rw-r--r--  2.0 unx      637 b- defN 23-Mar-21 08:26 towhee/models/repmlp/__init__.py
+-rw-r--r--  2.0 unx     8448 b- defN 23-Mar-21 08:26 towhee/models/repmlp/blocks.py
+-rw-r--r--  2.0 unx     2339 b- defN 23-Mar-21 08:26 towhee/models/repmlp/configs.py
+-rw-r--r--  2.0 unx     7259 b- defN 23-Mar-21 08:26 towhee/models/repmlp/repmlp.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/retina_face/__init__.py
+-rw-r--r--  2.0 unx     1503 b- defN 23-Mar-21 08:26 towhee/models/retina_face/configs.py
+-rw-r--r--  2.0 unx     2552 b- defN 23-Mar-21 08:26 towhee/models/retina_face/heads.py
+-rw-r--r--  2.0 unx     2057 b- defN 23-Mar-21 08:26 towhee/models/retina_face/mobilenet_v1.py
+-rw-r--r--  2.0 unx     2194 b- defN 23-Mar-21 08:26 towhee/models/retina_face/prior_box.py
+-rw-r--r--  2.0 unx     6531 b- defN 23-Mar-21 08:26 towhee/models/retina_face/retinaface.py
+-rw-r--r--  2.0 unx     2350 b- defN 23-Mar-21 08:26 towhee/models/retina_face/retinaface_fpn.py
+-rw-r--r--  2.0 unx     2140 b- defN 23-Mar-21 08:26 towhee/models/retina_face/ssh.py
+-rw-r--r--  2.0 unx     5194 b- defN 23-Mar-21 08:26 towhee/models/retina_face/utils.py
+-rw-r--r--  2.0 unx      672 b- defN 23-Mar-21 08:26 towhee/models/shunted_transformer/__init__.py
+-rw-r--r--  2.0 unx     2134 b- defN 23-Mar-21 08:26 towhee/models/shunted_transformer/configs.py
+-rw-r--r--  2.0 unx     5411 b- defN 23-Mar-21 08:26 towhee/models/shunted_transformer/shunted_transformer.py
+-rw-r--r--  2.0 unx    10283 b- defN 23-Mar-21 08:26 towhee/models/shunted_transformer/utils.py
+-rw-r--r--  2.0 unx      612 b- defN 23-Mar-21 08:26 towhee/models/svt/__init__.py
+-rw-r--r--  2.0 unx     2925 b- defN 23-Mar-21 08:26 towhee/models/svt/svt.py
+-rw-r--r--  2.0 unx     1375 b- defN 23-Mar-21 08:26 towhee/models/svt/svt_utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/swin_transformer/__init__.py
+-rw-r--r--  2.0 unx     3558 b- defN 23-Mar-21 08:26 towhee/models/swin_transformer/basic_layer.py
+-rw-r--r--  2.0 unx     9904 b- defN 23-Mar-21 08:26 towhee/models/swin_transformer/configs.py
+-rw-r--r--  2.0 unx     7134 b- defN 23-Mar-21 08:26 towhee/models/swin_transformer/model.py
+-rw-r--r--  2.0 unx     6114 b- defN 23-Mar-21 08:26 towhee/models/swin_transformer/swin_transformer_block.py
+-rw-r--r--  2.0 unx      620 b- defN 23-Mar-21 08:26 towhee/models/timesformer/__init__.py
+-rw-r--r--  2.0 unx     9537 b- defN 23-Mar-21 08:26 towhee/models/timesformer/timesformer.py
+-rw-r--r--  2.0 unx     6249 b- defN 23-Mar-21 08:26 towhee/models/timesformer/timesformer_block.py
+-rw-r--r--  2.0 unx     9090 b- defN 23-Mar-21 08:26 towhee/models/timesformer/timesformer_utils.py
+-rw-r--r--  2.0 unx      638 b- defN 23-Mar-21 08:26 towhee/models/transrac/__init__.py
+-rw-r--r--  2.0 unx     5724 b- defN 23-Mar-21 08:26 towhee/models/transrac/transrac.py
+-rw-r--r--  2.0 unx     3378 b- defN 23-Mar-21 08:26 towhee/models/transrac/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/tsm/__init__.py
+-rw-r--r--  2.0 unx     4683 b- defN 23-Mar-21 08:26 towhee/models/tsm/config.py
+-rw-r--r--  2.0 unx     5394 b- defN 23-Mar-21 08:26 towhee/models/tsm/mobilenet_v2.py
+-rw-r--r--  2.0 unx     5815 b- defN 23-Mar-21 08:26 towhee/models/tsm/temporal_shift.py
+-rw-r--r--  2.0 unx    13942 b- defN 23-Mar-21 08:26 towhee/models/tsm/tsm.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/uniformer/__init__.py
+-rw-r--r--  2.0 unx     4337 b- defN 23-Mar-21 08:26 towhee/models/uniformer/config.py
+-rw-r--r--  2.0 unx    21976 b- defN 23-Mar-21 08:26 towhee/models/uniformer/uniformer.py
+-rw-r--r--  2.0 unx       89 b- defN 23-Mar-21 08:26 towhee/models/utils/__init__.py
+-rw-r--r--  2.0 unx     4633 b- defN 23-Mar-21 08:26 towhee/models/utils/audio_preprocess.py
+-rw-r--r--  2.0 unx     1488 b- defN 23-Mar-21 08:26 towhee/models/utils/basic_ops.py
+-rw-r--r--  2.0 unx      808 b- defN 23-Mar-21 08:26 towhee/models/utils/causal_module.py
+-rw-r--r--  2.0 unx     4368 b- defN 23-Mar-21 08:26 towhee/models/utils/create_act.py
+-rw-r--r--  2.0 unx     2159 b- defN 23-Mar-21 08:26 towhee/models/utils/create_conv2d.py
+-rw-r--r--  2.0 unx     1328 b- defN 23-Mar-21 08:26 towhee/models/utils/create_conv2d_pad.py
+-rw-r--r--  2.0 unx     1664 b- defN 23-Mar-21 08:26 towhee/models/utils/create_model.py
+-rw-r--r--  2.0 unx     3929 b- defN 23-Mar-21 08:26 towhee/models/utils/create_resnet_basic_3d_module.py
+-rw-r--r--  2.0 unx     3658 b- defN 23-Mar-21 08:26 towhee/models/utils/download.py
+-rw-r--r--  2.0 unx     1208 b- defN 23-Mar-21 08:26 towhee/models/utils/fuse_bn.py
+-rw-r--r--  2.0 unx     1153 b- defN 23-Mar-21 08:26 towhee/models/utils/gelu_ignore_parameters.py
+-rw-r--r--  2.0 unx     1370 b- defN 23-Mar-21 08:26 towhee/models/utils/general_utils.py
+-rw-r--r--  2.0 unx     1720 b- defN 23-Mar-21 08:26 towhee/models/utils/get_relative_position_index.py
+-rw-r--r--  2.0 unx      958 b- defN 23-Mar-21 08:26 towhee/models/utils/get_window_size.py
+-rw-r--r--  2.0 unx     2642 b- defN 23-Mar-21 08:26 towhee/models/utils/init_vit_weights.py
+-rw-r--r--  2.0 unx     1439 b- defN 23-Mar-21 08:26 towhee/models/utils/pretrained_utils.py
+-rw-r--r--  2.0 unx     1556 b- defN 23-Mar-21 08:26 towhee/models/utils/round_width.py
+-rw-r--r--  2.0 unx     8505 b- defN 23-Mar-21 08:26 towhee/models/utils/video_transforms.py
+-rw-r--r--  2.0 unx     3957 b- defN 23-Mar-21 08:26 towhee/models/utils/weight_init.py
+-rw-r--r--  2.0 unx      527 b- defN 23-Mar-21 08:26 towhee/models/utils/window_partition.py
+-rw-r--r--  2.0 unx      891 b- defN 23-Mar-21 08:26 towhee/models/utils/window_partition3d.py
+-rw-r--r--  2.0 unx      690 b- defN 23-Mar-21 08:26 towhee/models/utils/window_reverse.py
+-rw-r--r--  2.0 unx      887 b- defN 23-Mar-21 08:26 towhee/models/utils/window_reverse3d.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/vggish/__init__.py
+-rw-r--r--  2.0 unx     2049 b- defN 23-Mar-21 08:26 towhee/models/vggish/torch_vggish.py
+-rw-r--r--  2.0 unx      824 b- defN 23-Mar-21 08:26 towhee/models/video_swin_transformer/__init__.py
+-rw-r--r--  2.0 unx     1643 b- defN 23-Mar-21 08:26 towhee/models/video_swin_transformer/compute_mask.py
+-rw-r--r--  2.0 unx     3992 b- defN 23-Mar-21 08:26 towhee/models/video_swin_transformer/get_configs.py
+-rw-r--r--  2.0 unx    14573 b- defN 23-Mar-21 08:26 towhee/models/video_swin_transformer/video_swin_transformer.py
+-rw-r--r--  2.0 unx     3925 b- defN 23-Mar-21 08:26 towhee/models/video_swin_transformer/video_swin_transformer_block.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/violet/__init__.py
+-rw-r--r--  2.0 unx     3916 b- defN 23-Mar-21 08:26 towhee/models/violet/violet.py
+-rw-r--r--  2.0 unx     1007 b- defN 23-Mar-21 08:26 towhee/models/vis4mer/__init__.py
+-rw-r--r--  2.0 unx     1471 b- defN 23-Mar-21 08:26 towhee/models/vis4mer/activation.py
+-rw-r--r--  2.0 unx     1944 b- defN 23-Mar-21 08:26 towhee/models/vis4mer/get_initializer.py
+-rw-r--r--  2.0 unx     2347 b- defN 23-Mar-21 08:26 towhee/models/vis4mer/linearactivation.py
+-rw-r--r--  2.0 unx     1662 b- defN 23-Mar-21 08:26 towhee/models/vis4mer/transposelinear.py
+-rw-r--r--  2.0 unx    10305 b- defN 23-Mar-21 08:26 towhee/models/vis4mer/utils.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Mar-21 08:26 towhee/models/visualization/__init__.py
+-rw-r--r--  2.0 unx     8524 b- defN 23-Mar-21 08:26 towhee/models/visualization/clip_visualization.py
+-rw-r--r--  2.0 unx     3193 b- defN 23-Mar-21 08:26 towhee/models/visualization/embedding_visualization.py
+-rw-r--r--  2.0 unx     7065 b- defN 23-Mar-21 08:26 towhee/models/visualization/transformer_visualization.py
+-rw-r--r--  2.0 unx      612 b- defN 23-Mar-21 08:26 towhee/models/vit/__init__.py
+-rw-r--r--  2.0 unx    10295 b- defN 23-Mar-21 08:26 towhee/models/vit/vit.py
+-rw-r--r--  2.0 unx     3878 b- defN 23-Mar-21 08:26 towhee/models/vit/vit_block.py
+-rw-r--r--  2.0 unx     1868 b- defN 23-Mar-21 08:26 towhee/models/vit/vit_utils.py
+-rw-r--r--  2.0 unx      617 b- defN 23-Mar-21 08:26 towhee/models/wave_vit/__init__.py
+-rw-r--r--  2.0 unx    10014 b- defN 23-Mar-21 08:26 towhee/models/wave_vit/wave_vit.py
+-rw-r--r--  2.0 unx     9731 b- defN 23-Mar-21 08:26 towhee/models/wave_vit/wave_vit_block.py
+-rw-r--r--  2.0 unx     6361 b- defN 23-Mar-21 08:26 towhee/models/wave_vit/wave_vit_utils.py
+-rw-r--r--  2.0 unx    11357 b- defN 23-May-04 08:05 towhee.models-1.0.0rc1.dist-info/LICENSE
+-rw-r--r--  2.0 unx    17255 b- defN 23-May-04 08:05 towhee.models-1.0.0rc1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 23-May-04 08:05 towhee.models-1.0.0rc1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       55 b- defN 23-May-04 08:05 towhee.models-1.0.0rc1.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx        7 b- defN 23-May-04 08:05 towhee.models-1.0.0rc1.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx    26320 b- defN 23-May-04 08:05 towhee.models-1.0.0rc1.dist-info/RECORD
+280 files, 2679966 bytes uncompressed, 1758198 bytes compressed:  34.4%
```

## zipnote {}

```diff
@@ -816,26 +816,26 @@
 
 Filename: towhee/models/wave_vit/wave_vit_block.py
 Comment: 
 
 Filename: towhee/models/wave_vit/wave_vit_utils.py
 Comment: 
 
-Filename: towhee.models-0.9.0.dist-info/LICENSE
+Filename: towhee.models-1.0.0rc1.dist-info/LICENSE
 Comment: 
 
-Filename: towhee.models-0.9.0.dist-info/METADATA
+Filename: towhee.models-1.0.0rc1.dist-info/METADATA
 Comment: 
 
-Filename: towhee.models-0.9.0.dist-info/WHEEL
+Filename: towhee.models-1.0.0rc1.dist-info/WHEEL
 Comment: 
 
-Filename: towhee.models-0.9.0.dist-info/entry_points.txt
+Filename: towhee.models-1.0.0rc1.dist-info/entry_points.txt
 Comment: 
 
-Filename: towhee.models-0.9.0.dist-info/top_level.txt
+Filename: towhee.models-1.0.0rc1.dist-info/top_level.txt
 Comment: 
 
-Filename: towhee.models-0.9.0.dist-info/RECORD
+Filename: towhee.models-1.0.0rc1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## towhee/models/clip4clip/clip4clip.py

```diff
@@ -107,15 +107,15 @@
         else:
             return None
 
     def get_sequence_output(self, input_ids, shaped=False):
         if shaped is False:
             input_ids = input_ids.view(-1, input_ids.shape[-1])
         bs_pair = input_ids.size(0)
-        sequence_hidden = self.clip.encode_text(input_ids, clip4clip=True).float()
+        sequence_hidden = self.clip.encode_text(input_ids, clip4clip=True, device=input_ids.device).float()
         sequence_hidden = sequence_hidden.view(bs_pair, -1, sequence_hidden.size(-1))
 
         return sequence_hidden
 
     def get_visual_output(self, video, video_mask, shaped=False):
         if shaped is False:
             video_mask = video_mask.view(-1, video_mask.shape[-1])
```

## towhee/models/clip4clip/utils.py

```diff
@@ -36,15 +36,15 @@
 
     Returns:
         Ndarray of ID list.
     """
     special_token = {"CLS_TOKEN": "<|startoftext|>", "SEP_TOKEN": "<|endoftext|>",
                      "MASK_TOKEN": "[MASK]", "UNK_TOKEN": "[UNK]", "PAD_TOKEN": "[PAD]"}
 
-    pairs_text = np.zeros((1, max_words), dtype=np.long)
+    pairs_text = np.zeros((1, max_words), dtype=np.int32)
 
     words = tokenize(words)
     words = [special_token["CLS_TOKEN"]] + words
     total_length_with_cls = max_words - 1
     if len(words) > total_length_with_cls:
         words = words[:total_length_with_cls]
     words = words + [special_token["SEP_TOKEN"]]
```

## towhee/models/drl/drl.py

```diff
@@ -198,15 +198,15 @@
             return None
 
     def get_text_feat(self, text_ids, shaped=False):
         if shaped is False:
             text_ids = text_ids.view(-1, text_ids.shape[-1])
 
         bs_pair = text_ids.size(0)
-        text_feat = self.clip.encode_text(text_ids, clip4clip=True, return_hidden=True)[1].float()
+        text_feat = self.clip.encode_text(text_ids, clip4clip=True, return_hidden=True, device=text_ids.device)[1].float()
         text_feat = text_feat.view(bs_pair, -1, text_feat.size(-1))
 
         return text_feat
 
     def get_video_feat(self, video, video_mask, shaped=False):
         if shaped is False:
             video_mask = video_mask.view(-1, video_mask.shape[-1])
```

## towhee/models/isc/isc.py

```diff
@@ -51,31 +51,36 @@
     def _init_params(self):
         nn.init.xavier_normal_(self.fc.weight)
         nn.init.constant_(self.bn.weight, 1)
         nn.init.constant_(self.bn.bias, 0)
 
     def forward(self, x):
         batch_size = x.shape[0]
+
         x = self.backbone(x)[-1]
         assert len(x.shape) == 4
+        _, _, height, width = x.shape
+        if torch.is_tensor(height):
+            height = height.item()
+            width = width.item()
         p = self.p if self.training else self.eval_p
-        x = nn.functional.avg_pool2d(x.clamp(min=1e-6).pow(p), (x.size(-2), x.size(-1))).pow(1./p)
+        x = nn.functional.avg_pool2d(x.clamp(min=1e-6).pow(p), (height, width)).pow(1./p)
         x = x.view(batch_size, -1)
         x = self.fc(x)
         x = self.bn(x)
         x = nn.functional.normalize(x)
         return x
 
 
 def create_model(timm_backbone=None, pretrained=False, checkpoint_path=None, device=None, **kwargs):
     if device is None:
         device = 'cuda' if torch.cuda.is_available() else 'cpu'
     if timm_backbone:
         import timm  # pylint: disable=C0415
-        backbone = timm.create_model(timm_backbone, features_only=True, pretrained=pretrained)
+        backbone = timm.create_model(timm_backbone, features_only=True, pretrained=False)
         kwargs.update(backbone=backbone)
     model = ISCNet(**kwargs).to(device)
     if pretrained:
         assert checkpoint_path, 'Checkpoint path is mandatory for pretrained model.'
         state_dict = torch.load(checkpoint_path, map_location=device)
         if 'state_dict' in state_dict:
             state_dict = state_dict['state_dict']
```

## Comparing `towhee.models-0.9.0.dist-info/LICENSE` & `towhee.models-1.0.0rc1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `towhee.models-0.9.0.dist-info/METADATA` & `towhee.models-1.0.0rc1.dist-info/METADATA`

 * *Files 19% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: towhee.models
-Version: 0.9.0
+Version: 1.0.0rc1
 Summary: Towhee is a framework that helps you encode your unstructured data into embeddings.
 Home-page: https://github.com/towhee-io/towhee
 Author: Towhee Team
 Author-email: towhee-team@zilliz.com
 License: http://www.apache.org/licenses/LICENSE-2.0
 Platform: unix
 Platform: linux
@@ -12,18 +12,16 @@
 Platform: win32
 Description-Content-Type: text/markdown
 License-File: LICENSE
 Requires-Dist: requests (>=2.12.5)
 Requires-Dist: tqdm (>=4.59.0)
 Requires-Dist: tabulate
 Requires-Dist: numpy
-Requires-Dist: pygit2 (<=1.10.1)
-Requires-Dist: pgzip
-Requires-Dist: pyarrow
 Requires-Dist: twine
+Requires-Dist: contextvars ; python_version <= "3.6"
 Requires-Dist: importlib-resources ; python_version<'3.7'
 
 &nbsp;
 
 <p align="center">
     <img src="towhee_logo.png#gh-light-mode-only" width="60%"/>
     <img src="assets/towhee_logo_dark.png#gh-dark-mode-only" width="60%"/>
@@ -71,19 +69,70 @@
 
 :package:&emsp;**Data Processing:** Towhee also provides traditional methods alongside neural network models to help you build practical data processing pipelines. We have a rich pool of operators available, such as video decoding, audio slicing, frame sampling, feature vector dimension reduction, ensembling, and database operations.
 
 :snake:&emsp;**Pythonic API:** Towhee includes a Pythonic method-chaining API for describing custom data processing pipelines. We also support schemas, which makes processing unstructured data as easy as handling tabular data.
 
 ## What's New
 
+**v0.9.0 Dec. 2, 2022**
+* Added one video classification model:
+[*Vis4mer*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/vis4mer)
+* Added three visual backbones:
+[*MCProp*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/mcprop), 
+[*RepLKNet*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/replknet), 
+[*Shunted Transformer*](https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/shunted_transformer)
+* Add two code search operators:
+[*code_search.codebert*](https://towhee.io/code-search/codebert), 
+[*code_search.unixcoder*](https://towhee.io/code-search/unixcoder)
+* Add five image captioning operators: 
+[*image_captioning.expansionnet-v2*](https://towhee.io/image-captioning/expansionnet-v2), 
+[*image_captioning.magic*](https://towhee.io/image-captioning/magic),
+[*image_captioning.clip_caption_reward*](https://towhee.io/image-captioning/clip-caption-reward), 
+[*image_captioning.blip*](https://towhee.io/image-captioning/blip), 
+[*image_captioning.clipcap*](https://towhee.io/image-captioning/clipcap)
+* Add five image-text embedding operators: 
+[*image_text_embedding.albef*](https://towhee.io/image-text-embedding/albef), 
+[*image_text_embedding.ru_clip*](https://towhee.io/image-text-embedding/ru-clip), 
+[*image_text_embedding.japanese_clip*](https://towhee.io/image-text-embedding/japanese-clip),
+[*image_text_embedding.taiyi*](https://towhee.io/image-text-embedding/taiyi),
+[*image_text_embedding.slip*](https://towhee.io/image-text-embedding/slip)
+* Add one machine-translation operator: 
+[*machine_translation.opus_mt*](https://towhee.io/machine-translation/opus-mt)
+* Add one filter-tiny-segments operator:
+[*video-copy-detection.filter-tiny-segments*](https://towhee.io/video-copy-detection/filter-tiny-segments)
+* Add an advanced tutorial for audio fingerprinting: 
+[*Audio Fingerprint II: Music Detection with Temporal Localization*](https://github.com/towhee-io/examples/blob/main/audio/audio_fingerprint/audio_fingerprint_advanced.ipynb) (increased accuracy from 84% to 90%)
+
+**v0.8.1 Sep. 30, 2022**
+
+* Added four visual backbones:
+[*ISC*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/isc),
+[*MetaFormer*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/metaformer),
+[*ConvNext*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/convnext),
+[*HorNet*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/hornet)
+* Add two video de-copy operators:
+[*select-video*](https://towhee.io/video-copy-detection/select-video), 
+[*temporal-network*](https://towhee.io/video-copy-detection/temporal-network)
+* Add one image embedding operator specifically designed for image retrieval and video de-copy with SOTA performance on VCSL dataset:
+[*isc*](https://towhee.io/image-embedding/isc)
+* Add one audio embedding operator specified for audio fingerprint:
+[*audio_embedding.nnfp*](https://towhee.io/audio-embedding/nnfp) (with pretrained weights)
+* Add one tutorial for video de-copy: 
+[*How to Build a Video Segment Copy Detection System*](https://github.com/towhee-io/examples/blob/main/video/video_deduplication/segment_level/video_deduplication_at_segment_level.ipynb)
+* Add one beginner tutorial for audio fingerprint:
+[*Audio Fingerprint I: Build a Demo with Towhee & Milvus*](https://github.com/towhee-io/examples/blob/main/audio/audio_fingerprint/audio_fingerprint_beginner.ipynb)
+
+
 **v0.8.0 Aug. 16, 2022**
 
 * Towhee now supports generating an Nvidia Triton Server from a Towhee pipeline, with aditional support for GPU image decoding.
-* Added one audio fingerprinting model: [**nnfp**](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/nnfp)
-* Added two image embedding models: [**RepMLP**](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/repmlp), [**WaveViT**](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/wave_vit)
+* Added one audio fingerprinting model: 
+[*nnfp*](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/nnfp)
+* Added two image embedding models: 
+[*RepMLP*](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/repmlp), [**WaveViT**](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/wave_vit)
 
 **v0.7.3 Jul. 27, 2022**
 * Added one multimodal (text/image) model:
 [*CoCa*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/coca).
 * Added two video models for grounded situation recognition & repetitive action counting:
 [*CoFormer*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/coformer),
 [*TransRAC*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/transrac).
@@ -143,23 +192,23 @@
 ```python
 import towhee
 
 # create image embeddings and build index
 (
     towhee.glob['file_name']('./*.png')
           .image_decode['file_name', 'img']()
-          .image_text_embedding.clip['img', 'vec'](model_name='clip_vit_b32', modality='image')
+          .image_text_embedding.clip['img', 'vec'](model_name='clip_vit_base_patch32', modality='image')
           .tensor_normalize['vec','vec']()
           .to_faiss[('file_name', 'vec')](findex='./index.bin')
 )
 
 # search image by text
 results = (
     towhee.dc['text'](['puppy Corgi'])
-          .image_text_embedding.clip['text', 'vec'](model_name='clip_vit_b32', modality='text')
+          .image_text_embedding.clip['text', 'vec'](model_name='clip_vit_base_patch32', modality='text')
           .tensor_normalize['vec', 'vec']()
           .faiss_search['vec', 'results'](findex='./index.bin', k=3)
           .select['text', 'results']()
 )
 ```
 <img src="assets/towhee_example.png" style="width: 60%; height: 60%">
 
@@ -179,15 +228,15 @@
 
 ## Contributing
 
 Writing code is not the only way to contribute! Submitting issues, answering questions, and improving documentation are just some of the many ways you can help our growing community. Check out our [contributing page](https://github.com/towhee-io/towhee/blob/main/CONTRIBUTING.md) for more information.
 
 Special thanks goes to these folks for contributing to Towhee, either on Github, our Towhee Hub, or elsewhere:
 <br><!-- Do not remove start of hero-bot --><br>
-<img src="https://img.shields.io/badge/all--contributors-34-orange"><br>
+<img src="https://img.shields.io/badge/all--contributors-33-orange"><br>
 <a href="https://github.com/AniTho"><img src="https://avatars.githubusercontent.com/u/34787227?v=4" width="30px" /></a>
 <a href="https://github.com/Chiiizzzy"><img src="https://avatars.githubusercontent.com/u/72550076?v=4" width="30px" /></a>
 <a href="https://github.com/GuoRentong"><img src="https://avatars.githubusercontent.com/u/57477222?v=4" width="30px" /></a>
 <a href="https://github.com/NicoYuan1986"><img src="https://avatars.githubusercontent.com/u/109071306?v=4" width="30px" /></a>
 <a href="https://github.com/Tumao727"><img src="https://avatars.githubusercontent.com/u/20420181?v=4" width="30px" /></a>
 <a href="https://github.com/YuDongPan"><img src="https://avatars.githubusercontent.com/u/88148730?v=4" width="30px" /></a>
 <a href="https://github.com/binbinlv"><img src="https://avatars.githubusercontent.com/u/83755740?v=4" width="30px" /></a>
@@ -195,15 +244,14 @@
 <a href="https://github.com/dreamfireyu"><img src="https://avatars.githubusercontent.com/u/47691077?v=4" width="30px" /></a>
 <a href="https://github.com/filip-halt"><img src="https://avatars.githubusercontent.com/u/81822489?v=4" width="30px" /></a>
 <a href="https://github.com/fzliu"><img src="https://avatars.githubusercontent.com/u/6334158?v=4" width="30px" /></a>
 <a href="https://github.com/gexy185"><img src="https://avatars.githubusercontent.com/u/103474331?v=4" width="30px" /></a>
 <a href="https://github.com/hyf3513OneGO"><img src="https://avatars.githubusercontent.com/u/67197231?v=4" width="30px" /></a>
 <a href="https://github.com/jaelgu"><img src="https://avatars.githubusercontent.com/u/86251631?v=4" width="30px" /></a>
 <a href="https://github.com/jeffoverflow"><img src="https://avatars.githubusercontent.com/u/24581746?v=4" width="30px" /></a>
-<a href="https://github.com/jennyli-z"><img src="https://avatars.githubusercontent.com/u/93511422?v=4" width="30px" /></a>
 <a href="https://github.com/jingkl"><img src="https://avatars.githubusercontent.com/u/34296482?v=4" width="30px" /></a>
 <a href="https://github.com/jinlingxu06"><img src="https://avatars.githubusercontent.com/u/106302799?v=4" width="30px" /></a>
 <a href="https://github.com/junjiejiangjjj"><img src="https://avatars.githubusercontent.com/u/14136703?v=4" width="30px" /></a>
 <a href="https://github.com/krishnakatyal"><img src="https://avatars.githubusercontent.com/u/37455387?v=4" width="30px" /></a>
 <a href="https://github.com/omartarek206"><img src="https://avatars.githubusercontent.com/u/40853054?v=4" width="30px" /></a>
 <a href="https://github.com/oneseer"><img src="https://avatars.githubusercontent.com/u/28955741?v=4" width="30px" /></a>
 <a href="https://github.com/pravee42"><img src="https://avatars.githubusercontent.com/u/65100038?v=4" width="30px" /></a>
```

### html2text {}

```diff
@@ -1,17 +1,16 @@
-Metadata-Version: 2.1 Name: towhee.models Version: 0.9.0 Summary: Towhee is a
-framework that helps you encode your unstructured data into embeddings. Home-
+Metadata-Version: 2.1 Name: towhee.models Version: 1.0.0rc1 Summary: Towhee is
+a framework that helps you encode your unstructured data into embeddings. Home-
 page: https://github.com/towhee-io/towhee Author: Towhee Team Author-email:
 towhee-team@zilliz.com License: http://www.apache.org/licenses/LICENSE-2.0
 Platform: unix Platform: linux Platform: osx Platform: win32 Description-
 Content-Type: text/markdown License-File: LICENSE Requires-Dist: requests
 (>=2.12.5) Requires-Dist: tqdm (>=4.59.0) Requires-Dist: tabulate Requires-
-Dist: numpy Requires-Dist: pygit2 (<=1.10.1) Requires-Dist: pgzip Requires-
-Dist: pyarrow Requires-Dist: twine Requires-Dist: importlib-resources ;
-python_version<'3.7' 
+Dist: numpy Requires-Dist: twine Requires-Dist: contextvars ; python_version <=
+"3.6" Requires-Dist: importlib-resources ; python_version<'3.7' 
 [towhee_logo.png#gh-light-mode-only] [assets/towhee_logo_dark.png#gh-dark-mode-
                                      only]
                    **** x2vec, Towhee is all you need! ****
                        **** ENGLISH |  ****
 [join-slack] [twitter] [license] [github_actions] [coverage]
  [Towhee](https://towhee.io) makes it easy to build neural data processing
 pipelines for AI applications. We provide hundreds of models, algorithms, and
@@ -27,94 +26,139 @@
 provides traditional methods alongside neural network models to help you build
 practical data processing pipelines. We have a rich pool of operators
 available, such as video decoding, audio slicing, frame sampling, feature
 vector dimension reduction, ensembling, and database operations. :snake:
 &emsp;**Pythonic API:** Towhee includes a Pythonic method-chaining API for
 describing custom data processing pipelines. We also support schemas, which
 makes processing unstructured data as easy as handling tabular data. ## What's
-New **v0.8.0 Aug. 16, 2022** * Towhee now supports generating an Nvidia Triton
-Server from a Towhee pipeline, with aditional support for GPU image decoding. *
-Added one audio fingerprinting model: [**nnfp**](https://github.com/towhee-io/
-towhee/tree/branch0.8.0/towhee/models/nnfp) * Added two image embedding models:
-[**RepMLP**](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/
-models/repmlp), [**WaveViT**](https://github.com/towhee-io/towhee/tree/
-branch0.8.0/towhee/models/wave_vit) **v0.7.3 Jul. 27, 2022** * Added one
-multimodal (text/image) model: [*CoCa*](https://github.com/towhee-io/towhee/
-tree/branch0.7.3/towhee/models/coca). * Added two video models for grounded
-situation recognition & repetitive action counting: [*CoFormer*](https://
-github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/coformer),
-[*TransRAC*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/
-models/transrac). * Added two SoTA models for image tasks (image retrieval,
-image classification, etc.): [*CVNet*](https://github.com/towhee-io/towhee/
-tree/branch0.7.3/towhee/models/cvnet), [*MaxViT*](https://github.com/towhee-io/
-towhee/tree/branch0.7.3/towhee/models/max_vit) **v0.7.1 Jul. 1, 2022** * Added
-one image embedding model: [*MPViT*](https://towhee.io/image-embedding/mpvit).
-* Added two video retrieval models: [*BridgeFormer*](https://towhee.io/video-
-text-embedding/bridge-former), [*collaborative-experts*](https://towhee.io/
-video-text-embedding/collaborative-experts). * Added FAISS-based ANNSearch
-operators: *to_faiss*, *faiss_search*. **v0.7.0 Jun. 24, 2022** * Added six
-video understanding/classification models: [*Video Swin Transformer*](https://
-towhee.io/action-classification/video-swin-transformer), [*TSM*](https://
-towhee.io/action-classification/tsm), [*Uniformer*](https://towhee.io/action-
-classification/uniformer), [*OMNIVORE*](https://towhee.io/action-
-classification/omnivore), [*TimeSformer*](https://towhee.io/action-
-classification/timesformer), [*MoViNets*](https://towhee.io/action-
-classification/movinet). * Added four video retrieval models: [*CLIP4Clip*]
-(https://towhee.io/video-text-embedding/clip4clip), [*DRL*](https://towhee.io/
-video-text-embedding/drl), [*Frozen in Time*](https://towhee.io/video-text-
-embedding/frozen-in-time), [*MDMMT*](https://towhee.io/video-text-embedding/
-mdmmt). **v0.6.1 May. 13, 2022** * Added three text-image retrieval models:
-[*CLIP*](https://towhee.io/image-text-embedding/clip), [*BLIP*](https://
-towhee.io/image-text-embedding/blip), [*LightningDOT*](https://towhee.io/image-
-text-embedding/lightningdot). * Added six video understanding/classification
-models from PyTorchVideo: [*I3D*](https://towhee.io/action-classification/
-pytorchvideo), [*C2D*](https://towhee.io/action-classification/pytorchvideo),
-[*Slow*](https://towhee.io/action-classification/pytorchvideo), [*SlowFast*]
-(https://towhee.io/action-classification/pytorchvideo), [*X3D*](https://
-towhee.io/action-classification/pytorchvideo), [*MViT*](https://towhee.io/
-action-classification/pytorchvideo). ## Getting started Towhee requires Python
-3.6+. You can install Towhee via `pip`: ```bash pip install towhee
-towhee.models ``` If you run into any pip-related install problems, please try
-to upgrade pip with `pip install -U pip`. Let's try your first Towhee pipeline.
-Below is an example for how to create a CLIP-based cross modal retrieval
-pipeline with only 15 lines of code. ```python import towhee # create image
-embeddings and build index ( towhee.glob['file_name']('./*.png') .image_decode
-['file_name', 'img']() .image_text_embedding.clip['img', 'vec']
-(model_name='clip_vit_b32', modality='image') .tensor_normalize['vec','vec']()
-.to_faiss[('file_name', 'vec')](findex='./index.bin') ) # search image by text
-results = ( towhee.dc['text'](['puppy Corgi']) .image_text_embedding.clip
-['text', 'vec'](model_name='clip_vit_b32', modality='text') .tensor_normalize
-['vec', 'vec']() .faiss_search['vec', 'results'](findex='./index.bin', k=3)
-.select['text', 'results']() ) ``` [assets/towhee_example.png] Learn more
-examples from the [Towhee Bootcamp](https://codelabs.towhee.io/). ## Core
-Concepts Towhee is composed of four main building blocks - `Operators`,
-`Pipelines`, `DataCollection API` and `Engine`. - __Operators__: An operator is
-a single building block of a neural data processing pipeline. Different
-implementations of operators are categorized by tasks, with each task having a
-standard interface. An operator can be a deep learning model, a data processing
-method, or a Python function. - __Pipelines__: A pipeline is composed of
-several operators interconnected in the form of a DAG (directed acyclic graph).
-This DAG can direct complex functionalities, such as embedding feature
-extraction, data tagging, and cross modal data analysis. - __DataCollection
-API__: A Pythonic and method-chaining style API for building custom pipelines.
-A pipeline defined by the DataColltion API can be run locally on a laptop for
-fast prototyping and then be converted to a docker image, with end-to-end
-optimizations, for production-ready environments. - __Engine__: The engine sits
-at Towhee's core. Given a pipeline, the engine will drive dataflow among
-individual operators, schedule tasks, and monitor compute resource usage (CPU/
-GPU/etc). We provide a basic engine within Towhee to run pipelines on a single-
-instance machine and a Triton-based engine for docker containers. ##
-Contributing Writing code is not the only way to contribute! Submitting issues,
-answering questions, and improving documentation are just some of the many ways
-you can help our growing community. Check out our [contributing page](https://
-github.com/towhee-io/towhee/blob/main/CONTRIBUTING.md) for more information.
-Special thanks goes to these folks for contributing to Towhee, either on
-Github, our Towhee Hub, or elsewhere:
+New **v0.9.0 Dec. 2, 2022** * Added one video classification model: [*Vis4mer*]
+(https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/vis4mer) *
+Added three visual backbones: [*MCProp*](https://github.com/towhee-io/towhee/
+tree/branch0.9.0/towhee/models/mcprop), [*RepLKNet*](https://github.com/towhee-
+io/towhee/tree/branch0.9.0/towhee/models/replknet), [*Shunted Transformer*]
+(https://github.com/towhee-io/towhee/tree/branch0.9.0/towhee/models/
+shunted_transformer) * Add two code search operators: [*code_search.codebert*]
+(https://towhee.io/code-search/codebert), [*code_search.unixcoder*](https://
+towhee.io/code-search/unixcoder) * Add five image captioning operators:
+[*image_captioning.expansionnet-v2*](https://towhee.io/image-captioning/
+expansionnet-v2), [*image_captioning.magic*](https://towhee.io/image-
+captioning/magic), [*image_captioning.clip_caption_reward*](https://towhee.io/
+image-captioning/clip-caption-reward), [*image_captioning.blip*](https://
+towhee.io/image-captioning/blip), [*image_captioning.clipcap*](https://
+towhee.io/image-captioning/clipcap) * Add five image-text embedding operators:
+[*image_text_embedding.albef*](https://towhee.io/image-text-embedding/albef),
+[*image_text_embedding.ru_clip*](https://towhee.io/image-text-embedding/ru-
+clip), [*image_text_embedding.japanese_clip*](https://towhee.io/image-text-
+embedding/japanese-clip), [*image_text_embedding.taiyi*](https://towhee.io/
+image-text-embedding/taiyi), [*image_text_embedding.slip*](https://towhee.io/
+image-text-embedding/slip) * Add one machine-translation operator:
+[*machine_translation.opus_mt*](https://towhee.io/machine-translation/opus-mt)
+* Add one filter-tiny-segments operator: [*video-copy-detection.filter-tiny-
+segments*](https://towhee.io/video-copy-detection/filter-tiny-segments) * Add
+an advanced tutorial for audio fingerprinting: [*Audio Fingerprint II: Music
+Detection with Temporal Localization*](https://github.com/towhee-io/examples/
+blob/main/audio/audio_fingerprint/audio_fingerprint_advanced.ipynb) (increased
+accuracy from 84% to 90%) **v0.8.1 Sep. 30, 2022** * Added four visual
+backbones: [*ISC*](https://github.com/towhee-io/towhee/tree/branch0.8.1/towhee/
+models/isc), [*MetaFormer*](https://github.com/towhee-io/towhee/tree/
+branch0.8.1/towhee/models/metaformer), [*ConvNext*](https://github.com/towhee-
+io/towhee/tree/branch0.8.1/towhee/models/convnext), [*HorNet*](https://
+github.com/towhee-io/towhee/tree/branch0.8.1/towhee/models/hornet) * Add two
+video de-copy operators: [*select-video*](https://towhee.io/video-copy-
+detection/select-video), [*temporal-network*](https://towhee.io/video-copy-
+detection/temporal-network) * Add one image embedding operator specifically
+designed for image retrieval and video de-copy with SOTA performance on VCSL
+dataset: [*isc*](https://towhee.io/image-embedding/isc) * Add one audio
+embedding operator specified for audio fingerprint: [*audio_embedding.nnfp*]
+(https://towhee.io/audio-embedding/nnfp) (with pretrained weights) * Add one
+tutorial for video de-copy: [*How to Build a Video Segment Copy Detection
+System*](https://github.com/towhee-io/examples/blob/main/video/
+video_deduplication/segment_level/video_deduplication_at_segment_level.ipynb) *
+Add one beginner tutorial for audio fingerprint: [*Audio Fingerprint I: Build a
+Demo with Towhee & Milvus*](https://github.com/towhee-io/examples/blob/main/
+audio/audio_fingerprint/audio_fingerprint_beginner.ipynb) **v0.8.0 Aug. 16,
+2022** * Towhee now supports generating an Nvidia Triton Server from a Towhee
+pipeline, with aditional support for GPU image decoding. * Added one audio
+fingerprinting model: [*nnfp*](https://github.com/towhee-io/towhee/tree/
+branch0.8.0/towhee/models/nnfp) * Added two image embedding models: [*RepMLP*]
+(https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/models/repmlp),
+[**WaveViT**](https://github.com/towhee-io/towhee/tree/branch0.8.0/towhee/
+models/wave_vit) **v0.7.3 Jul. 27, 2022** * Added one multimodal (text/image)
+model: [*CoCa*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/
+models/coca). * Added two video models for grounded situation recognition &
+repetitive action counting: [*CoFormer*](https://github.com/towhee-io/towhee/
+tree/branch0.7.3/towhee/models/coformer), [*TransRAC*](https://github.com/
+towhee-io/towhee/tree/branch0.7.3/towhee/models/transrac). * Added two SoTA
+models for image tasks (image retrieval, image classification, etc.): [*CVNet*]
+(https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/cvnet),
+[*MaxViT*](https://github.com/towhee-io/towhee/tree/branch0.7.3/towhee/models/
+max_vit) **v0.7.1 Jul. 1, 2022** * Added one image embedding model: [*MPViT*]
+(https://towhee.io/image-embedding/mpvit). * Added two video retrieval models:
+[*BridgeFormer*](https://towhee.io/video-text-embedding/bridge-former),
+[*collaborative-experts*](https://towhee.io/video-text-embedding/collaborative-
+experts). * Added FAISS-based ANNSearch operators: *to_faiss*, *faiss_search*.
+**v0.7.0 Jun. 24, 2022** * Added six video understanding/classification models:
+[*Video Swin Transformer*](https://towhee.io/action-classification/video-swin-
+transformer), [*TSM*](https://towhee.io/action-classification/tsm),
+[*Uniformer*](https://towhee.io/action-classification/uniformer), [*OMNIVORE*]
+(https://towhee.io/action-classification/omnivore), [*TimeSformer*](https://
+towhee.io/action-classification/timesformer), [*MoViNets*](https://towhee.io/
+action-classification/movinet). * Added four video retrieval models:
+[*CLIP4Clip*](https://towhee.io/video-text-embedding/clip4clip), [*DRL*](https:
+//towhee.io/video-text-embedding/drl), [*Frozen in Time*](https://towhee.io/
+video-text-embedding/frozen-in-time), [*MDMMT*](https://towhee.io/video-text-
+embedding/mdmmt). **v0.6.1 May. 13, 2022** * Added three text-image retrieval
+models: [*CLIP*](https://towhee.io/image-text-embedding/clip), [*BLIP*](https:/
+/towhee.io/image-text-embedding/blip), [*LightningDOT*](https://towhee.io/
+image-text-embedding/lightningdot). * Added six video understanding/
+classification models from PyTorchVideo: [*I3D*](https://towhee.io/action-
+classification/pytorchvideo), [*C2D*](https://towhee.io/action-classification/
+pytorchvideo), [*Slow*](https://towhee.io/action-classification/pytorchvideo),
+[*SlowFast*](https://towhee.io/action-classification/pytorchvideo), [*X3D*]
+(https://towhee.io/action-classification/pytorchvideo), [*MViT*](https://
+towhee.io/action-classification/pytorchvideo). ## Getting started Towhee
+requires Python 3.6+. You can install Towhee via `pip`: ```bash pip install
+towhee towhee.models ``` If you run into any pip-related install problems,
+please try to upgrade pip with `pip install -U pip`. Let's try your first
+Towhee pipeline. Below is an example for how to create a CLIP-based cross modal
+retrieval pipeline with only 15 lines of code. ```python import towhee # create
+image embeddings and build index ( towhee.glob['file_name']('./*.png')
+.image_decode['file_name', 'img']() .image_text_embedding.clip['img', 'vec']
+(model_name='clip_vit_base_patch32', modality='image') .tensor_normalize
+['vec','vec']() .to_faiss[('file_name', 'vec')](findex='./index.bin') ) #
+search image by text results = ( towhee.dc['text'](['puppy Corgi'])
+.image_text_embedding.clip['text', 'vec'](model_name='clip_vit_base_patch32',
+modality='text') .tensor_normalize['vec', 'vec']() .faiss_search['vec',
+'results'](findex='./index.bin', k=3) .select['text', 'results']() ) ```
+[assets/towhee_example.png] Learn more examples from the [Towhee Bootcamp]
+(https://codelabs.towhee.io/). ## Core Concepts Towhee is composed of four main
+building blocks - `Operators`, `Pipelines`, `DataCollection API` and `Engine`.
+- __Operators__: An operator is a single building block of a neural data
+processing pipeline. Different implementations of operators are categorized by
+tasks, with each task having a standard interface. An operator can be a deep
+learning model, a data processing method, or a Python function. -
+__Pipelines__: A pipeline is composed of several operators interconnected in
+the form of a DAG (directed acyclic graph). This DAG can direct complex
+functionalities, such as embedding feature extraction, data tagging, and cross
+modal data analysis. - __DataCollection API__: A Pythonic and method-chaining
+style API for building custom pipelines. A pipeline defined by the DataColltion
+API can be run locally on a laptop for fast prototyping and then be converted
+to a docker image, with end-to-end optimizations, for production-ready
+environments. - __Engine__: The engine sits at Towhee's core. Given a pipeline,
+the engine will drive dataflow among individual operators, schedule tasks, and
+monitor compute resource usage (CPU/GPU/etc). We provide a basic engine within
+Towhee to run pipelines on a single-instance machine and a Triton-based engine
+for docker containers. ## Contributing Writing code is not the only way to
+contribute! Submitting issues, answering questions, and improving documentation
+are just some of the many ways you can help our growing community. Check out
+our [contributing page](https://github.com/towhee-io/towhee/blob/main/
+CONTRIBUTING.md) for more information. Special thanks goes to these folks for
+contributing to Towhee, either on Github, our Towhee Hub, or elsewhere:
 
-[https://img.shields.io/badge/all--contributors-34-orange]
+[https://img.shields.io/badge/all--contributors-33-orange]
 [https://avatars.githubusercontent.com/u/34787227?v=4] [https://
 avatars.githubusercontent.com/u/72550076?v=4] [https://
 avatars.githubusercontent.com/u/57477222?v=4] [https://
 avatars.githubusercontent.com/u/109071306?v=4] [https://
 avatars.githubusercontent.com/u/20420181?v=4] [https://
 avatars.githubusercontent.com/u/88148730?v=4] [https://
 avatars.githubusercontent.com/u/83755740?v=4] [https://
@@ -122,15 +166,14 @@
 avatars.githubusercontent.com/u/47691077?v=4] [https://
 avatars.githubusercontent.com/u/81822489?v=4] [https://
 avatars.githubusercontent.com/u/6334158?v=4] [https://
 avatars.githubusercontent.com/u/103474331?v=4] [https://
 avatars.githubusercontent.com/u/67197231?v=4] [https://
 avatars.githubusercontent.com/u/86251631?v=4] [https://
 avatars.githubusercontent.com/u/24581746?v=4] [https://
-avatars.githubusercontent.com/u/93511422?v=4] [https://
 avatars.githubusercontent.com/u/34296482?v=4] [https://
 avatars.githubusercontent.com/u/106302799?v=4] [https://
 avatars.githubusercontent.com/u/14136703?v=4] [https://
 avatars.githubusercontent.com/u/37455387?v=4] [https://
 avatars.githubusercontent.com/u/40853054?v=4] [https://
 avatars.githubusercontent.com/u/28955741?v=4] [https://
 avatars.githubusercontent.com/u/65100038?v=4] [https://
```

## Comparing `towhee.models-0.9.0.dist-info/RECORD` & `towhee.models-1.0.0rc1.dist-info/RECORD`

 * *Files 5% similar despite different names*

```diff
@@ -22,17 +22,17 @@
 towhee/models/clip/__init__.py,sha256=IaeYF9bJeWL3q1X5GH_LJQx2m4yl7wYWKCVMHI7U3oU,639
 towhee/models/clip/auxilary.py,sha256=RaxNl99HbwiBlTDxjx1jQf0x-qeQbCEqJRaToT39HxI,20204
 towhee/models/clip/bpe_simple_vocab_16e6.txt.gz,sha256=kkaRrCiOVECSNhFWUq1KolD0ggPeUKnkcipuzUjWgEo,1356917
 towhee/models/clip/clip.py,sha256=C5I9TBXO3h4L79Bnw6guH85qg5Fi7RSzbYIkd4DpCII,30031
 towhee/models/clip/clip_utils.py,sha256=_4rcQ0PRefv_AxAPWbahWRIHVdv8gaC7NpjIy15evhA,9935
 towhee/models/clip/simple_tokenizer.py,sha256=zTiYCOHcB94uoVyZdWzqJzI5GZMgE-vuFJmp9Qk458o,5618
 towhee/models/clip4clip/__init__.py,sha256=QhA4vIRpuETlUOVYywNc8nPe60gRxcl0r-tM0jM3Q-Q,46
-towhee/models/clip4clip/clip4clip.py,sha256=mCp0DbSC2Im1HOSm3KJTAVUFJed38O1qQ7QH7kSXoac,10131
+towhee/models/clip4clip/clip4clip.py,sha256=pjJuQQLsywQb5f5SNacsuS1NXSW4alo43l8FwkQ3lMM,10156
 towhee/models/clip4clip/until_module.py,sha256=I_mP9z0QQQzg3KxYcSa7g9hEIp_EPgx1EpEmjZpglaY,2605
-towhee/models/clip4clip/utils.py,sha256=86T4UBs0ucVXzxVdN1rMjydUuM1XZ3ssSfkbM4exBAo,1836
+towhee/models/clip4clip/utils.py,sha256=oXXRWRIWFg7jt0t_y-ccC2-7gj-9o70zecdzueuKRDQ,1837
 towhee/models/coca/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 towhee/models/coca/coca.py,sha256=19aMdsJj_zq2wjEFzJ4zqXPpJ8dZRnmtRuhtIUNvpW0,11514
 towhee/models/coformer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 towhee/models/coformer/backbone.py,sha256=daSobWDRtjXXg-_v9Uzh3UssqlzfE4nakcr7lSOqPxU,5167
 towhee/models/coformer/coformer.py,sha256=tU_bRAB_jCmBZ13pqBaYg6TJhNMOuogos8IPNKWjnpU,10222
 towhee/models/coformer/config.py,sha256=cA-AEvqzluEqfmCZ3hy51oR7LPwmYLrBKktTR2DAFNk,10008
 towhee/models/coformer/transformer.py,sha256=_3nnNXJ5HD4wbcCc1Y4BOamcP7VYkQx6fncc-Jms2lQ,15471
@@ -47,29 +47,29 @@
 towhee/models/convnext/utils.py,sha256=nrm5J9agCfmn55h263sg0R6E9kP-agAWqpoIf4Hensk,4107
 towhee/models/cvnet/__init__.py,sha256=pHfteInPOBaVxm4c_-jetHWySrL2-WJ2I2Hzg-XhaVw,614
 towhee/models/cvnet/cvnet.py,sha256=CXghdOFn3heDbRcjmeEiXy0jiWTme__4i8HJxWA5Qdg,4665
 towhee/models/cvnet/cvnet_block.py,sha256=d-_LKK6oE9gQvGTYaK8D7mvnkowb8lV-D1DIDBJDWAM,6640
 towhee/models/cvnet/cvnet_utils.py,sha256=_dbGLWHSI14gvu7Wu4tyGAJLrPvpiPmAtzdyz47NE_I,2439
 towhee/models/cvnet/resnet.py,sha256=OiEo3KzbSPsPH_JCl1DMBhqZFxPMZiAVWZr-AO6Ba_w,8106
 towhee/models/drl/__init__.py,sha256=PgP1p-VWs-ePWvrYIxD5matG3lIz5muH_q7qP3MYrkk,19
-towhee/models/drl/drl.py,sha256=Z9ffeE-8kiBr47qKp7CPi09KheSSoRGdSnggVMidHd4,34544
+towhee/models/drl/drl.py,sha256=130rbEJYqTJV8TiXt9Tui2qj8fTBb4TDTlMzynkkY6E,34568
 towhee/models/drl/module_cross.py,sha256=g5xjqpUrp7M8wFBsK5Zk4lvVuwej_1xeVgpuINUYqIk,7936
 towhee/models/drl/until_module.py,sha256=37QJUYt6Vy_69VYMhFEfwltq1XPY5M30L44kB_pkoc0,4891
 towhee/models/embedding/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 towhee/models/embedding/embedding_extractor.py,sha256=rIDHuT2Twi1NSbkw57t4A4vpLyACuDhRZThYv0FoT-E,2061
 towhee/models/frozen_in_time/__init__.py,sha256=kUP9-Oplu2lj1C4Xv9qYFN0CZcY7FZokSr2wtViaXDM,623
 towhee/models/frozen_in_time/frozen_in_time.py,sha256=MWK4kyzbOGxs2g2Q9O6HAJ8Kv2PGP7mtLtLICEqxzC8,14921
 towhee/models/frozen_in_time/frozen_utils.py,sha256=TpMVcCebdKlkR9-km_L-J6Spbm9FFiUMhpF5XBSo3i4,2394
 towhee/models/frozen_in_time/frozen_video_transformer.py,sha256=vJUF_X85nwaZKGqWbiKKL3XLk-8iaTBWujxZHQP1B0A,15910
 towhee/models/hornet/__init__.py,sha256=lMpn0bHjNMkNtj6FJenyybsAe_pgSpq6biXS62ddgXg,659
 towhee/models/hornet/configs.py,sha256=eOJT-2JIjFsVxYQJ3p5cmaBnrMqKAFOuL3l71DOcFnw,4309
 towhee/models/hornet/hornet.py,sha256=O0q22iN4Iatrd3JVksYFe86t1juHqBXFyN_V0UFIz2M,5036
 towhee/models/hornet/utils.py,sha256=aYtfYn9AgjJhNx1vzodXxOqcISBqpZax7PLtIilzxzo,5983
 towhee/models/isc/__init__.py,sha256=yx6YmzvEl-PxrSJAP_PvCmlA6mRTojKW7a4YYk2qJiw,612
-towhee/models/isc/isc.py,sha256=aDK2XXfi4ZiPtKk_5ovCYzMLpZ0Urxx71NOg7DFEZxw,3341
+towhee/models/isc/isc.py,sha256=LKk3FBN5IhfSxbqp6zdF8mxbgi-on6gscACNKwvC_3Y,3470
 towhee/models/layers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 towhee/models/layers/aspp.py,sha256=P_vJjWww3QD7FmnT0TnJVb9zuYUx8EsxTQ0Jh1pkZg8,1638
 towhee/models/layers/attention.py,sha256=WbCCHDaje082ENtlbZB6OeHWpzyy98zT-5h-9elBG5I,6693
 towhee/models/layers/cond_conv2d.py,sha256=zWNz0Azc3xteEt-LPdgoiOprN5qls1xB3Yw9LwvafVs,5783
 towhee/models/layers/conv2d_same.py,sha256=0IuxYb__gomDjXhknu5QOxU7UKtUiPM5jqsooi73krI,2706
 towhee/models/layers/conv2d_separable.py,sha256=W80EScTE_5kdxUPQKIkRPRbgTBQ7nAbeExsEKeLr3RU,4180
 towhee/models/layers/conv4d.py,sha256=xndrtFSz6b4kam9tnW3LexW94bjI5LgNIfy3RcsxwcA,4354
@@ -268,13 +268,13 @@
 towhee/models/vit/vit.py,sha256=JCpAKp-e2U3ss45Ji-mJ9qRaAJpN7YBRdAHD98mTkaY,10295
 towhee/models/vit/vit_block.py,sha256=zprOJCOX2z273iMenk-ex9-Rr53SIvzQbCHCrJCqsm4,3878
 towhee/models/vit/vit_utils.py,sha256=y5PwJKR57TIwiO3ZssRO2r0esJLCiguYekhVkLUfZSI,1868
 towhee/models/wave_vit/__init__.py,sha256=oyVqphD9VR93Kj--l4TvVS-k-d3fqGuvEJcMJzohHaQ,617
 towhee/models/wave_vit/wave_vit.py,sha256=KqsLjc2DOQpjaIHvD909u6YNXSb7gGpv_VjbAYWC2aw,10014
 towhee/models/wave_vit/wave_vit_block.py,sha256=15DAbuYWb8SKr2crC4uS18W6bXmD_yClrPshaPYpriQ,9731
 towhee/models/wave_vit/wave_vit_utils.py,sha256=8nYx23xalB26swYnut2WlzWCj45md-1-hVHFoAydHPE,6361
-towhee.models-0.9.0.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-towhee.models-0.9.0.dist-info/METADATA,sha256=vdJhfTaOeHUBP6QjmRzjW-YxOno9yuxzDYkSdn5f8vQ,13931
-towhee.models-0.9.0.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
-towhee.models-0.9.0.dist-info/entry_points.txt,sha256=tGMn2QCTr-tOrFxMYYTyubVJ-ZOXIP-qf6P5GXOJWxI,55
-towhee.models-0.9.0.dist-info/top_level.txt,sha256=s8O0-CAA8lmENHKY-FR5ojkSAmqEOEkrpg6ITjplm4A,7
-towhee.models-0.9.0.dist-info/RECORD,,
+towhee.models-1.0.0rc1.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+towhee.models-1.0.0rc1.dist-info/METADATA,sha256=vTD7XpAd9hJM6-FRP3SMdyLqRgoGX4w2_YSDcetpWY0,17255
+towhee.models-1.0.0rc1.dist-info/WHEEL,sha256=G16H4A3IeoQmnOrYV4ueZGKSjhipXx8zc8nu9FGlvMA,92
+towhee.models-1.0.0rc1.dist-info/entry_points.txt,sha256=tGMn2QCTr-tOrFxMYYTyubVJ-ZOXIP-qf6P5GXOJWxI,55
+towhee.models-1.0.0rc1.dist-info/top_level.txt,sha256=s8O0-CAA8lmENHKY-FR5ojkSAmqEOEkrpg6ITjplm4A,7
+towhee.models-1.0.0rc1.dist-info/RECORD,,
```

