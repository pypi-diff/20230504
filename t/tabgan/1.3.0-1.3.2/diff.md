# Comparing `tmp/tabgan-1.3.0-py2.py3-none-any.whl.zip` & `tmp/tabgan-1.3.2-py2.py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,21 +1,21 @@
-Zip file size: 27152 bytes, number of entries: 19
--rw-r--r--  2.0 unx       59 b- defN 22-Jul-06 19:52 _ctgan/README.MD
--rw-r--r--  2.0 unx      191 b- defN 22-Jul-06 19:52 _ctgan/__init__.py
--rw-r--r--  2.0 unx     2955 b- defN 22-Jul-06 19:52 _ctgan/conditional.py
--rw-r--r--  2.0 unx     2271 b- defN 22-Jul-06 19:52 _ctgan/models.py
--rw-r--r--  2.0 unx     1160 b- defN 22-Jul-06 19:52 _ctgan/sampler.py
--rw-r--r--  2.0 unx    11113 b- defN 23-Jan-06 10:46 _ctgan/synthesizer.py
--rw-r--r--  2.0 unx     5787 b- defN 23-Jan-06 10:46 _ctgan/transformer.py
--rw-r--r--  2.0 unx      363 b- defN 21-Feb-18 20:07 tabgan/__init__.py
--rw-r--r--  2.0 unx     4477 b- defN 22-Jul-06 19:52 tabgan/abc_sampler.py
--rw-r--r--  2.0 unx     8051 b- defN 23-Jan-06 12:20 tabgan/adversarial_model.py
--rw-r--r--  2.0 unx    10734 b- defN 22-Jul-06 20:02 tabgan/encoders.py
--rw-r--r--  2.0 unx    14005 b- defN 22-Jul-06 19:52 tabgan/sampler.py
--rw-r--r--  2.0 unx     1140 b- defN 22-Jul-06 19:52 tabgan/utils.py
--rw-r--r--  2.0 unx       79 b- defN 23-Jan-06 12:26 tabgan-1.3.0.dist-info/AUTHORS.rst
--rw-r--r--  2.0 unx    11270 b- defN 23-Jan-06 12:26 tabgan-1.3.0.dist-info/LICENSE
--rw-r--r--  2.0 unx     9735 b- defN 23-Jan-06 12:26 tabgan-1.3.0.dist-info/METADATA
--rw-r--r--  2.0 unx      110 b- defN 23-Jan-06 12:26 tabgan-1.3.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       14 b- defN 23-Jan-06 12:26 tabgan-1.3.0.dist-info/top_level.txt
-?rw-rw-r--  2.0 unx     1458 b- defN 23-Jan-06 12:26 tabgan-1.3.0.dist-info/RECORD
-19 files, 84972 bytes uncompressed, 24818 bytes compressed:  70.8%
+Zip file size: 28090 bytes, number of entries: 19
+-rw-rw-rw-  2.0 fat       60 b- defN 23-May-04 19:07 _ctgan/README.MD
+-rw-rw-rw-  2.0 fat      202 b- defN 23-May-04 19:07 _ctgan/__init__.py
+-rw-rw-rw-  2.0 fat     4408 b- defN 23-May-04 19:07 _ctgan/conditional.py
+-rw-rw-rw-  2.0 fat     2466 b- defN 23-May-04 19:07 _ctgan/models.py
+-rw-rw-rw-  2.0 fat     1241 b- defN 23-May-04 19:07 _ctgan/sampler.py
+-rw-rw-rw-  2.0 fat    11533 b- defN 23-May-04 19:07 _ctgan/synthesizer.py
+-rw-rw-rw-  2.0 fat     5959 b- defN 23-May-04 19:07 _ctgan/transformer.py
+-rw-rw-rw-  2.0 fat      374 b- defN 23-May-04 19:07 tabgan/__init__.py
+-rw-rw-rw-  2.0 fat     4602 b- defN 23-May-04 19:07 tabgan/abc_sampler.py
+-rw-rw-rw-  2.0 fat     8263 b- defN 23-May-04 19:07 tabgan/adversarial_model.py
+-rw-rw-rw-  2.0 fat    10998 b- defN 23-May-04 19:07 tabgan/encoders.py
+-rw-rw-rw-  2.0 fat    14174 b- defN 23-May-04 19:07 tabgan/sampler.py
+-rw-rw-rw-  2.0 fat     1766 b- defN 23-May-04 19:07 tabgan/utils.py
+-rw-rw-rw-  2.0 fat       84 b- defN 23-May-04 19:13 tabgan-1.3.2.dist-info/AUTHORS.rst
+-rw-rw-rw-  2.0 fat    11474 b- defN 23-May-04 19:13 tabgan-1.3.2.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     9977 b- defN 23-May-04 19:13 tabgan-1.3.2.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      110 b- defN 23-May-04 19:13 tabgan-1.3.2.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       14 b- defN 23-May-04 19:13 tabgan-1.3.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     1458 b- defN 23-May-04 19:13 tabgan-1.3.2.dist-info/RECORD
+19 files, 89163 bytes uncompressed, 25756 bytes compressed:  71.1%
```

## zipnote {}

```diff
@@ -33,26 +33,26 @@
 
 Filename: tabgan/sampler.py
 Comment: 
 
 Filename: tabgan/utils.py
 Comment: 
 
-Filename: tabgan-1.3.0.dist-info/AUTHORS.rst
+Filename: tabgan-1.3.2.dist-info/AUTHORS.rst
 Comment: 
 
-Filename: tabgan-1.3.0.dist-info/LICENSE
+Filename: tabgan-1.3.2.dist-info/LICENSE
 Comment: 
 
-Filename: tabgan-1.3.0.dist-info/METADATA
+Filename: tabgan-1.3.2.dist-info/METADATA
 Comment: 
 
-Filename: tabgan-1.3.0.dist-info/WHEEL
+Filename: tabgan-1.3.2.dist-info/WHEEL
 Comment: 
 
-Filename: tabgan-1.3.0.dist-info/top_level.txt
+Filename: tabgan-1.3.2.dist-info/top_level.txt
 Comment: 
 
-Filename: tabgan-1.3.0.dist-info/RECORD
+Filename: tabgan-1.3.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## _ctgan/README.MD

 * *Ordering differences only*

```diff
@@ -1 +1 @@
-REFERENCE (initial code): https://github.com/sdv-dev/CTGAN
+REFERENCE (initial code): https://github.com/sdv-dev/CTGAN
```

## _ctgan/__init__.py

 * *Ordering differences only*

```diff
@@ -1,11 +1,11 @@
-# -*- coding: utf-8 -*-
-
-"""Top-level package for _ctgan."""
-
-__author__ = 'MIT Data To AI Lab'
-__email__ = 'dailabmit@gmail.com'
-__version__ = '0.2.1'
-
-__all__ = (
-    '_CTGANSynthesizer'
-)
+# -*- coding: utf-8 -*-
+
+"""Top-level package for _ctgan."""
+
+__author__ = 'MIT Data To AI Lab'
+__email__ = 'dailabmit@gmail.com'
+__version__ = '0.2.1'
+
+__all__ = (
+    '_CTGANSynthesizer'
+)
```

## _ctgan/conditional.py

```diff
@@ -1,98 +1,131 @@
-import numpy as np
-
-
-class ConditionalGenerator(object):
-    def __init__(self, data, output_info, log_frequency):
-        self.model = []
-
-        start = 0
-        skip = False
-        max_interval = 0
-        counter = 0
-        for item in output_info:
-            if item[1] == 'tanh':
-                start += item[0]
-                skip = True
-                continue
-
-            elif item[1] == 'softmax':
-                if skip:
-                    skip = False
-                    start += item[0]
-                    continue
-
-                end = start + item[0]
-                max_interval = max(max_interval, end - start)
-                counter += 1
-                self.model.append(np.argmax(data[:, start:end], axis=-1))
-                start = end
-
-            else:
-                assert 0
-
-        assert start == data.shape[1]
-
-        self.interval = []
-        self.n_col = 0
-        self.n_opt = 0
-        skip = False
-        start = 0
-        self.p = np.zeros((counter, max_interval))
-        for item in output_info:
-            if item[1] == 'tanh':
-                skip = True
-                start += item[0]
-                continue
-            elif item[1] == 'softmax':
-                if skip:
-                    start += item[0]
-                    skip = False
-                    continue
-                end = start + item[0]
-                tmp = np.sum(data[:, start:end], axis=0)
-                if log_frequency:
-                    tmp = np.log(tmp + 1)
-                tmp = tmp / np.sum(tmp)
-                self.p[self.n_col, :item[0]] = tmp
-                self.interval.append((self.n_opt, item[0]))
-                self.n_opt += item[0]
-                self.n_col += 1
-                start = end
-            else:
-                assert 0
-
-        self.interval = np.asarray(self.interval)
-
-    def random_choice_prob_index(self, idx):
-        a = self.p[idx]
-        r = np.expand_dims(np.random.rand(a.shape[0]), axis=1)
-        return (a.cumsum(axis=1) > r).argmax(axis=1)
-
-    def sample(self, batch):
-        if self.n_col == 0:
-            return None
-
-        batch = batch
-        idx = np.random.choice(np.arange(self.n_col), batch)
-
-        vec1 = np.zeros((batch, self.n_opt), dtype='float32')
-        mask1 = np.zeros((batch, self.n_col), dtype='float32')
-        mask1[np.arange(batch), idx] = 1
-        opt1prime = self.random_choice_prob_index(idx)
-        opt1 = self.interval[idx, 0] + opt1prime
-        vec1[np.arange(batch), opt1] = 1
-
-        return vec1, mask1, idx, opt1prime
-
-    def sample_zero(self, batch):
-        if self.n_col == 0:
-            return None
-
-        vec = np.zeros((batch, self.n_opt), dtype='float32')
-        idx = np.random.choice(np.arange(self.n_col), batch)
-        for i in range(batch):
-            col = idx[i]
-            pick = int(np.random.choice(self.model[col]))
-            vec[i, pick + self.interval[col, 0]] = 1
-
-        return vec
+import numpy as np
+
+
+class ConditionalGenerator(object):
+    """A class that generates conditional data based on the given input data and output information.
+
+    Args:
+        data (numpy.ndarray): The input data.
+        output_info (list): A list of tuples containing information about the output data.
+        log_frequency (bool): A boolean value indicating whether to use logarithmic frequency.
+
+    Attributes:
+        model (list): A list of models.
+        interval (numpy.ndarray): An array of intervals.
+        n_col (int): The number of columns.
+        n_opt (int): The number of options.
+        p (numpy.ndarray): An array of probabilities.
+    """
+    def __init__(self, data, output_info, log_frequency):
+        self.model = []
+
+        start = 0
+        skip = False
+        max_interval = 0
+        counter = 0
+        for item in output_info:
+            if item[1] == 'tanh':
+                start += item[0]
+                skip = True
+                continue
+
+            elif item[1] == 'softmax':
+                if skip:
+                    skip = False
+                    start += item[0]
+                    continue
+
+                end = start + item[0]
+                max_interval = max(max_interval, end - start)
+                counter += 1
+                self.model.append(np.argmax(data[:, start:end], axis=-1))
+                start = end
+
+            else:
+                raise AssertionError
+
+        if start != data.shape[1]:
+            raise AssertionError
+
+        self.interval = []
+        self.n_col = 0
+        self.n_opt = 0
+        skip = False
+        start = 0
+        self.p = np.zeros((counter, max_interval))
+        for item in output_info:
+            if item[1] == 'tanh':
+                skip = True
+                start += item[0]
+                continue
+            elif item[1] == 'softmax':
+                if skip:
+                    start += item[0]
+                    skip = False
+                    continue
+                end = start + item[0]
+                tmp = np.sum(data[:, start:end], axis=0)
+                if log_frequency:
+                    tmp = np.log(tmp + 1)
+                tmp = tmp / np.sum(tmp)
+                self.p[self.n_col, :item[0]] = tmp
+                self.interval.append((self.n_opt, item[0]))
+                self.n_opt += item[0]
+                self.n_col += 1
+                start = end
+            else:
+                raise AssertionError
+
+        self.interval = np.asarray(self.interval)
+
+    def random_choice_prob_index(self, idx):
+        """Randomly selects an index based on the given probabilities.
+        Args:
+            idx (numpy.ndarray): An array of indices.
+        Returns:
+            numpy.ndarray: An array of randomly selected indices.
+        """
+        a = self.p[idx]
+        r = np.expand_dims(np.random.rand(a.shape[0]), axis=1)
+        return (a.cumsum(axis=1) > r).argmax(axis=1)
+
+    def sample(self, batch):
+        """Samples data based on the given batch size.
+        Args:
+            batch (int): The batch size.
+        Returns:
+            tuple: A tuple containing the generated data, mask, index, and option.
+        """
+        if self.n_col == 0:
+            return None
+
+        batch = batch
+        idx = np.random.choice(np.arange(self.n_col), batch)
+
+        vec1 = np.zeros((batch, self.n_opt), dtype='float32')
+        mask1 = np.zeros((batch, self.n_col), dtype='float32')
+        mask1[np.arange(batch), idx] = 1
+        opt1prime = self.random_choice_prob_index(idx)
+        opt1 = self.interval[idx, 0] + opt1prime
+        vec1[np.arange(batch), opt1] = 1
+
+        return vec1, mask1, idx, opt1prime
+
+    def sample_zero(self, batch):
+        """Samples zero data based on the given batch size.
+        Args:
+            batch (int): The batch size.
+        Returns:
+            numpy.ndarray: An array of generated zero data.
+        """
+        if self.n_col == 0:
+            return None
+
+        vec = np.zeros((batch, self.n_opt), dtype='float32')
+        idx = np.random.choice(np.arange(self.n_col), batch)
+        for i in range(batch):
+            col = idx[i]
+            pick = int(np.random.choice(self.model[col]))
+            vec[i, pick + self.interval[col, 0]] = 1
+
+        return vec
```

## _ctgan/models.py

```diff
@@ -1,74 +1,75 @@
-import torch
-from torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential
-
-
-class Discriminator(Module):
-
-    def calc_gradient_penalty(self, real_data, fake_data, device='cpu', pac=10, lambda_=10):
-
-        alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)
-        alpha = alpha.repeat(1, pac, real_data.size(1))
-        alpha = alpha.view(-1, real_data.size(1))
-
-        interpolates = alpha * real_data + ((1 - alpha) * fake_data)
-
-        disc_interpolates = self(interpolates)
-
-        gradients = torch.autograd.grad(
-            outputs=disc_interpolates, inputs=interpolates,
-            grad_outputs=torch.ones(disc_interpolates.size(), device=device),
-            create_graph=True, retain_graph=True, only_inputs=True
-        )[0]
-
-        gradient_penalty = ((
-            gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1
-        ) ** 2).mean() * lambda_
-
-        return gradient_penalty
-
-    def __init__(self, input_dim, dis_dims, pack=10):
-        super(Discriminator, self).__init__()
-        dim = input_dim * pack
-        self.pack = pack
-        self.packdim = dim
-        seq = []
-        for item in list(dis_dims):
-            seq += [Linear(dim, item), LeakyReLU(0.2), Dropout(0.5)]
-            dim = item
-
-        seq += [Linear(dim, 1)]
-        self.seq = Sequential(*seq)
-
-    def forward(self, input):
-        assert input.size()[0] % self.pack == 0
-        return self.seq(input.view(-1, self.packdim))
-
-
-class Residual(Module):
-    def __init__(self, i, o):
-        super(Residual, self).__init__()
-        self.fc = Linear(i, o)
-        self.bn = BatchNorm1d(o)
-        self.relu = ReLU()
-
-    def forward(self, input):
-        out = self.fc(input)
-        out = self.bn(out)
-        out = self.relu(out)
-        return torch.cat([out, input], dim=1)
-
-
-class Generator(Module):
-    def __init__(self, embedding_dim, gen_dims, data_dim):
-        super(Generator, self).__init__()
-        dim = embedding_dim
-        seq = []
-        for item in list(gen_dims):
-            seq += [Residual(dim, item)]
-            dim += item
-        seq.append(Linear(dim, data_dim))
-        self.seq = Sequential(*seq)
-
-    def forward(self, input):
-        data = self.seq(input)
-        return data
+import torch
+from torch.nn import BatchNorm1d, Dropout, LeakyReLU, Linear, Module, ReLU, Sequential
+
+
+class Discriminator(Module):
+
+    def calc_gradient_penalty(self, real_data, fake_data, device='cpu', pac=10, lambda_=10):
+
+        alpha = torch.rand(real_data.size(0) // pac, 1, 1, device=device)
+        alpha = alpha.repeat(1, pac, real_data.size(1))
+        alpha = alpha.view(-1, real_data.size(1))
+
+        interpolates = alpha * real_data + ((1 - alpha) * fake_data)
+
+        disc_interpolates = self(interpolates)
+
+        gradients = torch.autograd.grad(
+            outputs=disc_interpolates, inputs=interpolates,
+            grad_outputs=torch.ones(disc_interpolates.size(), device=device),
+            create_graph=True, retain_graph=True, only_inputs=True
+        )[0]
+
+        gradient_penalty = ((
+            gradients.view(-1, pac * real_data.size(1)).norm(2, dim=1) - 1
+        ) ** 2).mean() * lambda_
+
+        return gradient_penalty
+
+    def __init__(self, input_dim, dis_dims, pack=10):
+        super(Discriminator, self).__init__()
+        dim = input_dim * pack
+        self.pack = pack
+        self.packdim = dim
+        seq = []
+        for item in list(dis_dims):
+            seq += [Linear(dim, item), LeakyReLU(0.2), Dropout(0.5)]
+            dim = item
+
+        seq += [Linear(dim, 1)]
+        self.seq = Sequential(*seq)
+
+    def forward(self, input):
+        if input.size()[0] % self.pack != 0:
+            raise ValueError("Batch size should be divisible to {}, but provided {}".format(self.pack, input.size()[0], ))
+        return self.seq(input.view(-1, self.packdim))
+
+
+class Residual(Module):
+    def __init__(self, i, o):
+        super(Residual, self).__init__()
+        self.fc = Linear(i, o)
+        self.bn = BatchNorm1d(o)
+        self.relu = ReLU()
+
+    def forward(self, input):
+        out = self.fc(input)
+        out = self.bn(out)
+        out = self.relu(out)
+        return torch.cat([out, input], dim=1)
+
+
+class Generator(Module):
+    def __init__(self, embedding_dim, gen_dims, data_dim):
+        super(Generator, self).__init__()
+        dim = embedding_dim
+        seq = []
+        for item in list(gen_dims):
+            seq += [Residual(dim, item)]
+            dim += item
+        seq.append(Linear(dim, data_dim))
+        self.seq = Sequential(*seq)
+
+    def forward(self, input):
+        data = self.seq(input)
+        return data
```

## _ctgan/sampler.py

```diff
@@ -1,46 +1,47 @@
-import numpy as np
-
-
-class Sampler(object):
-    """docstring for Sampler."""
-
-    def __init__(self, data, output_info):
-        super(Sampler, self).__init__()
-        self.data = data
-        self.model = []
-        self.n = len(data)
-
-        st = 0
-        skip = False
-        for item in output_info:
-            if item[1] == 'tanh':
-                st += item[0]
-                skip = True
-            elif item[1] == 'softmax':
-                if skip:
-                    skip = False
-                    st += item[0]
-                    continue
-
-                ed = st + item[0]
-                tmp = []
-                for j in range(item[0]):
-                    tmp.append(np.nonzero(data[:, st + j])[0])
-
-                self.model.append(tmp)
-                st = ed
-            else:
-                assert 0
-
-        assert st == data.shape[1]
-
-    def sample(self, n, col, opt):
-        if col is None:
-            idx = np.random.choice(np.arange(self.n), n)
-            return self.data[idx]
-
-        idx = []
-        for c, o in zip(col, opt):
-            idx.append(np.random.choice(self.model[c][o]))
-
-        return self.data[idx]
+import numpy as np
+
+
+class Sampler:
+    """docstring for Sampler."""
+
+    def __init__(self, data, output_info):
+        super(Sampler, self).__init__()
+        self.data = data
+        self.model = []
+        self.n = len(data)
+
+        st = 0
+        skip = False
+        for item in output_info:
+            if item[1] == 'tanh':
+                st += item[0]
+                skip = True
+            elif item[1] == 'softmax':
+                if skip:
+                    skip = False
+                    st += item[0]
+                    continue
+
+                ed = st + item[0]
+                tmp = []
+                for j in range(item[0]):
+                    tmp.append(np.nonzero(data[:, st + j])[0])
+
+                self.model.append(tmp)
+                st = ed
+            else:
+                raise AssertionError
+
+        if st != data.shape[1]:
+            raise AssertionError
+
+    def sample(self, n, col, opt):
+        if col is None:
+            idx = np.random.choice(np.arange(self.n), n)
+            return self.data[idx]
+
+        idx = []
+        for c, o in zip(col, opt):
+            idx.append(np.random.choice(self.model[c][o]))
+
+        return self.data[idx]
```

## _ctgan/synthesizer.py

```diff
@@ -1,311 +1,310 @@
-import logging
-
-import numpy as np
-import torch
-from torch import optim
-from torch.nn import functional
-from tqdm.autonotebook import tqdm
-
-from _ctgan.conditional import ConditionalGenerator
-from _ctgan.models import Discriminator, Generator
-from _ctgan.sampler import Sampler
-from _ctgan.transformer import DataTransformer
-
-
-class EarlyStopping:
-    """Early stops the training if validation loss doesn't improve after a given patience."""
-
-    def __init__(self, patience=7, verbose=False, delta=0):
-        """
-        Args:
-            patience (int): How long to wait after last time validation loss improved.
-                            Default: 7
-            delta (float): Minimum change in the monitored quantity to qualify as an improvement.
-                            Default: 0
-        """
-        self.patience = patience
-        self.counter = 0
-        self.best_score = None
-        self.early_stop = False
-        self.val_loss_min = np.Inf
-        self.delta = delta
-        self.verbose = verbose
-
-    def __call__(self, val_loss):
-
-        score = -val_loss
-
-        if self.best_score is None:
-            self.best_score = score
-        elif score < self.best_score + self.delta:
-            self.counter += 1
-            if self.counter >= self.patience:
-                logging.info("Early stoping for GAN. Best score: {:.2f} with patience = {}".format(self.best_score,
-                                                                                               self.patience))
-                self.early_stop = True
-        else:
-            self.best_score = score
-            self.counter = 0
-
-
-class _CTGANSynthesizer(object):
-    """Conditional Table GAN Synthesizer.
-
-    This is the core class of the CTGAN project, where the different components
-    are orchestrated together.
-
-    For more details about the process, please check the [Modeling Tabular data using
-    Conditional GAN](https://arxiv.org/abs/1907.00503) paper.
-
-    Args:
-        embedding_dim (int):
-            Size of the random sample passed to the Generator. Defaults to 128.
-        gen_dim (tuple or list of ints):
-            Size of the output samples for each one of the Residuals. A Resiudal Layer
-            will be created for each one of the values provided. Defaults to (256, 256).
-        dis_dim (tuple or list of ints):
-            Size of the output samples for each one of the Discriminator Layers. A Linear Layer
-            will be created for each one of the values provided. Defaults to (256, 256).
-        l2scale (float):
-            Wheight Decay for the Adam Optimizer. Defaults to 1e-6.
-        batch_size (int):
-            Number of data samples to process in each step.
-    """
-
-    def __init__(
-        self,
-        embedding_dim=128,
-        gen_dim=(256, 256),
-        dis_dim=(256, 256),
-        l2scale=1e-6,
-        batch_size=500,
-        patience=25,
-    ):
-
-        self.embedding_dim = embedding_dim
-        self.gen_dim = gen_dim
-        self.dis_dim = dis_dim
-        self.patience = patience
-        self.l2scale = l2scale
-        self.batch_size = batch_size
-        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
-
-    def _apply_activate(self, data):
-        data_t = []
-        st = 0
-        for item in self.transformer.output_info:
-            if item[1] == "tanh":
-                ed = st + item[0]
-                data_t.append(torch.tanh(data[:, st:ed]))
-                st = ed
-            elif item[1] == "softmax":
-                ed = st + item[0]
-                data_t.append(functional.gumbel_softmax(data[:, st:ed], tau=0.2))
-                st = ed
-            else:
-                assert 0
-
-        return torch.cat(data_t, dim=1)
-
-    def _cond_loss(self, data, c, m):
-        loss = []
-        st = 0
-        st_c = 0
-        skip = False
-        for item in self.transformer.output_info:
-            if item[1] == "tanh":
-                st += item[0]
-                skip = True
-
-            elif item[1] == "softmax":
-                if skip:
-                    skip = False
-                    st += item[0]
-                    continue
-
-                ed = st + item[0]
-                ed_c = st_c + item[0]
-                tmp = functional.cross_entropy(
-                    data[:, st:ed],
-                    torch.argmax(c[:, st_c:ed_c], dim=1),
-                    reduction="none",
-                )
-                loss.append(tmp)
-                st = ed
-                st_c = ed_c
-
-            else:
-                assert 0
-
-        loss = torch.stack(loss, dim=1)
-
-        return (loss * m).sum() / data.size()[0]
-
-    def fit(self, train_data, discrete_columns=(), epochs=300, log_frequency=True):
-        """Fit the CTGAN Synthesizer models to the training data.
-
-        Args:
-            train_data (numpy.ndarray or pandas.DataFrame):
-                Training Data. It must be a 2-dimensional numpy array or a
-                pandas.DataFrame.
-            discrete_columns (list-like):
-                List of discrete columns to be used to generate the Conditional
-                Vector. If ``train_data`` is a Numpy array, this list should
-                contain the integer indices of the columns. Otherwise, if it is
-                a ``pandas.DataFrame``, this list should contain the column names.
-            epochs (int):
-                Number of training epochs. Defaults to 300.
-            log_frequency (boolean):
-                Whether to use log frequency of categorical levels in conditional
-                sampling. Defaults to ``True``.
-        """
-
-        self.transformer = DataTransformer()
-        self.transformer.fit(train_data, discrete_columns)
-        train_data = self.transformer.transform(train_data)
-
-        data_sampler = Sampler(train_data, self.transformer.output_info)
-
-        data_dim = self.transformer.output_dimensions
-        self.cond_generator = ConditionalGenerator(
-            train_data, self.transformer.output_info, log_frequency
-        )
-
-        self.generator = Generator(
-            self.embedding_dim + self.cond_generator.n_opt, self.gen_dim, data_dim
-        ).to(self.device)
-
-        discriminator = Discriminator(
-            data_dim + self.cond_generator.n_opt, self.dis_dim
-        ).to(self.device)
-
-        optimizerG = optim.Adam(
-            self.generator.parameters(),
-            lr=2e-4,
-            betas=(0.5, 0.9),
-            weight_decay=self.l2scale,
-        )
-        optimizerD = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.9))
-
-        assert self.batch_size % 2 == 0
-        mean = torch.zeros(self.batch_size, self.embedding_dim, device=self.device)
-        std = mean + 1
-
-        train_losses = []
-        early_stopping = EarlyStopping(patience=self.patience, verbose=False)
-
-        steps_per_epoch = max(len(train_data) // self.batch_size, 1)
-
-        for i in tqdm(range(epochs), desc="Training CTGAN, epochs:"):
-            for id_ in range(steps_per_epoch):
-                fakez = torch.normal(mean=mean, std=std)
-
-                condvec = self.cond_generator.sample(self.batch_size)
-                if condvec is None:
-                    c1, m1, col, opt = None, None, None, None
-                    real = data_sampler.sample(self.batch_size, col, opt)
-                else:
-                    c1, m1, col, opt = condvec
-                    c1 = torch.from_numpy(c1).to(self.device)
-                    m1 = torch.from_numpy(m1).to(self.device)
-                    fakez = torch.cat([fakez, c1], dim=1)
-
-                    perm = np.arange(self.batch_size)
-                    np.random.shuffle(perm)
-                    real = data_sampler.sample(self.batch_size, col[perm], opt[perm])
-                    c2 = c1[perm]
-
-                fake = self.generator(fakez)
-                fakeact = self._apply_activate(fake)
-
-                real = torch.from_numpy(real.astype("float32")).to(self.device)
-
-                if c1 is not None:
-                    fake_cat = torch.cat([fakeact, c1], dim=1)
-                    real_cat = torch.cat([real, c2], dim=1)
-                else:
-                    real_cat = real
-                    fake_cat = fake
-
-                y_fake = discriminator(fake_cat)
-                y_real = discriminator(real_cat)
-
-                pen = discriminator.calc_gradient_penalty(
-                    real_cat, fake_cat, self.device
-                )
-                loss_d = -(torch.mean(y_real) - torch.mean(y_fake))
-                train_losses.append(loss_d.item())
-                optimizerD.zero_grad()
-                pen.backward(retain_graph=True)
-                loss_d.backward()
-                optimizerD.step()
-
-                fakez = torch.normal(mean=mean, std=std)
-                condvec = self.cond_generator.sample(self.batch_size)
-
-                if condvec is None:
-                    c1, m1, col, opt = None, None, None, None
-                else:
-                    c1, m1, col, opt = condvec
-                    c1 = torch.from_numpy(c1).to(self.device)
-                    m1 = torch.from_numpy(m1).to(self.device)
-                    fakez = torch.cat([fakez, c1], dim=1)
-
-                fake = self.generator(fakez)
-                fakeact = self._apply_activate(fake)
-
-                if c1 is not None:
-                    y_fake = discriminator(torch.cat([fakeact, c1], dim=1))
-                else:
-                    y_fake = discriminator(fakeact)
-
-                if condvec is None:
-                    cross_entropy = 0
-                else:
-                    cross_entropy = self._cond_loss(fake, c1, m1)
-
-                loss_g = -torch.mean(y_fake) + cross_entropy
-                train_losses.append(loss_g.item())
-                optimizerG.zero_grad()
-                loss_g.backward()
-                optimizerG.step()
-            early_stopping(np.average(train_losses))
-            if early_stopping.early_stop:
-                logging.info("Early stopping in GAN training!")
-                break
-            train_losses = []
-
-    def sample(self, n):
-        """Sample data similar to the training data.
-
-        Args:
-            n (int):
-                Number of rows to sample.
-
-        Returns:
-            numpy.ndarray or pandas.DataFrame
-        """
-
-        steps = n // self.batch_size + 1
-        data = []
-        for i in range(steps):
-            mean = torch.zeros(self.batch_size, self.embedding_dim)
-            std = mean + 1
-            fakez = torch.normal(mean=mean, std=std).to(self.device)
-
-            condvec = self.cond_generator.sample_zero(self.batch_size)
-            if condvec is None:
-                pass
-            else:
-                c1 = condvec
-                c1 = torch.from_numpy(c1).to(self.device)
-                fakez = torch.cat([fakez, c1], dim=1)
-
-            fake = self.generator(fakez)
-            fakeact = self._apply_activate(fake)
-            data.append(fakeact.detach().cpu().numpy())
-
-        data = np.concatenate(data, axis=0)
-        data = data[:n]
-
-        return self.transformer.inverse_transform(data, None)
+import logging
+
+import numpy as np
+import torch
+from torch import optim
+from torch.nn import functional
+from tqdm.autonotebook import tqdm
+
+from _ctgan.conditional import ConditionalGenerator
+from _ctgan.models import Discriminator, Generator
+from _ctgan.sampler import Sampler
+from _ctgan.transformer import DataTransformer
+
+
+class EarlyStopping:
+    """Early stops the training if validation loss doesn't improve after a given patience."""
+
+    def __init__(self, patience=7, verbose=False, delta=0):
+        """
+        Args:
+            patience (int): How long to wait after last time validation loss improved.
+                            Default: 7
+            delta (float): Minimum change in the monitored quantity to qualify as an improvement.
+                            Default: 0
+        """
+        self.patience = patience
+        self.counter = 0
+        self.best_score = None
+        self.early_stop = False
+        self.val_loss_min = np.Inf
+        self.delta = delta
+        self.verbose = verbose
+
+    def __call__(self, val_loss):
+
+        score = -val_loss
+
+        if self.best_score is None:
+            self.best_score = score
+        elif score < self.best_score + self.delta:
+            self.counter += 1
+            if self.counter >= self.patience:
+                logging.info("Early stoping for GAN. Best score: {:.2f} with patience = {}".format(self.best_score,
+                                                                                               self.patience))
+                self.early_stop = True
+        else:
+            self.best_score = score
+            self.counter = 0
+
+
+class _CTGANSynthesizer:
+    """Conditional Table GAN Synthesizer.
+
+    This is the core class of the CTGAN project, where the different components
+    are orchestrated together.
+
+    For more details about the process, please check the [Modeling Tabular data using
+    Conditional GAN](https://arxiv.org/abs/1907.00503) paper.
+
+    Args:
+        embedding_dim (int):
+            Size of the random sample passed to the Generator. Defaults to 128.
+        gen_dim (tuple or list of ints):
+            Size of the output samples for each one of the Residuals. A Resiudal Layer
+            will be created for each one of the values provided. Defaults to (256, 256).
+        dis_dim (tuple or list of ints):
+            Size of the output samples for each one of the Discriminator Layers. A Linear Layer
+            will be created for each one of the values provided. Defaults to (256, 256).
+        l2scale (float):
+            Wheight Decay for the Adam Optimizer. Defaults to 1e-6.
+        batch_size (int):
+            Number of data samples to process in each step.
+    """
+
+    def __init__(
+        self,
+        embedding_dim=128,
+        gen_dim=(256, 256),
+        dis_dim=(256, 256),
+        l2scale=1e-6,
+        batch_size=500,
+        patience=25,
+    ):
+
+        self.embedding_dim = embedding_dim
+        self.gen_dim = gen_dim
+        self.dis_dim = dis_dim
+        self.patience = patience
+        self.l2scale = l2scale
+        self.batch_size = batch_size
+        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
+
+    def _apply_activate(self, data):
+        data_t = []
+        st = 0
+        for item in self.transformer.output_info:
+            if item[1] == "tanh":
+                ed = st + item[0]
+                data_t.append(torch.tanh(data[:, st:ed]))
+                st = ed
+            elif item[1] == "softmax":
+                ed = st + item[0]
+                data_t.append(functional.gumbel_softmax(data[:, st:ed], tau=0.2))
+                st = ed
+            else:
+                raise AssertionError
+
+        return torch.cat(data_t, dim=1)
+
+    def _cond_loss(self, data, c, m):
+        loss = []
+        st = 0
+        st_c = 0
+        skip = False
+        for item in self.transformer.output_info:
+            if item[1] == "tanh":
+                st += item[0]
+                skip = True
+
+            elif item[1] == "softmax":
+                if skip:
+                    skip = False
+                    st += item[0]
+                    continue
+
+                ed = st + item[0]
+                ed_c = st_c + item[0]
+                tmp = functional.cross_entropy(
+                    data[:, st:ed],
+                    torch.argmax(c[:, st_c:ed_c], dim=1),
+                    reduction="none",
+                )
+                loss.append(tmp)
+                st = ed
+                st_c = ed_c
+
+            else:
+                raise AssertionError
+
+        loss = torch.stack(loss, dim=1)
+
+        return (loss * m).sum() / data.size()[0]
+
+    def fit(self, train_data, discrete_columns=(), epochs=300, log_frequency=True):
+        """Fit the CTGAN Synthesizer models to the training data.
+
+        Args:
+            train_data (numpy.ndarray or pandas.DataFrame):
+                Training Data. It must be a 2-dimensional numpy array or a
+                pandas.DataFrame.
+            discrete_columns (list-like):
+                List of discrete columns to be used to generate the Conditional
+                Vector. If ``train_data`` is a Numpy array, this list should
+                contain the integer indices of the columns. Otherwise, if it is
+                a ``pandas.DataFrame``, this list should contain the column names.
+            epochs (int):
+                Number of training epochs. Defaults to 300.
+            log_frequency (boolean):
+                Whether to use log frequency of categorical levels in conditional
+                sampling. Defaults to ``True``.
+        """
+        self.transformer = DataTransformer()
+        self.transformer.fit(train_data, discrete_columns)
+        train_data = self.transformer.transform(train_data)
+
+        data_sampler = Sampler(train_data, self.transformer.output_info)
+
+        data_dim = self.transformer.output_dimensions
+        self.cond_generator = ConditionalGenerator(
+            train_data, self.transformer.output_info, log_frequency
+        )
+
+        self.generator = Generator(
+            self.embedding_dim + self.cond_generator.n_opt, self.gen_dim, data_dim
+        ).to(self.device)
+
+        discriminator = Discriminator(
+            data_dim + self.cond_generator.n_opt, self.dis_dim
+        ).to(self.device)
+
+        optimizerG = optim.Adam(
+            self.generator.parameters(),
+            lr=2e-4,
+            betas=(0.5, 0.9),
+            weight_decay=self.l2scale,
+        )
+        optimizerD = optim.Adam(discriminator.parameters(), lr=2e-4, betas=(0.5, 0.9))
+
+        if self.batch_size % 2 != 0:
+            raise ValueError("batch_size should even, but {} is provided".format(self.batch_size))
+        mean = torch.zeros(self.batch_size, self.embedding_dim, device=self.device)
+        std = mean + 1
+
+        train_losses = []
+        early_stopping = EarlyStopping(patience=self.patience, verbose=False)
+
+        steps_per_epoch = max(len(train_data) // self.batch_size, 1)
+
+        for i in tqdm(range(epochs), desc="Training CTGAN, epochs:"):
+            for id_ in range(steps_per_epoch):
+                fakez = torch.normal(mean=mean, std=std)
+
+                condvec = self.cond_generator.sample(self.batch_size)
+                if condvec is None:
+                    c1, m1, col, opt = None, None, None, None
+                    real = data_sampler.sample(self.batch_size, col, opt)
+                else:
+                    c1, m1, col, opt = condvec
+                    c1 = torch.from_numpy(c1).to(self.device)
+                    m1 = torch.from_numpy(m1).to(self.device)
+                    fakez = torch.cat([fakez, c1], dim=1)
+
+                    perm = np.arange(self.batch_size)
+                    np.random.shuffle(perm)
+                    real = data_sampler.sample(self.batch_size, col[perm], opt[perm])
+                    c2 = c1[perm]
+
+                fake = self.generator(fakez)
+                fakeact = self._apply_activate(fake)
+
+                real = torch.from_numpy(real.astype("float32")).to(self.device)
+
+                if c1 is not None:
+                    fake_cat = torch.cat([fakeact, c1], dim=1)
+                    real_cat = torch.cat([real, c2], dim=1)
+                else:
+                    real_cat = real
+                    fake_cat = fake
+
+                y_fake = discriminator(fake_cat)
+                y_real = discriminator(real_cat)
+
+                pen = discriminator.calc_gradient_penalty(
+                    real_cat, fake_cat, self.device
+                )
+                loss_d = -(torch.mean(y_real) - torch.mean(y_fake))
+                train_losses.append(loss_d.item())
+                optimizerD.zero_grad()
+                pen.backward(retain_graph=True)
+                loss_d.backward()
+                optimizerD.step()
+
+                fakez = torch.normal(mean=mean, std=std)
+                condvec = self.cond_generator.sample(self.batch_size)
+
+                if condvec is None:
+                    c1, m1, col, opt = None, None, None, None
+                else:
+                    c1, m1, col, opt = condvec
+                    c1 = torch.from_numpy(c1).to(self.device)
+                    m1 = torch.from_numpy(m1).to(self.device)
+                    fakez = torch.cat([fakez, c1], dim=1)
+
+                fake = self.generator(fakez)
+                fakeact = self._apply_activate(fake)
+
+                if c1 is not None:
+                    y_fake = discriminator(torch.cat([fakeact, c1], dim=1))
+                else:
+                    y_fake = discriminator(fakeact)
+
+                if condvec is None:
+                    cross_entropy = 0
+                else:
+                    cross_entropy = self._cond_loss(fake, c1, m1)
+
+                loss_g = -torch.mean(y_fake) + cross_entropy
+                train_losses.append(loss_g.item())
+                optimizerG.zero_grad()
+                loss_g.backward()
+                optimizerG.step()
+            early_stopping(np.average(train_losses))
+            if early_stopping.early_stop:
+                logging.info("Early stopping in GAN training!")
+                break
+            train_losses = []
+
+    def sample(self, n):
+        """Sample data similar to the training data.
+
+        Args:
+            n (int):
+                Number of rows to sample.
+
+        Returns:
+            numpy.ndarray or pandas.DataFrame
+        """
+        steps = n // self.batch_size + 1
+        data = []
+        for i in range(steps):
+            mean = torch.zeros(self.batch_size, self.embedding_dim)
+            std = mean + 1
+            fakez = torch.normal(mean=mean, std=std).to(self.device)
+
+            condvec = self.cond_generator.sample_zero(self.batch_size)
+            if condvec is None:
+                pass
+            else:
+                c1 = condvec
+                c1 = torch.from_numpy(c1).to(self.device)
+                fakez = torch.cat([fakez, c1], dim=1)
+
+            fake = self.generator(fakez)
+            fakeact = self._apply_activate(fake)
+            data.append(fakeact.detach().cpu().numpy())
+
+        data = np.concatenate(data, axis=0)
+        data = data[:n]
+
+        return self.transformer.inverse_transform(data, None)
```

## _ctgan/transformer.py

```diff
@@ -1,180 +1,180 @@
-import numpy as np
-import pandas as pd
-from sklearn.exceptions import ConvergenceWarning
-from sklearn.mixture import BayesianGaussianMixture
-from sklearn.preprocessing import OneHotEncoder
-from sklearn.utils._testing import ignore_warnings
-from tqdm.autonotebook import tqdm
-
-
-class DataTransformer(object):
-    """Data Transformer.
-
-    Model continuous columns with a BayesianGMM and normalized to a scalar
-    [0, 1] and a vector.
-    Discrete columns are encoded using a scikit-learn OneHotEncoder.
-
-    Args:
-        n_cluster (int):
-            Number of modes.
-        epsilon (float):
-            Epsilon value.
-    """
-
-    def __init__(self, n_clusters=10, epsilon=0.005):
-        self.n_clusters = n_clusters
-        self.epsilon = epsilon
-
-    @ignore_warnings(category=ConvergenceWarning)
-    def _fit_continuous(self, column, data):
-        gm = BayesianGaussianMixture(
-            n_components=self.n_clusters,
-            weight_concentration_prior_type='dirichlet_process',
-            weight_concentration_prior=0.001,
-            n_init=1
-        )
-        gm.fit(data)
-        components = gm.weights_ > self.epsilon
-        num_components = components.sum()
-
-        return {
-            'name': column,
-            'model': gm,
-            'components': components,
-            'output_info': [(1, 'tanh'), (num_components, 'softmax')],
-            'output_dimensions': 1 + num_components,
-        }
-
-    @staticmethod
-    def _fit_discrete(column, data):
-        ohe = OneHotEncoder(sparse=False)
-        ohe.fit(data)
-        categories = len(ohe.categories_[0])
-
-        return {
-            'name': column,
-            'encoder': ohe,
-            'output_info': [(categories, 'softmax')],
-            'output_dimensions': categories
-        }
-
-    def fit(self, data, discrete_columns=()):
-        self.output_info = []
-        self.output_dimensions = 0
-
-        if not isinstance(data, pd.DataFrame):
-            self.dataframe = False
-            data = pd.DataFrame(data)
-        else:
-            self.dataframe = True
-
-        self.meta = []
-        for column in tqdm(data.columns, desc='Fitting CTGAN transformers for each column'):
-            column_data = data[[column]].values
-            if column in discrete_columns:
-                meta = self._fit_discrete(column, column_data)
-            else:
-                meta = self._fit_continuous(column, column_data)
-
-            self.output_info += meta['output_info']
-            self.output_dimensions += meta['output_dimensions']
-            self.meta.append(meta)
-
-    def _transform_continuous(self, column_meta, data):
-        components = column_meta['components']
-        model = column_meta['model']
-
-        means = model.means_.reshape((1, self.n_clusters))
-        stds = np.sqrt(model.covariances_).reshape((1, self.n_clusters))
-        features = (data - means) / (4 * stds)
-
-        probs = model.predict_proba(data)
-
-        n_opts = components.sum()
-        features = features[:, components]
-        probs = probs[:, components]
-
-        opt_sel = np.zeros(len(data), dtype='int')
-        for i in range(len(data)):
-            pp = probs[i] + 1e-6
-            pp = pp / pp.sum()
-            opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)
-
-        idx = np.arange((len(features)))
-        features = features[idx, opt_sel].reshape([-1, 1])
-        features = np.clip(features, -.99, .99)
-
-        probs_onehot = np.zeros_like(probs)
-        probs_onehot[np.arange(len(probs)), opt_sel] = 1
-        return [features, probs_onehot]
-
-    @staticmethod
-    def _transform_discrete(column_meta, data):
-        encoder = column_meta['encoder']
-        return encoder.transform(data)
-
-    def transform(self, data):
-        if not isinstance(data, pd.DataFrame):
-            data = pd.DataFrame(data)
-
-        values = []
-        for meta in self.meta:
-            column_data = data[[meta['name']]].values
-            if 'model' in meta:
-                values += self._transform_continuous(meta, column_data)
-            else:
-                values.append(self._transform_discrete(meta, column_data))
-
-        return np.concatenate(values, axis=1).astype(float)
-
-    def _inverse_transform_continuous(self, meta, data, sigma):
-        model = meta['model']
-        components = meta['components']
-
-        u = data[:, 0]
-        v = data[:, 1:]
-
-        if sigma is not None:
-            u = np.random.normal(u, sigma)
-
-        u = np.clip(u, -1, 1)
-        v_t = np.ones((len(data), self.n_clusters)) * -100
-        v_t[:, components] = v
-        v = v_t
-        means = model.means_.reshape([-1])
-        stds = np.sqrt(model.covariances_).reshape([-1])
-        p_argmax = np.argmax(v, axis=1)
-        std_t = stds[p_argmax]
-        mean_t = means[p_argmax]
-        column = u * 4 * std_t + mean_t
-
-        return column
-
-    @staticmethod
-    def _inverse_transform_discrete(meta, data):
-        encoder = meta['encoder']
-        return encoder.inverse_transform(data)
-
-    def inverse_transform(self, data, sigmas):
-        start = 0
-        output = []
-        column_names = []
-        for meta in self.meta:
-            dimensions = meta['output_dimensions']
-            columns_data = data[:, start:start + dimensions]
-
-            if 'model' in meta:
-                sigma = sigmas[start] if sigmas else None
-                inverted = self._inverse_transform_continuous(meta, columns_data, sigma)
-            else:
-                inverted = self._inverse_transform_discrete(meta, columns_data)
-
-            output.append(inverted)
-            column_names.append(meta['name'])
-            start += dimensions
-
-        output = np.column_stack(output)
-        if self.dataframe:
-            output = pd.DataFrame(output, columns=column_names)
-
-        return output
+import numpy as np
+import pandas as pd
+from sklearn.exceptions import ConvergenceWarning
+from sklearn.mixture import BayesianGaussianMixture
+from sklearn.preprocessing import OneHotEncoder
+from sklearn.utils._testing import ignore_warnings
+from tqdm.autonotebook import tqdm
+
+
+class DataTransformer:
+    """Data Transformer.
+
+    Model continuous columns with a BayesianGMM and normalized to a scalar
+    [0, 1] and a vector.
+    Discrete columns are encoded using a scikit-learn OneHotEncoder.
+
+    Args:
+        n_cluster (int):
+            Number of modes.
+        epsilon (float):
+            Epsilon value.
+    """
+
+    def __init__(self, n_clusters=10, epsilon=0.005):
+        self.n_clusters = n_clusters
+        self.epsilon = epsilon
+
+    @ignore_warnings(category=ConvergenceWarning)
+    def _fit_continuous(self, column, data):
+        gm = BayesianGaussianMixture(
+            n_components=self.n_clusters,
+            weight_concentration_prior_type='dirichlet_process',
+            weight_concentration_prior=0.001,
+            n_init=1
+        )
+        gm.fit(data)
+        components = gm.weights_ > self.epsilon
+        num_components = components.sum()
+
+        return {
+            'name': column,
+            'model': gm,
+            'components': components,
+            'output_info': [(1, 'tanh'), (num_components, 'softmax')],
+            'output_dimensions': 1 + num_components,
+        }
+
+    @staticmethod
+    def _fit_discrete(column, data):
+        ohe = OneHotEncoder(sparse=False)
+        ohe.fit(data)
+        categories = len(ohe.categories_[0])
+
+        return {
+            'name': column,
+            'encoder': ohe,
+            'output_info': [(categories, 'softmax')],
+            'output_dimensions': categories
+        }
+
+    def fit(self, data, discrete_columns=()):
+        self.output_info = []
+        self.output_dimensions = 0
+
+        if not isinstance(data, pd.DataFrame):
+            self.dataframe = False
+            data = pd.DataFrame(data)
+        else:
+            self.dataframe = True
+
+        self.meta = []
+        for column in tqdm(data.columns, desc='Fitting CTGAN transformers for each column'):
+            column_data = data[[column]].values
+            if column in discrete_columns:
+                meta = self._fit_discrete(column, column_data)
+            else:
+                meta = self._fit_continuous(column, column_data)
+
+            self.output_info += meta['output_info']
+            self.output_dimensions += meta['output_dimensions']
+            self.meta.append(meta)
+
+    def _transform_continuous(self, column_meta, data):
+        components = column_meta['components']
+        model = column_meta['model']
+
+        means = model.means_.reshape((1, self.n_clusters))
+        stds = np.sqrt(model.covariances_).reshape((1, self.n_clusters))
+        features = (data - means) / (4 * stds)
+
+        probs = model.predict_proba(data)
+
+        n_opts = components.sum()
+        features = features[:, components]
+        probs = probs[:, components]
+
+        opt_sel = np.zeros(len(data), dtype='int')
+        for i in range(len(data)):
+            pp = probs[i] + 1e-6
+            pp = pp / pp.sum()
+            opt_sel[i] = np.random.choice(np.arange(n_opts), p=pp)
+
+        idx = np.arange((len(features)))
+        features = features[idx, opt_sel].reshape([-1, 1])
+        features = np.clip(features, -.99, .99)
+
+        probs_onehot = np.zeros_like(probs)
+        probs_onehot[np.arange(len(probs)), opt_sel] = 1
+        return [features, probs_onehot]
+
+    @staticmethod
+    def _transform_discrete(column_meta, data):
+        encoder = column_meta['encoder']
+        return encoder.transform(data)
+
+    def transform(self, data):
+        if not isinstance(data, pd.DataFrame):
+            data = pd.DataFrame(data)
+
+        values = []
+        for meta in self.meta:
+            column_data = data[[meta['name']]].values
+            if 'model' in meta:
+                values += self._transform_continuous(meta, column_data)
+            else:
+                values.append(self._transform_discrete(meta, column_data))
+
+        return np.concatenate(values, axis=1).astype(float)
+
+    def _inverse_transform_continuous(self, meta, data, sigma):
+        model = meta['model']
+        components = meta['components']
+
+        u = data[:, 0]
+        v = data[:, 1:]
+
+        if sigma is not None:
+            u = np.random.normal(u, sigma)
+
+        u = np.clip(u, -1, 1)
+        v_t = np.ones((len(data), self.n_clusters)) * -100
+        v_t[:, components] = v
+        v = v_t
+        means = model.means_.reshape([-1])
+        stds = np.sqrt(model.covariances_).reshape([-1])
+        p_argmax = np.argmax(v, axis=1)
+        std_t = stds[p_argmax]
+        mean_t = means[p_argmax]
+        column = u * 4 * std_t + mean_t
+
+        return column
+
+    @staticmethod
+    def _inverse_transform_discrete(meta, data):
+        encoder = meta['encoder']
+        return encoder.inverse_transform(data)
+
+    def inverse_transform(self, data, sigmas):
+        start = 0
+        output = []
+        column_names = []
+        for meta in self.meta:
+            dimensions = meta['output_dimensions']
+            columns_data = data[:, start:start + dimensions]
+
+            if 'model' in meta:
+                sigma = sigmas[start] if sigmas else None
+                inverted = self._inverse_transform_continuous(meta, columns_data, sigma)
+            else:
+                inverted = self._inverse_transform_discrete(meta, columns_data)
+
+            output.append(inverted)
+            column_names.append(meta['name'])
+            start += dimensions
+
+        output = np.column_stack(output)
+        if self.dataframe:
+            output = pd.DataFrame(output, columns=column_names)
+
+        return output
```

## tabgan/__init__.py

 * *Ordering differences only*

```diff
@@ -1,11 +1,11 @@
-# -*- coding: utf-8 -*-
-from pkg_resources import DistributionNotFound, get_distribution
-
-try:
-    # Change here if project is renamed and does not equal the package name
-    dist_name = __name__
-    __version__ = get_distribution(dist_name).version
-except DistributionNotFound:
-    __version__ = "unknown"
-finally:
-    del get_distribution, DistributionNotFound
+# -*- coding: utf-8 -*-
+from pkg_resources import DistributionNotFound, get_distribution
+
+try:
+    # Change here if project is renamed and does not equal the package name
+    dist_name = __name__
+    __version__ = get_distribution(dist_name).version
+except DistributionNotFound:
+    __version__ = "unknown"
+finally:
+    del get_distribution, DistributionNotFound
```

## tabgan/abc_sampler.py

```diff
@@ -1,120 +1,115 @@
-import gc
-import logging
-from abc import ABC, abstractmethod
-from typing import Tuple
-
-import pandas as pd
-
-__author__ = "Insaf Ashrapov"
-__copyright__ = "Insaf Ashrapov"
-__license__ = "Apache 2.0"
-
-
-class SampleData(ABC):
-    """
-        Factory method for different sampler strategies. The goal is to generate more train data
-        which should be more close to test, in other word we trying to fix uneven distribution.
-    """
-
-    @abstractmethod
-    def get_object_generator(self):
-        """
-        Getter for object sampler aka generator, which is not a generator
-        """
-        raise NotImplementedError
-
-    def generate_data_pipe(
-        self,
-        train_df: pd.DataFrame,
-        target: pd.DataFrame,
-        test_df: pd.DataFrame,
-        deep_copy: bool = True,
-        only_adversarial: bool = False,
-        use_adversarial: bool = True,
-        only_generated_data: bool = False,
-    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
-        """
-        Defines logic for sampling
-        @param train_df: Train dataframe which has separate target
-        @param target: Input target for the train dataset
-        @param test_df: Test dataframe - newly generated train dataframe should be close to it
-        @param deep_copy: make copy of input files or not. If not input dataframes will be overridden
-        @param only_adversarial: only adversarial fitering to train dataframe will be performed
-        @param use_adversarial: perform or not adversarial filtering
-        @param only_generated_data: After generation get only newly generated, without concating input train dataframe.
-        Only works for SamplerGAN.
-        @return: Newly generated train dataframe and test data
-        """
-        generator = self.get_object_generator()
-        if deep_copy:
-            logging.info("Preprocessing input data with deep copying input data.")
-            if target is None or test_df is None:
-                new_train = generator.preprocess_data_df(train_df.copy())
-                new_target = None
-            else:
-                new_train, new_target, test_df = generator.preprocess_data(
-                    train_df.copy(), target.copy(), test_df
-                )
-        else:
-            logging.info("Preprocessing input data with deep copying input data.")
-            new_train, new_target, test_df = generator.preprocess_data(
-                train_df, target, test_df
-            )
-        if only_adversarial and use_adversarial:
-            logging.info("Applying adversarial filtering")
-            return generator.adversarial_filtering(new_train, new_target, test_df)
-        else:
-            logging.info("Starting generation step.")
-            new_train, new_target = generator.generate_data(
-                new_train, new_target, test_df, only_generated_data
-            )
-            logging.info("Starting postprocessing step.")
-            new_train, new_target = generator.postprocess_data(
-                new_train, new_target, test_df
-            )
-            if use_adversarial:
-                logging.info("Applying adversarial filtering")
-                new_train, new_target = generator.adversarial_filtering(
-                    new_train, new_target, test_df
-                )
-            gc.collect()
-
-            logging.info("Total finishing, returning data")
-            return new_train, new_target
-
-
-class Sampler(ABC):
-    """
-        Interface for each sampling strategy
-    """
-
-    def get_generated_shape(self, input_df):
-        """
-        Calculates final output shape
-        """
-        if self.gen_x_times <= 0:
-            raise ValueError(
-                "Passed gen_x_times = {} should be bigger than 0".format(
-                    self.gen_x_times
-                )
-            )
-        return int(self.gen_x_times * input_df.shape[0])
-
-    @abstractmethod
-    def preprocess_data(self, train, target, test_df):
-        """Before we can start data generation we might need some preprocessing, numpy to pandas
-        and etc"""
-        raise NotImplementedError
-
-    @abstractmethod
-    def generate_data(self, train_df, target, test_df):
-        raise NotImplementedError
-
-    @abstractmethod
-    def postprocess_data(self, train_df, target, test_df):
-        """Filtering data which far beyond from test_df data distribution"""
-        raise NotImplementedError
-
-    @abstractmethod
-    def adversarial_filtering(self, train_df, target, test_df):
-        raise NotImplementedError
+import gc
+import logging
+from abc import ABC, abstractmethod
+from typing import Tuple
+from .utils import seed_everything
+import pandas as pd
+
+__author__ = "Insaf Ashrapov"
+__copyright__ = "Insaf Ashrapov"
+__license__ = "Apache 2.0"
+
+
+class SampleData(ABC):
+    """
+        Factory method for different sampler strategies. The goal is to generate more train data
+        which should be more close to test, in other word we trying to fix uneven distribution.
+    """
+
+    @abstractmethod
+    def get_object_generator(self):
+        """Getter for object sampler aka generator, which is not a generator"""
+        raise NotImplementedError
+
+    def generate_data_pipe(
+        self,
+        train_df: pd.DataFrame,
+        target: pd.DataFrame,
+        test_df: pd.DataFrame,
+        deep_copy: bool = True,
+        only_adversarial: bool = False,
+        use_adversarial: bool = True,
+        only_generated_data: bool = False,
+    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
+        """
+        Defines logic for sampling
+        @param train_df: Train dataframe which has separate target
+        @param target: Input target for the train dataset
+        @param test_df: Test dataframe - newly generated train dataframe should be close to it
+        @param deep_copy: make copy of input files or not. If not input dataframes will be overridden
+        @param only_adversarial: only adversarial fitering to train dataframe will be performed
+        @param use_adversarial: perform or not adversarial filtering
+        @param only_generated_data: After generation get only newly generated, without concating input train dataframe.
+        Only works for SamplerGAN.
+        @return: Newly generated train dataframe and test data
+        """
+        seed_everything()
+        generator = self.get_object_generator()
+        if deep_copy:
+            logging.info("Preprocessing input data with deep copying input data.")
+            if target is None or test_df is None:
+                new_train = generator.preprocess_data_df(train_df.copy())
+                new_target = None
+            else:
+                new_train, new_target, test_df = generator.preprocess_data(
+                    train_df.copy(), target.copy(), test_df
+                )
+        else:
+            logging.info("Preprocessing input data with deep copying input data.")
+            new_train, new_target, test_df = generator.preprocess_data(
+                train_df, target, test_df
+            )
+        if only_adversarial and use_adversarial:
+            logging.info("Applying adversarial filtering")
+            return generator.adversarial_filtering(new_train, new_target, test_df)
+        else:
+            logging.info("Starting generation step.")
+            new_train, new_target = generator.generate_data(
+                new_train, new_target, test_df, only_generated_data
+            )
+            logging.info("Starting postprocessing step.")
+            new_train, new_target = generator.postprocess_data(
+                new_train, new_target, test_df
+            )
+            if use_adversarial:
+                logging.info("Applying adversarial filtering")
+                new_train, new_target = generator.adversarial_filtering(
+                    new_train, new_target, test_df
+                )
+            gc.collect()
+
+            logging.info("Total finishing, returning data")
+            return new_train, new_target
+
+
+class Sampler(ABC):
+    """Interface for each sampling strategy"""
+
+    def get_generated_shape(self, input_df):
+        """Calculates final output shape"""
+        if self.gen_x_times <= 0:
+            raise ValueError(
+                "Passed gen_x_times = {} should be bigger than 0".format(
+                    self.gen_x_times
+                )
+            )
+        return int(self.gen_x_times * input_df.shape[0])
+
+    @abstractmethod
+    def preprocess_data(self, train, target, test_df):
+        """Before we can start data generation we might need some preprocessing, numpy to pandas
+        and etc"""
+        raise NotImplementedError
+
+    @abstractmethod
+    def generate_data(self, train_df, target, test_df):
+        raise NotImplementedError
+
+    @abstractmethod
+    def postprocess_data(self, train_df, target, test_df):
+        """Filtering data which far beyond from test_df data distribution"""
+        raise NotImplementedError
+
+    @abstractmethod
+    def adversarial_filtering(self, train_df, target, test_df):
+        raise NotImplementedError
```

## tabgan/adversarial_model.py

```diff
@@ -1,212 +1,212 @@
-import numpy as np
-import pandas as pd
-from lightgbm import LGBMClassifier
-from scipy.stats import rankdata
-from sklearn.metrics import roc_auc_score
-from sklearn.model_selection import StratifiedKFold
-
-from tabgan.encoders import MultipleEncoder, DoubleValidationEncoderNumerical
-
-
-class AdversarialModel:
-    def __init__(
-            self,
-            cat_validation="Single",
-            encoders_names=("OrdinalEncoder",),
-            cat_cols=None,
-            model_validation=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
-            model_params=None,
-    ):
-        '''
-        Class for fit predicting tabular models, mostly - boostings. Several encoders for categorical features are
-        supported
-
-        Args:
-            cat_validation: categorical type of validation, examples: "None", "Single" and "Double"
-            encoders_names: different categorical encoders from category_encoders library, example CatBoostEncoder
-            cat_cols: list of categorical columns
-            model_validation: model training cross validation type from sklearn.model_selection,
-            example StratifiedKFold(5)
-            model_params: model training hyperparameters
-        '''
-        self.cat_validation = cat_validation
-        self.encoders_names = encoders_names
-        self.cat_cols = cat_cols
-        self.model_validation = model_validation
-        self.model_params = model_params
-
-    def adversarial_test(self, left_df, right_df):
-        """
-        Trains adversarial model to distinguish train from test
-        :param left_df:  dataframe
-        :param right_df: dataframe
-        :return: trained model
-        """
-        # sample to shuffle the data
-        left_df = left_df.copy().sample(frac=1).reset_index(drop=True)
-        right_df = right_df.copy().sample(frac=1).reset_index(drop=True)
-
-        left_df = left_df.head(right_df.shape[0])
-        right_df = right_df.head(left_df.shape[0])
-
-        left_df["gt"] = 0
-        right_df["gt"] = 1
-
-        concated = pd.concat([left_df, right_df], ignore_index=True)
-        lgb_model = Model(
-            cat_validation=self.cat_validation,
-            encoders_names=self.encoders_names,
-            cat_cols=self.cat_cols,
-            model_validation=self.model_validation,
-            model_params=self.model_params,
-        )
-        train_score, val_score, avg_num_trees = lgb_model.fit(
-            concated.drop("gt", axis=1), concated["gt"]
-        )
-        self.metrics = {"train_score": train_score,
-                        "val_score": val_score,
-                        "avg_num_trees": avg_num_trees}
-        self.trained_model = lgb_model
-
-
-class Model:
-    def __init__(
-            self,
-            cat_validation="None",
-            encoders_names=None,
-            cat_cols=None,
-            model_validation=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
-            model_params=None,
-    ):
-        '''
-        Class for fit predicting tabular models, mostly - boostings. Several encoders for categorical features are supported
-
-        Args:
-            cat_validation: categorical type of validation, examples: "None", "Single" and "Double"
-            encoders_names: different categorical encoders from category_encoders library, example CatBoostEncoder
-            cat_cols: list of categorical columns
-            model_validation: model training cross validation type from sklearn.model_selection, example StratifiedKFold(5)
-            model_params: model training hyperparameters
-        '''
-        self.cat_validation = cat_validation
-        self.encoders_names = encoders_names
-        self.cat_cols = cat_cols
-        self.model_validation = model_validation
-
-        if model_params is None:
-            self.model_params = {
-                "metrics": "AUC",
-                "n_estimators": 5000,
-                "learning_rate": 0.04,
-                "random_state": 42,
-            }
-        else:
-            self.model_params = model_params
-
-        self.encoders_list = []
-        self.models_list = []
-        self.scores_list_train = []
-        self.scores_list_val = []
-        self.models_trees = []
-
-    def fit(self, X: pd.DataFrame, y: np.array) -> tuple:
-        """
-        Fits model with speficified in init params
-        Args:
-            X: Input training dataframe
-            y: Target for X
-
-        Returns:
-            mean_score_train, mean_score_val, avg_num_trees
-        """
-        # process cat cols
-        if self.cat_validation == "None":
-            encoder = MultipleEncoder(
-                cols=self.cat_cols, encoders_names_tuple=self.encoders_names
-            )
-            X = encoder.fit_transform(X, y)
-
-        for n_fold, (train_idx, val_idx) in enumerate(
-                self.model_validation.split(X, y)
-        ):
-
-            X_train = X.loc[train_idx]
-            y_train = y.loc[train_idx]
-
-            X_val = X.loc[val_idx]
-            y_val = y.loc[val_idx]
-
-            if self.cat_cols is not None:
-                if self.cat_validation == "Single":
-                    encoder = MultipleEncoder(
-                        cols=self.cat_cols, encoders_names_tuple=self.encoders_names
-                    )
-
-                    X_train = encoder.fit_transform(X_train, y_train)
-                    X_val = encoder.transform(X_val)
-                if self.cat_validation == "Double":
-                    encoder = DoubleValidationEncoderNumerical(
-                        cols=self.cat_cols, encoders_names_tuple=self.encoders_names
-                    )
-                    X_train = encoder.fit_transform(X_train, y_train)
-                    X_val = encoder.transform(X_val)
-                self.encoders_list.append(encoder)
-
-                # check for OrdinalEncoder encoding
-                for col in [col for col in X_train.columns if "OrdinalEncoder" in col]:
-                    X_train[col] = X_train[col].astype("category")
-                    X_val[col] = X_val[col].astype("category")
-
-            # fit model            
-            model = LGBMClassifier(**self.model_params)
-            model.fit(
-                X_train,
-                y_train,
-                eval_set=[(X_train, y_train), (X_val, y_val)],
-                early_stopping_rounds=50,
-                verbose=False,
-            )
-            self.models_trees.append(model.best_iteration_)
-            self.models_list.append(model)
-
-            y_hat = model.predict_proba(X_train)[:, 1]
-            score_train = roc_auc_score(y_train, y_hat)
-            self.scores_list_train.append(score_train)
-            y_hat = model.predict_proba(X_val)[:, 1]
-            score_val = roc_auc_score(y_val, y_hat)
-            self.scores_list_val.append(score_val)
-
-        mean_score_train = np.mean(self.scores_list_train)
-        mean_score_val = np.mean(self.scores_list_val)
-        avg_num_trees = int(np.mean(self.models_trees))
-
-        return mean_score_train, mean_score_val, avg_num_trees
-
-    def predict(self, X: pd.DataFrame) -> np.array:
-        """
-        Making inference with trained models for input dataframe
-        Args:
-            X: input dataframe for inference
-
-        Returns: Predicted ranks
-
-        """
-        y_hat = np.zeros(X.shape[0])
-        if self.encoders_list is not None and self.encoders_list != []:
-            for encoder, model in zip(self.encoders_list, self.models_list):
-                X_test = X.copy()
-                X_test = encoder.transform(X_test)
-
-                # check for OrdinalEncoder encoding
-                for col in [col for col in X_test.columns if "OrdinalEncoder" in col]:
-                    X_test[col] = X_test[col].astype("category")
-
-                unranked_preds = model.predict_proba(X_test)[:, 1]
-                y_hat += rankdata(unranked_preds)
-        else:
-            for model in self.models_list:
-                X_test = X.copy()
-
-                unranked_preds = model.predict_proba(X_test)[:, 1]
-                y_hat += rankdata(unranked_preds)
-        return y_hat
+import numpy as np
+import pandas as pd
+from lightgbm import LGBMClassifier
+from scipy.stats import rankdata
+from sklearn.metrics import roc_auc_score
+from sklearn.model_selection import StratifiedKFold
+
+from tabgan.encoders import MultipleEncoder, DoubleValidationEncoderNumerical
+
+
+class AdversarialModel:
+    def __init__(
+            self,
+            cat_validation="Single",
+            encoders_names=("OrdinalEncoder",),
+            cat_cols=None,
+            model_validation=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
+            model_params=None,
+    ):
+        '''
+        Class for fit predicting tabular models, mostly - boosting. Several encoders for categorical features are
+        supported
+
+        Args:
+            cat_validation: categorical type of validation, examples: "None", "Single" and "Double"
+            encoders_names: different categorical encoders from category_encoders library, example CatBoostEncoder
+            cat_cols: list of categorical columns
+            model_validation: model training cross validation type from sklearn.model_selection,
+            example StratifiedKFold(5)
+            model_params: model training hyper-parameters
+        '''
+        self.cat_validation = cat_validation
+        self.encoders_names = encoders_names
+        self.cat_cols = cat_cols
+        self.model_validation = model_validation
+        self.model_params = model_params
+
+    def adversarial_test(self, left_df, right_df):
+        """
+        Trains adversarial model to distinguish train from test
+        :param left_df:  dataframe
+        :param right_df: dataframe
+        :return: trained model
+        """
+        # sample to shuffle the data
+        left_df = left_df.copy().sample(frac=1).reset_index(drop=True)
+        right_df = right_df.copy().sample(frac=1).reset_index(drop=True)
+
+        left_df = left_df.head(right_df.shape[0])
+        right_df = right_df.head(left_df.shape[0])
+
+        left_df["gt"] = 0
+        right_df["gt"] = 1
+
+        concated = pd.concat([left_df, right_df], ignore_index=True)
+        lgb_model = Model(
+            cat_validation=self.cat_validation,
+            encoders_names=self.encoders_names,
+            cat_cols=self.cat_cols,
+            model_validation=self.model_validation,
+            model_params=self.model_params,
+        )
+        train_score, val_score, avg_num_trees = lgb_model.fit(
+            concated.drop("gt", axis=1), concated["gt"]
+        )
+        self.metrics = {"train_score": train_score,
+                        "val_score": val_score,
+                        "avg_num_trees": avg_num_trees}
+        self.trained_model = lgb_model
+
+
+class Model:
+    def __init__(
+            self,
+            cat_validation="None",
+            encoders_names=None,
+            cat_cols=None,
+            model_validation=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
+            model_params=None,
+    ):
+        '''
+        Class for fit predicting tabular models, mostly - boosting. Several encoders for categorical features are supported
+
+        Args:
+            cat_validation: categorical type of validation, examples: "None", "Single" and "Double"
+            encoders_names: different categorical encoders from category_encoders library, example CatBoostEncoder
+            cat_cols: list of categorical columns
+            model_validation: model training cross validation type from sklearn.model_selection, example StratifiedKFold(5)
+            model_params: model training hyper-parameters
+        '''
+        self.cat_validation = cat_validation
+        self.encoders_names = encoders_names
+        self.cat_cols = cat_cols
+        self.model_validation = model_validation
+
+        if model_params is None:
+            self.model_params = {
+                "metrics": "AUC",
+                "n_estimators": 5000,
+                "learning_rate": 0.04,
+                "random_state": 42,
+            }
+        else:
+            self.model_params = model_params
+
+        self.encoders_list = []
+        self.models_list = []
+        self.scores_list_train = []
+        self.scores_list_val = []
+        self.models_trees = []
+
+    def fit(self, X: pd.DataFrame, y: np.array) -> tuple:
+        """
+        Fits model with speficified in init params
+        Args:
+            X: Input training dataframe
+            y: Target for X
+
+        Returns:
+            mean_score_train, mean_score_val, avg_num_trees
+        """
+        # process cat cols
+        if self.cat_validation == "None":
+            encoder = MultipleEncoder(
+                cols=self.cat_cols, encoders_names_tuple=self.encoders_names
+            )
+            X = encoder.fit_transform(X, y)
+
+        for n_fold, (train_idx, val_idx) in enumerate(
+                self.model_validation.split(X, y)
+        ):
+
+            X_train = X.loc[train_idx]
+            y_train = y.loc[train_idx]
+
+            X_val = X.loc[val_idx]
+            y_val = y.loc[val_idx]
+
+            if self.cat_cols is not None:
+                if self.cat_validation == "Single":
+                    encoder = MultipleEncoder(
+                        cols=self.cat_cols, encoders_names_tuple=self.encoders_names
+                    )
+
+                    X_train = encoder.fit_transform(X_train, y_train)
+                    X_val = encoder.transform(X_val)
+                if self.cat_validation == "Double":
+                    encoder = DoubleValidationEncoderNumerical(
+                        cols=self.cat_cols, encoders_names_tuple=self.encoders_names
+                    )
+                    X_train = encoder.fit_transform(X_train, y_train)
+                    X_val = encoder.transform(X_val)
+                self.encoders_list.append(encoder)
+
+                # check for OrdinalEncoder encoding
+                for col in [col for col in X_train.columns if "OrdinalEncoder" in col]:
+                    X_train[col] = X_train[col].astype("category")
+                    X_val[col] = X_val[col].astype("category")
+
+            # fit model            
+            model = LGBMClassifier(**self.model_params)
+            model.fit(
+                X_train,
+                y_train,
+                eval_set=[(X_train, y_train), (X_val, y_val)],
+                early_stopping_rounds=50,
+                verbose=False,
+            )
+            self.models_trees.append(model.best_iteration_)
+            self.models_list.append(model)
+
+            y_hat = model.predict_proba(X_train)[:, 1]
+            score_train = roc_auc_score(y_train, y_hat)
+            self.scores_list_train.append(score_train)
+            y_hat = model.predict_proba(X_val)[:, 1]
+            score_val = roc_auc_score(y_val, y_hat)
+            self.scores_list_val.append(score_val)
+
+        mean_score_train = np.mean(self.scores_list_train)
+        mean_score_val = np.mean(self.scores_list_val)
+        avg_num_trees = int(np.mean(self.models_trees))
+
+        return mean_score_train, mean_score_val, avg_num_trees
+
+    def predict(self, X: pd.DataFrame) -> np.array:
+        """
+        Making inference with trained models for input dataframe
+        Args:
+            X: input dataframe for inference
+
+        Returns: Predicted ranks
+
+        """
+        y_hat = np.zeros(X.shape[0])
+        if self.encoders_list is not None and self.encoders_list != []:
+            for encoder, model in zip(self.encoders_list, self.models_list):
+                X_test = X.copy()
+                X_test = encoder.transform(X_test)
+
+                # check for OrdinalEncoder encoding
+                for col in [col for col in X_test.columns if "OrdinalEncoder" in col]:
+                    X_test[col] = X_test[col].astype("category")
+
+                unranked_preds = model.predict_proba(X_test)[:, 1]
+                y_hat += rankdata(unranked_preds)
+        else:
+            for model in self.models_list:
+                X_test = X.copy()
+
+                unranked_preds = model.predict_proba(X_test)[:, 1]
+                y_hat += rankdata(unranked_preds)
+        return y_hat
```

## tabgan/encoders.py

```diff
@@ -1,297 +1,292 @@
-from typing import List
-
-import numpy as np
-import pandas as pd
-from category_encoders.backward_difference import BackwardDifferenceEncoder
-from category_encoders.cat_boost import CatBoostEncoder
-from category_encoders.helmert import HelmertEncoder
-from category_encoders.james_stein import JamesSteinEncoder
-from category_encoders.leave_one_out import LeaveOneOutEncoder
-from category_encoders.m_estimate import MEstimateEncoder
-from category_encoders.one_hot import OneHotEncoder
-from category_encoders.ordinal import OrdinalEncoder
-from category_encoders.sum_coding import SumEncoder
-from category_encoders.target_encoder import TargetEncoder
-from category_encoders.woe import WOEEncoder
-from sklearn.model_selection import RepeatedStratifiedKFold
-
-
-def get_single_encoder(encoder_name: str, cat_cols: list):
-    """
-    Get encoder by its name
-    :param encoder_name: Name of desired encoder
-    :param cat_cols: Cat columns for encoding
-    :return: Categorical encoder
-    """
-    if encoder_name == "FrequencyEncoder":
-        encoder = FrequencyEncoder(cols=cat_cols)
-
-    if encoder_name == "WOEEncoder":
-        encoder = WOEEncoder(cols=cat_cols)
-
-    if encoder_name == "TargetEncoder":
-        encoder = TargetEncoder(cols=cat_cols)
-
-    if encoder_name == "SumEncoder":
-        encoder = SumEncoder(cols=cat_cols)
-
-    if encoder_name == "MEstimateEncoder":
-        encoder = MEstimateEncoder(cols=cat_cols)
-
-    if encoder_name == "LeaveOneOutEncoder":
-        encoder = LeaveOneOutEncoder(cols=cat_cols)
-
-    if encoder_name == "HelmertEncoder":
-        encoder = HelmertEncoder(cols=cat_cols)
-
-    if encoder_name == "BackwardDifferenceEncoder":
-        encoder = BackwardDifferenceEncoder(cols=cat_cols)
-
-    if encoder_name == "JamesSteinEncoder":
-        encoder = JamesSteinEncoder(cols=cat_cols)
-
-    if encoder_name == "OrdinalEncoder":
-        encoder = OrdinalEncoder(cols=cat_cols)
-
-    if encoder_name == "CatBoostEncoder":
-        encoder = CatBoostEncoder(cols=cat_cols)
-
-    if encoder_name == "MEstimateEncoder":
-        encoder = MEstimateEncoder(cols=cat_cols)
-    if encoder_name == "OneHotEncoder":
-        encoder = OneHotEncoder(cols=cat_cols)
-    if encoder is None:
-        raise NotImplementedError("To be implemented")
-    return encoder
-
-
-class DoubleValidationEncoderNumerical:
-    """
-    Encoder with validation within
-    """
-
-    def __init__(self, cols, encoders_names_tuple=()):
-        """
-        :param cols: Categorical columns
-        :param encoders_names_tuple: Tuple of str with encoders
-        """
-        self.cols, self.num_cols = cols, None
-        self.encoders_names_tuple = encoders_names_tuple
-
-        self.n_folds, self.n_repeats = 5, 3
-        self.model_validation = RepeatedStratifiedKFold(
-            n_splits=self.n_folds, n_repeats=self.n_repeats, random_state=0
-        )
-        self.encoders_dict = {}
-
-        self.storage = None
-
-    def fit_transform(self, X: pd.DataFrame, y: np.array) -> pd.DataFrame:
-        self.num_cols = [col for col in X.columns if col not in self.cols]
-        self.storage = []
-
-        for encoder_name in self.encoders_names_tuple:
-            for n_fold, (train_idx, val_idx) in enumerate(
-                self.model_validation.split(X, y)
-            ):
-                encoder = get_single_encoder(encoder_name, self.cols)
-
-                X_train, X_val = (
-                    X.loc[train_idx].reset_index(drop=True),
-                    X.loc[val_idx].reset_index(drop=True),
-                )
-                y_train, y_val = y[train_idx], y[val_idx]
-                _ = encoder.fit_transform(X_train, y_train)
-
-                # transform validation part and get all necessary cols
-                val_t = encoder.transform(X_val)
-                val_t = val_t[
-                    [col for col in val_t.columns if col not in self.num_cols]
-                ].values
-
-                if encoder_name not in self.encoders_dict.keys():
-                    cols_representation = np.zeros((X.shape[0], val_t.shape[1]))
-                    self.encoders_dict[encoder_name] = [encoder]
-                else:
-                    self.encoders_dict[encoder_name].append(encoder)
-
-                cols_representation[val_idx, :] += val_t / self.n_repeats
-
-            cols_representation = pd.DataFrame(cols_representation)
-            cols_representation.columns = [
-                f"encoded_{encoder_name}_{i}"
-                for i in range(cols_representation.shape[1])
-            ]
-            self.storage.append(cols_representation)
-
-        for df in self.storage:
-            X = pd.concat([X, df], axis=1)
-
-        X.drop(self.cols, axis=1, inplace=True)
-        return X
-
-    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
-        self.storage = []
-        for encoder_name in self.encoders_names_tuple:
-            cols_representation = None
-
-            for encoder in self.encoders_dict[encoder_name]:
-                test_tr = encoder.transform(X)
-                test_tr = test_tr[
-                    [col for col in test_tr.columns if col not in self.num_cols]
-                ].values
-
-                if cols_representation is None:
-                    cols_representation = np.zeros(test_tr.shape)
-
-                cols_representation = (
-                    cols_representation + test_tr / self.n_folds / self.n_repeats
-                )
-
-            cols_representation = pd.DataFrame(cols_representation)
-            cols_representation.columns = [
-                f"encoded_{encoder_name}_{i}"
-                for i in range(cols_representation.shape[1])
-            ]
-            self.storage.append(cols_representation)
-
-        for df in self.storage:
-            X = pd.concat([X, df], axis=1)
-
-        X.drop(self.cols, axis=1, inplace=True)
-        return X
-
-
-class MultipleEncoder:
-    """
-    Multiple encoder for categorical columns
-    """
-
-    def __init__(self, cols: List[str], encoders_names_tuple=()):
-        """
-        :param cols: List of categorical columns
-        :param encoders_names_tuple: Tuple of categorical encoders names. Possible values in tuple are:
-        "FrequencyEncoder", "WOEEncoder", "TargetEncoder", "SumEncoder", "MEstimateEncoder", "LeaveOneOutEncoder",
-        "HelmertEncoder", "BackwardDifferenceEncoder", "JamesSteinEncoder", "OrdinalEncoder""CatBoostEncoder"
-        """
-
-        self.cols = cols
-        self.num_cols = None
-        self.encoders_names_tuple = encoders_names_tuple
-        self.encoders_dict = {}
-
-        # list for storing results of transformation from each encoder
-        self.storage = None
-
-    def fit_transform(self, X: pd.DataFrame, y: np.array) -> pd.DataFrame:
-        self.num_cols = [col for col in X.columns if col not in self.cols]
-        self.storage = []
-        for encoder_name in self.encoders_names_tuple:
-            encoder = get_single_encoder(encoder_name=encoder_name, cat_cols=self.cols)
-
-            cols_representation = encoder.fit_transform(X, y)
-            self.encoders_dict[encoder_name] = encoder
-            cols_representation = cols_representation[
-                [col for col in cols_representation.columns if col not in self.num_cols]
-            ].values
-            cols_representation = pd.DataFrame(cols_representation)
-            cols_representation.columns = [
-                f"encoded_{encoder_name}_{i}"
-                for i in range(cols_representation.shape[1])
-            ]
-            self.storage.append(cols_representation)
-
-        # concat cat cols representations with initial dataframe
-        for df in self.storage:
-            df.index = X.index
-            X = pd.concat([X, df], axis=1)
-
-        # remove all columns as far as we have their representations
-        X.drop(self.cols, axis=1, inplace=True)
-        return X
-
-    def transform(self, X) -> pd.DataFrame:
-        self.storage = []
-        for encoder_name in self.encoders_names_tuple:
-            # get representation of cat columns and form a pd.DataFrame for it
-            cols_representation = self.encoders_dict[encoder_name].transform(X)
-            cols_representation = cols_representation[
-                [col for col in cols_representation.columns if col not in self.num_cols]
-            ].values
-            cols_representation = pd.DataFrame(cols_representation)
-            cols_representation.columns = [
-                f"encoded_{encoder_name}_{i}"
-                for i in range(cols_representation.shape[1])
-            ]
-            self.storage.append(cols_representation)
-
-        # concat cat cols representations with initial dataframe
-        for df in self.storage:
-            df.index = X.index
-            X = pd.concat([X, df], axis=1)
-
-        # remove all columns as far as we have their representations
-        X.drop(self.cols, axis=1, inplace=True)
-        return X
-
-
-class FrequencyEncoder:
-    def __init__(self, cols):
-        self.cols = cols
-        self.counts_dict = None
-
-    def fit(self, X: pd.DataFrame):
-        counts_dict = {}
-        for col in self.cols:
-            values, counts = np.unique(X[col], return_counts=True)
-            counts_dict[col] = dict(zip(values, counts))
-        self.counts_dict = counts_dict
-
-    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
-        counts_dict_test = {}
-        res = []
-        for col in self.cols:
-            values, counts = np.unique(X[col], return_counts=True)
-            counts_dict_test[col] = dict(zip(values, counts))
-
-            # if value is in "train" keys - replace "test" counts with "train" counts
-            for k in [
-                key
-                for key in counts_dict_test[col].keys()
-                if key in self.counts_dict[col].keys()
-            ]:
-                counts_dict_test[col][k] = self.counts_dict[col][k]
-
-            res.append(X[col].map(counts_dict_test[col]).values.reshape(-1, 1))
-        res = np.hstack(res)
-
-        X[self.cols] = res
-        return X
-
-    def fit_transform(self, X: pd.DataFrame, y=None) -> pd.DataFrame:
-        self.fit(X, y)
-        X = self.transform(X)
-        return X
-
-
-if __name__ == "__main__":
-    df = pd.DataFrame({})
-    df["cat_col"] = [1, 2, 3, 1, 2, 3, 1, 1, 1]
-    df["target"] = [0, 1, 0, 1, 0, 1, 0, 1, 0]
-
-    #
-    temp = df.copy()
-    enc = CatBoostEncoder(cols=["cat_col"])
-    print(enc.fit_transform(temp, temp["target"]))
-
-    #
-    temp = df.copy()
-    enc = MultipleEncoder(cols=["cat_col"], encoders_names_tuple=("CatBoostEncoder",))
-    print(enc.fit_transform(temp, temp["target"]))
-
-    #
-    temp = df.copy()
-    enc = DoubleValidationEncoderNumerical(
-        cols=["cat_col"], encoders_names_tuple=("CatBoostEncoder",)
-    )
-    print(enc.fit_transform(temp, temp["target"]))
+from typing import List
+
+import numpy as np
+import pandas as pd
+from category_encoders.backward_difference import BackwardDifferenceEncoder
+from category_encoders.cat_boost import CatBoostEncoder
+from category_encoders.helmert import HelmertEncoder
+from category_encoders.james_stein import JamesSteinEncoder
+from category_encoders.leave_one_out import LeaveOneOutEncoder
+from category_encoders.m_estimate import MEstimateEncoder
+from category_encoders.one_hot import OneHotEncoder
+from category_encoders.ordinal import OrdinalEncoder
+from category_encoders.sum_coding import SumEncoder
+from category_encoders.target_encoder import TargetEncoder
+from category_encoders.woe import WOEEncoder
+from sklearn.model_selection import RepeatedStratifiedKFold
+
+
+def get_single_encoder(encoder_name: str, cat_cols: list):
+    """
+    Get encoder by its name
+    :param encoder_name: Name of desired encoder
+    :param cat_cols: Cat columns for encoding
+    :return: Categorical encoder
+    """
+    if encoder_name == "FrequencyEncoder":
+        encoder = FrequencyEncoder(cols=cat_cols)
+
+    if encoder_name == "WOEEncoder":
+        encoder = WOEEncoder(cols=cat_cols)
+
+    if encoder_name == "TargetEncoder":
+        encoder = TargetEncoder(cols=cat_cols)
+
+    if encoder_name == "SumEncoder":
+        encoder = SumEncoder(cols=cat_cols)
+
+    if encoder_name == "MEstimateEncoder":
+        encoder = MEstimateEncoder(cols=cat_cols)
+
+    if encoder_name == "LeaveOneOutEncoder":
+        encoder = LeaveOneOutEncoder(cols=cat_cols)
+
+    if encoder_name == "HelmertEncoder":
+        encoder = HelmertEncoder(cols=cat_cols)
+
+    if encoder_name == "BackwardDifferenceEncoder":
+        encoder = BackwardDifferenceEncoder(cols=cat_cols)
+
+    if encoder_name == "JamesSteinEncoder":
+        encoder = JamesSteinEncoder(cols=cat_cols)
+
+    if encoder_name == "OrdinalEncoder":
+        encoder = OrdinalEncoder(cols=cat_cols)
+
+    if encoder_name == "CatBoostEncoder":
+        encoder = CatBoostEncoder(cols=cat_cols)
+
+    if encoder_name == "MEstimateEncoder":
+        encoder = MEstimateEncoder(cols=cat_cols)
+    if encoder_name == "OneHotEncoder":
+        encoder = OneHotEncoder(cols=cat_cols)
+    if encoder is None:
+        raise NotImplementedError("To be implemented")
+    return encoder
+
+
+class DoubleValidationEncoderNumerical:
+    """Encoder with validation within"""
+
+    def __init__(self, cols, encoders_names_tuple=()):
+        """
+        :param cols: Categorical columns
+        :param encoders_names_tuple: Tuple of str with encoders
+        """
+        self.cols, self.num_cols = cols, None
+        self.encoders_names_tuple = encoders_names_tuple
+
+        self.n_folds, self.n_repeats = 5, 3
+        self.model_validation = RepeatedStratifiedKFold(
+            n_splits=self.n_folds, n_repeats=self.n_repeats, random_state=0
+        )
+        self.encoders_dict = {}
+
+        self.storage = None
+
+    def fit_transform(self, X: pd.DataFrame, y: np.array) -> pd.DataFrame:
+        self.num_cols = [col for col in X.columns if col not in self.cols]
+        self.storage = []
+
+        for encoder_name in self.encoders_names_tuple:
+            for n_fold, (train_idx, val_idx) in enumerate(
+                self.model_validation.split(X, y)
+            ):
+                encoder = get_single_encoder(encoder_name, self.cols)
+
+                X_train, X_val = (
+                    X.loc[train_idx].reset_index(drop=True),
+                    X.loc[val_idx].reset_index(drop=True),
+                )
+                y_train, y_val = y[train_idx], y[val_idx]
+                _ = encoder.fit_transform(X_train, y_train)
+
+                # transform validation part and get all necessary cols
+                val_t = encoder.transform(X_val)
+                val_t = val_t[
+                    [col for col in val_t.columns if col not in self.num_cols]
+                ].values
+
+                if encoder_name not in self.encoders_dict:
+                    cols_representation = np.zeros((X.shape[0], val_t.shape[1]))
+                    self.encoders_dict[encoder_name] = [encoder]
+                else:
+                    self.encoders_dict[encoder_name].append(encoder)
+
+                cols_representation[val_idx, :] += val_t / self.n_repeats
+
+            cols_representation = pd.DataFrame(cols_representation)
+            cols_representation.columns = [
+                f"encoded_{encoder_name}_{i}"
+                for i in range(cols_representation.shape[1])
+            ]
+            self.storage.append(cols_representation)
+
+        for df in self.storage:
+            X = pd.concat([X, df], axis=1)
+
+        X.drop(self.cols, axis=1, inplace=True)
+        return X
+
+    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
+        self.storage = []
+        for encoder_name in self.encoders_names_tuple:
+            cols_representation = None
+
+            for encoder in self.encoders_dict[encoder_name]:
+                test_tr = encoder.transform(X)
+                test_tr = test_tr[
+                    [col for col in test_tr.columns if col not in self.num_cols]
+                ].values
+
+                if cols_representation is None:
+                    cols_representation = np.zeros(test_tr.shape)
+
+                cols_representation = (
+                    cols_representation + test_tr / self.n_folds / self.n_repeats
+                )
+
+            cols_representation = pd.DataFrame(cols_representation)
+            cols_representation.columns = [
+                f"encoded_{encoder_name}_{i}"
+                for i in range(cols_representation.shape[1])
+            ]
+            self.storage.append(cols_representation)
+
+        for df in self.storage:
+            X = pd.concat([X, df], axis=1)
+
+        X.drop(self.cols, axis=1, inplace=True)
+        return X
+
+
+class MultipleEncoder:
+    """Multiple encoder for categorical columns"""
+
+    def __init__(self, cols: List[str], encoders_names_tuple=()):
+        """
+        :param cols: List of categorical columns
+        :param encoders_names_tuple: Tuple of categorical encoders names. Possible values in tuple are:
+        "FrequencyEncoder", "WOEEncoder", "TargetEncoder", "SumEncoder", "MEstimateEncoder", "LeaveOneOutEncoder",
+        "HelmertEncoder", "BackwardDifferenceEncoder", "JamesSteinEncoder", "OrdinalEncoder""CatBoostEncoder"
+        """
+        self.cols = cols
+        self.num_cols = None
+        self.encoders_names_tuple = encoders_names_tuple
+        self.encoders_dict = {}
+
+        # list for storing results of transformation from each encoder
+        self.storage = None
+
+    def fit_transform(self, X: pd.DataFrame, y: np.array) -> pd.DataFrame:
+        self.num_cols = [col for col in X.columns if col not in self.cols]
+        self.storage = []
+        for encoder_name in self.encoders_names_tuple:
+            encoder = get_single_encoder(encoder_name=encoder_name, cat_cols=self.cols)
+
+            cols_representation = encoder.fit_transform(X, y)
+            self.encoders_dict[encoder_name] = encoder
+            cols_representation = cols_representation[
+                [col for col in cols_representation.columns if col not in self.num_cols]
+            ].values
+            cols_representation = pd.DataFrame(cols_representation)
+            cols_representation.columns = [
+                f"encoded_{encoder_name}_{i}"
+                for i in range(cols_representation.shape[1])
+            ]
+            self.storage.append(cols_representation)
+
+        # concat cat cols representations with initial dataframe
+        for df in self.storage:
+            df.index = X.index
+            X = pd.concat([X, df], axis=1)
+
+        # remove all columns as far as we have their representations
+        X.drop(self.cols, axis=1, inplace=True)
+        return X
+
+    def transform(self, X) -> pd.DataFrame:
+        self.storage = []
+        for encoder_name in self.encoders_names_tuple:
+            # get representation of cat columns and form a pd.DataFrame for it
+            cols_representation = self.encoders_dict[encoder_name].transform(X)
+            cols_representation = cols_representation[
+                [col for col in cols_representation.columns if col not in self.num_cols]
+            ].values
+            cols_representation = pd.DataFrame(cols_representation)
+            cols_representation.columns = [
+                f"encoded_{encoder_name}_{i}"
+                for i in range(cols_representation.shape[1])
+            ]
+            self.storage.append(cols_representation)
+
+        # concat cat cols representations with initial dataframe
+        for df in self.storage:
+            df.index = X.index
+            X = pd.concat([X, df], axis=1)
+
+        # remove all columns as far as we have their representations
+        X.drop(self.cols, axis=1, inplace=True)
+        return X
+
+
+class FrequencyEncoder:
+    def __init__(self, cols):
+        self.cols = cols
+        self.counts_dict = None
+
+    def fit(self, X: pd.DataFrame):
+        counts_dict = {}
+        for col in self.cols:
+            values, counts = np.unique(X[col], return_counts=True)
+            counts_dict[col] = dict(zip(values, counts))
+        self.counts_dict = counts_dict
+
+    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
+        counts_dict_test = {}
+        res = []
+        for col in self.cols:
+            values, counts = np.unique(X[col], return_counts=True)
+            counts_dict_test[col] = dict(zip(values, counts))
+
+            # if value is in "train" keys - replace "test" counts with "train" counts
+            for k in [
+                key
+                for key in counts_dict_test[col].keys()
+                if key in self.counts_dict[col].keys()
+            ]:
+                counts_dict_test[col][k] = self.counts_dict[col][k]
+
+            res.append(X[col].map(counts_dict_test[col]).values.reshape(-1, 1))
+        res = np.hstack(res)
+
+        X[self.cols] = res
+        return X
+
+    def fit_transform(self, X: pd.DataFrame, y=None) -> pd.DataFrame:
+        self.fit(X, y)
+        X = self.transform(X)
+        return X
+
+
+if __name__ == "__main__":
+    df = pd.DataFrame({})
+    df["cat_col"] = [1, 2, 3, 1, 2, 3, 1, 1, 1]
+    df["target"] = [0, 1, 0, 1, 0, 1, 0, 1, 0]
+
+    #
+    temp = df.copy()
+    enc = CatBoostEncoder(cols=["cat_col"])
+    print(enc.fit_transform(temp, temp["target"]))
+
+    #
+    temp = df.copy()
+    enc = MultipleEncoder(cols=["cat_col"], encoders_names_tuple=("CatBoostEncoder",))
+    print(enc.fit_transform(temp, temp["target"]))
+
+    #
+    temp = df.copy()
+    enc = DoubleValidationEncoderNumerical(
+        cols=["cat_col"], encoders_names_tuple=("CatBoostEncoder",)
+    )
+    print(enc.fit_transform(temp, temp["target"]))
```

## tabgan/sampler.py

```diff
@@ -1,354 +1,349 @@
-# -*- coding: utf-8 -*-
-"""
-todo write description
-"""
-
-import gc
-import logging
-import warnings
-from typing import Tuple
-
-import numpy as np
-import pandas as pd
-
-from _ctgan.synthesizer import _CTGANSynthesizer as CTGAN
-from tabgan.abc_sampler import Sampler, SampleData
-from tabgan.adversarial_model import AdversarialModel
-from tabgan.utils import setup_logging, get_year_mnth_dt_from_date, collect_dates
-
-warnings.filterwarnings("ignore", category=FutureWarning)
-
-__author__ = "Insaf Ashrapov"
-__copyright__ = "Insaf Ashrapov"
-__license__ = "Apache 2.0"
-
-__all__ = ["OriginalGenerator", "GANGenerator"]
-
-
-class OriginalGenerator(SampleData):
-    def __init__(self, *args, **kwargs):
-        self.args = args
-        self.kwargs = kwargs
-
-    def get_object_generator(self) -> Sampler:
-        return SamplerOriginal(*self.args, **self.kwargs)
-
-
-class GANGenerator(SampleData):
-    def __init__(self, *args, **kwargs):
-        self.args = args
-        self.kwargs = kwargs
-
-    def get_object_generator(self) -> Sampler:
-        return SamplerGAN(*self.args, **self.kwargs)
-
-
-class SamplerOriginal(Sampler):
-    def __init__(
-            self,
-            gen_x_times: float = 1.1,
-            cat_cols: list = None,
-            bot_filter_quantile: float = 0.001,
-            top_filter_quantile: float = 0.999,
-            is_post_process: bool = True,
-            adversarial_model_params: dict = {
-                "metrics": "AUC",
-                "max_depth": 2,
-                "max_bin": 100,
-                "n_estimators": 500,
-                "learning_rate": 0.02,
-                "random_state": 42,
-            },
-            pregeneration_frac: float = 2,
-            only_generated_data: bool = False,
-            gan_params: dict = {'batch_size': 500, 'patience': 25, "epochs": 500, }
-    ):
-        """
-
-        @param gen_x_times: float = 1.1 - how much data to generate, output might be less because of postprocessing and
-        adversarial filtering
-        @param cat_cols: list = None - categorical columns
-        @param bot_filter_quantile: float = 0.001 - bottom quantile for postprocess filtering
-        @param top_filter_quantile: float = 0.999 - bottom quantile for postprocess filtering
-        @param is_post_process: bool = True - perform or not postfiltering, if false bot_filter_quantile
-         and top_filter_quantile ignored
-        @param adversarial_model_params: dict params for adversarial filtering model, default values for binary task
-        @param pregeneration_frac: float = 2 - for generation step gen_x_times * pregeneration_frac amount of data
-        will generated. However in postprocessing (1 + gen_x_times) % of original data will be returned
-        @param only_generated_data: bool = False If True after generation get only newly generated, without
-        concating input train dataframe.
-        @param gan_params: dict params for GAN training
-        Only works for SamplerGAN.
-        """
-        self.gen_x_times = gen_x_times
-        self.cat_cols = cat_cols
-        self.is_post_process = is_post_process
-        self.bot_filter_quantile = bot_filter_quantile
-        self.top_filter_quantile = top_filter_quantile
-        self.adversarial_model_params = adversarial_model_params
-        self.pregeneration_frac = pregeneration_frac
-        self.only_generated_data = only_generated_data
-        self.gan_params = gan_params
-        self.TEMP_TARGET = "TEMP_TARGET"
-
-    @staticmethod
-    def preprocess_data_df(df) -> pd.DataFrame:
-        logging.info("Input shape: {}".format(df.shape))
-        if isinstance(df, pd.DataFrame) is False:
-            raise ValueError(
-                "Input dataframe aren't pandas dataframes: df is {}".format(type(df))
-            )
-        return df
-
-    def preprocess_data(
-            self, train, target, test_df
-    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
-        train = self.preprocess_data_df(train)
-        target = self.preprocess_data_df(target)
-        test_df = self.preprocess_data_df(test_df)
-        self.TEMP_TARGET = target.columns[0]
-        if self.TEMP_TARGET in train.columns:
-            raise ValueError(
-                "Input train dataframe already have {} column, consider removing it".format(
-                    self.TEMP_TARGET
-                )
-            )
-        if "test_similarity" in train.columns:
-            raise ValueError(
-                "Input train dataframe already have test_similarity, consider removing it"
-            )
-
-        return train, target, test_df
-
-    def generate_data(
-            self, train_df, target, test_df, only_generated_data
-    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
-        if only_generated_data:
-            Warning(
-                "For SamplerOriginal setting only_generated_data doesn't change anything, "
-                "because generated data sampled from the train!"
-            )
-        self._validate_data(train_df, target, test_df)
-        train_df[self.TEMP_TARGET] = target
-        generated_df = train_df.sample(
-            frac=(1 + self.pregeneration_frac), replace=True, random_state=42
-        )
-        generated_df = generated_df.reset_index(drop=True)
-        gc.collect()
-        logging.info(
-            "Generated shape: {} and {}".format(
-                generated_df.drop(self.TEMP_TARGET, axis=1).shape,
-                generated_df[self.TEMP_TARGET].shape,
-            )
-        )
-        return (
-            generated_df.drop(self.TEMP_TARGET, axis=1),
-            generated_df[self.TEMP_TARGET],
-        )
-
-    def postprocess_data(self, train_df, target, test_df):
-        if not self.is_post_process or test_df is None:
-            logging.info("Skipping postprocessing")
-            return train_df, target
-
-        self._validate_data(train_df, target, test_df)
-        train_df[self.TEMP_TARGET] = target
-
-        for num_col in test_df.columns:
-            if self.cat_cols is None or num_col not in self.cat_cols:
-                min_val = test_df[num_col].quantile(self.bot_filter_quantile)
-                max_val = test_df[num_col].quantile(self.top_filter_quantile)
-                filtered_df = train_df.loc[
-                    (train_df[num_col] >= min_val) & (train_df[num_col] <= max_val)
-                    ]
-                if filtered_df.shape[0] < 10:
-                    raise ValueError(
-                        "After post-processing generated data's shape less than 10. For columns {} test "
-                        "might be highly skewed. Filter conditions are min_val = {} and max_val = {}.".format(
-                            num_col, min_val, max_val
-                        )
-                    )
-                train_df = filtered_df
-
-        if self.cat_cols is not None:
-            for cat_col in self.cat_cols:
-                filtered_df = train_df[
-                    train_df[cat_col].isin(test_df[cat_col].unique())
-                ]
-                if filtered_df.shape[0] < 10:
-                    raise ValueError(
-                        "After post-processing generated data's shape less than 10. For columns {} test "
-                        "might be highly skewed.".format(num_col)
-                    )
-                train_df = filtered_df
-        gc.collect()
-        logging.info(
-            "Generated shapes after postprocessing: {} plus target".format(
-                train_df.drop(self.TEMP_TARGET, axis=1).shape
-            )
-        )
-        return (
-            train_df.drop(self.TEMP_TARGET, axis=1).reset_index(drop=True),
-            train_df[self.TEMP_TARGET].reset_index(drop=True),
-        )
-
-    def adversarial_filtering(self, train_df, target, test_df):
-        if test_df is None:
-            logging.info("Skipping adversarial filtering, because test_df is None.")
-            return train_df, target
-        ad_model = AdversarialModel(
-            cat_cols=self.cat_cols, model_params=self.adversarial_model_params
-        )
-        self._validate_data(train_df, target, test_df)
-        train_df[self.TEMP_TARGET] = target
-        ad_model.adversarial_test(test_df, train_df.drop(self.TEMP_TARGET, axis=1))
-
-        train_df["test_similarity"] = ad_model.trained_model.predict(
-            train_df.drop(self.TEMP_TARGET, axis=1)
-        )
-        train_df.sort_values("test_similarity", ascending=False, inplace=True)
-        train_df = train_df.head(self.get_generated_shape(train_df) * train_df.shape[0])
-        del ad_model
-        gc.collect()
-        return (
-            train_df.drop(["test_similarity", self.TEMP_TARGET], axis=1).reset_index(
-                drop=True
-            ),
-            train_df[self.TEMP_TARGET].reset_index(drop=True),
-        )
-
-    @staticmethod
-    def _validate_data(train_df, target, test_df):
-        if test_df is not None:
-            if train_df.shape[0] < 10 or test_df.shape[0] < 10:
-                raise ValueError(
-                    "Shape of train is {} and test is {}. Both should at least 10! "
-                    "Consider disabling adversarial filtering".format(
-                        train_df.shape[0], test_df.shape[0]
-                    )
-                )
-        if target is not None:
-            if train_df.shape[0] != target.shape[0]:
-                raise ValueError(
-                    "Something gone wrong: shape of train_df = {} is not equal to target = {} shape".format(
-                        train_df.shape[0], target.shape[0]
-                    )
-                )
-
-
-class SamplerGAN(SamplerOriginal):
-    def generate_data(
-            self, train_df, target, test_df, only_generated_data: bool
-    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
-        self._validate_data(train_df, target, test_df)
-        if target is not None:
-            train_df[self.TEMP_TARGET] = target
-        ctgan = CTGAN(batch_size=self.gan_params["batch_size"], patience=self.gan_params["patience"])
-        logging.info("training GAN")
-        if self.cat_cols is None:
-            ctgan.fit(train_df, [], epochs=self.gan_params["epochs"])
-        else:
-            ctgan.fit(train_df, self.cat_cols, epochs=self.gan_params["epochs"])
-        logging.info("Finished training GAN")
-        generated_df = ctgan.sample(
-            self.pregeneration_frac * self.get_generated_shape(train_df)
-        )
-        data_dtype = train_df.dtypes.values
-
-        for i in range(len(generated_df.columns)):
-            generated_df[generated_df.columns[i]] = generated_df[
-                generated_df.columns[i]
-            ].astype(data_dtype[i])
-        gc.collect()
-        if not only_generated_data:
-            train_df = pd.concat([train_df, generated_df]).reset_index(drop=True)
-            logging.info(
-                "Generated shapes: {} plus target".format(
-                    _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
-                )
-            )
-            return (
-                _drop_col_if_exist(train_df, self.TEMP_TARGET),
-                get_columns_if_exists(train_df, self.TEMP_TARGET),
-            )
-        else:
-            logging.info(
-                "Generated shapes: {} plus target".format(
-                    _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
-                )
-            )
-            return (
-                _drop_col_if_exist(generated_df, self.TEMP_TARGET),
-                get_columns_if_exists(generated_df, self.TEMP_TARGET),
-            )
-        gc.collect()
-
-        return (
-            _drop_col_if_exist(train_df, self.TEMP_TARGET),
-            get_columns_if_exists(train_df, self.TEMP_TARGET),
-        )
-
-
-def _sampler(creator: SampleData, in_train, in_target, in_test) -> None:
-    _logger = logging.getLogger(__name__)
-    _logger.info("Starting generating data")
-    train, test = creator.generate_data_pipe(in_train, in_target, in_test)
-    _logger.info(train, test)
-    _logger.info("Finished generation\n")
-    return train, test
-
-
-def _drop_col_if_exist(df, col_to_drop) -> pd.DataFrame:
-    """
-    Drops col_to_drop from input dataframe df if such column exists
-    """
-    if col_to_drop in df.columns:
-        return df.drop(col_to_drop, axis=1)
-    else:
-        return df
-
-
-def get_columns_if_exists(df, col) -> pd.DataFrame:
-    if col in df.columns:
-        return df[col]
-    else:
-        return None
-
-
-if __name__ == "__main__":
-    setup_logging(logging.DEBUG)
-    train_size = 100
-    train = pd.DataFrame(
-        np.random.randint(-10, 150, size=(train_size, 4)), columns=list("ABCD")
-    )
-    logging.info(train)
-    target = pd.DataFrame(np.random.randint(0, 2, size=(train_size, 1)), columns=list("Y"))
-    test = pd.DataFrame(np.random.randint(0, 100, size=(train_size, 4)), columns=list("ABCD"))
-    _sampler(OriginalGenerator(gen_x_times=15), train, target, test)
-    _sampler(
-        GANGenerator(gen_x_times=10, only_generated_data=False,
-                     gan_params={"batch_size": 500, "patience": 25, "epochs" : 500,}), train, target, test
-    )
-
-    _sampler(OriginalGenerator(gen_x_times=15), train, None, train)
-    _sampler(
-        GANGenerator(cat_cols=["A"], gen_x_times=20, only_generated_data=True),
-        train,
-        None,
-        train,
-    )
-    min_date = pd.to_datetime('2019-01-01')
-    max_date = pd.to_datetime('2021-12-31')
-
-    d = (max_date - min_date).days + 1
-
-    train['Date'] = min_date + pd.to_timedelta(pd.np.random.randint(d, size=train_size), unit='d')
-    train = get_year_mnth_dt_from_date(train, 'Date')
-
-    new_train, new_target = GANGenerator(gen_x_times=1.1, cat_cols=['year'], bot_filter_quantile=0.001,
-                                         top_filter_quantile=0.999,
-                                         is_post_process=True, pregeneration_frac=2, only_generated_data=False).\
-                                         generate_data_pipe(train.drop('Date', axis=1), None,
-                                                            train.drop('Date', axis=1)
-                                                                        )
-    new_train = collect_dates(new_train)
+# -*- coding: utf-8 -*-
+
+import gc
+import logging
+import warnings
+from typing import Tuple
+
+import numpy as np
+import pandas as pd
+
+from _ctgan.synthesizer import _CTGANSynthesizer as CTGAN
+from tabgan.abc_sampler import Sampler, SampleData
+from tabgan.adversarial_model import AdversarialModel
+from tabgan.utils import setup_logging, get_year_mnth_dt_from_date, collect_dates
+
+warnings.filterwarnings("ignore")
+
+__author__ = "Insaf Ashrapov"
+__copyright__ = "Insaf Ashrapov"
+__license__ = "Apache 2.0"
+
+__all__ = ["OriginalGenerator", "GANGenerator"]
+
+
+class OriginalGenerator(SampleData):
+    def __init__(self, *args, **kwargs):
+        self.args = args
+        self.kwargs = kwargs
+
+    def get_object_generator(self) -> Sampler:
+        return SamplerOriginal(*self.args, **self.kwargs)
+
+
+class GANGenerator(SampleData):
+    def __init__(self, *args, **kwargs):
+        self.args = args
+        self.kwargs = kwargs
+
+    def get_object_generator(self) -> Sampler:
+        return SamplerGAN(*self.args, **self.kwargs)
+
+
+class SamplerOriginal(Sampler):
+    def __init__(
+            self,
+            gen_x_times: float = 1.1,
+            cat_cols: list = None,
+            bot_filter_quantile: float = 0.001,
+            top_filter_quantile: float = 0.999,
+            is_post_process: bool = True,
+            adversarial_model_params: dict = {
+                "metrics": "AUC",
+                "max_depth": 2,
+                "max_bin": 100,
+                "n_estimators": 500,
+                "learning_rate": 0.02,
+                "random_state": 42,
+            },
+            pregeneration_frac: float = 2,
+            only_generated_data: bool = False,
+            gan_params: dict = {'batch_size': 500, 'patience': 25, "epochs": 500, }
+    ):
+        """
+
+        @param gen_x_times: float = 1.1 - how much data to generate, output might be less because of postprocessing and
+        adversarial filtering
+        @param cat_cols: list = None - categorical columns
+        @param bot_filter_quantile: float = 0.001 - bottom quantile for postprocess filtering
+        @param top_filter_quantile: float = 0.999 - top quantile for postprocess filtering
+        @param is_post_process: bool = True - perform or not postfiltering, if false bot_filter_quantile
+         and top_filter_quantile ignored
+        @param adversarial_model_params: dict params for adversarial filtering model, default values for binary task
+        @param pregeneration_frac: float = 2 - for generation step gen_x_times * pregeneration_frac amount of data
+        will generated. However in postprocessing (1 + gen_x_times) % of original data will be returned
+        @param only_generated_data: bool = False If True after generation get only newly generated, without
+        concating input train dataframe.
+        @param gan_params: dict params for GAN training
+        Only works for SamplerGAN.
+        """
+        self.gen_x_times = gen_x_times
+        self.cat_cols = cat_cols
+        self.is_post_process = is_post_process
+        self.bot_filter_quantile = bot_filter_quantile
+        self.top_filter_quantile = top_filter_quantile
+        self.adversarial_model_params = adversarial_model_params
+        self.pregeneration_frac = pregeneration_frac
+        self.only_generated_data = only_generated_data
+        self.gan_params = gan_params
+        self.TEMP_TARGET = "TEMP_TARGET"
+
+    @staticmethod
+    def preprocess_data_df(df) -> pd.DataFrame:
+        logging.info("Input shape: {}".format(df.shape))
+        if isinstance(df, pd.DataFrame) is False:
+            raise ValueError(
+                "Input dataframe aren't pandas dataframes: df is {}".format(type(df))
+            )
+        return df
+
+    def preprocess_data(
+            self, train, target, test_df
+    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
+        train = self.preprocess_data_df(train)
+        target = self.preprocess_data_df(target)
+        test_df = self.preprocess_data_df(test_df)
+        self.TEMP_TARGET = target.columns[0]
+        if self.TEMP_TARGET in train.columns:
+            raise ValueError(
+                "Input train dataframe already have {} column, consider removing it".format(
+                    self.TEMP_TARGET
+                )
+            )
+        if "test_similarity" in train.columns:
+            raise ValueError(
+                "Input train dataframe already have test_similarity, consider removing it"
+            )
+
+        return train, target, test_df
+
+    def generate_data(
+            self, train_df, target, test_df, only_generated_data
+    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
+        if only_generated_data:
+            Warning(
+                "For SamplerOriginal setting only_generated_data doesn't change anything, "
+                "because generated data sampled from the train!"
+            )
+        self._validate_data(train_df, target, test_df)
+        train_df[self.TEMP_TARGET] = target
+        generated_df = train_df.sample(
+            frac=(1 + self.pregeneration_frac), replace=True, random_state=42
+        )
+        generated_df = generated_df.reset_index(drop=True)
+        gc.collect()
+        logging.info(
+            "Generated shape: {} and {}".format(
+                generated_df.drop(self.TEMP_TARGET, axis=1).shape,
+                generated_df[self.TEMP_TARGET].shape,
+            )
+        )
+        return (
+            generated_df.drop(self.TEMP_TARGET, axis=1),
+            generated_df[self.TEMP_TARGET],
+        )
+
+    def postprocess_data(self, train_df, target, test_df):
+        if not self.is_post_process or test_df is None:
+            logging.info("Skipping postprocessing")
+            return train_df, target
+
+        self._validate_data(train_df, target, test_df)
+        train_df[self.TEMP_TARGET] = target
+
+        for num_col in test_df.columns:
+            if self.cat_cols is None or num_col not in self.cat_cols:
+                min_val = test_df[num_col].quantile(self.bot_filter_quantile)
+                max_val = test_df[num_col].quantile(self.top_filter_quantile)
+                filtered_df = train_df.loc[
+                    (train_df[num_col] >= min_val) & (train_df[num_col] <= max_val)
+                    ]
+                if filtered_df.shape[0] < 10:
+                    raise ValueError(
+                        "After post-processing generated data's shape less than 10. For columns {} test "
+                        "might be highly skewed. Filter conditions are min_val = {} and max_val = {}.".format(
+                            num_col, min_val, max_val
+                        )
+                    )
+                train_df = filtered_df
+
+        if self.cat_cols is not None:
+            for cat_col in self.cat_cols:
+                filtered_df = train_df[
+                    train_df[cat_col].isin(test_df[cat_col].unique())
+                ]
+                if filtered_df.shape[0] < 10:
+                    raise ValueError(
+                        "After post-processing generated data's shape less than 10. For columns {} test "
+                        "might be highly skewed.".format(num_col)
+                    )
+                train_df = filtered_df
+        gc.collect()
+        logging.info(
+            "Generated shapes after postprocessing: {} plus target".format(
+                train_df.drop(self.TEMP_TARGET, axis=1).shape
+            )
+        )
+        return (
+            train_df.drop(self.TEMP_TARGET, axis=1).reset_index(drop=True),
+            train_df[self.TEMP_TARGET].reset_index(drop=True),
+        )
+
+    def adversarial_filtering(self, train_df, target, test_df):
+        if test_df is None:
+            logging.info("Skipping adversarial filtering, because test_df is None.")
+            return train_df, target
+        ad_model = AdversarialModel(
+            cat_cols=self.cat_cols, model_params=self.adversarial_model_params
+        )
+        self._validate_data(train_df, target, test_df)
+        train_df[self.TEMP_TARGET] = target
+        ad_model.adversarial_test(test_df, train_df.drop(self.TEMP_TARGET, axis=1))
+
+        train_df["test_similarity"] = ad_model.trained_model.predict(
+            train_df.drop(self.TEMP_TARGET, axis=1)
+        )
+        train_df.sort_values("test_similarity", ascending=False, inplace=True)
+        train_df = train_df.head(self.get_generated_shape(train_df) * train_df.shape[0])
+        del ad_model
+        gc.collect()
+        return (
+            train_df.drop(["test_similarity", self.TEMP_TARGET], axis=1).reset_index(
+                drop=True
+            ),
+            train_df[self.TEMP_TARGET].reset_index(drop=True),
+        )
+
+    @staticmethod
+    def _validate_data(train_df, target, test_df):
+        if test_df is not None:
+            if train_df.shape[0] < 10 or test_df.shape[0] < 10:
+                raise ValueError(
+                    "Shape of train is {} and test is {}. Both should at least 10! "
+                    "Consider disabling adversarial filtering".format(
+                        train_df.shape[0], test_df.shape[0]
+                    )
+                )
+        if target is not None:
+            if train_df.shape[0] != target.shape[0]:
+                raise ValueError(
+                    "Something gone wrong: shape of train_df = {} is not equal to target = {} shape".format(
+                        train_df.shape[0], target.shape[0]
+                    )
+                )
+
+
+class SamplerGAN(SamplerOriginal):
+    def generate_data(
+            self, train_df, target, test_df, only_generated_data: bool
+    ) -> Tuple[pd.DataFrame, pd.DataFrame]:
+        self._validate_data(train_df, target, test_df)
+        if target is not None:
+            train_df[self.TEMP_TARGET] = target
+        ctgan = CTGAN(batch_size=self.gan_params["batch_size"], patience=self.gan_params["patience"])
+        logging.info("training GAN")
+        if self.cat_cols is None:
+            ctgan.fit(train_df, [], epochs=self.gan_params["epochs"])
+        else:
+            ctgan.fit(train_df, self.cat_cols, epochs=self.gan_params["epochs"])
+        logging.info("Finished training GAN")
+        generated_df = ctgan.sample(
+            self.pregeneration_frac * self.get_generated_shape(train_df)
+        )
+        data_dtype = train_df.dtypes.values
+
+        for i in range(len(generated_df.columns)):
+            generated_df[generated_df.columns[i]] = generated_df[
+                generated_df.columns[i]
+            ].astype(data_dtype[i])
+        gc.collect()
+        if not only_generated_data:
+            train_df = pd.concat([train_df, generated_df]).reset_index(drop=True)
+            logging.info(
+                "Generated shapes: {} plus target".format(
+                    _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
+                )
+            )
+            return (
+                _drop_col_if_exist(train_df, self.TEMP_TARGET),
+                get_columns_if_exists(train_df, self.TEMP_TARGET),
+            )
+        else:
+            logging.info(
+                "Generated shapes: {} plus target".format(
+                    _drop_col_if_exist(train_df, self.TEMP_TARGET).shape
+                )
+            )
+            return (
+                _drop_col_if_exist(generated_df, self.TEMP_TARGET),
+                get_columns_if_exists(generated_df, self.TEMP_TARGET),
+            )
+        gc.collect()
+
+        return (
+            _drop_col_if_exist(train_df, self.TEMP_TARGET),
+            get_columns_if_exists(train_df, self.TEMP_TARGET),
+        )
+
+
+def _sampler(creator: SampleData, in_train, in_target, in_test) -> None:
+    _logger = logging.getLogger(__name__)
+    _logger.info("Starting generating data")
+    train, test = creator.generate_data_pipe(in_train, in_target, in_test)
+    _logger.info(train, test)
+    _logger.info("Finished generation\n")
+    return train, test
+
+
+def _drop_col_if_exist(df, col_to_drop) -> pd.DataFrame:
+    """Drops col_to_drop from input dataframe df if such column exists"""
+    if col_to_drop in df.columns:
+        return df.drop(col_to_drop, axis=1)
+    else:
+        return df
+
+
+def get_columns_if_exists(df, col) -> pd.DataFrame:
+    if col in df.columns:
+        return df[col]
+    else:
+        return None
+
+
+if __name__ == "__main__":
+    setup_logging(logging.DEBUG)
+    train_size = 100
+    train = pd.DataFrame(
+        np.random.randint(-10, 150, size=(train_size, 4)), columns=list("ABCD")
+    )
+    logging.info(train)
+    target = pd.DataFrame(np.random.randint(0, 2, size=(train_size, 1)), columns=list("Y"))
+    test = pd.DataFrame(np.random.randint(0, 100, size=(train_size, 4)), columns=list("ABCD"))
+    _sampler(OriginalGenerator(gen_x_times=15), train, target, test)
+    _sampler(
+        GANGenerator(gen_x_times=10, only_generated_data=False,
+                     gan_params={"batch_size": 500, "patience": 25, "epochs" : 500,}), train, target, test
+    )
+
+    _sampler(OriginalGenerator(gen_x_times=15), train, None, train)
+    _sampler(
+        GANGenerator(cat_cols=["A"], gen_x_times=20, only_generated_data=True),
+        train,
+        None,
+        train,
+    )
+    min_date = pd.to_datetime('2019-01-01')
+    max_date = pd.to_datetime('2021-12-31')
+
+    d = (max_date - min_date).days + 1
+
+    train['Date'] = min_date + pd.to_timedelta(np.random.randint(d, size=train_size), unit='d')
+    train = get_year_mnth_dt_from_date(train, 'Date')
+
+    new_train, new_target = GANGenerator(gen_x_times=1.1, cat_cols=['year'], bot_filter_quantile=0.001,
+                                         top_filter_quantile=0.999,
+                                         is_post_process=True, pregeneration_frac=2, only_generated_data=False). \
+        generate_data_pipe(train.drop('Date', axis=1), None,
+                           train.drop('Date', axis=1)
+                           )
+    new_train = collect_dates(new_train)
```

## tabgan/utils.py

```diff
@@ -1,43 +1,65 @@
-import logging
-import sys
-
-import pandas as pd
-
-
-def setup_logging(loglevel):
-    """Setup basic logging
-
-    Args:
-      loglevel (int): minimum loglevel for emitting messages
-    """
-
-    logformat = "[%(asctime)s] %(levelname)s:%(name)s:%(message)s"
-    logging.basicConfig(
-        level=loglevel, stream=sys.stdout, format=logformat, datefmt="%Y-%m-%d %H:%M:%S"
-    )
-
-
-def make_two_digit(num_as_str: str) -> pd.DataFrame:
-    if len(num_as_str) == 2:
-        return num_as_str
-    else:
-        return '0' + num_as_str
-
-
-def get_year_mnth_dt_from_date(df: pd.DataFrame, date_col='Date') -> pd.DataFrame:
-    df[date_col] = pd.to_datetime(df[date_col])
-    df['year'] = df[date_col].dt.year
-    df['month'] = df[date_col].dt.month
-    df['day'] = df[date_col].dt.day
-    return df
-
-
-def collect_dates(df: pd.DataFrame)-> pd.DataFrame:
-    df["Date"] = df['year'].astype(str) + '-' \
-                        + df['month'].astype(str).apply(make_two_digit) + '-' \
-                        + df['day'].astype(str).apply(make_two_digit)
-    df.drop(['year','month','day'], axis=1,inplace=True)
-    return df
-
-
-TEMP_TARGET = "_temp_target"
+import logging
+import sys
+import os
+import random
+
+import pandas as pd
+import numpy as np
+import torch
+
+
+def setup_logging(loglevel):
+    """Setup basic logging
+
+    Args:
+      loglevel (int): minimum loglevel for emitting messages
+    """
+    logformat = "[%(asctime)s] %(levelname)s:%(name)s:%(message)s"
+    logging.basicConfig(
+        level=loglevel, stream=sys.stdout, format=logformat, datefmt="%Y-%m-%d %H:%M:%S"
+    )
+
+
+def make_two_digit(num_as_str: str) -> pd.DataFrame:
+    if len(num_as_str) == 2:
+        return num_as_str
+    else:
+        return '0' + num_as_str
+
+
+def get_year_mnth_dt_from_date(df: pd.DataFrame, date_col='Date') -> pd.DataFrame:
+    """
+    Extracts year, month, and day from a date column in a pandas DataFrame.
+
+    Args:
+        df (pd.DataFrame): Input DataFrame.
+        date_col (str): Name of the date column.
+
+    Returns:
+        pd.DataFrame: DataFrame with year, month, and day columns added.
+    """
+    df[date_col] = pd.to_datetime(df[date_col])
+    df['year'] = df[date_col].dt.year
+    df['month'] = df[date_col].dt.month
+    df['day'] = df[date_col].dt.day
+    return df
+
+
+def collect_dates(df: pd.DataFrame) -> pd.DataFrame:
+    df["Date"] = df['year'].astype(str) + '-' \
+                 + df['month'].astype(str).apply(make_two_digit) + '-' \
+                 + df['day'].astype(str).apply(make_two_digit)
+    df.drop(['year', 'month', 'day'], axis=1, inplace=True)
+    return df
+
+
+def seed_everything(seed=1234):
+    random.seed(seed)
+    os.environ['PYTHONHASHSEED'] = str(seed)
+    np.random.seed(seed)
+    torch.manual_seed(seed)
+    torch.cuda.manual_seed(seed)
+    torch.backends.cudnn.deterministic = True
+
+
+TEMP_TARGET = "_temp_target"
```

## Comparing `tabgan-1.3.0.dist-info/LICENSE` & `tabgan-1.3.2.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 12% similar despite different names*

```diff
@@ -1,204 +1,204 @@
-Apache License
-
-Version 2.0, January 2004
-
-http://www.apache.org/licenses/
-
-TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
-
-
-1. Definitions.
-
-      "License" shall mean the terms and conditions for use, reproduction,
-      and distribution as defined by Sections 1 through 9 of this document.
-
-      "Licensor" shall mean the copyright owner or entity authorized by
-      the copyright owner that is granting the License.
-
-      "Legal Entity" shall mean the union of the acting entity and all
-      other entities that control, are controlled by, or are under common
-      control with that entity. For the purposes of this definition,
-      "control" means (i) the power, direct or indirect, to cause the
-      direction or management of such entity, whether by contract or
-      otherwise, or (ii) ownership of fifty percent (50%) or more of the
-      outstanding shares, or (iii) beneficial ownership of such entity.
-
-      "You" (or "Your") shall mean an individual or Legal Entity
-      exercising permissions granted by this License.
-
-      "Source" form shall mean the preferred form for making modifications,
-      including but not limited to software source code, documentation
-      source, and configuration files.
-
-      "Object" form shall mean any form resulting from mechanical
-      transformation or translation of a Source form, including but
-      not limited to compiled object code, generated documentation,
-      and conversions to other media types.
-
-      "Work" shall mean the work of authorship, whether in Source or
-      Object form, made available under the License, as indicated by a
-      copyright notice that is included in or attached to the work
-      (an example is provided in the Appendix below).
-
-      "Derivative Works" shall mean any work, whether in Source or Object
-      form, that is based on (or derived from) the Work and for which the
-      editorial revisions, annotations, elaborations, or other modifications
-      represent, as a whole, an original work of authorship. For the purposes
-      of this License, Derivative Works shall not include works that remain
-      separable from, or merely link (or bind by name) to the interfaces of,
-      the Work and Derivative Works thereof.
-
-      "Contribution" shall mean any work of authorship, including
-      the original version of the Work and any modifications or additions
-      to that Work or Derivative Works thereof, that is intentionally
-      submitted to Licensor for inclusion in the Work by the copyright owner
-      or by an individual or Legal Entity authorized to submit on behalf of
-      the copyright owner. For the purposes of this definition, "submitted"
-      means any form of electronic, verbal, or written communication sent
-      to the Licensor or its representatives, including but not limited to
-      communication on electronic mailing lists, source code control systems,
-      and issue tracking systems that are managed by, or on behalf of, the
-      Licensor for the purpose of discussing and improving the Work, but
-      excluding communication that is conspicuously marked or otherwise
-      designated in writing by the copyright owner as "Not a Contribution."
-
-      "Contributor" shall mean Licensor and any individual or Legal Entity
-      on behalf of whom a Contribution has been received by Licensor and
-      subsequently incorporated within the Work.
-
-   2. Grant of Copyright License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      copyright license to reproduce, prepare Derivative Works of,
-      publicly display, publicly perform, sublicense, and distribute the
-      Work and such Derivative Works in Source or Object form.
-
-   3. Grant of Patent License. Subject to the terms and conditions of
-      this License, each Contributor hereby grants to You a perpetual,
-      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
-      (except as stated in this section) patent license to make, have made,
-      use, offer to sell, sell, import, and otherwise transfer the Work,
-      where such license applies only to those patent claims licensable
-      by such Contributor that are necessarily infringed by their
-      Contribution(s) alone or by combination of their Contribution(s)
-      with the Work to which such Contribution(s) was submitted. If You
-      institute patent litigation against any entity (including a
-      cross-claim or counterclaim in a lawsuit) alleging that the Work
-      or a Contribution incorporated within the Work constitutes direct
-      or contributory patent infringement, then any patent licenses
-      granted to You under this License for that Work shall terminate
-      as of the date such litigation is filed.
-
-   4. Redistribution. You may reproduce and distribute copies of the
-      Work or Derivative Works thereof in any medium, with or without
-      modifications, and in Source or Object form, provided that You
-      meet the following conditions:
-
-      (a) You must give any other recipients of the Work or
-          Derivative Works a copy of this License; and
-
-      (b) You must cause any modified files to carry prominent notices
-          stating that You changed the files; and
-
-      (c) You must retain, in the Source form of any Derivative Works
-          that You distribute, all copyright, patent, trademark, and
-          attribution notices from the Source form of the Work,
-          excluding those notices that do not pertain to any part of
-          the Derivative Works; and
-
-      (d) If the Work includes a "NOTICE" text file as part of its
-          distribution, then any Derivative Works that You distribute must
-          include a readable copy of the attribution notices contained
-          within such NOTICE file, excluding those notices that do not
-          pertain to any part of the Derivative Works, in at least one
-          of the following places: within a NOTICE text file distributed
-          as part of the Derivative Works; within the Source form or
-          documentation, if provided along with the Derivative Works; or,
-          within a display generated by the Derivative Works, if and
-          wherever such third-party notices normally appear. The contents
-          of the NOTICE file are for informational purposes only and
-          do not modify the License. You may add Your own attribution
-          notices within Derivative Works that You distribute, alongside
-          or as an addendum to the NOTICE text from the Work, provided
-          that such additional attribution notices cannot be construed
-          as modifying the License.
-
-      You may add Your own copyright statement to Your modifications and
-      may provide additional or different license terms and conditions
-      for use, reproduction, or distribution of Your modifications, or
-      for any such Derivative Works as a whole, provided Your use,
-      reproduction, and distribution of the Work otherwise complies with
-      the conditions stated in this License.
-
-   5. Submission of Contributions. Unless You explicitly state otherwise,
-      any Contribution intentionally submitted for inclusion in the Work
-      by You to the Licensor shall be under the terms and conditions of
-      this License, without any additional terms or conditions.
-      Notwithstanding the above, nothing herein shall supersede or modify
-      the terms of any separate license agreement you may have executed
-      with Licensor regarding such Contributions.
-
-   6. Trademarks. This License does not grant permission to use the trade
-      names, trademarks, service marks, or product names of the Licensor,
-      except as required for reasonable and customary use in describing the
-      origin of the Work and reproducing the content of the NOTICE file.
-
-   7. Disclaimer of Warranty. Unless required by applicable law or
-      agreed to in writing, Licensor provides the Work (and each
-      Contributor provides its Contributions) on an "AS IS" BASIS,
-      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
-      implied, including, without limitation, any warranties or conditions
-      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
-      PARTICULAR PURPOSE. You are solely responsible for determining the
-      appropriateness of using or redistributing the Work and assume any
-      risks associated with Your exercise of permissions under this License.
-
-   8. Limitation of Liability. In no event and under no legal theory,
-      whether in tort (including negligence), contract, or otherwise,
-      unless required by applicable law (such as deliberate and grossly
-      negligent acts) or agreed to in writing, shall any Contributor be
-      liable to You for damages, including any direct, indirect, special,
-      incidental, or consequential damages of any character arising as a
-      result of this License or out of the use or inability to use the
-      Work (including but not limited to damages for loss of goodwill,
-      work stoppage, computer failure or malfunction, or any and all
-      other commercial damages or losses), even if such Contributor
-      has been advised of the possibility of such damages.
-
-   9. Accepting Warranty or Additional Liability. While redistributing
-      the Work or Derivative Works thereof, You may choose to offer,
-      and charge a fee for, acceptance of support, warranty, indemnity,
-      or other liability obligations and/or rights consistent with this
-      License. However, in accepting such obligations, You may act only
-      on Your own behalf and on Your sole responsibility, not on behalf
-      of any other Contributor, and only if You agree to indemnify,
-      defend, and hold each Contributor harmless for any liability
-      incurred by, or claims asserted against, such Contributor by reason
-      of your accepting any such warranty or additional liability.
-
-   END OF TERMS AND CONDITIONS
-
-   APPENDIX: How to apply the Apache License to your work.
-
-      To apply the Apache License to your work, attach the following
-      boilerplate notice, with the fields enclosed by brackets "[]"
-      replaced with your own identifying information. (Don't include
-      the brackets!)  The text should be enclosed in the appropriate
-      comment syntax for the file format. We also recommend that a
-      file or class name and description of purpose be included on the
-      same "printed page" as the copyright notice for easier
-      identification within third-party archives.
-
-   Copyright [yyyy] [name of copyright owner]
-
-   Licensed under the Apache License, Version 2.0 (the "License");
-   you may not use this file except in compliance with the License.
-   You may obtain a copy of the License at
-
-       http://www.apache.org/licenses/LICENSE-2.0
-
-   Unless required by applicable law or agreed to in writing, software
-   distributed under the License is distributed on an "AS IS" BASIS,
-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
-   See the License for the specific language governing permissions and
-   limitations under the License.
+Apache License
+
+Version 2.0, January 2004
+
+http://www.apache.org/licenses/
+
+TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+
+1. Definitions.
+
+      "License" shall mean the terms and conditions for use, reproduction,
+      and distribution as defined by Sections 1 through 9 of this document.
+
+      "Licensor" shall mean the copyright owner or entity authorized by
+      the copyright owner that is granting the License.
+
+      "Legal Entity" shall mean the union of the acting entity and all
+      other entities that control, are controlled by, or are under common
+      control with that entity. For the purposes of this definition,
+      "control" means (i) the power, direct or indirect, to cause the
+      direction or management of such entity, whether by contract or
+      otherwise, or (ii) ownership of fifty percent (50%) or more of the
+      outstanding shares, or (iii) beneficial ownership of such entity.
+
+      "You" (or "Your") shall mean an individual or Legal Entity
+      exercising permissions granted by this License.
+
+      "Source" form shall mean the preferred form for making modifications,
+      including but not limited to software source code, documentation
+      source, and configuration files.
+
+      "Object" form shall mean any form resulting from mechanical
+      transformation or translation of a Source form, including but
+      not limited to compiled object code, generated documentation,
+      and conversions to other media types.
+
+      "Work" shall mean the work of authorship, whether in Source or
+      Object form, made available under the License, as indicated by a
+      copyright notice that is included in or attached to the work
+      (an example is provided in the Appendix below).
+
+      "Derivative Works" shall mean any work, whether in Source or Object
+      form, that is based on (or derived from) the Work and for which the
+      editorial revisions, annotations, elaborations, or other modifications
+      represent, as a whole, an original work of authorship. For the purposes
+      of this License, Derivative Works shall not include works that remain
+      separable from, or merely link (or bind by name) to the interfaces of,
+      the Work and Derivative Works thereof.
+
+      "Contribution" shall mean any work of authorship, including
+      the original version of the Work and any modifications or additions
+      to that Work or Derivative Works thereof, that is intentionally
+      submitted to Licensor for inclusion in the Work by the copyright owner
+      or by an individual or Legal Entity authorized to submit on behalf of
+      the copyright owner. For the purposes of this definition, "submitted"
+      means any form of electronic, verbal, or written communication sent
+      to the Licensor or its representatives, including but not limited to
+      communication on electronic mailing lists, source code control systems,
+      and issue tracking systems that are managed by, or on behalf of, the
+      Licensor for the purpose of discussing and improving the Work, but
+      excluding communication that is conspicuously marked or otherwise
+      designated in writing by the copyright owner as "Not a Contribution."
+
+      "Contributor" shall mean Licensor and any individual or Legal Entity
+      on behalf of whom a Contribution has been received by Licensor and
+      subsequently incorporated within the Work.
+
+   2. Grant of Copyright License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      copyright license to reproduce, prepare Derivative Works of,
+      publicly display, publicly perform, sublicense, and distribute the
+      Work and such Derivative Works in Source or Object form.
+
+   3. Grant of Patent License. Subject to the terms and conditions of
+      this License, each Contributor hereby grants to You a perpetual,
+      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+      (except as stated in this section) patent license to make, have made,
+      use, offer to sell, sell, import, and otherwise transfer the Work,
+      where such license applies only to those patent claims licensable
+      by such Contributor that are necessarily infringed by their
+      Contribution(s) alone or by combination of their Contribution(s)
+      with the Work to which such Contribution(s) was submitted. If You
+      institute patent litigation against any entity (including a
+      cross-claim or counterclaim in a lawsuit) alleging that the Work
+      or a Contribution incorporated within the Work constitutes direct
+      or contributory patent infringement, then any patent licenses
+      granted to You under this License for that Work shall terminate
+      as of the date such litigation is filed.
+
+   4. Redistribution. You may reproduce and distribute copies of the
+      Work or Derivative Works thereof in any medium, with or without
+      modifications, and in Source or Object form, provided that You
+      meet the following conditions:
+
+      (a) You must give any other recipients of the Work or
+          Derivative Works a copy of this License; and
+
+      (b) You must cause any modified files to carry prominent notices
+          stating that You changed the files; and
+
+      (c) You must retain, in the Source form of any Derivative Works
+          that You distribute, all copyright, patent, trademark, and
+          attribution notices from the Source form of the Work,
+          excluding those notices that do not pertain to any part of
+          the Derivative Works; and
+
+      (d) If the Work includes a "NOTICE" text file as part of its
+          distribution, then any Derivative Works that You distribute must
+          include a readable copy of the attribution notices contained
+          within such NOTICE file, excluding those notices that do not
+          pertain to any part of the Derivative Works, in at least one
+          of the following places: within a NOTICE text file distributed
+          as part of the Derivative Works; within the Source form or
+          documentation, if provided along with the Derivative Works; or,
+          within a display generated by the Derivative Works, if and
+          wherever such third-party notices normally appear. The contents
+          of the NOTICE file are for informational purposes only and
+          do not modify the License. You may add Your own attribution
+          notices within Derivative Works that You distribute, alongside
+          or as an addendum to the NOTICE text from the Work, provided
+          that such additional attribution notices cannot be construed
+          as modifying the License.
+
+      You may add Your own copyright statement to Your modifications and
+      may provide additional or different license terms and conditions
+      for use, reproduction, or distribution of Your modifications, or
+      for any such Derivative Works as a whole, provided Your use,
+      reproduction, and distribution of the Work otherwise complies with
+      the conditions stated in this License.
+
+   5. Submission of Contributions. Unless You explicitly state otherwise,
+      any Contribution intentionally submitted for inclusion in the Work
+      by You to the Licensor shall be under the terms and conditions of
+      this License, without any additional terms or conditions.
+      Notwithstanding the above, nothing herein shall supersede or modify
+      the terms of any separate license agreement you may have executed
+      with Licensor regarding such Contributions.
+
+   6. Trademarks. This License does not grant permission to use the trade
+      names, trademarks, service marks, or product names of the Licensor,
+      except as required for reasonable and customary use in describing the
+      origin of the Work and reproducing the content of the NOTICE file.
+
+   7. Disclaimer of Warranty. Unless required by applicable law or
+      agreed to in writing, Licensor provides the Work (and each
+      Contributor provides its Contributions) on an "AS IS" BASIS,
+      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+      implied, including, without limitation, any warranties or conditions
+      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+      PARTICULAR PURPOSE. You are solely responsible for determining the
+      appropriateness of using or redistributing the Work and assume any
+      risks associated with Your exercise of permissions under this License.
+
+   8. Limitation of Liability. In no event and under no legal theory,
+      whether in tort (including negligence), contract, or otherwise,
+      unless required by applicable law (such as deliberate and grossly
+      negligent acts) or agreed to in writing, shall any Contributor be
+      liable to You for damages, including any direct, indirect, special,
+      incidental, or consequential damages of any character arising as a
+      result of this License or out of the use or inability to use the
+      Work (including but not limited to damages for loss of goodwill,
+      work stoppage, computer failure or malfunction, or any and all
+      other commercial damages or losses), even if such Contributor
+      has been advised of the possibility of such damages.
+
+   9. Accepting Warranty or Additional Liability. While redistributing
+      the Work or Derivative Works thereof, You may choose to offer,
+      and charge a fee for, acceptance of support, warranty, indemnity,
+      or other liability obligations and/or rights consistent with this
+      License. However, in accepting such obligations, You may act only
+      on Your own behalf and on Your sole responsibility, not on behalf
+      of any other Contributor, and only if You agree to indemnify,
+      defend, and hold each Contributor harmless for any liability
+      incurred by, or claims asserted against, such Contributor by reason
+      of your accepting any such warranty or additional liability.
+
+   END OF TERMS AND CONDITIONS
+
+   APPENDIX: How to apply the Apache License to your work.
+
+      To apply the Apache License to your work, attach the following
+      boilerplate notice, with the fields enclosed by brackets "[]"
+      replaced with your own identifying information. (Don't include
+      the brackets!)  The text should be enclosed in the appropriate
+      comment syntax for the file format. We also recommend that a
+      file or class name and description of purpose be included on the
+      same "printed page" as the copyright notice for easier
+      identification within third-party archives.
+
+   Copyright [yyyy] [name of copyright owner]
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
```

## Comparing `tabgan-1.3.0.dist-info/METADATA` & `tabgan-1.3.2.dist-info/METADATA`

 * *Files 8% similar despite different names*

```diff
@@ -1,199 +1,199 @@
-Metadata-Version: 2.1
-Name: tabgan
-Version: 1.3.0
-Summary: Applying GAN in tabular data generation for uneven distribution
-Home-page: https://github.com/Diyago/GAN-for-tabular-data
-Author: Insaf Ashrapov
-Author-email: iashrapov@gmail.com
-License: Apache License 2.0
-Project-URL: Documentation, https://github.com/Diyago/GAN-for-tabular-data
-Platform: any
-Classifier: Development Status :: 5 - Production/Stable
-Classifier: Programming Language :: Python
-Requires-Python: >=3.5
-Description-Content-Type: text/markdown; charset=UTF-8
-Requires-Dist: pandas
-Requires-Dist: numpy
-Requires-Dist: category-encoders
-Requires-Dist: torch (>=1.0)
-Requires-Dist: lightgbm (>=2.2.3)
-Requires-Dist: scikit-learn (>=1.0.2)
-Requires-Dist: torchvision
-Requires-Dist: python-dateutil
-Requires-Dist: tqdm
-Provides-Extra: testing
-Requires-Dist: pytest ; extra == 'testing'
-Requires-Dist: pytest-cov ; extra == 'testing'
-
-[![CodeFactor](https://www.codefactor.io/repository/github/diyago/gan-for-tabular-data/badge)](https://www.codefactor.io/repository/github/diyago/gan-for-tabular-data)
-[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
-# GANs for tabular  data
-<img src="https://raw.githubusercontent.com/Diyago/GAN-for-tabular-data/e5a4d437655261755de962b9779c73203611d921/images/logo%20tabular%20gan.svg" height="15%" width="15%">
-
-We well know GANs for success in the realistic image generation. However, they can be applied in tabular data generation. We will review and examine some recent papers about tabular GANs in action.
-
-* Github project: ["GAN-for-tabular-data"](https://github.com/Diyago/GAN-for-tabular-data)
-* Arxiv article: ["Tabular GANs for uneven distribution"](https://arxiv.org/abs/2010.00638)
-* Medium post: [GANs for tabular data](https://towardsdatascience.com/review-of-gans-for-tabular-data-a30a2199342)
-
-### Library goal
-
-Let say we have **T_train** and **T_test** (train and test set respectively).
-We need to train the model on **T_train** and make predictions on **T_test**.
-However, we will increase the train by generating new data by GAN,
-somehow similar to **T_test**, without using ground truth labels.
-
-### How to use library
-
-* Installation: `pip install tabgan`
-* To generate new data to train by sampling and then filtering by adversarial
-  training call `GANGenerator().generate_data_pipe`:
-
-``` python
-from tabgan.sampler import OriginalGenerator, GANGenerator
-import pandas as pd
-import numpy as np
-
-# random input data
-train = pd.DataFrame(np.random.randint(-10, 150, size=(150, 4)), columns=list("ABCD"))
-target = pd.DataFrame(np.random.randint(0, 2, size=(150, 1)), columns=list("Y"))
-test = pd.DataFrame(np.random.randint(0, 100, size=(100, 4)), columns=list("ABCD"))
-
-# generate data
-new_train1, new_target1 = OriginalGenerator().generate_data_pipe(train, target, test, )
-new_train1, new_target1 = GANGenerator().generate_data_pipe(train, target, test, )
-
-# example with all params defined
-new_train3, new_target3 = GANGenerator(gen_x_times=1.1, cat_cols=None,
-           bot_filter_quantile=0.001, top_filter_quantile=0.999, is_post_process=True,
-           adversarial_model_params={
-               "metrics": "AUC", "max_depth": 2, "max_bin": 100, 
-               "learning_rate": 0.02, "random_state": 42, "n_estimators": 500,
-           }, pregeneration_frac=2, only_generated_data=False,
-           gan_params = {"batch_size": 500, "patience": 25, "epochs" : 500,}).generate_data_pipe(train, target,
-                                          test, deep_copy=True, only_adversarial=False, use_adversarial=True)
-```
-
-Both samplers `OriginalGenerator` and `GANGenerator` have same input parameters:
-
-* **gen_x_times**: float = 1.1 - how much data to generate, output might be less because of postprocessing and
-adversarial filtering
-* **cat_cols**: list = None - categorical columns
-* **bot_filter_quantile**: float = 0.001 - bottom quantile for postprocess filtering
-* **top_filter_quantile**: float = 0.999 - bottom quantile for postprocess filtering
-* **is_post_process**: bool = True - perform or not postfiltering, if false bot_filter_quantile
- and top_filter_quantile ignored
-* **adversarial_model_params**: dict params for adversarial filtering model, default values for binary task
-* **pregeneration_frac**: float = 2 - for generataion step gen_x_times * pregeneration_frac amount of data
-will be generated. However, in postprocessing (1 + gen_x_times) % of original data will be returned
-* **gan_params**: dict params for GAN training
-
-
-For `generate_data_pipe` methods params:
-
-* **train_df**: pd.DataFrame Train dataframe which has separate target
-* **target**: pd.DataFrame Input target for the train dataset
-* **test_df**: pd.DataFrame Test dataframe - newly generated train dataframe should be close to it
-* **deep_copy**: bool = True - make copy of input files or not. If not input dataframes will be overridden
-* **only_adversarial**: bool = False - only adversarial fitering to train dataframe will be performed
-* **use_adversarial**: bool = True - perform or not adversarial filtering
-* **only_generated_data**: bool = False  - After generation get only newly generated, without 
-  concating input train dataframe.  
-
-* **@return**: -> Tuple[pd.DataFrame, pd.DataFrame] -  Newly generated train dataframe and test data
-
-Thus, you may use this library to improve your dataset quality:
-
-``` python
-def fit_predict(clf, X_train, y_train, X_test, y_test):
-    clf.fit(X_train, y_train)
-    return sklearn.metrics.roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])
-
-
-if __name__ == "__main__":
-    dataset = sklearn.datasets.load_breast_cancer()
-    clf = sklearn.ensemble.RandomForestClassifier(n_estimators=25, max_depth=6)
-    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
-        pd.DataFrame(dataset.data), pd.DataFrame(dataset.target, columns=["target"]), test_size=0.33, random_state=42)
-    print("initial metric", fit_predict(clf, X_train, y_train, X_test, y_test))
-
-    new_train1, new_target1 = OriginalGenerator().generate_data_pipe(X_train, y_train, X_test, )
-    print("OriginalGenerator metric", fit_predict(clf, new_train1, new_target1, X_test, y_test))
-
-    new_train1, new_target1 = GANGenerator().generate_data_pipe(X_train, y_train, X_test, )
-    print("GANGenerator metric", fit_predict(clf, new_train1, new_target1, X_test, y_test))
-```
-
-## Timeseries GAN generation TimeGAN
-
-You can easily adjust code to generate multidimensional timeseries data.
-Basically it extracts days, months and year from _date_. Demo how to use in the example below:
-```python
-import pandas as pd
-import numpy as np
-from tabgan.utils import get_year_mnth_dt_from_date,make_two_digit,collect_dates
-from tabgan.sampler import OriginalGenerator, GANGenerator
-
-
-train_size = 100
-train = pd.DataFrame(
-        np.random.randint(-10, 150, size=(train_size, 4)), columns=list("ABCD")
-    )
-min_date = pd.to_datetime('2019-01-01')
-max_date = pd.to_datetime('2021-12-31')
-d = (max_date - min_date).days + 1
-
-train['Date'] = min_date + pd.to_timedelta(pd.np.random.randint(d, size=train_size), unit='d')
-train = get_year_mnth_dt_from_date(train, 'Date')
-
-new_train, new_target = GANGenerator(gen_x_times=1.1, cat_cols=['year'], bot_filter_quantile=0.001,
-                                     top_filter_quantile=0.999,
-                                     is_post_process=True, pregeneration_frac=2, only_generated_data=False).\
-                                     generate_data_pipe(train.drop('Date', axis=1), None,
-                                                        train.drop('Date', axis=1)
-                                                                    )
-new_train = collect_dates(new_train)
-```
-
-## Experiments
-### Datasets and experiment design
-
-**Running experiment**
-
-To run experiment follow these steps:
-1. Clone the repository. All required dataset are stored in `./Research/data` folder
-2. Install requirements `pip install -r requirements.txt`
-4. Run all experiments  `python ./Research/run_experiment.py`. Run all experiments  `python run_experiment.py`. You may add more datasets, adjust validation type and categorical encoders.
-5. Observe metrics across all experiment in console or
-   in `./Research/results/fit_predict_scores.txt`
-
-
-
-## Acknowledgments
-
-The author would like to thank Open Data Science community [7] for many
-valuable discussions and educational help in the growing field of machine and
-deep learning. Also, special big thanks to Sber [8] for allowing solving
-such tasks and providing computational resources.
-
-## References
-
-[1] Jonathan Hui. GAN — What is Generative Adversarial Networks GAN? (2018), medium article
-
-[2]Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. Generative Adversarial Networks (2014). arXiv:1406.2661
-
-[3] Lei Xu LIDS, Kalyan Veeramachaneni. Synthesizing Tabular Data using Generative Adversarial Networks (2018). arXiv:1811.11264v1 [cs.LG]
-
-[4] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, Kalyan Veeramachaneni. Modeling Tabular Data using Conditional GAN (2019). arXiv:1907.00503v2 [cs.LG]
-
-[5] Denis Vorotyntsev. Benchmarking Categorical Encoders (2019). Medium post
-
-[6] Insaf Ashrapov. GAN-for-tabular-data (2020). Github repository.
-
-[7] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila. Analyzing and Improving the Image Quality of StyleGAN (2019) arXiv:1912.04958v2 [cs.CV]
-
-[8]  ODS.ai: Open data science (2020), https://ods.ai/
-
-[9]  Sber (2020), https://www.sberbank.ru/
-
-
+Metadata-Version: 2.1
+Name: tabgan
+Version: 1.3.2
+Summary: Applying GAN in tabular data generation for uneven distribution
+Home-page: https://github.com/Diyago/GAN-for-tabular-data
+Author: Insaf Ashrapov
+Author-email: iashrapov@gmail.com
+License: Apache License 2.0
+Project-URL: Documentation, https://github.com/Diyago/GAN-for-tabular-data
+Platform: any
+Classifier: Development Status :: 5 - Production/Stable
+Classifier: Programming Language :: Python
+Requires-Python: >=3.5
+Description-Content-Type: text/markdown; charset=UTF-8
+License-File: LICENSE
+License-File: AUTHORS.rst
+Requires-Dist: pandas
+Requires-Dist: numpy
+Requires-Dist: category-encoders
+Requires-Dist: torch (>=1.0)
+Requires-Dist: lightgbm (>=2.2.3)
+Requires-Dist: scikit-learn (>=1.0.2)
+Requires-Dist: torchvision
+Requires-Dist: python-dateutil
+Requires-Dist: tqdm
+Provides-Extra: testing
+Requires-Dist: pytest ; extra == 'testing'
+Requires-Dist: pytest-cov ; extra == 'testing'
+
+[![CodeFactor](https://www.codefactor.io/repository/github/diyago/gan-for-tabular-data/badge)](https://www.codefactor.io/repository/github/diyago/gan-for-tabular-data)
+[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black) [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
+# GANs for tabular  data
+<img src="https://raw.githubusercontent.com/Diyago/GAN-for-tabular-data/e5a4d437655261755de962b9779c73203611d921/images/logo%20tabular%20gan.svg" height="15%" width="15%">
+
+We well know GANs for success in the realistic image generation. However, they can be applied in tabular data generation. We will review and examine some recent papers about tabular GANs in action.
+
+* Github project: ["GAN-for-tabular-data"](https://github.com/Diyago/GAN-for-tabular-data)
+* Arxiv article: ["Tabular GANs for uneven distribution"](https://arxiv.org/abs/2010.00638)
+* Medium post: [GANs for tabular data](https://towardsdatascience.com/review-of-gans-for-tabular-data-a30a2199342)
+
+### Library goal
+
+Let say we have **T_train** and **T_test** (train and test set respectively).
+We need to train the model on **T_train** and make predictions on **T_test**.
+However, we will increase the train by generating new data by GAN,
+somehow similar to **T_test**, without using ground truth labels.
+
+### How to use library
+
+* Installation: `pip install tabgan`
+* To generate new data to train by sampling and then filtering by adversarial
+  training call `GANGenerator().generate_data_pipe`:
+
+``` python
+from tabgan.sampler import OriginalGenerator, GANGenerator
+import pandas as pd
+import numpy as np
+
+# random input data
+train = pd.DataFrame(np.random.randint(-10, 150, size=(150, 4)), columns=list("ABCD"))
+target = pd.DataFrame(np.random.randint(0, 2, size=(150, 1)), columns=list("Y"))
+test = pd.DataFrame(np.random.randint(0, 100, size=(100, 4)), columns=list("ABCD"))
+
+# generate data
+new_train1, new_target1 = OriginalGenerator().generate_data_pipe(train, target, test, )
+new_train1, new_target1 = GANGenerator().generate_data_pipe(train, target, test, )
+
+# example with all params defined
+new_train3, new_target3 = GANGenerator(gen_x_times=1.1, cat_cols=None,
+           bot_filter_quantile=0.001, top_filter_quantile=0.999, is_post_process=True,
+           adversarial_model_params={
+               "metrics": "AUC", "max_depth": 2, "max_bin": 100, 
+               "learning_rate": 0.02, "random_state": 42, "n_estimators": 500,
+           }, pregeneration_frac=2, only_generated_data=False,
+           gan_params = {"batch_size": 500, "patience": 25, "epochs" : 500,}).generate_data_pipe(train, target,
+                                          test, deep_copy=True, only_adversarial=False, use_adversarial=True)
+```
+
+Both samplers `OriginalGenerator` and `GANGenerator` have same input parameters:
+
+* **gen_x_times**: float = 1.1 - how much data to generate, output might be less because of postprocessing and
+adversarial filtering
+* **cat_cols**: list = None - categorical columns
+* **bot_filter_quantile**: float = 0.001 - bottom quantile for postprocess filtering
+* **top_filter_quantile**: float = 0.999 - top quantile for postprocess filtering
+* **is_post_process**: bool = True - perform or not postfiltering, if false bot_filter_quantile
+ and top_filter_quantile ignored
+* **adversarial_model_params**: dict params for adversarial filtering model, default values for binary task
+* **pregeneration_frac**: float = 2 - for generataion step gen_x_times * pregeneration_frac amount of data
+will be generated. However, in postprocessing (1 + gen_x_times) % of original data will be returned
+* **gan_params**: dict params for GAN training
+
+
+For `generate_data_pipe` methods params:
+
+* **train_df**: pd.DataFrame Train dataframe which has separate target
+* **target**: pd.DataFrame Input target for the train dataset
+* **test_df**: pd.DataFrame Test dataframe - newly generated train dataframe should be close to it
+* **deep_copy**: bool = True - make copy of input files or not. If not input dataframes will be overridden
+* **only_adversarial**: bool = False - only adversarial fitering to train dataframe will be performed
+* **use_adversarial**: bool = True - perform or not adversarial filtering
+* **only_generated_data**: bool = False  - After generation get only newly generated, without 
+  concating input train dataframe.  
+
+* **@return**: -> Tuple[pd.DataFrame, pd.DataFrame] -  Newly generated train dataframe and test data
+
+Thus, you may use this library to improve your dataset quality:
+
+``` python
+def fit_predict(clf, X_train, y_train, X_test, y_test):
+    clf.fit(X_train, y_train)
+    return sklearn.metrics.roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])
+
+
+if __name__ == "__main__":
+    dataset = sklearn.datasets.load_breast_cancer()
+    clf = sklearn.ensemble.RandomForestClassifier(n_estimators=25, max_depth=6)
+    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(
+        pd.DataFrame(dataset.data), pd.DataFrame(dataset.target, columns=["target"]), test_size=0.33, random_state=42)
+    print("initial metric", fit_predict(clf, X_train, y_train, X_test, y_test))
+
+    new_train1, new_target1 = OriginalGenerator().generate_data_pipe(X_train, y_train, X_test, )
+    print("OriginalGenerator metric", fit_predict(clf, new_train1, new_target1, X_test, y_test))
+
+    new_train1, new_target1 = GANGenerator().generate_data_pipe(X_train, y_train, X_test, )
+    print("GANGenerator metric", fit_predict(clf, new_train1, new_target1, X_test, y_test))
+```
+
+## Timeseries GAN generation TimeGAN
+
+You can easily adjust code to generate multidimensional timeseries data.
+Basically it extracts days, months and year from _date_. Demo how to use in the example below:
+```python
+import pandas as pd
+import numpy as np
+from tabgan.utils import get_year_mnth_dt_from_date,make_two_digit,collect_dates
+from tabgan.sampler import OriginalGenerator, GANGenerator
+
+
+train_size = 100
+train = pd.DataFrame(
+        np.random.randint(-10, 150, size=(train_size, 4)), columns=list("ABCD")
+    )
+min_date = pd.to_datetime('2019-01-01')
+max_date = pd.to_datetime('2021-12-31')
+d = (max_date - min_date).days + 1
+
+train['Date'] = min_date + pd.to_timedelta(pd.np.random.randint(d, size=train_size), unit='d')
+train = get_year_mnth_dt_from_date(train, 'Date')
+
+new_train, new_target = GANGenerator(gen_x_times=1.1, cat_cols=['year'], bot_filter_quantile=0.001,
+                                     top_filter_quantile=0.999,
+                                     is_post_process=True, pregeneration_frac=2, only_generated_data=False).\
+                                     generate_data_pipe(train.drop('Date', axis=1), None,
+                                                        train.drop('Date', axis=1)
+                                                                    )
+new_train = collect_dates(new_train)
+```
+
+## Experiments
+### Datasets and experiment design
+
+**Running experiment**
+
+To run experiment follow these steps:
+1. Clone the repository. All required dataset are stored in `./Research/data` folder
+2. Install requirements `pip install -r requirements.txt`
+4. Run all experiments  `python ./Research/run_experiment.py`. Run all experiments  `python run_experiment.py`. You may add more datasets, adjust validation type and categorical encoders.
+5. Observe metrics across all experiment in console or
+   in `./Research/results/fit_predict_scores.txt`
+
+
+
+## Acknowledgments
+
+The author would like to thank Open Data Science community [7] for many
+valuable discussions and educational help in the growing field of machine and
+deep learning. Also, special big thanks to Sber [8] for allowing solving
+such tasks and providing computational resources.
+
+## References
+
+[1] Jonathan Hui. GAN — What is Generative Adversarial Networks GAN? (2018), medium article
+
+[2]Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio. Generative Adversarial Networks (2014). arXiv:1406.2661
+
+[3] Lei Xu LIDS, Kalyan Veeramachaneni. Synthesizing Tabular Data using Generative Adversarial Networks (2018). arXiv:1811.11264v1 [cs.LG]
+
+[4] Lei Xu, Maria Skoularidou, Alfredo Cuesta-Infante, Kalyan Veeramachaneni. Modeling Tabular Data using Conditional GAN (2019). arXiv:1907.00503v2 [cs.LG]
+
+[5] Denis Vorotyntsev. Benchmarking Categorical Encoders (2019). Medium post
+
+[6] Insaf Ashrapov. GAN-for-tabular-data (2020). Github repository.
+
+[7] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila. Analyzing and Improving the Image Quality of StyleGAN (2019) arXiv:1912.04958v2 [cs.CV]
+
+[8]  ODS.ai: Open data science (2020), https://ods.ai/
+
+[9]  Sber (2020), https://www.sberbank.ru/
```

